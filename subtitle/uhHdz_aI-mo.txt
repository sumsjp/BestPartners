HBM
大家好，这里是最佳拍档，我是大飞
对于如今AI大模型的训练来说
大量并行数据的处理可谓是重中之重
算力决定了每秒处理数据的速度
而带宽决定了每秒可访问的数据
GPU负责提供算力
而存储器负责提供带宽
正是因为这样的需求
英伟达才得以靠着近乎于垄断的地位
问鼎史上最高的市值
然而
GPU的垄断还只是整个硬件产业的冰山一角
HBM其实才是海面之下的庞然大物
大家可以不用英伟达的GPU
但是绝对离不开海力士、三星或者美光的HBM
英伟达虽然有CUDA这条护城河
但是也不能完全阻止用户向其他厂商迁移
但是HBM就不同了
不论是AMD还是英特尔
还是其他的定制芯片
上面无一例外都镶嵌着密密麻麻的HBM
但是，HBM并不是普通的DRAM
它的价格早已到了一个令人咋舌的地步
在相同密度的情况下
HBM 的价格大约是DDR5的5倍
目前HBM的成本
在AI服务器成本中占比排名第三
大约占9%，
单机平均售价高达18000美元
不过，即便是如此昂贵的HBM
依旧处于供不应求的状态
甚至还在不断涨价
TrendForce在今年5月表示
2025年的HBM定价谈判已经开启
但是由于DRAM整体产能有限
供应商为了管理产能限制
已经初步涨价5~10%，
影响范围涵盖HBM2e、HBM3与HBM3e
TrendForce还指出
从各大AI方案商来看
HBM的规格需求将明显朝HBM3e转移
12Hi堆叠的产品预期将会增加
同时将带动单颗芯片HBM容量的提升
预计2024年HBM需求的年增长率
将逼近200%，2025年则有望再翻一番
不差钱的巨头可以继续加价购买更大容量的HBM
但是对于中小厂商来说
昂贵的HBM已经成为了它们踏上大模型之路以后的最大阻碍
那么，是否存在一块不需要HBM的产品
来给昂贵的AI芯片解围呢？
今天我们就来聊聊这个事情
在众多挑战HBM垄断地位的选手中
号称“硅仙人”的吉姆·凯勒（Jim Keller）估计是名气最大的一个了
他的职业生涯横跨 DEC、AMD、SiByte、Broadcom、PA Semi、Apple、Tesla、Intel等等知名公司
从AMD的K8架构
到苹果的A4和A5处理器
再到AMD的Zen架构
最后是特斯拉的FSD自动驾驶芯片
背后都有着这位大神的身影
而在2021年，他离开了英特尔
加入了位于加拿大多伦多的AI芯片初创公司Tenstorrent
担任这家公司的CTO
负责开发下一代的AI芯片
凯勒一直致力于解决人工智能硬件成本高昂的问题
将它视为Tenstorrent 等初创公司挑战英伟达等巨头的切入点
他曾经提出
英伟达在开发 Blackwell GPU 的时候
如果使用以太网互连技术
就可以节省下 10 亿美元
在这位大佬的鼎立加持下
Tenstorrentu的开发速度突飞猛进
公司正准备在今年年底发售第二代的多功能 AI 芯片
并且表示
在某些领域
这款芯片的能效和处理效率
要优于英伟达的 AI GPU
据 Tenstorrent 自己称
他们 Galaxy 系统的效率
是英伟达AI 服务器 DGX 的三倍
而且成本降低了33%。
凯勒表示，取得这个成就的原因之一
就是公司不再使用高带宽内存HBM
虽然这种先进的内存芯片能够快速传输大量的数据
但也是 AI 芯片高能耗和高价格的罪魁祸首之一
在典型的 AI 芯片组中
GPU 在每次执行过程的时候
都会将数据发送到内存
这就需要 HBM 高速的数据传输能力
然而
Tenstorrent 特别设计了新的芯片
大幅减少了这类传输
凯勒表示，通过这种新的方法
公司设计的芯片在某些 AI 开发领域
可以替代 GPU 和 HBM
同时他补充道
许多其他公司也在寻找更好的内存解决方案
但是他也谨慎地承认
要想颠覆现有的庞大 HBM 产业
还需要很多年的时间
不过
市场上将会有更多新的玩家出现
来填补英伟达未能提供服务的各种 AI 市场
而不是由某一家公司取代英伟达
对于吉姆·凯勒来说
过于昂贵的HBM似乎已经阻碍到了AI的发展
尽管大公司有雄厚的财力来承担这一切
但是小公司早就难以为继了
而他负责的Tenstorrent芯片
就是为了解决这个问题而来的
2020年5月
Tenstorrent 推出了自己的首款产品
Grayskull
这是一款基于 GF 12nm 工艺、大约 620 平方毫米的处理器
最初设计为推理加速器和主机
它包含 120 个定制核心
采用 2D 双向网格结构
提供 368 TeraOPs 的 8 位计算能力
功耗仅为 65 瓦
每个定制核心都配备了数据控制的包管理引擎、包含 Tenstorrent 自定义的TENSIX Core包计算引擎
以及用于非标准操作的五个 RISC 核心
这款芯片侧重于稀疏张量运算
可以将矩阵运算优化为压缩数据包
再通过图形编译器和数据包管理器
实现计算步骤的流水线并行化
同时它也实现了动态图形执行
相比其他一些人工智能芯片
它允许计算和数据的异步传输
而不需要在特定时间段内进行同步作业
今年3月
Tenstorrent开始销售基于Grayskull的两款开发板
其中
Grayskull e75 和 e150是 Tenstorrent 的基础、仅用于推理的 AI 图形处理器
每个都采用 Tensix Core 构建
包括一个计算单元、片上网络、本地缓存和“小型 RISC-V”核心
从而可以在芯片中实现独特高效的数据移动
为机器学习开发人员提供更加经济高效、可定制的传统 GPU 替代方案
其中Grayskull e75是一款75瓦的PCIe Gen 4卡
售价为600美元
拥有一颗1GHz的NPU芯片
集成了96颗Tensix Core和96MB的SRAM
这个板还包含8GB的标准LPDDR4 DRAM
而Grayskull e150将时钟频率提高到了1.2GHz
核心数量增加到了120个
片上内存相应增加到了120MB
但是片外DRAM仍然为8GB的LPDDR4
功耗提升到了200瓦，售价为800美元
Tenstorrent Grayskull架构不同于其他数据中心的AI加速器
比如GPU和NPU
排列的Tensix Core包含了多个CPU
可供计算单元使用
而后者包括了向量和矩阵引擎
这种结构化的颗粒方法可以增加数学单元的利用率
从而提高每瓦性能
每个Tensix Core还具有1MB的SRAM
提供了充足的片上内存总量
因此与其他大内存的NPU不同
Grayskull可以连接到外部的内存
当然最重要的
还是Grayskull使用了标准的DRAM
而不是昂贵的HBM
仅仅这一项，就省去了一大半的成本
这也符合吉姆·凯勒提出的追求成本效益的目标
此外
软件是NPU和其他处理器挑战者的一个薄弱环节
也是Grayskull与竞争对手相比的一个强项
Tenstorrent提供了两种软件流程
分别是TT-Buda和TT-Metalium
前者基于标准的AI框架
比如PyTorch和TensorFlow
可以将模型映射到Tenstorrent硬件上
而后者则为开发人员提供了直接的硬件访问
并且允许他们创建用于更高级框架的库
在Grayskull架构的支持下
TT-Metalium因为可以提供类似计算机编程模型的能力脱颖而出
并且可能吸引有低级编程资源的客户
另外，从一开始
Tenstorrent就将功耗效率作为一个差异化的因素
比如e75相对较低的75瓦
符合标准PCIe和OCP功率范围
可以作为一个很好的服务器附加板
用于推理
除了Grayskull芯片和板卡以外
Tenstorrent还开始授权高性能的RISC-V CPU和Tensix Core
与合作伙伴共同开发Chiplet
当然，这还只是一个开始
在吉姆·凯勒加盟之后
Tenstorrent的野心也开始变得更大
今年7月
Tenstorrent推出了了新一代Wormhole处理器
专门为AI工作负载设计
承诺以较低的价格提供不错的性能
目前提供两种附加的PCIe卡
分别搭载一个或者两个Wormhole处理器
还有TT-LoudBox和TT-QuietBox工作站
专门为软件开发人员设计
Wormhole处理器可以提供灵活的可扩展性
满足各种工作负载的需求
在标准的工作站设置中
四张Wormhole n300卡可以合并为一个单元
同时在软件中显示为一个统一的、广泛的Tensix Core网络
这种配置允许加速器处理相同的工作负载、分配给四个开发人员或者同时运行多达八个不同的AI模型
在数据中心的环境中
Wormhole处理器不仅可以通过PCIe在一台机器内部扩展
也可以通过以太网在多台机器之间扩展
从性能角度来看
Tenstorrent的单芯片Wormhole n150卡
包含了72个Tensix Core
1GHz AI 时钟，108MB SRAM
12GB GDDR6，带宽为每秒288GB
在160W功耗下可以提供262 FP8 的TFLOPS
而双芯片Wormhole n300板
包含了128个Tensix Core
1GHz AI时钟，192MB SRAM
24GB GDDR6，带宽为每秒576GB
在300W功耗下可以提供高达466 FP8的 TFLOPS
与英伟达的产品相比
英伟达的A100不支持FP8
但是支持INT8
峰值性能为624 TFLOPS
稀疏时为1,248 TFLOPS
而英伟达的H100不仅支持FP8
峰值性能更是高达1,670 TFLOPS
稀疏时为3,341 TFLOPS
Wormhole完全无法与之相比
不过价格弥补了性能上的不足
Wormhole n150的售价仅为999美元
而n300售价为1,399美元
相比之下
一张英伟达H100的售价可能高达30
000美元
除了板卡以外
Tenstorrent还为开发者提供了预装四张n300的工作站
包括低端的基于Xeon的TT-LoudBox
和高端的基于EPYC的TT-QuietBox。
真正的重头戏还在后面
根据Tenstorrent披露的路线图
第二代架构Blackhole 芯片有 140 个 Tensix Core
以及更多的 DRAM 和更快的以太网
同时具备16 个 RISC-V 内核
独立于 Tensix Core
可以脱离x86 CPU来运行操作系统
目前已经在台积电 N6 上流片并且进展顺利
而Tenstorrent 的第三代架构将基于芯片组
并将迁移到三星 SF4工艺
产品分别为Quasar 和 Grendel
将采用更新一代的 Tensix Core
同时会将四个 Tensix Core与共享的 L2 集成在一起
以便更好地重用内存中已有的权重
预计将在2025 年推出
同样，路线图中后续的三款芯片
也都没有采用HBM，而是选择了GDDR6
Tenstorrent的目标只有一个
那就是打破HBM这个昂贵的神话
当然
Tenstorrent并不是唯一一个想要用其他内存替代HBM的公司
比如我们之前介绍过的Groq
就选择了SRAM
速度比GPU用的存储器快大概20倍
这个特点不仅有助于避免HBM短缺的问题
还能有效的降低成本
但是Groq并非没有缺点
它选择SRAM的主要理由
是因为它只负责推理、不负责训练
而推理所需要的存储空间比训练要小得多
所以Groq的单板卡只有230MB的内存
因此，虽然SRAM确实要比DRAM快
但是因为它价格贵，容量小
所以大量使用SRAM的时候还需要有所权衡
除了Tenstorrent以外
2012 年在加利福尼亚州圣何塞创立的Neo Semiconductor
也提出了自己的HBM替代方案
公司近日宣布
他们自己开发出了带有附加神经元电路的三维 DRAM
通过避免从高带宽内存到 GPU 的数据传输
从而可以加速 AI 的处理过程
据了解，Neo以3D DRAM 技术作为基础
推出了3D X-AI 芯片
包括300 层3D DRAM、128 Gb DRAM
每个芯片有 8
000 个神经元和 10 TBps 的 AI 处理能力
3D X-AI 芯片的容量和性能还可以扩展 12 倍
最多可堆叠 12 个 3D X-AI 芯片
就像HBM一样
从而提供 192 GB的容量和 120 TBps 的处理吞吐量
Neo 公司表示
由于使用了基于内存的神经网络
所以3D X-AI非常适合加速下一代的 AI 应用
每个 3D X-AI 芯片中都有突触和神经元
可以大幅减少执行 AI 操作时
GPU 和 HBM 之间数据传输的繁重工作
而在此之前
SK海力士和三星等NAND供应商
已经尝试过计算内存
但是由于使用场景过于小众
不足以证明大规模生产是合理的
而Neo 则相信AI 处理将会变得普及
从而不再是一个小众的场景
而且3D X-AI 芯片还可以与标准 GPU 一起使用
从而以更低的成本提供更快的 AI 处理能力
好了，以上就是有关Tenstorrent公司
以及当下HBM产业的一些介绍了
对于内存厂商，尤其是SK海力士来说
HBM可以说是在坚持多年后获得的一笔意外之财
事实上
就连行业领先了三十多年的三星都出现了误判
在AI浪潮来临的前夜错失了机会
HBM因为这次的AI浪潮而兴起
在大模型中扮演了不可或缺的角色
这一点目前看是毋庸置疑的
但是HBM也面临着各种挑战
尤其是许多更加具备成本优势的方案在不断地涌现
如果HBM不能通过其他的方式来降低成本
未来的地位恐怕也会有点危险了
那么大家是如何看待HBM的发展呢
它还能持续垄断多久呢？
欢迎在评论区留言
感谢大家的观看，我们下期再见
