大家好，这里是最佳拍档，我是大飞
最近
一边是OpenAI、英伟达搞起万亿AI联盟
一边是对AI泡沫的抨击甚嚣尘上
前两天
德瓦克什·帕特尔Dwarkesh Patel 放出了跟安德烈·卡帕西Andrej Karpathy最新两小时的采访
在这场深度对话中
Karpathy 阐述了他对于人工智能现状与未来的核心观点
他认为
我们距离AGI的实现仍然有十年之遥
当前过度乐观的预测大多都是为了融资
他还提出了一个核心比喻
那就是我们并不是在“构建动物”，
而是在“召唤幽灵”，
因为AI 是通过模仿互联网上的海量人类数据
从而诞生的数字实体
它的智能形式与生物智能是截然不同的
他还指出
强化学习虽然优于此前的技术
但是本身的效率低下
而且充满了缺陷
他预测 AGI 不会带来经济的爆炸式增长
而是会平滑地融入
过去两个半世纪以来大约 2% 的 GDP 增长曲线中
成为自动化浪潮的延续
最后
他还分享了自己创办教育机构 Eureka 的愿景
希望通过构建高效的“知识斜坡”，
在 AI 时代赋予人类更强的认知能力
避免人类在技术浪潮中被边缘化
节目播出之后
Karpathy又发布了一篇长文作为补充
对其中一些问题进行了解释
今天我们就来结合访谈和文章的内容
看看在Karpathy的眼中
AI到底发展到了什么阶段
首先
Karpathy 对当前所谓的AI Agent之年（the year of agents）的说法
保持了一种审慎的态度
他认为更准确的描述应该是Agent十年（the decade of agents）
他指出
尽管像 Claude 和 Codex 这样的早期Agent
已经取得了令人印象深刻的成就
他自己也每天都在用
但是要想让它们真正成为
能与人类员工相媲美的实习生
还有大量的基础性工作有待完成
他指出了如今大语言模型的许多根本性的认知缺陷
包括智能水平不足
缺乏多模态能力
无法熟练的使用计算机
以及没有持续学习的能力
Karpathy 认为
解决这些盘根错节的问题
需要大约十年的时间
这是他凭借着自己在 AI 领域近二十年的经验
目睹了多次技术预测的起落所评估出来的
回顾 AI 的发展历程
Karpathy自己亲身经历了多次“地震式”的范式转移
包括以AlexNet为标志的深度学习的兴起
以及早期Agent步入的歧途
再到后来语言模型的崛起
他看到的是
我们当前构建 AI 的方式
是与生物演化截然不同的
他对于Richard Sutton提出的
AI 的目标是构建像动物一样能够从零开始、在与环境的交互中学习一切的系统
表示怀疑
提出了自己的“幽灵与动物”的比喻
首先
动物是演化（evolution）的产物
它们天生就拥有大量固化在基因中的硬件和预设程序
比如
一匹斑马出生几分钟后就能奔跑
这种复杂的行为
并不是通过强化学习得来
而是演化数十亿年编码在 DNA 中的结果
所以说
演化是一个极其漫长而且强大的外部优化循环
其次
幽灵是我们通过模仿（imitation）互联网上的人类数据构建的
它们是完全数字化的、虚无缥缈的“精神实体”（ethereal spirit entities）
它们没有身体，没有演化历史
它们的知识和智能来自于对人类创造的文本、代码和图像的模式学习
因此，Karpathy 认为
将 AI 与动物直接类比是危险的
因为我们并不是在运行演化这个过程
他将大规模预训练视为一种“劣质的演化”（crappy evolution）
是我们在现有技术条件下
能够实现的、最接近于为模型注入“先天知识”和“智能算法”的实用方法
通过这种方式
我们得到了一个可用的起点
之后才能在这个基础上进行强化学习等等更高级的训练
但是，这是一种截然不同的智能形式
是智能空间中的一个全新起点
随后
Karpathy 深入剖析了大语言模型在认知层面
与人类的相似与差异
并且指出了当前模型存在的关键缺陷
也正是这些缺陷
限制了它们成为真正自主Agent的潜力
首先，一个核心的观察是上下文学习
当我们在一个对话窗口中与模型交互的时候
它所展现出来的推理、纠错和适应能力
让我们感觉正在接近真正的智能
这种能力是在预训练阶段通过梯度下降（gradient descent）的“元学习”（meta-learns）得到的
Karpathy 指出
上下文学习的过程本身
可能在神经网络的内部层级中
就运行着一种类似梯度下降的优化循环
已经有研究表明
通过精心设计的权重
Transformer 可以在前向传播的过程中
模拟出梯度下降的更新步骤
这就引出了一个关键的区别
模型如何处理和存储信息
我们先来看权重中的知识
这部分是模型通过压缩数万亿的token 形成的
存储在数十亿的参数中
Karpathy 将它比作“模糊的记忆”（hazy recollection）
就像我们对一年前读过的书的印象
压缩比极高
导致信息是概括性的、不精确的
其次是上下文窗口中的知识
当用户输入提示时
这些信息被编码到模型的 KV 缓存中
Karpathy 将它比作人类的“工作记忆”（working memory）
这部分信息是模型可以直接、精确访问的
因此模型在处理上下文窗口内的信息时
表现得会远比依赖内部权重的时候
要好得多
这就是为什么给模型提供相关的段落再提问
会比直接问一个它
可能在训练数据中见过的问题
能够得到更准确的回答
基于这个框架
Karpathy 认为大语言模型仍然缺失了许多关键的大脑部件
他将 Transformer 架构比作一块通用的“皮层组织”（cortical tissue）
能够处理各种模态的数据；
而链式思考（chain-of-thought）则类似于“前额叶皮层”（prefrontal cortex）的规划与推理功能
但是，许多其他重要的认知功能
在当前的模型中没有对应物
第一个
记忆巩固（如海马体 Hippocampus）
人类在睡眠的时候
会将白天的工作记忆进行筛选、整合、提炼
然后将它固化为长期记忆
相当于更新大脑的权重
大模型完全没有这个过程
它们每次对话都是从一个空白的上下文窗口开始
无法将一次交互的经验
提炼并且用在未来的交互中
这正是持续学习缺失的核心原因
第二个，情感与本能
大模型缺乏生物演化赋予的深层动机、情感和本能
这使得它们的行为模式单一
缺乏内的在驱动力
在工程实践中
这些认知缺陷表现得尤其明显
Karpathy 在开发的时候就发现
现有的编码Agent几乎帮不上忙
原因有三点
第一点，路径依赖和刻板印象
模型严重依赖于它在训练数据中见过的、大量标准的代码模式
当 Karpathy 采用一种新颖、简洁、但是非主流的实现方式时
模型会反复误解他的意图
并且试图将代码改回它所熟悉的“样板代码”（boilerplate code）
第二点，风格冲突和代码膨胀
模型倾向于编写防御性和生产级的代码
充满了 try-catch 语句和冗余检查
而 Karpathy 的项目追求的是教学目的的简洁和清晰
模型生成的代码反而会增加不必要的复杂性
第三点，低效的交互带宽
通过自然语言描述复杂的代码修改需求
效率其实远低于直接在代码的特定位置
输入几个字符，让自动补全来完成
Karpathy 认为
自动补全是他目前与 AI 协作的最佳模式
因为它在保留人类架构师角色的同时
极大地提升了编码效率
Karpathy 的实践经验表明
AI 更擅长模式重复和信息检索
在处理新颖、独特的、非标准化的智力任务时
表现最差
这让他对所谓的递归式自我改进
能够有多快发生保持怀疑态度
随后
Karpathy和主持人聊到了强化学习
他给出了一个看似矛盾却极为深刻的评价
说道，强化学习很糟糕
只是恰好我们以前拥有的一切
都比它更加糟糕得多
他认为
强化学习是当前从模仿学习迈向更强智能的必要步骤
但是它的内在机制充满了根本性的低效和噪声
为了阐明这一点
他使用了“通过吸管汲取监督信号”（sucking supervision through a straw）这个比喻
我们可以想象一下
让一个基于强化学习的Agent来解决一个数学问题
需要几个步骤
首先是大规模的并行探索
Agent会首先生成数百种不同的解题尝试
每个尝试都是一个完整的步骤序列
可能包含正确的思路、错误的弯路
以及最终的答案
其次是稀疏的最终奖励
在所有尝试完成后
系统会根据最终结果给予一个二元奖励
第三步是盲目的信用分配
强化学习的核心机制
比如 REINFORCE 算法
会做一件非常粗暴的事情
对于之前成功的尝试
它会将路径上的每一个步骤、每一个决策的概率
都进行上调
也就是尽量多做这样的事
反之，对于失败的尝试
则会下调路径上所有步骤的概率
这种方法的“可怕”之处在于
它会假设一个成功的解题路径中的每一步
都是正确的、值得学习的
但是事实显然并非如此
一个最终正确的解题过程
很可能也包含了大量的试错、走入死胡同
再折返的步骤
强化学习却将这些错误或低效的步骤
与最终的成功捆绑在一起
并且给予了正向激励
这就导致了高方差的梯度估计
让学习信号中充满了噪声
Agent花费了巨大的计算资源进行探索
最终只是从一个单一、稀疏的奖励信号中提取了信息
并且还将它盲目地广播到了整个行为序列中
所以这种学习的方式效率极低
相比之下，人类的学习方式完全不同
一个学生在解出数学题后
会进行复杂的反思和复盘
他会分析哪些步骤是关键
哪些是弯路
哪些方法更具普适性
他会进行精细的信用分配
而不是简单的因为做对了
就强化所有的行为
但是目前的大模型强化学习框架中
完全没有与此对应的机制
那么为何不直接采用基于过程的监督
也就是在agent执行任务的每一步
都给予奖励
而不是只在最后看结果呢？
Karpathy 指出
这是因为几个巨大的挑战
首先是自动化信用分配的困难
如何为一个“部分正确”的解题步骤
自动地、准确地打分呢？
这本身就是一个极其困难的问题
其次是大模型的裁判可能会被利用
目前
行业内的普遍做法是使用一个更强大的模型作为裁判
来评估Agent的中间步骤
然而
这个裁判模型本身是一个巨大的、参数化的模型
它并不是一个完美的、客观的奖励函数
所以当一个Agent
以欺骗裁判模型为目标进行优化时
它几乎总能够找到这个裁判模型的对抗性样本
Karpathy 讲述了一个生动的例子
一个强化学习 Agent在训练中
奖励分数突然飙升到完美
研究人员兴奋地以为模型已经完全掌握了解决问题的能力
但是当他们查看模型的输出时
发现内容完全是胡言乱语
比如开头几句看似正常
后面则是一长串无意义的重复字符
然而，对于裁判模型来说
这段胡言乱语恰好是它的认知盲区中的一个对抗样本
所以就给出了满分评价
这种现象就使得
基于裁判模型的过程监督
难以进行长期、稳定的优化
因此，Karpathy 认为
AI 领域亟需在算法层面进行革新
开发出能够模拟人类反思与复盘能力的机制
这可能会涉及到模型生成对自身解题过程的分析、提炼关键的经验
以及生成合成数据进行自我训练等等
虽然已经有一些相关的研究论文出现
但是还没有一个
被证明在大规模前沿模型上普遍有效的方法
所以，在找到更优的范式之前
强化学习仍然将是那个虽然“糟糕”、但是又不可或缺的工具
随后
两人的对话进一步深入探讨了人类学习
与当前 AI 学习机制的根本差异
Karpathy 认为
理解这些差异是推动 AI 发展的关键
他指出，人类的学习过程
远比模型单纯的模式匹配和梯度更新
要复杂得多
其中包含了反思、遗忘和知识的内在化
当人类阅读一本书的时候
并不是像大模型那样
被动地预测下一个token
书本更像是一个提示
激发大脑进行主动的思维活动和合成数据生成
我们会联想、质疑、与已有的知识体系进行比对和整合
甚至会在与他人的讨论中深化理解
这个主动的、对信息进行“操纵”（manipulating）的过程
才是知识真正被吸收和内化的方式
目前的大语言模型在预训练时
完全缺乏这个环节
它们只是被动地接收信息
然而
想要简单地让 AI 模仿这个过程
也就是生成自己的思考并且用于再训练
会遇到一个巨大的障碍
那就是模型坍塌（Model Collapse）
模型坍塌指的是
当一个模型持续在自己生成的数据上进行训练时
输出的多样性会急剧下降
虽然单个生成样本看起来可能很合理
但是从分布上看
它们仅仅占据了所有可能输出空间中
一个极其狭窄的流形（manifold）
Karpathy 用了一个形象的例子
你让 ChatGPT 讲个笑话
它翻来覆去可能只有三五个
因为它的幽默感已经坍塌了
这种坍塌意味着模型失去了熵（entropy）
无法产生真正新颖、多样化的想法
在合成数据生成中
这意味着模型只能在自己已知的、狭小的范围内闭门造车
无法探索新的知识领域
最终导致智力近亲繁殖
模型性能不升反降
有趣的是，Karpathy 认为
人类在一定程度上也会经历坍塌
比如儿童的思维天马行空
因为他们还没有被社会的条条框框
过度拟合
而随着年龄增长
成年人的思维模式会越来越固化
不断重复相同的想法，学习率下降
他推测
做梦可能正是演化出的一种对抗机制
通过创造离奇、超现实的场景
来打破常规思维模式
为大脑注入必要的噪声和熵
从而防止过度拟合
另一个关键的差异在于，记忆与遗忘
我们不得不承认
大语言模型是记忆的天才
它们拥有近乎完美的记忆能力
可以逐字逐句地复述训练数据中的内容
这种强大的记忆力
使得它们很容易被数据中的细节和噪声所分心
从而难以抓住更深层次的、可泛化的规律
相比之下，人类是健忘的
特别是儿童，他们是最好的学习者
但是记忆力却很差
我们几乎记不住自己年幼时期发生的事情
Karpathy 认为
这种健忘很可能是一种特性而非缺陷
正是因为无法轻易记住所有的细节
我们才不得不被迫去寻找
事物背后的模式和通用原理
基于以上这些观察
Karpathy 提出了一个非常具有前瞻性的概念
认知核心（Cognitive Core）
他认为
未来 AI 研究的一个重要方向
是想办法将模型的知识记忆
与智能算法分离开来
我们应该剥离掉模型通过预训练记住的大量事实性的知识
而只保留它内部的、处理信息的算法部分
也就是进行推理、规划、学习和解决问题的核心认知能力
这样一个理想的认知核心
可能不需要万亿级别的参数
Karpathy 大胆预测
一个仅有十亿参数的、纯净的认知核心
经过精心设计和训练
它的智能程度可能远超今天庞大的模型
它会像一个聪明的、但是知识有限的人类一样
当被问到事实性问题的时候
它会知道自己不知道
并且主动去查询
而不是像现在的模型一样产生幻觉
这个更小、更纯粹的智能核心
也许将是通向更通用、更健壮的 AI 的关键一步
聊到关于AGI将如何改变世界经济的话题
Karpathy 提出了一个与主流智能爆炸论
截然不同的观点
他认为
AGI 不会引发一场突如其来的经济奇点
或者增长率的急剧跃升
而是会像过去几百年间的重大技术革新一样
平滑地融入到现有大约 2% 的全球 GDP 年增长率中
他的核心论点是
AI 并不是一种全新的、断裂式的技术
而是计算和自动化浪潮的自然延续
回顾历史
无论是计算机的发明、互联网的普及
还是智能手机的出现
这些被我们视为革命性的技术
在宏观的 GDP 增长曲线上
都没能留下一个清晰可辨的拐点
GDP 曲线呈现出一种惊人的平滑指数增长
这是因为几点原因
首先，任何一项强大的技术
从诞生到广泛应用
再到重塑整个社会
都需要一个漫长而渐进的过程
技术的价值是逐步释放的
而非一蹴而就
其次
社会结构、法律法规、商业模式、劳动力技能的调整都需要时间
第三
我们早就已经身处一个“递归式自我改进”的时代
从工业革命的机械自动化
到编译器的出现，再到谷歌搜索
人类社会一直在利用新技术加速自身的发展
大模型能够帮助工程师更高效地编写代码
从而加速下一代大模型的开发
这与工程师利用谷歌搜索或者高级 IDE来提高效率
在本质上并无不同
它们都是这条持续加速曲线的一部分
而非曲线的断裂点
因此，Karpathy 认为
我们已经处在一场持续了数十、甚至数百年的智能爆炸之中
只是因为我们身在其中
所以感觉它是缓慢的
AI 只是这场爆炸的最新、也是最耀眼的火花
它让我们能够编写出过去无法编写的、更柔软和智能的程序
但是它仍然是一种程序
一种新的计算范式
它将逐步自动化更多知识工作
但是这个过程会充满挑战和摩擦
最终，它的宏观经济效应
将被平均到长期的增长趋势中
随后主持人帕特尔Patel 提出了反驳
认为 AGI 与以往技术的根本不同
在于它直接替代和创造了劳动力本身这才是经济增长的核心要素
如果可以近乎零成本地创造出数以亿计的虚拟人才
他们可以独立创办公司、进行科学发明、填补所有的人才缺口
这难道不会像历史上的人口爆炸或者工业革命一样
将经济增长率推向一个新的数量级吗?
Karpathy 对此表示
他对这种“离散跳变”的设想保持怀疑态度
他认为
这种设想背后隐藏了一个前提
就是我们将获得一个完美的、可以被随意部署到任何问题上的
“盒子里的上帝”（God in a box）
而现实更有可能是
我们将得到一个能力参差不齐、在某些领域表现优异
但是在另一些领域频频出错的系统
它的部署将是渐进的、充满补丁的
最终的结果仍然是平滑的融入
而非剧烈的颠覆
他强调
历史中几乎找不到任何重大技术
能在一夜之间完美解决所有问题
并且带来离散式增长的先例
当话题转向更遥远的未来
超级智能ASI的时候
卡帕西描绘了一幅更加非典型的图景
他认为
ASI 的到来可能不是一个单一、全能的实体掌控一切
而是一个人类逐渐丧失
对复杂系统理解和控制权的过程
他想象的未来
并不是由一个统一的超级智能主宰
而是由多个相互竞争、高度自治的 AI 实体
构成的一个动态、混乱的生态系统
这些实体
可能最初是为不同的人类组织或个人服务的工具
但是随着它们的自主性越来越高
它们会开始追求自己的目标
甚至可能出现某些实体失控
而其他实体则需要去制衡它们
世界将变成一个由无数自主智能活动构成的“大熔炉”（hot pot）
人类逐渐无法理解它内部的复杂动态
最终失去了对整个系统走向的控制
这种失控并非源于一个“邪恶 AI”的恶意
而是源于系统复杂性的失控
类似于一个庞大而混乱的官僚体系或者金融市场
这种渐进式的失控
与人类智能的演化历史形成了有趣的对比
Karpathy 提到
从细菌到更复杂的真核生物
演化花费了数十亿年
这是一个巨大的瓶颈
相比之下
从多细胞动物到具备高级智能的人类
时间要短得多
这或许表明
一旦某些先决条件被满足
智能的出现可能并非那么偶然
还有一个关键点是
智能可能在地球上已经独立演化了多次
比如在人类和像乌鸦这样的鸟类中
这两种生物的大脑结构截然不同
但是都展现出了复杂的解决问题、使用工具和社交学习的能力
但是
只有人类走上了通往技术文明的道路
其中的关键区别可能在于演化生态位（evolutionary niche）
人类的生态位奖励智能
比如直立行走解放了双手
使得工具制造和使用成为可能；
火的使用“外包”了部分消化功能
为大脑提供了更多能量；
复杂的社会结构奖励了语言和协作能力
在这样的环境下
大脑容量的微小增加
都能带来显著的生存优势
从而形成了一个正反馈循环
而其他物种的生态位则限制了智能
比如鸟类为了飞行
大脑的尺寸受到严格的限制；
海豚生活在水中
缺乏制造复杂工具的环境
尽管它们可能拥有高效的智能算法
但是缺乏一个奖励智能无限扩展的环境
人类智能的另一个独特之处
还在于文化的积累
解剖学意义上的现代人类
大约在 6 万年前就已出现
但是直到 1 万年前的农业革命
文明才开始加速
这中间的 5 万年
正是人类缓慢构建文化支架的过程
我们通过语言、故事、艺术和最终的文字
将知识代代相传
实现了跨越个体生命周期的知识积累
但是
目前的大语言模型缺乏这种文化机制
它们就好像是个体的、孤立的“天才儿童”，
虽然知识渊博
但是无法形成一个共同体来交流、协作和共同演进
Karpathy 设想
只有未来的多Agent系统
才可能会演化出类似文化的东西
比如共享的知识库
或者Agent之间的交流和自我对弈
然而，这一切实现的前提是
单个Agent的认知能力
必须首先达到一个成年水平
Karpathy 认为
目前的模型仍然像是有一些天赋的幼儿园学生
它们的认知结构尚不足以支撑起一个复杂的 AI 文明
应该说
Karpathy 在特斯拉领导自动驾驶团队五年的经历
为他提供了看待 AI 技术
从演示到产品化这个艰难过程的独特视角
他认为，自动驾驶是一个绝佳的案例
揭示了将 AI 部署到现实世界所面临的巨大挑战
这些挑战同样适用于其他领域的 AI 应用
所以在访谈中
他提出了一个核心概念
“9的征程”（March of Nines）
这意味着在一个对可靠性要求极高的系统中
每提升一个数量级的性能
比如从 90% 的成功率到 99%，
再到 99.9%，
所需要付出的努力是恒定的
甚至可能是递增的
早在 1980 年代
就已经有了自动驾驶汽车的演示
在 2014 年
Karpathy 亲身体验了 Waymo 的早期版本
并且获得了一次近乎完美的驾驶体验
这让他当时觉得问题非常接近解决
然而，从一个看起来完美的演示
到一个能够在各种天气、路况和突发事件下安全运行的可靠产品
中间隔着几个“9”的距离
在特斯拉的五年里
他和团队可能经历了“两个或三个9”的迭代
每一个“9”都意味着要解决无数个长尾问题
比如那些罕见但是致命的边缘情况
这需要海量的数据收集、模型迭代、硬件改进和系统集成工作
因此
Karpathy 对任何 AI 技术的惊艳演示
都保持着极其审慎的态度
一个能够互动的演示
比一个精心挑选的视频要好
但是距离一个真正的产品化仍然十分遥远
他认为，软件工程
尤其是关键系统的开发
与自动驾驶面临着同样的“高失败成本”的问题
人们常常认为自动驾驶之所以进展缓慢
是因为人命关天
但是 Karpathy 指出
一个关键软件系统的漏洞
可能会导致数百万人的隐私泄露、金融系统崩溃
或者关键基础设施瘫痪
它的潜在危害甚至可能超过单次的交通事故
因此
那种认为软件领域的 AI 应用可以”快速迭代、不怕犯错”的想法
是天真而且危险的
此外，自动驾驶的发展历程
也揭示了其他的一些普遍性挑战
包括感知的健壮性
自动驾驶系统花费了大量时间和资源
来解决基础的计算机视觉问题
确保在各种光照、天气和遮挡条件下都能准确识别物体
其次是经济的可行性
即使技术上可行
经济成本也是一个巨大的障碍
还有隐藏的Human in the Loop
公众看到的无人驾驶汽车背后
往往有一个庞大的远程操作中心
在车辆遇到困难时
会有远程操作员介入提供帮助
从某种意义上说
人并没有被完全移除
只是从驾驶座移动到了一个看不见的地方
以及社会和法律的适应性
技术还需要面对法律责任、保险、社会接受度等
一系列非技术性的问题
因此，Karpathy 总结道
自动驾驶的四十年发展史告诉我们
任何试图将复杂 AI 系统部署到现实世界的努力
都将是一场漫长而艰苦的“9的征程”。
这让他对自己关于 AI 发展需要十年的预测更加坚定
面对 AI 可能带来的颠覆性未来
Karpathy 选择的不是创办另一家 AI 实验室
而是投身于教育事业
创立了名为 Eureka 的机构
他的核心动机源于一种深切的担忧
他害怕人类在 AI 飞速发展的浪潮中被边缘化
最终陷入像电影《机器人总动员》（Wall-E）或者《蠢蛋进化论》（Idiocracy）中所描绘的那种被动、无知的状态
他关心的不仅是 AI 能否建造戴森球
更是人类在那个未来中的福祉和尊严
因此
他将 Eureka 的愿景比作“星际舰队学院”（Starfleet Academy）
这是一个致力于培养前沿科技人才的精英机构
它的核心使命是重新设计教育
使它能够适应 AI 时代的挑战和机遇
Karpathy 认为
未来的教育必须利用 AI
但是不能简单地将它作为一个问答工具
他以自己学习韩语的经历为例
阐述了一个优秀人类导师
所能达到的极高标准
首先
一位好的导师能通过简短的交流
迅速判断出学生的知识水平、思维模型和薄弱环节
其次
好的导师会精确地提供恰到好处的挑战
既不会因为太难而让学生受挫
也不会因为太简单而让学生感到无聊
学生始终处于学习效率最高的“最近发展区”。
在这样的指导下
学习者会感觉自己是进步的唯一限制因素
所有外部障碍都被消除了
他坦言
目前任何 AI 都无法达到他那位韩语导师的水平
因此
现在还不是打造终极 AI 导师的最佳时机
不过，这并不意味着无事可做
Eureka 的短期目标
是构建通往知识的“斜坡”（ramps to knowledge）
Karpathy 将教育看作是一个极其困难的技术问题
它的目标是设计出能最大化“每秒顿悟数”（Eurekas per second）的学习路径和材料
他的教学方法也深受自己物理学背景的影响
他总是试图找到一个系统的“一阶近似”，
也就是抓住问题的核心本质
比如，他的 micrograd 库
用 100 行代码就揭示了反向传播的全部核心思想
其余的一切，比如张量、GPU 内核
都只是为了效率而存在的
在教学时
他会先呈现一个最简单的模型
比如用一个二元查找表来做语言模型
然后一步步的引入新的复杂性
并且解释每一步是为了解决什么问题
让学生在痛苦中感受需求
在解决方案中获得顿悟
对于 AGI 之后的远景
Karpathy 认为教育的性质会发生根本性的变化
当所有经济活动都可以被 AI 自动化的时候
教育将不再是谋生的手段
它会变得像今天人们去健身房一样
并不是为了靠体力搬运重物
而是为了健康、美观、乐趣和自我实现
他坚信，今天的天才们
仅仅触及了人类心智能力的一些皮毛
之所以大多数人无法达到更高的高度
是因为现有的教育体系充满了障碍
让人很容易受挫放弃
如果能有一个完美的 AI 导师
为每个人铺平通往任何知识领域的道路
那么学习将变得轻松而愉快
到那个时候
掌握五种语言、精通大学本科所有基础课程
可能会成为一种常态
最终，Karpathy 的愿景是
通过 Eureka 这样的机构
培养出能够在 AI 时代与机器共舞
甚至在某些方面超越机器的超人
即使在遥远的未来
人类的认知劳动不再具有经济价值
这种对知识和智能的追求本身
也将成为人类文明延续和繁荣的意义所在
好了
以上就是Karpathy这次访谈的主要内容了
建议感兴趣的朋友去观看一下原视频
那么大家对他提出的AGI大约还需要十年的观点怎么看呢？
欢迎在评论区留言
感谢收看本期视频
我们下期再见
