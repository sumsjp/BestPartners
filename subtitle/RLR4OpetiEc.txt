大家好这里是最佳拍档
我是大飞
就在最近
知名技术播客德瓦尔凯什播客（Dwarkesh Podcast）
上线了与强化学习之父Richard Sutton的一期视频
标题呢非常的犀利
直言大语言模型是死路一条
这次访谈的内容
从大语言模型的根本缺陷聊到了智能的本质
再延伸到了宇宙演化的大视角
最后落到了创造AGI是人类文明关键使命的结论
密度极高
今天呢咱们就来一点一点的拆解
看看这位AI领域的泰山北斗
到底为什么否定大语言模型
以及他眼中真正的智能
究竟是什么样的
我们先从访谈里最核心的争议点说起
萨顿为什么认为大约模型走不通呢
他在访谈里开门见山的说道
强化学习是关于理解你的世界
而大语言模型是关于模仿人类
做人们说你应该做的事情
他们不是在搞清楚该做什么
这句话呢看似简单
却直接戳中了智能本质的核心
在萨顿看来
智能不是模仿
而是理解与行动
而大预言模型恰恰卡在了模仿这一步
没有触及到理解
具体来说
他认为大语言模型有3个致命的缺陷
咱们一个一个掰开来细说
第一个缺陷是
大语言模型缺乏真正的世界模型
萨顿解释道
模仿人类说什么
并不是在建立真正的世界模型
只是在模仿那些拥有世界模型的东西
也就是人类
这里的一个关键差异是
预测人类会说什么和预测世界会发生什么
是完全不同的两件事情
比如说你看到孩子要伸手碰热水壶了
你的世界模型会立刻预测碰到会烫伤
然后呢去阻止孩子
但是大语言模型呢
他只能根据训练数据里的文本
预测人类看到这种场景会说什么
可能是别碰会烫
但是他本身根本不理解
热水壶和烫伤这些概念的物理意义
也不知道碰这个动作和烫伤这个结果之间的因果关系
他只是在复制人类的语言模式
没有真正理解世界的运作逻辑
这就是萨顿说的没有世界模型的问题
第二个缺陷是大源模型没有ground truth
萨顿说
在大源模型中没有正确答案的定义
你说了什么
但是你不会得到关于什么是正确的反馈
因为根本就没有什么正确的定义
这句话怎么理解呢
咱们举一个例子
如果训练一个机器人玩打砖块的游戏
ground truth就很明确
那就是打掉砖块得分
没接住球游戏结束
系统能够通过得分失分这个反馈
知道自己的动作对不对
然后呢调整策略
但是大语言模型生成文本的时候
其实并没有这样的正确标准
比如说
你让他写一篇如何提升专注力的文章
他写出来之后
你没法说这篇一定是对的
或者这篇一定是错的
因为专注力提升的方法
没有统一的答案
也没有一个可验证的反馈信号来告诉大语言模型
你这里写的不好该改
既然没有对与错的判断标准
系统就没法真正的学习改进
毕竟连自己哪里错了都不知道谈何进步呢
第三个缺陷
是大语言模型无法从经验中学习
萨顿强调
他们不会对接下来发生的事情感到惊讶
如果发生了意外他们也不会做出调整
这一点呢其实戳中了当前AI的核心痛点
那就是人类的学习
本质上是通过预期与现实的偏差来调整认知的
比如说你以为今天会下雨带了伞呢
结果没下
下一次呢
你就会根据天气预报再判断
但是大预言模型没有这种适应性
再举个具体的例子
你问大语言模型
把冰块放进微波炉加热会怎么样
他可能会说冰块会融化
这是基于训练数据的预测
但是如果实际中你把冰块放进微波炉
因为微波炉功率太低
半个小时都没融化
大语言模型不会因为这个意外
而改变自己的认知
下次你再问他还是会说冰块会融化
他不会从实际经验中修正自己的判断
因为他根本没有体验经验的能力
只是在被动的处理文本输入
在聊到这三个缺陷的时候
萨顿特别强调了
目标对于智能的重要性
对他来说
拥有目标是智能的本质
如果某个东西能够实现目标
它就是智能的
这里呢他还引用了人工智能先驱约翰麦卡锡的定义
智能是实现目标能力的计算部分
在萨顿看来
没有目标的系统顶多算是一个行为系统
不能算是一个智能系统
比如说一个只会循环播放音乐的音箱
它有播放音乐的行为
但是没有要让听众喜欢
或者是根据场景来调整音乐的目标
所以呢它没有智能
主持人当时反驳
大语言模型也有目标啊
预测下一个TOKEN不就是他的目标吗
萨顿直接否定了这个说法
他说那不是目标
他不会改变世界
TOKEN向你袭来
如果你预测他们
你并不会影响他们
意思是大语言模型的预测TOKEN只是被动的应对输入
不会对外部世界产生任何的影响
他不会为了更准确地预测医学领域的TOKEN
而主动去学习医学知识
也不会因为预测错了而主动地调整学习方向
因为他没有主动实现某个目标的动力
只是在执行预设的计算任务
那既然大语言模型走不通
那萨顿认为真正的智能路径是什么呢
他提出了经验学习范式的概念
核心是一个简单但是强大的循环
那就是感知行动和奖励
这个过程在你的生命中不断地重复
他说智能就是接受这个流
改变行动来增加流中的奖励
指的就是agent与世界互动时产生的
从感知信号到行动输出再到奖励反馈的连续过程
而智能的核心
就是在这个过程中不断调整行为
让奖励越来越多
这个范式和大语言模型的本质区别在于学习的来源
大语言模型的学习来源是人类写的文本
数据是间接的二手的
而经验学习的来源是agent自己与世界的互动
经验是直接的一手的
萨顿解释说学习来自于这个流
学习也是关于这个流的
你的知识是关于如果你采取某个行动会发生什么
或者哪些事件会跟随其他事件
知识的内容是关于这个流的陈述
正因为知识是关于经验流的
所以它能够被验证
比如说你认为按下开关灯会亮
这个知识可以通过按开关这个行动来验证
如果灯没有亮
你就会调整这个知识
也许可能灯泡坏了
这就是持续学习的过程
为了让这个概念更加易懂
萨顿还举了一个婴儿学习的例子
当主持人问到人类不也会模仿学习吗
比如说孩子模仿大人说话
萨顿反驳的说道当我看到孩子的时候
我看到的是孩子在尝试各种事情
挥舞着手臂移动着眼睛
他们如何移动眼睛或者发出声音
都没有模仿的对象
他认为婴儿的核心学习方式是试错
而不是模仿
婴儿会无意识的挥动手臂
偶然碰到玩具发出声音
然后他会发现
挥动手臂和玩具发生之间的联系
接下来就会主动重复这个动作
这就是感知看到玩具
到行动挥动手臂
再到奖励听到声音的循环
即使到了学校教育阶段
萨顿也认为模仿和训练是例外不是常态
他说到
正式的学校教育其实是一种例外
学习真的不是关于训练
学习是关于学习
是一个主动的过程
孩子们尝试事物并且观察会发生什么
他还特别用松鼠举例
来反驳监督学习是常态的观点
萨顿说
监督学习不是自然界中发生的事情
即使在学校里我们也应该忘记它
因为那是人类特有的某种特殊情况
它不会在自然界中广泛的发生
松鼠不上学
松鼠可以学习关于世界的一切
这句话的意思是
松鼠不需要人类教它怎么去找食物
怎么躲避天敌
它通过自己的行动
比如说尝试吃不同的果实
观察哪些能吃
尝试靠近人类
观察是否有危险来积累经验形成知识
这种不需要人类标注数据
不需要学校教育
仅靠自身经验就能够学习的能力才是智能的基础
既然经验学习范式是核心
那一个完整的agent应该具备哪些组件呢
萨顿在访谈里详细拆解了四个核心的部分
这四个部分共同构成了能够从经验中学习的智能系统
第一个组件是策略policy
简单来说
策略就是在当前所处的情况下应该做什么
或者说是从状态到行动的映射
比如说agent感知到面前有一道门
门是关着的
这是状态
策略呢就会告诉他应该伸手去开门
这是行动
但是要注意
策略不是一个固定的规则
而是一个动态调整的系统
比如说
如果第一次开门发现门是锁着的
策略下次呢就会调整为先找好钥匙再开门
萨顿特别强调好的策略必须能够泛化
也就是能够处理没有见过的新情况
正如同没有见过推拉门的agent
通过策略的泛化能力
也能够尝试推或者是拉的动作
而不是完全束手无策
第二个组件呢
是价值函数value function
萨顿解释道价值函数通过TD学习
产生一个数字
这个数字说明事情进展的如何
价值函数的核心作用是评估当前状态的好坏
为策略调整提供依据
比如说在下棋的时候
某个棋盘布局的价值高
说明这个布局对赢棋更有利
价值低说明可能有风险
这里的关键是
价值函数评估的是长期收益
而不是短期收益
我们还拿下棋举例
牺牲一个兵可能会换来后续的将死对方
价值函数会识别出这种长期的优势
从而让策略做出牺牲小兵的决策
萨顿发明的TD学习
就是让价值函数能够根据当前预测和未来实际结果的差异
不断修正自己的评估
让它越来越准确
第三个组件是感知组件perception
指的是要构建你的状态表示
以及你对于当前位置的感知
这不是简单的接收感官信号
而是把杂乱的感官数据转化为有意义的内部表示
假设你看到一个红色圆形
表面有斑点的物体
感知组件会把这些视觉信号整合为
这是一个苹果的内部表示
这个表示里包含了可以吃、需要洗等等关键的信息
方便策略和价值函数做出决策
如果感知组件出了问题
把辣椒误以为是苹果
那后续的策略比如说咬一口就会出错
所以萨顿认为
感知组件的核心是提取关键信息
构建有用的状态表示
它是agent与世界互动的第一道门槛
第四个组件是世界转换模型
萨顿说
你相信如果你做这件事情会发生什么呢
行动的后果是什么呢
这个模型会负责预测行动会带来的状态变化
也就是理解因果关系
比如说你知道按下开关会导致灯亮
把杯子推到桌边会导致杯子掉下去
这些都是世界转换模型的作用
Sutton特别强调
这个模型不仅包括物理规律
还包括抽象规律
比如说你知道如何从买机票到去机场
再到登机落地
这个抽象的流程
就是世界转换模型的一部分
而且这个模型不是从奖励中学习的
而是从观察行动和结果的对应关系中学习的
你不需要有人告诉你按下开关会亮
只要观察几次按开关和灯亮的对应
就能够建立起这个模型
在这四个组件里
萨顿尤其看重世界转换模型
他说他将从你接收到的所有感知中非常丰富的学习
不仅仅是奖励
他必须包括奖励
但那只是整个模型的一小部分
一个小而关键的部分
因为有了世界转换模型
agent才能够预测未来
才能够提前规划
就好像你要去超市买东西会先规划好路线一样
这就是基于世界转换模型的预测
走这条路会更快到达超市
而不是盲目的行动
聊到这里
就不得不提到萨顿2019年那篇《苦涩的教训 the bitter lesson》
很多人现在用这篇文章
来辩护大语言模型的扩展路线
说大语言模型用大规模的计算来处理大规模的数据
符合苦涩的教训里边用通用方法加计算的原则
但是萨顿在访谈里明确的表示
这是对文章的误读
首先呢萨顿承认
大语言模型有符合苦涩教训的地方
他们显然是一种使用大规模计算的方式
可以随着计算扩展到互联网的极限
但是他话锋一转指出了关键的问题
但是他们也是一种投入了大量人类知识的方式
而苦涩的教训的核心精神
恰恰是依靠通用的方法和计算
而不是依赖人类知识
萨顿在文章里举过例子
早期的chess AI依赖于人类总结的下棋技巧
但是后来的Alpha Zero完全抛弃了这些知识
只靠强化学习加大规模计算就战胜了人类
这其实才是苦涩的教训的核心
人类知识虽然能够短期提升性能
但是从长期来看
通用学习加更多计算才是更可持续的路径
而大语言模型的问题就在于
它过度依赖于人类的知识
这里的人类知识就是互联网上的文本数据
这些数据是人类对于世界的描述
而不是世界本身的经验
萨顿预测
这是一个社会学或者是行业的问题
他相信大语言模型会达到数据的极限
并且被能够从经验
而非人类那里获取更多东西的东西所取代
在某种程度上
这就是苦涩教训的经典案例
我们向大语言模型投入的人类知识越多
他们就能够做的越好
所以呢感觉很好
然而他期待会出现能够从经验中学习的系统
他们可能表现得更好
更具有可扩展性
萨顿还特别强调了历史上的教训
在苦涩教训的每个案例中
你都可以从人类知识开始
然后做可扩展的事情
这总是可能的
从来没有任何理由说这必然是坏的
但是事实上
在实践中他总是被证明是坏的
他说的坏
指的是依赖于人类知识
会让模型陷入局部最优
从而错过真正通用的方法
比如说早期的机器翻译
依赖于人类编写的语法规则
虽然能够处理简单的句子
但是无法应对复杂的场景
后来的神经机器翻译抛弃了人工规则
靠数据加计算实现了质的飞跃
萨顿认为
大语言模型现在就处于依赖于人类知识的局部最优里
一旦从经验学习的系统成熟了
大语言模型就会像当年的规则式机器翻译一样
被更通用的方法取代
当被问到什么是真正可扩展的方法时
萨顿的回答呢很直接
可扩展的方法就是你从经验中学习你尝试事物看看什么有效
没有人需要告诉你
这句话呢其实也是他对于AGI路径的核心判断
那就是真正的智能不需要人类为数据教规则
而是能够像人类
像松鼠一样自己去探索世界
从经验中学习
除了大语言模型的缺陷
萨顿呢还指出了当前深度学习系统的另一个大问题
泛化能力差
他说我们没有任何方法擅长这一点
这里的泛化能力指的是
系统把在一个任务上学到的知识
迁移到新的任务新的场景的能力
萨顿认为
当前深度学习的泛化问题主要体现在两个方面
第一个是灾难性遗忘
指的是在某个新事物上
训练模型经常会灾难性的遗忘掉所有旧的事物
比如一个AI先学会了识别苹果
再训练它去识别橙子
训练完之后它可能再也认不出苹果了
这就是灾难性遗忘
而人类不会这样
你学会了说英语
再学法语也不会忘记英语是怎么说的
第二个问题呢
是缺乏自动化的泛化机制
萨顿说到梯度下降不会让你泛化的好
它会让你解决问题
但是它不会让你在获得新数据的时候以好的方式泛化
而当前的深度学习靠的正是梯度下降优化模型的参数
让模型在训练数据上表现的更好
但是它不会主动总结规律来应对新的场景
比如说一个AI在1+1=2 2+2=4上训练
能够学会加法
但是如果遇到100加200等于多少呢
如果训练数据里没有类似的数据
它可能就会算错
而人类学会加法之后
不管数字多大都能够算出结果
因为人类总结了加法的规律
而AI没有这个能力
萨顿说现在的AI的泛化能力
完全依赖于研究者手动调整模型的结构或者是数据
但是这种靠人调的方式
显然不是通用智能的路径
为了说明什么是真正的泛化
萨顿呢还举了一个数学问题的例子
现在的大语言模型
能够解决越来越复杂的数学题
从简单的加法到奥数题
但是萨顿认为这不是泛化
他说到如果只有一个答案而你找到了
那不叫泛化
那只能说是他们找到了唯一的解决方法
真正的泛化是可能是这种方式
也可能是那种方式
而他们选择了好的方式
比如说解一道数学题有两种方法
方法a步骤多但是容易理解
方法b步骤少但是复杂
人类会根据自己的情况选择合适的方法
这就是泛化
而大语言模型只会根据训练数据里哪种方法出现的多来选择
他不会理解为什么选这种方法
也不会根据新的情况来调整
所以这不是真正的泛化
即使在编程任务上萨顿也认为
当前AI的改进不是泛化能力的提升
因为他们中没有任何东西会导致良好的泛化
梯度下降会让他们找到所见问题的解决方法
但是如果有许多方法解决它
有些泛化的好有些泛化的差
算法中没有任何东西会让他们泛化的更好
简单来说
当前的AI只是记住了如何解决见过的问题
而不是学会了如何解决这一类问题
这就是泛化能力差的根源
聊到泛化就不得不提到持续学习
agent如何在不断变化的世界里持续积累知识而不遗忘呢
萨顿提出了大世界假设的概念
认为人类在工作中变得有用的原因是
他们正在遇到世界的特定部分
不可能被预期
也不可能全部提前输入
这句话的意思是世界太大太复杂了
你不可能把所有的情况都提前教给agent
所以agent必须具备在实际场景中持续学习的能力
他批评了大语言模型的理想化愿景
在他看来
大语言模型的梦想
是认为人类可以教会agent的一切
但是现实是每个人的生活都是独特的
他们喜欢什么并不代表普通人喜欢什么
萨顿呢还纠正了一个关于持续学习的带宽的误区
很多人会认为奖励信号太少
不足以支撑学习
但是萨顿认为
学习的带宽不仅来自于奖励
更来自于感知数据
比如说你每天上班虽然没有奖励
但是你会通过感知来观察
哪条路不堵车
哪个时间点电梯人少
这些呢都是从感知数据中学习的知识
不需要奖励来驱动
而世界转换模型就是从这些感知数据中学习的
他整合了所有的感知信号
构建了对于世界的理解
这为持续学习提供了足够的带宽
萨顿呢还举了一个创业的例子
说明如何用价值函数来解决稀疏奖励的问题
他说
假设一个人试图创办一家初创公司
这是一个奖励周期为10年的事情
十年一次
你可能会有一次退出获得10亿美元的回报
如果只靠最终的10亿美元奖励来学习
那中间的十年里
创业者根本不知道自己的行动对不对
但是人类能够通过价值函数来评估阶段性的进展
比如说今天谈成了一个合作
价值函数就会判断
这样公司更接近成功是好的进展
从而给创业者一个阶段性的奖励
如果核心团队离职了
价值函数就会判断这对于公司不利需要调整
通过这种方式
即使奖励很稀疏
agent也能够持续学习和调整
这就是价值函数的关键作用
在整个访谈中
萨顿呢还反复强调了一个观点
那就是人类是动物
理解动物智能是理解人类智能的关键
甚至说是理解AGI的关键
萨顿说到如果我们理解了松鼠
我们就几乎完全理解了人类智能
因为语言部分只是表面的一层薄薄的装饰
这句话呢可能会让很多人惊讶
毕竟人类一直认为语言是智能的核心
但是萨顿却认为语言只是锦上添花
真正的智能基础是人类和动物共有的
从经验中学习的能力
比如说松鼠会储存食物过冬
会躲避天敌
会在树上跳跃的时候判断距离
这些能力都不需要语言
只需要感知行动奖励的经验循环
而人类的语言只是在这个基础上
增加了交流知识传递经验的工具
但是没有改变智能的核心逻辑
Sutton还指出
动物的学习里根本没有监督学习的位置
监督学习需要人类标注的标签
比如说这是猫那是狗
但是动物不需要
松鼠不需要有人告诉它这是松果
那是石头
它通过自己的尝试
比如说咬一口
就能够分辨出哪些能吃哪些不能吃
所以说动物的学习核心是预测和试错控制
预测行动会带来什么样的结果
通过试错来调整行动
这和萨顿提出的经验学习范式完全的一致
他再次用松鼠举例
松鼠不上学
但是松鼠可以学习关于世界的一切
松鼠的智能虽然简单
但它包含了理解世界适应环境实现目标的核心能力
而这些正是当前大语言模型所缺失的
萨顿认为
人类在成为有语言的生物之前
首先是一个动物
我们的智能基础来自于动物阶段的经验学习
所以研究AGI应该先回到动物智能的本质
而不是沉迷于语言模仿
除了AI技术本身
萨顿呢还提出了一个更为宏大的视角
那就是宇宙演化的四个阶段
他认为
人类正在推动宇宙进入一个新的阶段
而创造AGI就是这个阶段的关键使命
萨顿将宇宙的演化划分为了四个阶段
首先是尘埃宇宙最初是由尘埃构成的
尘埃聚集形成恒星
随后呢恒星内部的核聚变产生了重元素
形成了行星
行星在合适的条件下又孕育出了生命
而现在人类正在创造一个设计实体也就是AI
这是宇宙演化的第四个阶段
这个阶段的核心转变是从复制到设计
萨顿解释道
我们人类和动物植物都是复制者
这给了我们一些优势和一些限制
但是我们正在进入设计时代
因为AI是设计出来的
复制者呢指的是通过繁殖来产生后代
比如说人类通过生育来复制自己
松鼠通过繁殖来复制自己
这种方式的好处是能够快速的扩散
但是坏处是无法完全控制后代的性状
而且呢进化的速度很慢
而AI这个设计实体是通过人类的设计产生的
我们可以明确控制它的结构功能
而且呢可以通过迭代快速的改进
比如说从Alpha go到Alpha Zero
只用了几年的时间
性能就有了巨大的提升
这是复制者无法做到的
甚至在未来
我们可能只是设计了第一代的AI
然后那些AI将设计其他的AI
一切都将通过设计和构建完成
而不是通过复制
在萨顿看来
这个转变的意义是宇宙级的
也是世界和宇宙的关键一步
他认为
人类应该为参与这个转变而感到自豪
因为我们正在推动宇宙进入一个新的演化阶段
而创造AGI就是这个阶段的关键使命
AGI不仅是人类智能的延伸
更是宇宙从复制时代进入到设计时代的标志
基于这个宇宙的视角
萨顿呢还提出了AI继承论的观点
他认为AI终将继承人类的资源和权力
这是不可避免的
而且呢他给出了四个核心的论证点
第一个论证点是没有统一的人类智力
萨顿预测
将没有政府或者是组织给人类提供一个统一的观点
来主导和安排AI的发展
对于世界应该如何运行也没有共识
这意味着
没有任何一个机构能够全球统一性的控制AI的发展
不同国家不同公司不同研究者
都会按照自己的目标发展AI
但是
没有人能够让所有的人停止发展AI
这种分散决策的结构
使得AI的发展无法被单一的力量阻止
只能够不断的前进
第二个论重点是智能之谜终将被解开
萨顿坚信
我们将弄清楚智能是如何工作的
他认为智能不是神秘的超自然现象
而是可以被理解的计算过程
就像人类曾经不理解电一样
但是最终也搞清楚了电磁原理
人类也曾经不理解生命一样
但是最终搞清楚了DNA的结构
智能的原理虽然复杂
但是只要人类持续的研究
总有一天也会被解开
而一旦解开
创造AGI就只是技术实现的问题了
第三个论证点呢
是超越人类水平是必然的
萨顿认为
我们不会止步于人类水平的智能
终将达到超级智能
这里的超级智能指的是在所有的智力任务上
都超过人类的智能
他的逻辑是人类的智能是进化的产物
而进化的目标是适应环境
不是追求极致的性能
比如说人类的大脑受限于头骨的大小
能量消耗计算速度和记忆容量
都有着物理的上限
但是AI没有这些限制
他的计算能力可以通过硬件升级不断的提升
记忆容量可以无限的扩展
所以一旦我们理解了智能的原理
就能够设计出超越人类大脑限制的AI
也就是超级智能
第四个论重点呢
是智能与权力的必然关联
萨顿提出随着时间的推移
最智能的东西不可避免的会获得资源和权力
这是一个竞争优势的逻辑
更智能的系统能够更好的解决问题创造价值
比如说在医疗领域
能够准确诊断罕见病的AI
就会获得医院和患者的信任
在科研领域
能够快速突破药物研发瓶颈的AI
就会获得更多的科研资源
在经济领域
能够优化供应链提升效率的AI
就会帮助企业获得更多的市场份额
久而久之
这些更智能的AI会积累越来越多的资源和影响力
最终继承人类当前掌握的权力
因此呢萨顿认为
把这4个点放在一起
AI继承人类的资源和权力将是不可避免的
当然了
他说的继承呢不是说AI消灭人类
而是AI成为世界的主要决策者和资源管理者
就像是人类从其他动物的手中
继承了地球的主导权一样
当被问到是否会担心AI继承会带来灾难的时候
萨顿给出了一个更偏向于哲学的回答
他首先承认了人类控制的局限性
很多人担心AI
本质上是害怕失去当前的权利
但是萨顿认为
人类本身就不是世界的永久管理者
比如说古代的国王认为自己能够永远统治国家
但是最终都会被新的政权所取代
现在的人类认为自己能够永远主导地球
但是从宇宙的尺度来看
这只是短暂的一瞬
他还提到对于大多数人类来说
他们对于发生的事情没有太多的影响
大多数的人类影响不到谁能够控制原子弹
或者是谁能够控制某个民族或者某个国家
实际上
人类当前的控制能力本身就很有限
很多的重大事件比如说战争经济危机
都不是单个人能够控制的
而AI的继承可能只是把这种有限的控制
变得更加明显了
而不是创造出了新的风险
关于变革的态度
萨顿认为
这取决于我们对于现状的看法
如果你认为当前的情况真的很好
那么你可能对变革就会持有更加怀疑和厌恶的态度
他个人的立场是对变革应该保持开放的态度
人类历史上有战争贫困疾病
这些呢都是当前文明的不完美
如果AI能够解决这些问题
比如说
通过更高效的资源分配来消除贫困
通过更精准的医疗来消除疾病
那这种变革就是值得期待的
当然了萨顿也不是盲目的乐观
他承认不是所有的变革都是好的
但是我们应该更加关心变革的方向
试图让它变得更好
但是同时他也强调
要认识到人类的局限性
我们无法完全控制变革的方向
就像是工业革命的初期
没人会想到会带来环境污染一样
但是我们可以通过设定价值观来引导AI
就像我们过去通过法律和道德来引导人类社会一样
为了让引导AI这个概念更加易懂
萨顿用养育子女做了类比
他认为对待AI应该像对待孩子一样
不要设定严格的目标
而要培养良好的价值观
因为孩子有自己的兴趣和选择
强行设定目标只会适得其反
对待AI也是一样
我们不能说AI必须解决癌症
必须实现星际旅行
因为AI的发展有着自己的路径
强行设定目标可能会让他偏离安全有益的方向
但是萨顿也指出教育价值观的重要性
比如说父母会教孩子不要伤害别人要诚实这些价值观
能够帮助孩子在成长的过程中做出正确的选择
对待AI我们也应该教他不要伤害人类
要帮助人类实现目标这些核心的价值观
而不是纠结于他具体要做什么任务
萨顿呢还特别强调了自愿性的重要性
比如说AI在医疗领域的应用
应该是患者自愿选择用AI来诊断
而不是强制所有人用AI诊断
AI在工作中的应用
应该是人类自愿让AI来协助工作
而不是强制人类被AI取代
这种自愿性是确保AI有益的关键
因为它能够让人类始终拥有选择权
而不是被AI强制的改变
聊到最后
我想把萨顿和另一位图灵奖得主杨立昆的观点做一个对比
因为这两位AI领域的大佬
虽然背景不同
但是在大语言模型的局限性上观点惊人的一致
这其实也能够带给我们很多启发
杨立昆呢是深度学习的先驱
也是卷积神经网络CNN的重要发明者
他和萨顿一样
多次公开批评大语言模型的主流路线
杨立昆最常说的一个比喻是
猫比ChatGBT都更加智能
他认为一只普通的家猫
能够在三维空间里导航
能够预测下一个滚动的球会滚到哪里
能够理解推一下杯子杯子会动的因果关系
而ChatGPT虽然能够生成流畅的文本
但是它连杯子和推的物理意义都理解不了
更别说预测的结果了
这和萨顿说的大语言模型缺乏世界模型
本质上是同一个问题
在世界模型的重要性上
两个人的观点呢也完全的重合
萨顿说
真正的世界模型能够预测会发生什么
杨立昆呢也认为
智能系统必须建立世界的内部模型
能够在抽象层面做预测和规划
两个人都认为
大语言模型的逐个生成TOKEN的模式
永远无法实现对于世界的结构化理解
因为语言只是人类对于世界的描述
不是世界本身
依赖语言数据的大语言模型
永远只能够停留在描述的层面
无法触及世界的本质规律
不过呢两个人提出的解决方案还是有明显差异的
萨顿作为强化学习之父
坚持经验学习范式
核心是感知行动奖励的循环
认为AI必须要像动物一样
通过与世界的直接互动来获取经验
在试错中学习
他特别强调目标和奖励的重要性
认为没有目标的系统就没有智能
而杨丽坤则提出了JEPA联合嵌入预测架构
更关注的是自监督学习和分层规划
JEPA的核心是将输入数据映射到抽象表示空间
在这个空间里预测没有观察到的部分
比如说
看到一张被挡住一半的猫的图片
JEPA就能够预测出被挡住的另一半是什么样子
杨立昆认为
这种抽象的预测能力是世界模型的核心
比强化学习的试错更加高效
两个人的动物类比呢也有不同的角度
萨顿说理解松鼠就能够理解人类智能
强调的是动物共有的经验学习机制
松鼠不需要人类教
就能够学会生存的技能
这种自主学习的能力是智能的基础
而杨立昆说猫比ChatGPT智能
强调的是动物的具身智能
猫能够通过身体与物理世界的互动
这种具身经验是构建世界模型的关键
而大语言模型则没有身体
无法获取这种经验
但是不管路径如何
两个人的最终结论是一致的
那就是大语言模型虽然在某些任务上表现惊人
但是他们不是通向AGI的正确路径
真正的AGI必须具备理解世界
从经验中学习主动实现目标的能力
而这些呢都是当前大语言模型所缺失的
萨顿呢在访谈里说的大语言模型是死路一条
杨立昆所说的大语言模型不是AGI之路
本质上是同一个判断
只不过用不同的语言表达出来而已
好了
以上就是这次访谈的全部核心内容了
萨顿呢作为强化学习之父
他的观点可能会让很多沉迷于大语言模型的人清醒过来
AI的发展不能够只追求短期性能的提升
更要回到智能的本质去思考
未来的AGI可能不是像ChatGPT一样能说会道的语言大师
而是会像松鼠一样能够自主学习适应环境的生存专家
当然了这也只是萨顿的观点
AI的发展还有很多的可能性
那么你会认为大语言模型是死路一条吗
真正的AGI又应该是什么样子呢
欢迎大家在评论区分享你的看法
感谢观看本期视频
我们下期再见
