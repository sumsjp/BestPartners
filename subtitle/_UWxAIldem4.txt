大家好，这里是最佳拍档，我是大飞
没想到
有望超越Transformer的新的大语言架构
这么快就出现了
而且性能比Mamba还要好
这两天最火爆的人工智能话题
相信非TTT的论文莫属了
简单来说
论文作者们设计了一种新的大语言架构Test-Time Training
简称TTT
中文翻译过来就是测试时间训练层
将机器学习模型
作为一种新的信息压缩和模型记忆机制
取代了RNN的隐藏状态
并且通过输入token的实际梯度下降
实现了对上下文的压缩
研究作者之一的卡兰达拉尔(Karan Dalal)在推文中表示
他相信这将根本性的改变大语言模型
同时指出
TTT层直接取代了Attention
并且通过表达性记忆
解锁了线性复杂性架构
让我们能够在上下文中训练具有数百万、甚至十亿个token的大语言模型
与125M到1.3B参数规模的大模型相比
TTT均能够匹敌或者击败基于Transformers和Mamba架构
并且与Mamba相比
TTT-Linear 的困惑度更低
FLOP更少，对长上下文的利用更好
今天我们来简单介绍一下这篇论文
首先我们要先说一下RNN的问题
它最本质的问题就是长上下文面临的挑战
与自注意力机制不同
RNN层必须将上下文压缩为固定大小的隐藏状态
更新规则需要发现数千甚至数百万个token之间的底层结构和关系
相对于Transformer
RNN的主要优势就是它的线性复杂度
但是这种渐进优势
实际上只存在于长上下文中
而一旦上下文足够的长
现有的RNN反而很难真正利用额外的条件信息
于是，研究团队首先观察到
自监督学习可以将大量训练集
压缩为大语言模型的权重
而大语言模型通常会表现出
对训练数据之间语义联系的深刻理解
受此观察的启发
研究团队设计了一种新的序列建模层
其中的隐藏状态是一个模型
更新规则是自监督学习的一个步骤
由于更新测试序列上的隐藏状态的过程
相当于在测试时训练模型
因此研究团队将这种新的层
称为测试时训练Test-Time Training
也就是TTT层
随后，研究团队引入两个简单的实例
TTT-Linear和TTT-MLP
其中隐藏状态分别是线性模型和两层的多层感知器MLP
而TTT层可以集成到任何网络架构中并进行端到端优化
类似于RNN层和自注意力
为了让TTT层更加的高效
研究团队还采取了一些改进技巧
首先，类似于在常规训练期间
对小批量序列采取gradient step的做法
研究团队在TTT期间也使用了小批量的token
其次，他们为每个TTT小批量内的操作
开发了一种对偶形式dual form
以便更好地利用现代GPU和TPU
训练速度更加快了5倍以上
如图所示
TTT-Linear在8k上下文中比Transformer 更快
与Mamba相当
研究团队认为
所有序列建模层都可以看作是将历史上下文存储到隐藏状态
比方说
像LSTM、RWKV和 Mamba这样的RNN层
可以将上下文压缩为跨时间的固定大小状态
这种压缩会产生两种后果，一方面
将输入标记x_t映射到输出token z_t是高效的
因为每个token的更新规则和输出规则
都需要恒定的时间
另一方面，RNN层在长上下文中的性能
受限于其隐藏状态s_t的表现力
我们也可以从这个角度来看待自注意力
只不过它的隐藏状态是一个随着t线性增长的KV列表
它的更新规则
只是将当前的KV元组追加到这个列表中
而输出规则则需要扫描t前面的所有元组
从而形成注意力矩阵
隐藏状态明确存储了所有的历史上下文
无需压缩
这使得自注意力在长上下文方面
比RNN层更具表现力
然而
扫描这个线性增长的隐藏状态所需的时间
也是线性增长的，因此
为了保持长上下文的高效和表现力
研究团队需要一种更好的压缩启发式
具体来说
就是需要将成千上万、或者可能上百万的token压缩到一个隐藏状态中
从而有效捕捉它们的底层结构和关系
而将任何RNN层集成到更大架构中的最简洁方法
就是直接替换Transformer中的自注意力
在这里称为骨干backbone
然而，现有的像Mamba和Griffin的RNN
都使用了与Transformer不同的骨干层
值得注意的是
它们的骨干层在RNN层之前包含了时间卷积
这可能有助于收集跨时间的局部信息
在对Mamba进行试验后
研究团队发现它也能改善TTT层的困惑度
因此也把它纳入到了建议方法中
在实验中
研究团队将TTT-Linear、TTT-MLP与 Transformer、Mamba这两种基线进行了比较
我们先看短文本的实验结果
在2k上下文中
TTT-Linear (M)、Mamba和 Transformer的性能相当
因为线条大多重叠
在FLOP预算较大的情况下
TTT-MLP (M) 的性能稍差
尽管TTT-MLP在各种模型大小下
都比TTT-Linear有更好的困惑度
但是FLOPs的额外成本抵消了这一优势
在8k上下文中
TTT-Linear (M) 和TTT-MLP (M) 的表现都明显优于Mamba
这与2k上下文中的观察结果截然不同
即使是使用Transformer的TTT-MLP (T) ，
在1.3B左右规模的时候
也比Mamba略胜一筹
同时
实验中可以看到的一个显著现象是
随着上下文长度的增加
TTT 层相对于Mamba层的优势也在扩大
当上下文长度达到8k以后
Transformer虽然在每种模型尺寸下的困惑度
依旧表现不错
但是由于FLOPs成本的原因
已经不再具有竞争力
而在长上下文能力的评估实验中
研究者使用了Pile的一个流行子集Books3
以2倍的增量
对1k到32k的上下文长进行了实验
这里的训练方法与Pile相同
并且TTT层的所有实验
都是在一次训练运行中完成的
从结果子集中研究人员得出了以下观察结果
在Books的 2k 上下文中
Pile 2k的所有观察结果仍然成立
只是Mamba的表现要略好于TTT-Linear
在32k上下文中
TTT-Linear (M) 和 TTT-MLP (M) 的表现都优于Mamba
类似于Pile 8k的观察结果
即使是采用Transformer的 TTT-MLP (T) ，
在32k上下文中的表现也略好于Mamba
而TTT-MLP (T) 在1.3B的规模下
仅略差于TTT-MLP (M)。
最后是速度方面的实验
由于大语言模型的训练和推理
可以分解为前向、后向和生成三个过程
推理过程中的提示词处理、也称为预填充
与训练过程中的前向运算相同
只是后向操作不需要存储中间激活值
由于训练和推理中的前向和后向过程
都可以并行处理
因此这里使用了对偶形式dual form
而生成新token，也被称为解码的过程
本质上是顺序性的
因此使用了原始形式primal form
论文作者提到，由于受到资源的限制
所以实验是用JAX编写的
并且在TPU 上运行
在 v5e-256 TPU pod 上
Transformer 基线在上下文为2k 的情况下
每次迭代训练需要0.30 秒
而 TTT-Linear 每次迭代需要 0.27 秒
也就是说
在没有任何系统优化的情况下
比Transformer快了10%。
而由于Mamba只能在 GPU 上运行
所以论文作者还特意重写了TTT的实现方法
让它也能在 GPU 上运行
从实验结果可以看到
各个模型的前向内核在批大小为16时的延迟
除了Mamba为是1.4B参数以外
所有模型都是 1.3B
值得注意的是，因为此处使用了vLLM
而不是HuggingFace Transformer
所以这里的Transformer基线要比 Mamba 论文中的快得多
此外
论文作者还编写了另一个用来生成的 GPU 内核
并且以批大小 512 为基准
测试了TTT的速度
另一个常用的挂钟时间指标是吞吐量
它考虑了使用更大的批大小的好处
而对于吞吐量
之前所有的观察结果和方法之间的排序仍然有效
好了，以上就是这篇论文的主要内容
目前来看，如果scaling law依然存在
那么TTT将带来非常重大的影响
尤其对于长序列
Transformer的计算成本往往很高
当长序列变得更长的时候
RNN则会发生遗忘
而TTT训练巧妙地利用了神经网络
来解决了RNN的不足
我们再来简单介绍下论文的作者
三位主要作者分别来自于自斯坦福大学、加州大学伯克利分校、加州大学圣迭戈分校
其中Yu Sun是斯坦福大学的博士后
他博士毕业于 UC Berkeley EECS
长期以来一直的研究方向就是TTT
Xinhao Li是UCSD 在读博士
他本科毕业于电子科技大学
在论文发表后他更是发推表示
TTT的研究持续了一年半
但是TTT这个想法从诞生到现在
其实已经过去了五年时间
而卡兰达拉尔Karan Dalal 是 UC Berkeley 在读博士
本科毕业于UC Berkeley电子工程科学系
论文作者已经公开了代码、训练和测试用的jax
以及PyTorch的推理代码
大家有兴趣可以去研究一下
那各位对于TTT的发展是如何看的呢
会不会有机会超越如今的Transformer架构呢
欢迎在评论区留言
感谢大家的观看，我们下期再见
