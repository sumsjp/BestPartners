大家好，这里是最佳拍档，我是大飞
随着 Mixtral 8x7B 模型的推出
一种被称为混合专家模型
英文全称为Mixed Expert Models
简称 MoEs 的 Transformer模型
在开源人工智能社区引起了广泛关注
今天我们来分享HuggingFace的一篇文章
深入探讨 了MoEs的核心组件、训练方法
以及在推理过程中需要考量的各种因素
文章很长，总共有一万多字
大飞我总结了一下其中的精华内容
帮助大家快速梳理一下
有兴趣的朋友可以去阅读一下原文
首先我们先简短总结一下混合专家模型的几个特点
1、与稠密模型相比
它的预训练速度更快
2、与具有相同参数数量的模型相比
混合专家模型具有更快的推理速度
3、混合专家模型需要大量的显存
因为所有专家系统都需要加载到内存中
4、混合专家模型在微调方面存在诸多挑战
但是近期有研究表明
对混合专家模型进行指令调优具有很大的潜力
那么究竟什么是混合专家模型呢？
我们现在都知道
模型规模是提升模型性能的关键因素之一
在有限的计算资源预算下
用更少的训练步数训练一个更大的模型
往往比用更多的步数训练一个较小的模型
效果更佳
混合专家模型的一个显著优势
就是它们能够在远少于稠密模型所需的计算资源下进行有效的预训练
这意味着在相同的计算预算条件下
你可以显著扩大模型或数据集的规模
特别是在预训练阶段
与稠密模型相比
混合专家模型通常能够更快地达到相同的质量水平
混合专家模型MoE作为一种基于 Transformer 架构的模型
主要由两个关键部分组成
一个是稀疏 MoE 层
这些层代替了传统 Transformer 模型中的前馈网络层
可能会包含若干个“专家”，
比方Mixtral就是8
每个专家本身就是一个独立的神经网络
在实际应用中
这些专家通常是前馈网络FFN
但是它们也可以是更复杂的网络结构
甚至可以是 MoE 层本身
从而形成层级式的 MoE 结构
第二个部分是门控网络或者路由
这个部分用于决定哪些token被发送到哪个专家
例如
“More”这个令牌可能被发送到第二个专家
而“Parameters”这个令牌被发送到第一个专家
有时候
一个令牌甚至可以被发送到多个专家
令牌的路由方式是 MoE 使用中的一个关键点
因为路由器是由学习的参数组成的
并且与网络的其他部分一同进行预训练
总结来说，在混合专家模型中
我们将传统 Transformer模型中的每个前馈网络层替换为MoE层
其中MoE层由两个核心部分组成
分别是一个门控网络和若干数量的专家
我们再说一下混合专家模型的历史
混合专家模型的理念起源于1991年的论文 Adaptive Mixture of Local Experts
这个概念与集成学习方法类似
目的是为由多个单独网络组成的系统建立一个监管机制
在这种系统中
每个网络处理训练样本的不同子集
专注于输入空间的特定区域
也被称为“专家”。
而门控网络决定了分配给每个专家的权重
在训练过程中
这些专家和门控网络都同时接受训练
从而优化它们的性能和决策能力
在 2010 至 2015 年间
有两个独立的研究领域
为混合专家模型的后续发展做出了显著贡献
一个是组件专家
在传统的 MoE 设置中
整个系统由一个门控网络和多个专家组成
在支持向量机SVM、高斯过程和其他方法的研究中
MoE通常被视为整个模型的一部分
然而
Eigen、Ranzato 和 Ilya 的研究探索了将 MoE 作为更深层网络的一个组件
这种方法允许将MoE嵌入到多层网络中的某一层
使得模型既大又高效
第二个是条件计算
传统的神经网络通过每一层处理所有输入数据
在这一时期
Yoshua Bengio 等研究人员开始探索基于输入令牌动态激活、或者停用网络组件的方法
这些研究的融合促进了在自然语言处理 (NLP) 领域对混合专家模型的探索
特别是在 2017 年
Shazeer 等人将这一概念应用于 137B 的 LSTM
通过引入稀疏性
这项工作在保持极高规模的同时实现了快速的推理速度
混合专家模型的引入使得训练具有数千亿甚至万亿参数的模型成为可能
比如开源的 1.6 万亿参数的 Switch Transformers 等等
这种技术不仅在自然语言处理NLP领域得到了广泛应用
也开始在计算机视觉领域进行探索
刚才我们提到了稀疏性
这里简单解释一下
稀疏性的概念采用了条件计算的思想
在传统的稠密模型中
所有的参数都会对所有输入数据进行处理
相比之下
稀疏性允许我们仅针对整个系统的某些特定部分执行计算
这意味着并非所有参数都会在处理每个输入的时候
被激活或者使用
而是根据输入的特定特征或者需求
只有部分参数集合被调用和运行
这种条件计算的概念
使得在不增加额外计算负担的情况下扩展模型规模成为可能
并且这一策略在每个 MoE 层中实现了数以千计甚至更多的专家的有效利用
不过
这种稀疏性设置确实也带来了一些挑战
例如，在混合专家模型中
尽管较大的批量大小通常有利于提高性能
但是当数据通过激活的专家的时候
实际的批量大小可能会减少
比如
假设我们的输入批量包含 10 个令牌
可能会有五个令牌被路由到同一个专家
而剩下的五个令牌分别被路由到不同的专家
这就导致了批量大小的不均匀分配和资源利用效率不高的问题
那我们应该如何解决这个问题呢？
答案就是用一个可学习的门控网络
来决定将输入的哪一部分发送给哪些专家
在这种设置下
虽然所有专家都会对所有输入进行运算
但是通过门控网络的输出会进行加权乘法操作
而一个典型的门控函数
通常是一个带有 softmax 函数的简单的网络
在Shazeer 等人的工作中
还探索了其他的门控机制
其中包括带噪声的 TopK 门控
这种门控方法主要是引入了一些可调整的噪声
然后保留前 k 个值
最初的混合专家模型设计采用了分支结构
这导致了计算效率低下
这种低效主要是因为 GPU 并不是为处理这种结构而设计的
而且由于设备间需要传递数据
网络带宽常常成为性能瓶颈
要优化 MoE 模型
可以从并行计算、优化容量因子和通信开销、改进部署技术
以及高效训练几个方面着手
首先在并行计算方面
存在着数据并行、模型并行、模型和数据并行、专家并行几种形式
其中数据并行是指相同的权重在所有节点上复制
数据在节点之间分割
模型并行是指模型在节点之间分割
相同的数据在所有节点上复制
而模型和数据并行指的是
我们可以在节点之间
同时的分割模型和数据
爸爸哎米团爸爸要录个视频
而模型和数据并行指的是
我们可以在节点之间
同时的分割模型和数据
这里要注意的是
不同的节点处理不同批次的数据
最后的专家并行
指的是专家被放置在不同的节点上
如果与数据并行结合
每个节点拥有不同的专家
数据在所有节点之间分割
在专家并行中
专家被放置在不同的节点上
每个节点处理不同批次的训练样本
对于非 MoE 层
专家并行的行为与数据并行相同
对于 MoE 层
序列中的token会被发送到拥有所需专家的节点
此外，在设备通信带宽有限的情况下
选择较小的容量因子可能是更佳的策略
一个合理的初始设置是采用 Top-2 路由、1.25 的容量因子
同时每个节点配置一个专家
在部署技术方面
像预先蒸馏实验、任务级别路由、专家网络聚合等方法
可能会更适合于混合专家模型的本地部署
在训练方面
FasterMoE深入分析了 MoE 在不同并行策略下的理论性能极限
并且探索了一系列创新技术
包括用于专家权重调整的方法、减少延迟的细粒度通信调度技术
以及一个基于最低延迟进行专家选择的拓扑感知门控机制
而Megablocks 则专注于通过开发新的 GPU kernel 来处理 MoE 模型中的动态性
从而实现更高效的稀疏预训练
它的核心优势在于
它不会丢弃任何令牌
并且能够高效地适应现代硬件架构
比如支持块稀疏矩阵乘法
从而达到显著的加速效果
Megablocks 的创新之处在于
它不像传统 MoE 那样使用批量矩阵乘法
而是将MoE层表示为块稀疏操作
从而可以灵活适应不均衡的令牌分配
尽管混合专家模型有着很多显著的优势
例如更高效的预训练和与稠密模型相比更快的推理速度
但是它们也伴随着一些挑战
比方说训练挑战
虽然 MoE 能够实现更高效的计算预训练
但是它们在微调阶段往往面临泛化能力不足的问题
长期以来易于引发过拟合现象
还有推理方面的挑战
MoE 模型虽然可能拥有大量参数
但是在推理过程中只使用其中的一部分
这使得它们的推理速度快于具有相同数量参数的稠密模型
然而
这种模型需要将所有参数加载到内存中
因此对内存的需求非常高
以 Mixtral 8x7B 这样的 MoE 为例
需要足够的 虚拟内存来容纳一个 47B 参数的稠密模型
之所以是 47B 而不是 8 x 7B = 56B
是因为在 MoE 模型中
只有前馈网络层被视为独立的专家
而模型的其他参数是共享的
此外，假设每个令牌只使用两个专家
那么推理速度类似于使用 12B 模型
而不是 14B 模型
因为虽然它进行了2x7B的矩阵乘法计算
但是某些层是共享的
目前在混合专家模型方面
还有一些有趣的研究方向
首先是尝试将稀疏混合专家模型蒸馏回到具有更少实际参数、但是相似等价参数量的稠密模型
MoE的量化 也是一个有趣的研究领域
例如
QMoE通过将 MoE 量化到每个参数不到 1 位
将 1.6 万亿参数的 Switch Transformer 所需的存储
从 3.2TB 压缩到仅 160GB
简而言之，此外
像如何将Mixtral蒸馏成一个稠密模型、探索合并专家模型的技术及其对推理时间的影响、尝试对 Mixtral 进行极端量化的实验等等
都是一些值得探索的有趣领域
好了
以上就是对混合专家模型MoE的概括性介绍
希望能对大家带来一些帮助
出于现实成本和商业情况的考虑
现在很多AI厂家都开始用混合专家模型
来代替一个超大的基础大模型
包括OpenAI和ChatGPT
大飞我相信，在Mixtral的开源基础上
以后应该会有越来越多开源的MoE模型出来
从而在有限的计算资源条件下
尽可能的去扩大模型和数据集规模
并且进一步提高训练和推理的速度
不过，从目前实际的使用情况来看
MoE模型的工程化也还是有非常多的坑要去填
即使是Mixtral的开源模型
想完整的跑起来并进行微调
也并不容易
那么有关于MoE模型的更多内容
大家如果有想了解的
欢迎在评论区留言
有机会再做一些节目来讨论
本期视频内容就到这里
感谢大家的观看
我们下期再见
