大家好，这里是最佳拍档，我是大飞
临时插播一条资讯，还没消停两天
AI大模型又开始卷起来了
几个小时前
OpenAI突然宣布了「Mini」版本的GPT-4o模型
而且是立即上线
直接取代了之前的GPT-3.5 Turbo
免费用户已经可以使用
新的GPT-4o mini模型
在MMLU上的得分为 82%，
目前在LMSYS排行榜的聊天方面分数
优于GPT-4
除了在ChatGPT上立即可用以外
GPT-4o mini的商用价格是每百万输入token 15美分
每百万输出token 60美分
比之前的 SOTA 模型便宜一个数量级
比OpenAI此前最便宜的GPT-3.5 Turbo
还要便宜60%以上
比GPT-4o更是便宜了96%-97%。
OpenAI CEO 山姆・奥特曼发推文形容
通往智能的成本已经是too cheap to meter
好了
接下来我们就来详细介绍一下GPT-4o mini
按照OpenAI官方发布的内容
GPT-4o mini模型具有128K token的上下文窗口
知识截至2023年10月
而且因为使用了GPT-4o的改进版 tokenizer
所以处理非英语文本的能力
也变得更加经济和高效
凭借低成本和低延迟的性能
GPT-4o mini可以实现广泛的任务
比如说链接或者并行化多个模型调用的应用程序、将大量的上下文传递给模型
或者通过快速、实时的文本响应与人进行互动
目前GPT-4o mini的API中仅支持文本和视觉
不过按照奥特曼给网友的回复
7月晚些时候推出语音等模态的测试版
公众访问权限会在更晚些时间开放
很快也将支持文本、图像、视频和音频输入和输出
从性能上看
GPT-4omini目前在小模型系列中是最能打的
不仅在文本智能和多模态推理方面
超越了GPT-3.5 Turbo和其他小型模型
而且支持与GPT-4o相同范围的语言
甚至还在函数调用方面
表现出强大的性能
以OpenAI发布的多个关键基准测试评估为参考
我们可以看到，在推理方面
GPT-4o mini在涉及文本和视觉的推理任务上
优于其他的小型模型
在文本智能和推理基准MMLU上的得分为82.0%，
而Gemini Flash为77.9%，
Claude Haiku为73.8%。
在数学和编码能力方面
GPT-4o mini表现出色
优于以前的小型模型
在MGSM上，对于数学推理任务
GPT-4o mini得分为87.0%，
而Gemini Flash为75.5%，
Claude Haiku为71.7%。
在编码性能方面
GPT-4o mini在HumanEval基准上得分为87.2%，
而 Gemini Flash的得分为71.5%，
Claude Haiku的得分为75.9%。
在多模态推理方面
GPT-4o mini在MMMU上表现出了强劲的性能
得分为59.4%，
而Gemini Flash为56.1%，
Claude Haiku为50.2%。
在Artificial Analysis的质量指数上
GPT-4o mini得分为85
和Gemini 1.5 Flash、Llama 3 70B基本处于同一水平
胜过Mixtral系列的8×22B和8×7B型号
推理效率方面
每秒183个token的生成
让GPT-4o mini成为这个榜单上的绝对王者
比第二名Gemini 1.5 Flash还要快18个token
作为模型开发过程的一部分
OpenAI也与一些合作伙伴合作测试了GPT-4o mini
发现GPT-4o mini在一些任务上明显优于GPT-3.5 Turbo
比如说从收据文件中提取结构化的数据
或者生成高质量的电子邮件回复等等
这其实也印证了业界一直在讨论的一个观点
那就是模型的大小
并不重要
从性价比上来看
GPT-4o mini可以说是大幅上升
现在可在Assistant API、Chat Completions API和Batch API中作为文本和视觉模型使用
每100万输入token，价格为15美分
每100万输出token
大约相当于一本2500页的书
价格为60美分
这个价格基本已经卷到了头部模型的最低档
仅次于Llama 3 8B
不仅比OpenAI自己之前的模型要便宜
也比Claude 3 Haiku的25美分和125美分
以及Gemini 1.5 Flash 的35美分和70美分更便宜
从安全方面来看
这次OpenAI从模型开发一开始
就内置了安全措施
并且在开发过程中的每一步都加以强化
在前期训练中
团队会过滤掉他们不希望模型学习或者输出的信息
比如仇恨言论、成人内容、包含个人信息的网站和垃圾邮件
在后期训练中，会使用RLHF等技术
让模型的行为与自身策略保持一致
从而提高模型响应的准确性和可靠性
GPT-4o mini还内置了与GPT-4o相同的安全缓释措施
根据Preparedness Framework和自愿承诺
OpenAI通过自动和人工评估的方式
对模型进行了仔细评估
其中包含70 多名社会心理学和错误信息等领域的外部专家
对模型进行的仔细测试
在这些经验的基础上
团队还利用了新的技术来提高 GPT-4o mini的安全性
比如
它是第一个应用指令分层方法的模型
这个方法有助于提高模型抵御越狱、提示注入和系统提示提取的能力
从而让模型的响应更加可靠
在大规模应用中更加安全
此外
OpenAI计划在未来几天推出GPT-4o mini的微调版本
从今天开始
ChatGPT的Free、Plus和Team用户
已经可以使用GPT-4o mini来代替GPT-3.5 Turbo
从下周开始，企业用户也将可以访问
OpenAI 负责新模型的产品经理奥利维尔·戈德门特（Olivier Godement）表示
OpenAI的全部意义在于安全地构建和分发AI
并且让它能够得到广泛的普及，其中
以更低的成本提供AI是实现目标的有效方法之一
除了OpenAI的动作以外
昨天正好MistralAI和NVIDIA
也发布了新的小模型Mistral NeMo 12B
开发人员可以轻松定制和部署
支持聊天机器人、多语言任务、编程和摘要等任务
Mistral NeMo支持128k Tokens的上下文窗口
可以直接替代任何使用Mistral 7B的系统
由于Mistral NeMo在训练时考虑了量化
所以能够在不降低性能的情况下
进行FP8推理
在多项基准测试任务上
Mistral NeMo相比Gemma 2 9B和Llama 3 8B
都有较大幅度的提升
这次Mistral NeMo还使用了新的分词器Tekken
训练了超过100种语言
非常适用于全球多语言应用
Tekken基于Tiktoken分词器
相比之前Mistral模型中使用的SentencePiece分词器
可以更加高效地压缩自然语言文本和源代码
尤其是在压缩源代码、中文、意大利语、法语、德语、西班牙语和俄语的时候
效率提高了大约30%。
而在压缩韩语和阿拉伯语的时候
效率分别提高了2倍和3倍
与Llama 3的分词器相比
Tekken在大约85%的语言上表现出了更高的压缩效率
此外
由于Nemo模型经过了更先进的微调和对齐过程
所以与Mistral 7B相比
它在遵循精确指令、推理、处理多轮对话和生成代码方面
也有更佳的表现
对于这两个小模型的同时发布
Andrej Karpathy表示
大模型的参数规模竞争正在加剧
但是朝着相反的方向
他预测，我们将会看到非常小
但是可以「思考」得非常好、而且可靠的模型
当前大语言模型如此庞大的原因是
我们在训练过程中非常浪费
我们要求它要记住整个互联网
令人惊讶的是，它们确实做到了
比如可以背诵常见数字的SHA哈希值
或者回忆起非常冷僻的事实
如今模型的标准预训练目标
就好像是在闭卷考试中
根据前几句话来背诵互联网上的任意段落
但是真正的难点在于，在训练数据中
思考的展示与知识是交织在一起的
因此，模型必须先变大
然后才能变小
因为我们需要自动化的帮助
才能将训练数据重构并塑造成理想的格式
这是一个阶梯式的改进过程
一个模型帮助生成下一个模型的训练数据
直到我们拥有完美的训练集
HuggingFace的创始人也表示
这个星期是小模型的一周
这次价格的大幅下降
也让有网友表示
如果你想对美国24小时内所说或者所听到的每一个单词进行推理
现在只需要花费不到20万美元了
好了，以上就是最新的一些热点事件
按照计划
Meta下周也会发布400B参数的Llama 3模型了
OpenAI虽然又一次采用了抢先发布的营销策略
但是总让人有点开始挤牙膏的感觉了
大家最期待的肯定还是GPT-5了
估计很快其他AI公司也会有相应的发布动作
我们会持续关注和报道
感谢大家的观看，我们下期再见
