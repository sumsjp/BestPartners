大家好，这里是最佳拍档，我是大飞
不知道大家有没有这种感觉
现在很多AI Agent看起来技术很先进
但是实际用起来却经常掉链子
比如在处理复杂任务的时候半途而废
你可能会想
是不是因为模型不够强呢？
其实根据业内的观察
多数AI Agent的失败
并不是模型能力的失败
而是上下文工程的失败
那么，这个所谓的上下文工程
英文为"Context Engineering"的概念
它和我们熟悉的提示词工程（Prompt Engineering）、检索增强生成（RAG）
甚至模型上下文协议（MCP）
到底是什么关系呢？
今天我们就用一期视频来彻底搞懂这个话题
首先，我们得明确一个核心的问题
什么是上下文（Context）？
很多人可能觉得，不就是聊天记录吗？
其实这个理解只对了一部分
从本质上讲
上下文是提供给大语言模型的、用来完成下一步推理或者生成任务的全部信息集合
这个定义听起来简单
但是里面包含的内容
远比我们想象的要丰富
为了更好的帮助我们理解
可以把上下文分为三个核心的类别
第一类是指导性上下文（Guiding Context）
它的核心功能是指导模型该做什么
以及如何去做
主要为模型的行为去设定框架、目标和规则
我们平时做的提示词工程
其实主要就是在优化这类上下文
它包括系统提示词（System Prompt）、任务描述（Task Description）、少样本示例（Few-shot Examples）
以及输出格式定义（Output Schema）
第二类是信息性的上下文（Informational Context）
这类上下文的核心功能是告诉模型需要知道什么知识
为模型提供解决问题所必备的事实、数据与知识
它包括我们常说的RAG和记忆Memory
后者又为分短期记忆和长期记忆
以及State和Scratchpad
比如Claude Code的临时TODO
以及Thinking模型下的思考"草稿本"。
第三类是行动性上下文（Actionable Context）
这类上下文的核心功能是告诉模型能做什么
以及做了之后的结果
为模型提供与外部世界交互的能力
它包括工具定义（Tool Definition）、工具调用和结果（Tool Calls & Results）
以及工具追踪（ Tool Traces）
所以从整体上来看
上下文其实是一个多维、动态、服务于特定任务的系统性的概念
远不止我们以为的聊天记录那么简单
在理解了上下文的定义之后
我们再来看看什么是上下文工程（Context Engineering）
两位AI大佬托比·卢特克（Tobi Lütke）和安德烈·卡帕西（Andrej Karpathy）对这个概念的描述很有启发性
托比·卢特克说
它更好地描述了一种核心技能
那就是提供所有上下文的艺术
以便让大语言模型能够合理的解决任务
安德烈·卡帕西则补充道
在所有的工业级的大模型应用中
上下文工程是一门微妙的艺术与科学
目的是在上下文窗口中填入恰到好处的信息
为下一步的推理做准备
结合两人的观点，我们可以这样理解
上下文工程是一门系统性的学科
专注于设计、构建并且维护一个动态系统
负责为Agent执行任务的每一步
智能地组装出最优的上下文组合
从而确保任务能够被可靠、高效地完成
安德烈·卡帕西在他的一次演讲中
用了一个很形象的类比
如果把Agent视为一种新型的操作系统
那么模型就像是中央处理器（CPU）
而上下文窗口（Context Window）就像是内存
上下文工程则是这个操作系统中的内存管理器
它的职责不是简单地把数据塞满内存
而是通过复杂的调度算法
决定在每一个"时钟周期"，
哪些数据应该被加载、哪些应该被换出、哪些应该被优先处理
从而保证整个系统的流畅运行
以及最终结果的准确性
从这个类比中我们可以看到
上下文工程标志着我们和大语言模型交互模式的一种升级
也就是从原来的用提示词工程来优化指导性上下文
转向为构建一个最高效的信息供给系统
可能说到这里
你可能还是会有点迷糊
上下文工程和提示词工程、RAG到底有什么区别呢？
简单来说
它们并不是互相排斥的关系
而是处于不同层级、互相协作的关系
具体来说
提示词工程主要任务是优化单次交互的指令部分
比如在系统提示词中定义角色
添加少样本示例和输出模式
来引导模型的行文
因此
它是一种更细粒度的、面向具体问题的、单轮交互的工程实践
而RAG的作用则是从外部的知识库中检索相关信息
将它作为信息性上下文的一部分
填充到上下文窗口中
显然，上下文工程的范畴远大于RAG
它不仅要负责"检索什么"，
还需要考虑如何将得到的信息性上下文
与另外两类上下文进行动态组合
甚至在RAG失败之后考虑使用其他工具
那搞清楚了什么是上下文工程
接下来我们就要探讨
为什么我们需要这样一个新的概念呢？
在没有这个概念之前
当我们发现Agent的输出不及预期的时候
问题的根源往往可以归结为两个方面
一是模型本身的能力局限
需要对模型进行优化
二是上下文信息的缺失
也就是模型没有接收到
生成高质量回复所需要的恰当的上下文
通常的情况下
尤其是在现在基础模型的智能水平显著提升之后
输出不及预期的原因更多的已经指向了后者
也就是说
我们没有实现有效的上下文工程
导致模型缺失了一些解决问题的关键信息
在这种情况下
模型只能进行不确定的假设
甚至陷入不必要的幻觉
让我们通过两个具体的例子
来更直观地理解上下文工程的必要性
第一个例子来自菲利普·施密德（Philipp Schmid）的观察
想象你的AI助手收到了一封简单的邮件
Hi
明天有空聚一下吗？
一个上下文贫乏的Agent
它看到的只有这句请求
没有其他的上下文
它的回应可能是机械的，比如说
感谢你的消息
我明天有空
请问你希望约在什么时间？
这种回应其实无法真正的推进任务
因为它缺乏做出下一步决策所需要的信息性上下文
而一个上下文充足的Agent
它在调用模型之前
首要任务是动态地组装一个包含多维信息的上下文
在信息性上下文方面
它会检索你的日历
发现明天日程已满
然后识别到发件人吉姆（Jim）是重要的合作伙伴
再通过分析过往邮件
确定沟通应该采用非正式的语气
在行动性上下文方面
它会提供发送日历邀请（send_calendar_invite）工具的描述
基于这个完整的上下文
这回它就能生成一个真正高效的回应了
比如，Hi Jim！
我明天日程完全满了
周四上午有空，你看方便吗？
我发了一个暂定的邀请链接
如果你时间合适请确认一下
这里的"魔力"，
并非来自于更智能的模型
而是一个能够为特定任务动态组装合适上下文的系统
这个例子也清晰地展示了
缺乏上下文
将造成显著的性能鸿沟
不过
是不是直接把所有可能的上下文
都提供给模型就行了呢？
我们暂且不考虑大部分模型并没有较长的上下文容量和经济成本
实际上
当任务变得复杂和长时间的时候
这种简单的累加策略不仅会失效
甚至会降低模型的表现
所以第二个例子是
假设一个Agent被赋予了一项长期任务
比如在一个大型代码库中
用几天的时间来实现一个涉及多个文件和依赖项的新功能
为了简单起见
我们假设采用的是线性的Agent
不考虑并行架构下的决策复杂性
一种朴素的策略是
为了保证模型无所不知
系统将每一次的交互都记录下来
包括每一条用户指令、文件读取、编译错误、成功的工具调用等等
并将这整段历史作为上下文
在每次调用Agent的时候
都完整地传递给模型
但是这必然会带来几个后果
首先，性能会悄然下降
早期任务中各种无关紧要的细节
会不断稀释当前步骤所需要的核心信息
造成上下文干扰
其次，成本与延迟激增
随着上下文线性增长
每次API调用的Token数量会急剧膨胀
导致成本失控和更高的延迟
最终，系统将撞上架构限制
也就是所谓的上下文溢出
当累积的信息总量超过模型的上下文窗口上限后
API调用将直接失败
这也许会导致任务中断
或者因为信息截断而引发错误的结果
而上下文工程的目的
就是要解决这类问题
例如
它可以通过对上下文进行智能的管理与压缩
只将高价值的结论与信息注入上下文
从而规避上述风险
所以说
随着基础模型的能力普遍越过一个关键阈值以后
上下文工程不仅成为了提升系统表现的、更高优先级的选项
在许多场景下
它甚至是唯一可行的路径
上下文的缺失固然会导致Agent系统的性能下降、幻觉频发
但是无差别地把所有历史信息都注入上下文
同样会引发各种上下文退化的问题
为了应对上述的挑战
业界已经逐步收敛出了一套系统性的应对框架
如果我们参考兰斯（Lance）的博客
我们可以将上下文工程分为四个部分
分别是写入（Write）、选取（Select）、压缩（Compress）和隔离（Isolate）
我们逐一来看
首先是写入（Write）
它主要是将上下文持续久化
超越上下文窗口的限制
在未来按需取用
通常情况下分为会话内写入和持久化写入
会话内写入（Session-level Write）是指Agent将中间的思考、计划或者临时数据
写入一个会话内的草稿纸（Scratchpads）
这是一种轻量级的、非持久化的写入
用来管理当前任务的复杂性
持久化写入（Persistent Write）
是指系统将具有长期价值的信息
比如用户偏好的总结、关键事实等等
写入外部的记忆（Memory）系统
例如向量数据库或知识图谱
从而实现跨会话的知识积累
像ChatGPT和Cursor等应用就是通过这种方式
让Agent在与用户的持续交互中"学习"和"成长"，
使得用户在解决某些问题的时候
无需再手动引入上下文
Anthropic也曾经在博客中建议
可以通过将子agent的输出写入文件系统
来避免无意义的上下文传递和上下文窗口占用
其次是选取（Select）
它的目的是在每次模型调用前
从所有可用的信息源中
动态地拉取与当前子任务最相关的信息
这是保证上下文信噪比的关键
上下文工程中的选取包括三类
首先是确定性选取（Deterministic Select）
指的是根据预设的规则来加载上下文
例如，Claude Code在启动的时候
会固定加载项目根目录下的CLAUDE
md文件
这是一种简单高效的先验知识注入
其次是模型驱动选取（Model-driven Select）
指的是当可用的信息源过多的时候
可以利用模型自身的能力进行筛选
第三个是检索式的选取（Retrieval-based Select）
也是最主流的方式
它的核心范式是通过相似度检索
从记忆、草稿纸或者外部知识库中选取信息
因此，选取操作的成败
在很大程度上依赖于底层检索系统的质量
第三个部分是压缩（Compress）
它的目的是在信息进入上下文窗口之前
对信息进行有损或无损的压缩
用更少的Token来承载最核心的信号
这是在上下文窗口容量有限的情况下
容纳更多有效信息的直接手段
像Claude Code等Agent系统
由于需要大量的token开销，因此
它们在上下文窗口接近溢出的时候
通常会采取所谓的"自动压缩（auto-compact）"，
自动总结上下文
保留它认为最重要的部分
因此，自动压缩的策略及性能
对后续任务的处理影响非常大
根据实际使用情况
Claude Code的自动压缩功能目前还不完善
所以直接用最小上下文来重启会更加稳妥
另外还有一个是修剪策略
比如硬截断超过限度的历史
但是它的代价是可能会失去部分的语境
最后是隔离（Isolate）
相较于前三个在"信息流"内部进行优化的原则
隔离是一种在系统架构层面进行的、更根本的上下文管理策略
隔离的目的是在多信息流之间设置边界
由子流程先行消化，仅上交要点信息
所以可以视为跨流层面的一种压缩
Anthropic提到过一个很有意思的观点
那就是搜索即压缩
从庞大的语料库中提取洞见
这不禁会让人想到Ilya压缩即智能的观点
在多Agent系统中
子Agent扮演了一种智能过滤器的角色
它们在各自的领域内隔离且并行工作
消化大量的原始信息
然后再将最关键的压缩后的洞见
提交给主Agent
这种机制极大地减轻了主Agent的认知负担
它无需亲自阅读每一份原始文档
只需要处理由各个专家团队提交上来的、经过预处理和提炼的摘要报告
因此，上下文工程中的隔离
最经典的表现就是多Agent架构
由于主Agent只接收隔离上下文中最有价值的部分
可以很大程度的避免长上下文带来的潜在问题
比如上下文的干扰和冲突
也侧面提高了上下文中的信息密度
实际上
工具调用以及类似的沙盒环境
也都体现了同等的隔离思想
这里我们可以思考一下压缩与隔离的区别
压缩主要作用于单一信息流的内容
目的是为了提升其内在的信息密度；
而隔离则作用在多条信息流上
目的在于管理系统的复杂性
同时也能起到广义的压缩作用
可以说，它们的目的殊途同归
最终都是为了提升上下文中的信息密度
所以一个成熟的系统
往往会同时运用这两种策略
到这里
我们已经完整解构了上下文工程"是什么"、"为什么"以及"如何做"。
可以看到
它并非是一个简单的概念炒作
而是AI应用开发从演示阶段走向工业级应用阶段
所自然产生的开发哲学与准则
我们的工作重心，也将不可逆转地
从如何找到那句完美的提示词
转向如何设计出一个
能够为模型在每一步都动态组装出完美上下文的、健壮可靠的系统
而理解并且熟练运用写入、选取、压缩、隔离这四大最佳实践
将是区分一个AI系统
究竟是一个有趣的演示
还是一个可靠的、可以规模化的应用的关键要素
最后
让我们再回过头来看看模型上下文协议MCP
理解了上下文工程
也许我们就能更好地体会这个命名的用意
MCP是面向工具与数据的一个标准化接口
或者说上下文交换协议
它本质上是为"行动性上下文"和部分"信息性上下文"的标准化交互
所做出的努力
让Agent能够更顺畅、更安全地与外部工具和数据源进行通信
从这个角度看
MCP可以说是实现稳健的上下文工程
所需要的基础设施之一
因为它为这个复杂的系统工程
提供了标准化的接口
总而言之，我们要认识到
多数AI Agent的失败
并不是模型能力上的失败
而是上下文工程的失败
归根结底，无论是精巧的提示词
还是RAG，抑或是MCP
它们都指向的是同一个目标
那就是在模型做出决策之前
为它准备好一份恰到好处的上下文
好了
希望今天的视频能够帮助大家更好的理解上下文工程这个概念
感谢收看，我们下期再见
