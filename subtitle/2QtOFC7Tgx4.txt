大家好，这里是最佳拍档，我是大飞
最近OpenAI发布了DALL-E 3
而且终于Open了一下
还放出了DALL-E 3的技术报告
今天我们就来解读一下这份报告
DALL-E 3这篇论文的题目为Improving Image Generation with Better Captions
翻译过来就是通过改善图像标题
来提升图像生成的能力
其实主要提升的是prompt following
也就是提示遵循的能力
即生成的图像和输入的文本提示的一致性
而对于DALL-E 3具体的模型架构和实现细节
论文并没有给出
所以OpenAI还是保留了很多细节
从整体来看
OpenAI这个技术报告可以分成两个部分
第一个部分是讲如何通过合成训练数据集中图像的标题
Caption，来提升模型的生成能力
而第二部分主要是讲DALL-E 3和其他文生图模型的评测对比
按照文章结构，我们也分这两个部分
分别进行解读
首先是关于合成图像的标题
其实在合成数据上进行训练并不是一个新概念
目前文生图模型的一个很大的问题是模型的文本理解能力
这个文本理解能力指的是生成的图像是否能和文本保持一致
也就是论文里面所说的提示遵循的能力
对于稍微复杂的文本
目前的文生图模型生成的图像
往往会容易忽略掉部分的文本描述
甚至无法生成文本所描述的图像
这个问题主要还是由于训练数据集本身较差所造成的
更具体的是说是图像的标题不够准确
一方面
图像常规的文本描述往往过于简单
比如COCO数据集
它们大部分只描述图像中的主体
而忽略了图像中其它的很多信息
比如背景、物体的位置和数量、图像中的文字等等
另外一方面
目前训练文生图的图像文本对数据集
比如LAION数据集
都是从网页上爬取下来的的
图像的文本描述其实就是网页上alt-text标签的内容
但是这种文本描述很多都是一些不太相关的东西
比如广告
既然训练数据的标题不行
训练的模型也就自然而然
无法充分学习到文本和图像的对应关系
那么提示遵循的能力也就必然存在问题
OpenAI给出的解决方案是训练一个图像标题生成器
image captioner
来合成图像的标题
这里选用的模型架构是谷歌的CoCa
CoCa相比CLIP额外增加了一个多模态文本编码器
来生成标题
它训练的损失也包含CLIP的对比损失
以及生成标题的交叉熵损失
所以CoCa不仅可以像CLIP那样进行多模态检索
也可以用于标题生成
为了提升模型生成标题的质量
OpenAI对预训练好的图像标题生成器进行了进一步的微调
这个微调包括两个不同的方案
两个方案所构建的微调数据集不同
第一个方案的微调数据集
是只描述图像主体的短标题
而第二个方案的微调数据集
是详细描述图像内容的长标题
相应地
两个微调模型分别可以生成短标题和长标题
这张图展示了三个示例图像的原始标题
以及生成的短标题和长标题
这里的原始标题是从网页上alt-text标签得到的内容
质量较差
而合成的短标题简洁地描述了图像的主体内容
合成的长标题详细描述了图像的内容
细节也比较丰富
接下来就是通过实验
来分析合成标题对文生图模型性能的影响
另外一点是探讨训练过程中合成标题和原始标题的最佳混合比例
这里之所以要混合合成标题和原始标题
主要是为了防止模型过拟合合成标题的某些范式
比如最常见的例子是合成的标题
往往以"a"和"an"开头
在训练过程中
在合成标题中混入原始标题
相当于一种模型正则化
首先是合成标题对模型性能的影响
这里共训练了三个模型
它们的差异是采用不同类型的标题
分别是只用原始标题
5%的原始标题+95%的合成短标题
5%的原始标题+95%的合成长标题
这里训练的文生图是基于latent diffusion模型
这是图像生成的核心技术
它可以将图像生成问题分解为多次对噪声向量的小规模扰动
逐步邻近目标图像
关键在于设计恰当的前向过程和反向过程
它的VAE和Stable Diffusion一样
都是8倍下采样
而文本编码器则采用的是T5-XXL
之所以用T5-XXL
我觉得主要有两个原因
一方面T5-XXL可以编码更长的文本
另外一方面是T5-XXL的文本编码能力也更强
这里训练的图像尺寸为256x256
采用的batch size为2048
一共训练50万步
这相当于采样了1个Billion的样本
这里并没有说明UNet模型的具体架构
只是说它包含了3个stage
应该和StableDiffusion XL类似
只不过StableDiffusion XL只下采样了2次
由于这里只是想分析模型的指令遵循的能力
所以采用了CLIP 得分来评价模型
这里的CLIP得分是基于CLIP ViT-B/32来计算生成图像的image embedding和文本提示对应的text embedding的余弦相似度
总共生成了5万个图像
并取平均值再乘以100
采用三种标题训练的模型在CLIP得分上有所差异
左边的图在计算CLIP得分时
文本采用原始标题，从整体上来看
无论是采用合成的长标题还是短标题
其CLIP得分要比只采用原始标题要好一点
但是波动比较大
右图计算CLIP得分时
文本采用合成的长标题
这里就可以明显看到
长标题 > 短标题 > 原始标题
而且CLIP得分要比左图高很多
这说明了采用长标题来计算CLIP得分是比较合理的
因为图像的image embedding信息很大
而短文本信息少
所以两者的相似度就会低一些
而且还可能存在一定的波动
总之，从上面的实验来看
采用合成的长标题
对模型的提示遵循能力是有比较大的提升的
接下来的问题就是通过实验来找到最佳的数据混合比例
所以又增加了混合比例为65%、80%、90%的实验
这张图展示了不同混合比例训练出来的模型
其CLIP得分的差异
可以看到采用95%的合成标题训练的模型
在效果上要明显高于采用更低比例的标题训练的模型
所以最终的结论是采用合成标题对模型的提升帮助比较大
而且要采用描述详细的长标题
训练的混合比例高达95%，
这也是后面DALL-E 3的数据训练策略
不过采用95%的合成长标题来训练
得到的模型也会“过拟合”到长标题上
如果采用常规的短标题来生成图像
效果可能就会变差
为了解决这个问题
OpenAI采用GPT-4来上采样用户的标题
不论用户输入什么样的标题
经过GPT-4优化后就得到了长标题：
在这三个具体的例子中
可以看到使用优化后的长标题生成图像的效果
要优于原来的短标题
所以
DALL-E 3接入ChatGPT其实是不得已的事情
因为这样才能保证DALL-E 3的输入不会偏离训练的分布
论文的第二部分主要是关于模型评测
而对于DALL-E 3的具体实现只有两点
首先
DALL-E 3也采用95%的合成长标题和5%的原始标题混合训练
这也应该是DALL-E 3性能提升的关键
另外
DALL-E 3的模型是上述实验中所采用的模型的一个更大版本
也就是我们之前提到的
基于T5-XXL的latent diffusion模型
同时加上其它的改进
但是这里也没有说是具体的哪些改进
而且技术报告中也说
DALL-E 3也不应简单地归功于
在合成标题的数据集上训练
所以DALL-E 3应该也有一些其它比较重要的改进
目前DALL-E 3生成的图像的分辨率都是在1024x1024以上
所以DALL-E 3的模型应该类似于SDXL
采用递进式的训练策略
从256到512再到1024
而且最后也是采用了多尺度训练策略
来使模型能够输出各种长宽比的图像
另外附录里面给出了一个细节是
DALL-E 3额外训练了一个latent decoder来提升图像的细节
特别是文字和人脸方面
这个应该是为了解决VAE所产生的图像畸变
这里采用的扩散模型是基于DDPM中的架构
所以是pixel diffusion
同时为了加速
基于Consistency Models中提出的蒸馏策略
将去噪步数降低为2步
所以推理是相当高效的
从直观上来看
这个latent decoder就是替换原始的VAE decoder
因此这个扩散模型的条件就是VAE的latent
但是具体是怎么嵌入到扩散模型的UNet中
这里没有说明
最简单的方式应该是通过上采样或者一个可学习的网络
将latent转变为和噪音图像一样的维度
然后与噪音图像拼接在一起
对于DALL-E 3的评测
论文是选取了DALL-E 2和带refiner模块的SDXL来进行对比的
模型评测包括自动评测和人工评测
自动评测主要有3个指标
首先是计算CLIP得分
评测数据集是从COCO 2014数据集中选择4096个标题
然后是采用GPT-4V来进行评测
这里的评测数据集是Imagen中所提出的DrawBench评测集
总共包括了200个包含不同类型的提示语
这里的评测是将生成的图像和对应的文本输入到GPT-4V
然后让模型判断生成的图像是否和文本一致
如果一致就输出正确
否则就输出不正确
最后是采用T2I-CompBench来评测
这个评测集包含6000个组合类型的文本提示
它包括了很多个方面
这里只选择颜色绑定、形状绑定和纹理绑定三个方面进行评测
评测是通过BLIP-VQA model来得到评分
从DALL-E 3与其它模型的对比结果可以看出
DALL-E 3还是明显优于DALL-E 2和SDXL的
不过自动评测所选择的三个指标都是评测模型的指令遵循能力
并不涉及到图像质量
人工评测主要包括三个方面
第一个是提示遵循
给出两张不同的模型生成的图像
让人来选择哪个图像和文本更一致
第二个是风格，这里不给文本
只给两张图像
让人选择更喜欢的那个图像
第三个是一致性，或者称为连贯性
这里也不给文本
让人从两张图像中选择包含更多真实物体的图像
可以看到后面的两个方面其实就是评测模型生成图像的质量
对于提示遵循和风格评测
这里使用的评测集是DALL-E 3 Eval
它共包含170个标题
是从真实应用场景收集得到的
而一致性方面的评测集是从COCO数据集抽样的250个标题
因为一致性是评测真实性
所以采用更偏真实场景的COCO数据集
另外
之前DrawBench评测集是采用GPT-4V来自动评测的
但是GPT-4V也会犯错
比如在计数方面
所以这里也额外增加了DrawBench评测集的人工评测
所有人工评测的对比结果如表所示
可以看到DALL-E 3具有明显优势
不过，论文中也指出
尽管DALL-E 3在提示遵循方面有了很大的提升
但是它依然有一定的局限性
首先是模型在空间位置关系上还是会比较容易出错
当提示包含一些位置关系描述
比如"在左边、在下面"，
模型生成的图像并不一定会符合这样的位置关系
这主要是因为合成的标题在这方面也并不可靠
另外就是文字生成能力
虽然DALL-E 3在生成文字方面很强
但是还是会出现多词或者少词的情况
一个可能的原因
是由于T5-XXL的分词器并不是字符级的
还有一个比较大的问题是合成的标题会幻想图像中的重要细节
比如给定一幅植物的绘图
image captioner可能会虚构出一个植物
并体现在合成的标题上
这就意味着模型可能会学习到错误的绑定关系
也会导致模型在生成特定种类的东西时
并不完全可靠
最后一点是模型的安全性和偏见
这个是所有大模型所面临的问题
也是由于训练集所导致的，对于这点
DALL·E 3的系统卡有更多的评测
我们回头有机会介绍一下
论文中也给出了三个失败的例子
前两个图像应该属于在位置关系上处理失败
而最后一个图像属于生成了错误类型的植物
总的来说，DALL-E 3最大的提升
还是在于通过合成标题来构建了更高质量的训练数据集
而在技术上并没有太大的创新
其实在合成标题方面
LAION在2022年就发布了采用BLIP来生成标题的laion-coco数据集
但是这个工作并没有引起太大的注意
而且这里生成的标题是短文本
个人猜测可能因为成本的问题
OpenAI并没有采用GPT-4V来生成标题
而是专门训练了一个模型
不过相信随着DALLE-3的这个技术报告的发布
后续文生图的很多重点工作
都会转向去优化标题来提升模型效果了
相信过不了多久
开原版的DALLE-3也很快就会来到了
好了本期的视频内容就到这里
感谢大家的观看
我们下期再见
