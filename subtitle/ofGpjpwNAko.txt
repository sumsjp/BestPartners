大家好，这里是最佳拍档，我是大飞
2024年
诺贝尔物理学奖首次授予了人工智能领域的科学家约翰·霍普菲尔德John Hopfield 和 杰弗里·辛顿Geoffrey Hinton
他们因为在神经网络领域的开创性贡献而荣获殊荣
在12月8日
瑞典斯德哥尔摩的颁奖典礼上
辛顿发表了一场名为“玻尔兹曼机”的主题演讲
在这次演讲中，辛顿尝试挑战了一下
不借助任何公式
向观众们解释这个复杂的技术概念
也希望借此让大家了解到
人工智能是如何像人类的大脑一样
来理解和思考的
今天，大飞就在我个人的理解之上
跟大家分享一下辛顿的这次演讲
首先
辛顿介绍了一下 Hopfield 网络
想象有一个由二元神经元组成的网络
这些神经元的状态只有两种
1 或者 0
就像开关一样，要么开要么关
我们可以把这个网络的整体状态
称为“配置configuration”。
每个配置都有一个“良度goodness”，
这个良度是怎么计算的呢？
就是把所有同时激活的神经元对的权重加起来
比如说
左边三个神经元对的权重之和是3+2+（-1）
所以它的良度就是4
而能量呢，就是良度的相反数
也就是-4
Hopfield 网络有个很重要的特点
就是它会自动演化到能量最低点
这是怎么做到的呢？
每个神经元会根据接收到的、来自其他神经元的信号
来降低能量
也就是降低“坏度badness”。
如果接收到的总加权输入是正的
这个神经元就会被激活；
如果是负的，就会关闭
如果每个神经元都按照这个规则行动
并且我们随机选择神经元来持续的应用这个规则
那么网络最终会稳定在一个能量最低点
另外
一个Hopfield 网络可能会有多个能量最低点
但是网络的最终状态
取决于它的初始配置和神经元的更新顺序
Hopfield 网络还可以用来存储记忆
怎么存储呢？
就是把能量最低点对应到记忆上
比如说，我们有一个部分记忆
通过不断的应用神经元的决策规则
网络最终会稳定在一个能量最低点
这个过程就像是把零散的记忆碎片拼凑完整
从而实现了一种“内容可寻址的记忆”。
也就是说
虽然你只记得一件事情的一部分
但是通过这个网络
它能够自动补全剩下的部分
除了存储记忆
Hopfield 网络还有另外一个用途
就是构建对感官输入的解释
其中的核心思想是
一个网络可以同时包含可见神经元和隐藏神经元
可见神经元用来接收感官输入
而隐藏神经元则用来构建对感官输入的解释
如果我们用网络配置的能量来表示解释的“坏度”，
那么就会希望获得能量较低的解释
考虑到当我们看到一幅模糊的画时
大脑通常会有不同的理解方式
那能不能让神经网络也具有这种能力呢？
为了解决这个问题
我们需要设计一个特殊的神经网络
首先，我们需要用“线条神经元”，
来表示图像中的线条
这些神经元的激活状态
对应着图像中出现的具体线条
然后
因为每根线条可能对应着多个三维边缘
所以我们就需要把这些线条神经元
连接到一系列三维的“边缘神经元”上
但是
由于每条二维的线只能对应一个真实的三维边缘
所以这些边缘神经元之间需要相互抑制
除此之外
我们还需要加入一些基本的视觉解释原则
比如说，当图像中的两条线相连时
我们通常会认为它们在三维空间中也是相连的
为了实现这个
我们就在那些共享端点的三维边缘神经元之间
添加强化连接
特别是当两条边缘呈直角相交时
这种连接会更强
因为直角是一种很常见也很重要的视觉特征
通过这样的方式
神经网络就能模拟人类视觉系统
对二维图像进行三维解释的过程
而且可能会产生多种合理的解释结果
但是，这里会引出两个主要问题
第一个是搜索问题
如果我们想用隐藏神经元来解读由可见神经元表示的图像
怎么避免陷入局部最优解呢？
网络可能会停留在一个相对较差的解读中
而无法跳到一个更好的解读
第二个是学习问题
之前我们说的这些连接
好像是我们手动添加的
但实际上我们希望神经网络能够自己学会添加这些连接
就像小孩子自己学会认识世界一样
不需要别人一直告诉他该怎么做
在解决搜索问题的时候
神经元的随机性就起到了关键作用
在标准的 Hopfield 网络中
神经元的决策规则是确定性的
一旦进入到某个能量最低点
就只能沿着能量下降的方向移动
这就可能导致系统被困在局部最优解中
找不到全局最优解
为了克服这个限制
我们就需要引入随机二元神经元的概念
这种神经元虽然还是只有两种状态
但是它们的行为是概率性的
当接收到强烈的正输入信号时
它几乎肯定会被激活；
当接收到强烈的负输入信号时
它几乎肯定会被关闭
但是，当输入信号接近于零的时候
它的行为就不确定了
比如说，即使接收到正输入
它也可能偶尔保持关闭的状态；
接收到负输入的时候
也可能偶尔被激活
这种概率性的决策机制
在处理二元图像的时候特别有用
我们可以把图像的二元数据固定在可见单元上
然后对隐藏神经元随机初始化
在更新过程中
随机选择一个隐藏神经元
计算它从其他激活神经元接收到的总输入
然后根据输入的强度和符号
做出概率性的决策
如果总输入为强正值
那么这个神经元很可能被激活；
如果总输入为强负值
这个神经元很可能被关闭；
如果总输入接近零
神经元的状态就通过概率分布来决定
通过持续应用这个随机更新的规则
系统最终会达到一种叫做“热平衡”的状态
热平衡是一个来自物理学的概念
它描述的是系统在随机波动中达到的一种动态平衡状态
这个机制就像是给网络装上了一对翅膀
让它能够跳出局部最优解
去探索更广阔的解空间
从而有可能找到更好的全局解
一旦达到热平衡
隐藏神经元的状态就成为了对输入的一种解读
比如说
对于奈克方块（Necker Cube）这幅线条图
如果我们能够学习到二维线条神经元和三维边缘神经元之间的正确权重
以及三维边缘神经元之间的正确权重
那么网络的低能量状态
就可能对应着对图像的良好解读
也就是看到一个 3D 的矩形物体
这里的热平衡
并不是说系统的状态就完全稳定不变了
真正稳定下来的
其实是一个更抽象、更难理解的东西
也就是系统所有配置的概率分布
对于普通人来说
这确实有点难以理解
简单来说
系统会趋向于一种特定的分布
叫做“玻尔兹曼分布”（Boltzmann distribution）
在达到热平衡后
系统处于某个特定配置的概率
只由这个配置的能量决定
能量越低，概率就越高
打个比方，就好像在一个班级里
处于低能量状态的、成绩好的同学
被老师表扬的机会就更大
概率也越高
为了帮助大家理解热平衡
物理学家有个小技巧
你可以想象有一个非常大的集合
里面包含无数个完全相同的网络
这些 Hopfield 网络的权重都一样
本质上是同一个系统
但是每个网络的初始状态是随机的
而且它们各自可以独立地做出随机决策
一开始
每个可能的配置都会对应一定比例的网络
而这个比例只取决于初始状态
比如随机初始化的时候
所有的配置会相等概率出现
但是，当你开始运行算法
不断更新神经元的状态
让它倾向于降低能量之后，慢慢地
每个配置对应的网络比例就会稳定下来
虽然每个网络可能会在不同的配置之间跳跃
但是所有网络中某个特定配置的比例
会保持稳定
这种现象就叫做细致平衡（Detailed Balance）
接下来
辛顿讲了一下人工智能是怎么生成图像的
生成图像的过程是这样的
先让所有神经元
包括隐藏神经元和可见神经元
都处于随机的状态
然后随机选择一个隐藏神经元或可见神经元
根据随机规则来更新它的状态
如果它接收到大量正输入的话
就可能被激活；
如果它接收到大量负输入
就可能关闭；
如果输入接近零，行为就会有点随机
不断重复这个过程
直到系统接近热平衡的状态
这时
可见单元的状态就是网络生成的图像
这个图像来源于网络所“相信”的玻尔兹曼分布
在这个分布里
低能量的配置比高能量的配置更可能出现
这个网络会“相信”很多可能的图像
但是你可以从中选择一个它“相信”的图像
在玻尔兹曼机（Boltzmann Machine）里
学习目标就是让网络在生成图像的时候
让这些图像看起来像是它在真实感知中看到的图像
如果能做到这一点
隐藏神经元的状态就能成为解读真实图像的有效方式
它们就能捕捉到图像中的结构信息
就像我们大脑中的神经元能够捕捉到视觉场景中的结构一样
那玻尔兹曼机是怎么学习的呢？
这就涉及到它的学习算法了
这个算法有两个阶段
第一个阶段叫做唤醒阶段
在这个阶段
我们把一个训练图像固定在可见单元上
然后让隐藏单元根据前面提到的随机规则进行更新
直到达到热平衡
在这个过程中
我们会计算每对神经元同时被激活的频率
并且根据这个频率
来调整神经元之间的连接权重
让连接更有可能被激活的神经元对的权重增加
让连接不太可能被激活的神经元对的权重减少
第二个阶段叫做睡眠阶段
在这个阶段
我们会让网络自由地运行
就像它在“做梦”一样
从一个随机状态开始
然后更新神经元
直到再次达到热平衡
同样
我们也会计算每对神经元同时被激活的频率
不过这次是在网络“做梦”的过程中
然后根据这个频率再次调整权重
但是调整的方向和唤醒阶段相反
让在“做梦”时经常一起激活的神经元对的权重减少
让不经常一起激活的神经元对的权重增加
通过不断地重复这两个阶段
网络就能逐渐学习到正确的权重
使得它生成的图像越来越像真实的图像
这个算法的原理是基于这样一个假设
如果一个神经元对在真实图像中经常一起出现
那么它们之间的连接应该被加强；
如果一个神经元对在真实图像中很少一起出现
但是在网络“做梦”时经常一起出现
那么它们之间的连接应该被削弱
这样
网络就能逐渐学会捕捉图像中的统计规律
从而生成更逼真的图像
不过
玻尔兹曼机的学习算法也有它的局限性
在实际应用中，它的学习速度非常慢
尤其是当网络规模比较大的时候
这是因为在计算神经元对的激活频率时
需要大量的计算资源和时间
而且，随着网络层数的增加
这个问题会变得更加严重
这就限制了玻尔兹曼机在处理复杂图像和大规模数据时的应用
为了解决这个问题
一种叫做受限玻尔兹曼机（Restricted Boltzmann Machine）
简称RBM的变体被提了出来
RBM 和普通玻尔兹曼机的主要区别在于
它的神经元连接是受限的
只允许可见单元和隐藏单元之间有连接
而隐藏单元之间没有连接
这种结构大大简化了计算过程
使得学习速度得到了显著提高
在 RBM 中
有一个叫做捷径学习算法的方法
这个算法的基本思想是这样的，首先
我们把一个训练图像固定在可见单元上
然后计算每个隐藏单元被激活的概率
根据这个概率来激活隐藏单元
这样就得到了一个隐藏单元的激活模式
然后，我们再根据这个激活模式
来计算每个可见单元被激活的概率
根据这个概率来激活可见单元
这样就得到了一个新的可见单元的激活模式
通过比较原始的训练图像和新生成的可见单元的激活模式
我们就可以计算出每个神经元对的激活频率的差值
然后根据这个差值来调整神经元之间的连接权重
通过不断地重复这个过程
RBM 就能逐渐学习到正确的权重
使得它生成的图像越来越像真实的图像
虽然 RBM 本身只能学习一层特征检测器
但是我们可以通过堆叠多个 RBM 来构建一个多层的特征检测器网络
具体来说，我们可以先训练一个 RBM
让它学习到第一层特征检测器
然后把这个 RBM 的隐藏单元作为下一个 RBM 的可见单元
再训练这个新的 RBM
让它学习到第二层特征检测器
以此类推
通过这样的方式
我们就可以构建一个深度神经网络
这个网络学习速度不仅比随机初始化快得多
而且泛化能力更强
能够学习到非常复杂的特征表示
从而在图像识别、语音识别等领域取得很好的效果
在实际应用中
这种通过堆叠 RBM 构建的深度神经网络已经取得了很多成功的案例
比如说，2012 年
基于叠加受限玻尔兹曼机的方法被 Google 投入生产
显著提升了语音识别的效果
所以大家会突然之间感到
Android 设备上的语音识别性能有了大幅的提升
不过
如今人们已经找到了一些其他初始化权重的方法
不再需要使用 RBM了
从某种意义上说
这些早期的学习算法更像是一种“历史性的酶”。
它们在人工智能的发展历程中起到了关键的催化作用
虽然它们可能不是最完美的解决方案
但它们为后来更先进的算法和技术的发展奠定了基础
尽管如此
辛顿仍然对用“睡眠”来进行去学习（unlearning）的方法抱有极大的乐观
他认为这是一种更具备生物学合理性的算法
也会避免反向传播的逆向路径
最终会为我们理解人脑如何学习
指明道路
好了
以上就是辛顿在这次颁奖典礼上的演讲了
希望能够带给大家一些启发
演讲的时间虽然不长
但是里面涉及很多专业概念
大飞水平有限
难免会有解读不到位的地方
还请大家有机会亲自去看一下原视频
相信会有更多的收获
感谢大家观看本期视频
我们下期再见
