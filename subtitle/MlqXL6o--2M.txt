大家好，这里是最佳拍档，我是大飞
现在微调大模型
到底应该用LoRA还是全量微调呢？
其实这个问题的核心
本质是“资源”和“效果”的平衡
全量微调（FullFT）虽然能把模型性能拉满
但是太“吃资源”了
一个万亿参数的模型
全量微调时不仅要存原权重
还要存梯度、优化器动量
而且这些数据得用FP32精度
比推理时的BF16多占一倍空间
普通团队根本扛不住硬件成本
而LoRA作为参数高效微调的主流方法
确实省内存、省算力
但是大家总是感觉没底
它的效果到底能不能跟全量微调比？
会不会调了半天，性能差一截呢？
而Thinking Machines Lab最近发布的一篇博客
恰恰把这个问题给讲透了
文章的标题是《LoRA无悔（LoRA Without Regret）》，
作者是约翰·舒尔曼（John Schulman）和他的团队
文章不仅回答了“LoRA能不能媲美全量微调”的问题
还给出了具体的操作条件
甚至连超参数怎么设都讲清楚了
不管你是想入门LoRA
还是已经在落地中遇到问题
这篇内容都值得你仔细阅读
首先，我们得先搞清楚
为什么现在大家这么需要LoRA？
这还得从大模型的现状说起
现在主流的语言模型
比如Llama 3、Qwen3
参数都在上万亿
预训练时用了数万亿个Token
这么大的规模
就是为了让模型学到人类书面知识里的所有模式
基础性能才能上去
但是到了“训练后处理”阶段
情况就发生了变化
不仅数据集变小了
关注的领域也变窄了
这时候如果还用全量微调
这就像用一辆卡车去拉一颗鸡蛋
太浪费了
就是因为这个“浪费”的问题
参数高效微调（PEFT）才发展起来
而低秩适应LoRA就是其中最成熟的方法
LoRA的核心思路很巧妙
它不直接改原模型的权重矩阵W
而是给W加了一个“低秩增量项”，
公式是W' = W + γBA
这里的W'是微调后的权重
B和A是两个低秩矩阵
它们加起来的参数数量
比原矩阵W少得多
γ是个常数缩放因子
用来调整增量项的大小
简单说，LoRA就是用两个小矩阵
“模拟”出原模型权重的更新效果
这样既不用动原模型的海量参数
又能让模型适配新任务
这就是它“参数高效”的关键
而且LoRA不止省参数
还有三个很实际的优势
第一个是“多租户服务（Multi-tenant serving）”，
因为LoRA只训练A和B这两个“适配器”，
原模型权重不动
所以一个推理服务器可以在内存里存多个适配器
对应不同的任务
比如一个适配器处理客服对话
另一个处理产品摘要生成
还能批量处理这些请求
像陈（Chen）等人2023年提出的Punica框架
还有现在常用的vLLM、SGLang这些推理引擎
都已经支持这个功能
对企业来说能省不少硬件钱
第二个优势是“训练布局优化”，
全量微调的时候
硬件布局得专门设计
你想，除了原权重
还要存梯度、优化器动量，精度还高
需要的加速器数量往往是推理时的10倍以上
但LoRA要更新的参数少
占用内存也少
训练时的硬件布局只比推理时稍微大一点
普通团队用几台GPU就能跑
门槛低多了
第三个优势是“加载传输方便”，
适配器矩阵的参数少
文件体积小
比如一个秩为128的LoRA适配器
可能就几MB或几十MB
不管是在机器间传
还是加载到模型里
都比传整个模型快得多，运维也灵活
正是这些优势
LoRA从2021年胡（Hu）等人发表的第一篇论文（《LoRA:
Low-Rank Adaptation of Large Language Models》）后
就越来越火
但是问题也来了，现有的研究里
LoRA和全量微调的性能对比一直不明确
大家公认的是
在“类似预训练”的场景里
比如数据集特别大
超过LoRA的参数存储极限的时候
LoRA性能会差一些；
但是在训练后处理的常见数据集规模下
LoRA虽然容量足够存下关键信息
可是没人能保证它的“样本效率”和“计算效率”，
能跟全量微调相比
所以约翰·舒尔曼团队的实验
核心就是回答了
LoRA到底能不能匹配全量微调的性能？
如果能，需要满足哪些条件呢？
他们的实验结论很明确
只要把几个关键细节做对
LoRA的样本效率和最终性能
就能和全量微调完全一致
那这些“关键细节”到底是什么？
我们得先从他们的实验设计说起
因为好的实验设计
是结论可信的基础
和之前的LoRA研究相比
他们的实验有两个很重要的特点
第一个是“不盯着具体任务
而是研究通用关系”。
很多研究只会选一两个数据集
比如只测情感分析
或者只测文本摘要，但是他们不一样
专门研究“训练集大小”和“LoRA参数数量”之间的通用规律
这样得出的结论
不管你是做医疗还是教育任务
都能参考
第二个是采用了“对数损失（log loss）评估”，
之前很多研究采用的是“基于采样的评估”，
比如让模型生成文本，再人工打分
但是这种方法主观性强
不同任务里标准还不一样
而对数损失能直接反映模型的预测误差
而且能够清晰地看到不同训练步骤、不同参数设置下的模型表现规律
通用性强得多
就是靠着这样的实验设计
他们得出了五个关键发现
这些发现也是LoRA能够媲美全量微调的核心条件
第一个发现
在中小规模的指令微调或者推理数据集上
LoRA和全量微调的性能完全一样
比如他们用的Tulu3和OpenThoughts3数据集
只要规模没超过LoRA的容量
两者的损失曲线、收敛速度都几乎重合
这说明在大多数企业的落地场景里
LoRA完全够用
第二个发现
如果数据集规模超过了LoRA的容量
LoRA就会比全量微调差
但不是“损失降到一定程度就降不下去”了
而是“训练效率变慢”了
具体会慢多少
要取决于LoRA的参数容量和数据集大小的比例
比如一个秩为32的LoRA
处理100万Token的数据集没问题
但是如果数据集涨到1亿个Token
它的训练效率就会明显下降
损失下降速度变慢
第三个发现
LoRA对“批量大小（batch size）”的容忍度比全量微调低
如果批量 size 太大
超过某个阈值后
LoRA的性能损失就会比全量微调大得多
而且这个问题
就算增大LoRA的秩也解决不了
因为这是LoRA“矩阵乘积参数化（BA）”的固有属性
它的优化动态和原权重矩阵（W）不一样
批量一大，这种动态差异就会被放大
第四个发现
LoRA一定要应用到模型的所有层
尤其是MLP层或MoE层，效果才好
之前很多研究受到第一篇LoRA论文的影响
只把LoRA用到注意力层
但是约翰·舒尔曼团队发现
仅在注意力层使用LoRA的性能很差
而MLP层才是LoRA发挥作用的关键
比德曼（Biderman）等人2024年的研究（《LoRA Learns Less and Forgets Less》）也得出了类似结论
进一步验证了这个发现
第五个发现，在强化学习的场景里
就算用很低秩的LoRA
也能和全量微调性能一致
他们用数学推理任务做实验
比如让模型去解初中数学题
以答案正确性为奖励，结果发现
哪怕LoRA的秩只有1
最终的准确率也和全量微调一样
这背后有个信息论的解释
那就是监督学习里
每一轮训练能给模型提供“O(Token数量)”个比特的信息
比如一句话有100个Token
就能提供100比特左右的信息；
但是强化学习不一样
政策梯度算法的学习
靠的是“优势函数（Advantage Function）”，
每一轮只能提供“O(1)”比特的信息
简单来说
就是强化学习需要学的信息少
就算是低秩LoRA，容量也完全够
基于这些发现
他们提出了一个“低遗憾区间（low-regret regime）”的概念
只要LoRA的参数容量能覆盖数据集的信息需求
并且应用到所有层
就能和全量微调性能相当
而这个区间
刚好覆盖了大多数企业的训练后处理场景
这也意味着，LoRA在很多实际应用里
完全可以替代全量微调
当然，光有结论不够
我们还得知道这些结论是怎么来的
也就是他们的实验细节
这能帮我们在自己做实验的时候少走弯路
我们挑几个部分来重点讲一下
首先，他们的实验设置非常严谨
首先是LoRA的秩
覆盖了1到512三个数量级
从最低的秩1
到能覆盖大部分场景的秩128、256
再到接近全量微调的秩512
这样能全面看到秩对性能的影响
然后是学习率（LR）
为了避免“因为学习率选得不好
导致LoRA性能差”的情况出现
他们对每个实验条件都做了“学习率扫描”，
比如对秩32的LoRA
测试0.0001到0.01的不同学习率
找到最佳值；
而且采用了“恒定学习率”，
没有热身或冷却阶段
这样能够排除学习率调度的干扰
更准确对比LoRA和全量微调的差异
模型方面，他们用了两类主流模型
一类是Llama 3系列
另一类是Qwen3系列
还包括了混合专家（MoE）模型
因为MoE模型的激活效率高
现在很多大模型都用这种架构
所以加入MoE的实验，能让结论更全面
数据集方面
监督学习用了Tulu3和OpenThoughts3
Tulu3是艾维森（Ivison）等人在2024年提出的
侧重“指令遵循”，
比如让模型根据用户指令生成邮件、写代码；
OpenThoughts3是古哈（Guha）等人在2025年提出的
侧重“推理”，
比如数学题、逻辑推理题
这两个数据集在任务类型、数据结构上差异很大
强化学习实验则用了MATH数据集和GSM8K数据集
都是数学推理任务
以答案正确与否作为奖励信号
以便专注测试强化学习场景下的LoRA性能
先看“LoRA秩”的影响实验
他们在Tulu3和OpenThoughts3上训练了一个epoch
对每个数据集、每个模型大小
都扫描了不同的LoRA秩和学习率
结果发现，高秩LoRA，比如256、512
和全量微调的学习曲线几乎重合
损失都是“随步骤的对数线性下降”，
也就是说
每训练10倍的步骤
损失就会按固定比例下降
这和全量微调的规律完全一样
而低秩LoRA，比如1、4
在训练初期还能跟上
但是到了某个步骤阈值后
就会“偏离最小损失曲线”。
这就是之前说的“容量耗尽”，
低秩矩阵存不下更多的任务信息
所以学习速度变慢
更重要的是“学习率”的发现
他们绘制了“学习率和最终损失”的曲线
发现高秩LoRA的最小损失和全量微调几乎一样
但是最佳学习率是全量微调的10倍
比如全量微调的最佳学习率是0.001
LoRA就需要0.01
比德曼（Biderman）等人2024年的研究也发现了类似的10倍比例
这说明这个规律不是偶然的
而是LoRA的固有属性
而且不同秩的LoRA
最佳学习率很接近
比如秩4和秩512的最佳学习率差异不到2倍
这为我们设置LoRA学习率提供了很大便利
不用频繁调整
按全量微调的10倍开始试就行
再看“批量大小效应”的实验
他们用了OpenThoughts3里一个1万样本的子集
测试不同批量大小的影响
结果很明显，当批量大小比较小
比如32的时候
LoRA和全量微调的性能差距很小
而且随着训练推进，差距还会缩小；
但当批量大小变大
比如256、512的时候
LoRA的损失会明显比全量微调高
而且这个差距会一直存在
不会随训练步骤消失
更关键的是
这个差距和LoRA的秩无关
就算用秩512的LoRA
大批次下还是比全量微调差
他们分析
这是因为LoRA的“矩阵乘积（BA）”参数化
和原权重矩阵（W）的优化动态不同
批量一大
原矩阵能更好地利用批次信息更新
而BA矩阵的更新效率会下降
导致性能差距
不过好在，不管是LoRA还是全量微调
都是“小批量下性能更好”，
所以实际应用里
只要别用太大的批量
这个问题就能够缓解
接下来
我们来重点说一说跟前面第4点发现
“LoRA该应用到哪些层”有关的实验过程
这是很多人调参时容易踩坑的地方
之前很多人习惯只把LoRA用到注意力层
觉得注意力层负责“理解上下文”，
所以是关键，但是实验结果刚好相反
约翰·舒尔曼团队做了三组对比实验
第一组只在注意力层用LoRA
第二组只在MLP层用LoRA
第三组在所有层
也就是注意力+MLP+MoE都用了LoRA
结果发现
仅注意力层的LoRA性能最差
就算用更高的秩
参数数量和仅MLP层的秩128差不多
性能还是差一截
比如在Llama-3.1-8B模型上，仅MLP层
秩128，0.24B参数的损失
比仅注意力层
秩256，0.25B参数低了0.15左右
要知道，在语言模型里
损失降低0.1已经是很明显的提升了
而“所有层用LoRA”和“仅MLP层用LoRA”的性能几乎一样
这说明MLP层才是LoRA发挥作用的核心
注意力层加不加LoRA
对最终性能影响不大
他们还在MoE模型上做了实验
对Qwen3-30B-A3B-Base的每个专家都单独训练LoRA
秩设为“总秩除以活跃专家数量”，
这样能保证MoE层的LoRA参数比例和其他层一致
结果和稠密模型一样
仅MLP层的LoRA优于仅注意力层
所有层和仅MLP层相当
为什么会这样？
这就要用到马拉迪（Malladi）等人在2022年提出的“经验神经正切核（eNTK）”理论了
eNTK是用来描述模型微调初期学习动态的工具
它的核心是“梯度的点积”。
比如对每个Token的预测
梯度g_i是损失对参数的偏导
eNTK的核函数K(i
j)就是g_i和g_j的点积
这个核函数决定了模型怎么“学习”数据里的模式
而参数越多的层
对核函数的影响越大
马拉迪等人的研究指出
当LoRA应用到所有层的时候
它的eNTK和全量微调的eNTK几乎一样
所以学习动态也相似；
但是如果只应用到注意力层
而忽略了参数更多的MLP层
eNTK就会和全量微调有明显差异
学习速度自然慢下来
这也解释了为什么仅注意力层的LoRA效果差
聊完实验
我们最后再来总结一下这篇研究的主要结论和背后的深层意义
它不止告诉了我们LoRA怎么用
还帮我们深化了对大模型微调的理解
首先是“LoRA匹配全量微调的两个核心条件”，
一是要应用到所有层
尤其是MLP/MoE层
这样才能保证eNTK和全量微调的一致；
二是LoRA的容量要能覆盖数据集的信息需求
也就是“非容量约束”。
只要满足这两个条件
LoRA就能和全量微调性能相当
这为我们选择微调方法提供了明确的判断标准
如果你的数据集是中小规模
模型有MLP层
就用LoRA；
如果数据集特别大
比如接近预训练规模
再考虑全量微调
其次是“监督学习和RL的容量需求的差异”，
监督学习需要的信息多
每个token 1比特
所以需要更高秩的LoRA；
强化学习需要的信息少，每轮次1比特
低秩LoRA就够
这个差异能帮我们节省资源
也就是做强化学习任务时
不用追求高秩
秩1、8就够，参数更少，训练更快
然后是“LoRA的计算效率优势”，
他们算了一笔账
LoRA每轮训练的FLOPs
是全量微调的2/3左右
具体怎么算的大家可以去看一下这一段
这里我们就不一一推导了
简单来说，对于一个N×N的权重矩阵W
LoRA的总FLOPs是2N²
全量微调是3N²，所以比例就是2/3
这意味着，同样的硬件
LoRA能比全量微调多训练50%的样本
计算效率更高
当然，研究也提出了一些“开放问题”，
这些也是未来值得关注的方向
比如怎么更精准地预测LoRA的性能
不用每次都做实验；
为什么LoRA的最佳学习率是全量微调的10倍
目前还没有完整的理论解释；
LoRA的变体在这种评估方法下表现如何；
还有MoE层的LoRA
怎么和张量并行、专家并行这些技术结合
适配更大的模型，等等等等
最后，约翰·舒尔曼团队在结语里提到
他们研究LoRA的目标
不只是为了省资源
更是为了“让大模型的微调能力更易获取”，
毕竟不是每个团队都有能力做全量微调
但是LoRA能让普通团队也能用大模型来解决具体的问题
而且通过研究LoRA
他们也深化了对“模型容量”、“数据集复杂度”、“样本效率”这些基础问题的理解
这其实也是AI研究的魅力
一个实用的技术
背后往往藏着对核心原理的新认知
好了，今天的内容就到这里
如果你在做LoRA相关的工作
希望今天的拆解能帮你少走弯路；
如果你只是感兴趣
也希望能够帮助你增加对LoRA的了解
感谢大家观看本期视频
我们下期再见
