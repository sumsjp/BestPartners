大家好，这里是最佳拍档，我是大飞
前一段时间，Sora刚出来的时候
我做了一期节目，当时在节目最后
我问了一个问题
就是为什么又是OpenAI
是什么样的信仰在支持他们做这件事
后来我发现
这并不是我一个人的问题
在社交媒体上也引发了很热烈的讨论
其中，在一篇知乎文章中
加州大学伯克利分校计算机科学博士、作者 @SIY
Z 分析了 OpenAI 成功的一些方法论
他认为 OpenAI 的方法论就是通往 AGI 的方法论
并且该方法论构建在几个重要的「公理」之上
包括了 The bitter lesson、Scaling Law 和 Emerging properties
其中 The bitter lesson 源自机器学习先驱 Rich Sutton 在 2019 年的一篇经典文章《苦涩的教训》，
通过探讨人工智能近几十年所走过的弯路
他抛出的核心观点是
人工智能如果想要长期获得提升
利用强大的算力才是王道
这里的算力隐含了大量的训练数据和大模型
因此，作者 @SIY
Z 认为，某种意义上
强大算力加持的通用 AI 算法才是 AGI 路径的王道和 AI 技术真正进步的方向
有了大模型、大算力和大数据
The bitter lesson 构成了 AGI 的必要条件
再加上 Scaling Law 这一充分条件
通过算法使大模型、大算力和大数据获得更好的结果
无独有偶
前一段被疯传的 OpenAI 研究人员 Jason Wei 的每日工作时间线中
也提到了 Rich Sutton 的 The bitter lesson
他会在每天早上9点45到公司后
背诵OpenAI章程，向优化之神祈祷
学习《The Bitter Lesson》。
虽然有一些玩笑的成分在
但是也由此可见
很多业内人士都将 The bitter lesson 视为圭臬
与此同时
在另一个关于「大语言模型是否可以作为自身结果的验证者」的讨论中
有人认为大语言模型验证自身结果时
根本不够准确，并且会导致性能更差
对于这个观点
又有推特网友在 Rich Sutton 二十多年前的一篇博客中有了重要的发现
博客中是这样说的
考虑到任何 AI 系统以及它所拥有的知识
它可能是一个专家系统或者像 CYC 这样的大型数据库
或者它可能是一个熟悉建筑物布局的机器人
或者了解在各种处境下如何做出反应
在所有这些情况下
我们可以问 AI 系统是否可以验证自己的知识
或者是否需要人们干预来验证
或者是否需要人们干预来检测误差和不可预见的交互
并进行纠正
在后者这种情况下
我们永远无法建立真正庞大的知识系统
它们总是脆弱且不可靠的
并且规模仅限于人们可以监控和了解的范畴
没想到，Rich Sutton 进行了回帖
表示这篇写了一半的博客就是 The bitter lesson 的原型
其实，在 OpenAI 刚发布 Sora 不久
就有很多人意识到了 The bitter lesson 发挥了重要作用
还有人将 The bitter lesson 与 Transformer 论文 Attention is All You Need 并列看待
今天我们就来我们回顾一下 Rich Sutton 的这篇《苦涩的教训》。
过去70年以来对人工智能研究
给我们带来的最深刻的教训是
那些能够发挥计算力的通用方法
终将大获成功
这背后的根本原因就是摩尔定律
也就是计算单位成本会持续呈指数型下降这一普遍现象规律
绝大多数的AI研究都会基于一种假设
那就是智能体可使用的计算资源是不变的
在这种情况下
发挥人类的知识就成了提升性能的主要手段
然而，随着时间的推移
一旦放到更长的时间尺度上来看
我们将不可避免地需要大量的计算资源
虽然研究人员希望在短时间内
依靠人类在某些领域的知识来取得突破
但是从长远来看
真正重要的还是计算能力的发挥
这两种方法不是对立的
但是在实践中它们往往是此消彼长的
在一个上的投入就意味着要牺牲另一个
因为人们对某种方法的投入
往往会让他在心理上选择不断地坚持
基于人类知识的方法通常会使程序变得复杂化
所以不能够很好地发挥计算能力
有许多AI的研究人员
在项目后期才痛苦地吸取到这一教训
我们可以通过回顾几个最突出的例子
来帮助我们获得一些宝贵的启示
在计算机国际象棋领域
1997年战胜世界冠军卡斯帕罗夫的方法
依靠的是大量的深度搜索
当时，大部分研究人员感到沮丧
因为他们致力于发挥人类对象棋独特结构的理解
当一种更简单的、基于搜索的方法
在结合了专门的硬件与软件
并且被证实极为有效之后
这些基于人类知识的象棋研究者们
并没有好好地接受失败
他们声称这次“暴力”搜索可能赢了
但是并不是一种普遍有效的策略
并且这并不是人类下棋的方式
这些研究人员期待能够找到一种基于人类直觉的方法
来取得胜利
结果却令他们感到失望
对计算机围棋的研究也出现了类似的进展
只不过比国际象棋晚了20年
最初
研究者努力想避免通过搜索来解决问题
而是试图利用人类对围棋特殊性的认知
但是所有这些努力
一旦在大规模搜索得到有效应用之后
就显得无关紧要，甚至是错误的
对于很多其他游戏和国际象棋来说
同样重要的是利用自我对弈的学习策略来学习某种价值函数
尽管在1997年第一次击败世界冠军的程序中
学习并没有起到什么作用
无论是自我对弈的学习
还是一般意义上的学习都类似于搜索
因为它们能够利用海量的计算资源
在人工智能研究中
搜索和学习是利用海量计算资源的两种最关键的技术类别
无论在计算机围棋还是计算机国际象棋
研究的初衷都是为了减少搜索的需求
希望通过人类的认知来达到目的
然而，只有在很多年之后
通过接受并使用搜索和学习
才获得了巨大的成功
在语音识别领域
20世纪70年代有一场由DARPA赞助的早期竞赛
参与者呈现了多种特殊方法
有些方法利用了对词汇、音素、人类声道等方面的人类知识
而另外则是采用了更多基于统计的方法
基础是隐马尔可夫模型 HMMs
可以执行更大量的计算
结果是统计方法再次胜出
这引发了自然语言处理领域的重大转变
数十年来统计和计算逐渐占据了主导地位
深度学习在语音识别中的崛起
正是沿这一方向迈出的最新步伐
深度学习方法更少依赖人类的知识
而是利用了大量的计算资源
并且通过在海量训练数据集上进行学习
大幅提升了语音识别系统的性能
就像在游戏领域一样
研究人员总是试图打造出与他们心中想象的思维方式相匹配的系统
所以他们会尝试将这种认知融入到系统中
但是最终证明这是适得其反的
而且让研究者浪费了巨大的时间
因为摩尔定律（Moore's law ）使得大量计算成为可能
并且找到了有效利用这些计算资源的方法
在计算机视觉领域
也出现了类似的模式
早期方法将视觉处理想象为寻找边缘、广义圆柱体或者基于SIFT的特征
但是现在这些做法都被淘汰了
现代深度学习神经网络采用卷积以及某种不变性的概念
取得了更好的成绩
这是一个重要的教训
我们对人工智能领域还未完全领域
还在重复着同样的错误
为了吸取这些错误的教训
并有效地克服它们
我们需要学习到的是，从长远来看
试图构建一个符合我们人类思维方式的系统是行不通的
这个苦涩的教训来自于这样一些历史观察
1、AI研究者经常会试图将知识植入他们的智能体
2、这在短期内似乎总是有益的
并且能给研究者带来满意感
3、但是从长期看
这种方法迟早会遇到发展瓶颈
甚至阻碍进一步的进展
4、真正的突破性进展
最终可能都是通过相反的方法实现的
那就是基于以大规模计算为基础的搜索和学习
最后的成功往往会带有一点苦涩
并且往往难以被完全接受
因为它推翻了那些人们偏爱的、以人为中心的方法
从这个苦涩的教训中我们应该明白
通用的方法具有强大的力量
而且会随着算力的增加继续扩展
即使可用的计算资源变得非常非常的大
只有搜索和学习这两种方法能够做到如此无限的扩展
从这苦涩的教训中我们应该吸取的第二个要点是
意识的真实内容是极其复杂和深邃的
我们应当停止寻找想要简化理解意识思考的方法
比如思考空间感、物体、多智能体交互或者对称性等等
这些都是外在世界复杂性的一部分
而这种复杂性是无穷数量的
因此我们不应该将它们固化到系统中
相反
我们应该只构建那些能够发现并捕捉这些任意复杂性的元方法
这些方法的核心在于它们能找到良好的近似值
但是发现它们的过程应该由我们的方法来完成
而不是我们个人
我们希望创建的AI智能体能像我们一样发现新事物
而不是仅仅包含我们所发现的东西
将我们的发现植入到系统中
只会让理解发现的过程本身变得更加困难
好了
以上就是苦涩的教训这篇文章的全文内容
建议所有想要从事AI行业的朋友们可以经常回顾一下
时刻清晰的认识到
人工智能并不是要按照我们人类思考方式去构建一个系统
而是要通过发挥海量的计算能力
让它自己学会发现新的知识
这也就是Scaling Law缩放法则的由来吧
感谢收看本期节目，我们下期再见
