大家好，这里是最佳拍档
2025年12月
原OpenAI研究员姚顺雨正式官宣加盟腾讯
这位年仅27岁的青年直接出任腾讯首席AI科学家
成为了国内大厂中最年轻的首席AI科学家之一
姚顺雨的加盟
带来的不仅是顶尖的技术研究能力
更带来了一套彻底颠覆传统AI研发思路的核心理念
这位在OpenAI长期研究推理到行动范式的科学家
早在2025年就以一句
AI下半场更重要的是定义问题与评估
在行业内出圈，我们也做过一期节目
而在加入腾讯后的内部会议中
他更是直言不讳地提出
希望团队以后不要打榜
也不要盯着榜单做事
在他看来
真正决定一个大模型能否走出实验室的demo阶段
走向真实世界的商业应用
从来都不是再刷几个榜单的高分
也不是再提升几个百分点的评测指标
而是到底有没有把模型系统放进真实世界的约束里
并用真实世界的方式去评估它的能力
这篇名为《上下文学习基准测试：
一个面向上下文学习的评测基准》的论文
由腾讯混元团队与复旦大学可信具身智能研究院联合发布
既是姚顺雨理念在腾讯的首个落地成果
更是直接直击了当前大模型与Agent领域最核心、也被所有人忽视的行业痛点
那就是上下文处理能力的缺失
今天，我们就来深度拆解这篇论文
看看它到底揭露了当前AI发展的哪些核心问题
又为行业指出了怎样的破局方向
要理解这篇论文的价值
我们首先要搞清楚
当前的大模型在上下文处理上
到底遇到了什么样的问题
当前大模型在处理长上下文时的核心困境是
当模型在使用工具、执行复杂推理任务时
token的消耗如同流水一般
但往往上下文窗口还没有被填满
模型就已经开始遗忘其中的重要信息
要么直接停止推理
要么给出错误的结果
也正因为如此
当前的Agent系统不得不被拆解到最细的步骤
遵循严格的作业流程才能勉强运行
一旦跳过某个步骤
或者给出稍复杂的上下文
整个系统就会陷入混乱
这个现象背后
反映的是一个残酷的事实
当前的大模型
在长上下文的处理能力上
远不如现有的各类评测基准所显示的那般优异
在现有的评测体系中
模型似乎能轻松应对各类长上下文任务
但是一进入真实的Agent应用场景
就会出现各种混乱、遗忘、难以遵守规则的问题
而这
正是制约人工智能真正走向实用化的最大卡点
而姚顺雨这次打造的CL-BENCH
本质上就是为了精准测试这个卡点到底有多大
为行业提供一个能真正反映真实世界需求的上下文能力评测标准
为什么现有的评测基准
无法测出大模型上下文处理的真实水平呢？
这就要从现有上下文基准的设计逻辑说起
目前行业内主流的上下文评测基准主要分为两大派系
第一派是以大海捞针以及它的升级版本Ruler为代表的
这类基准的核心考察点
是模型在海量上下文中准确检索一个或多个特定信息点的能力
简单来说
就是考验模型找东西的能力
另一派则是以LongBench、ZeroScrolls为代表的
这类基准主要考察模型的上下文理解能力
比如对长文本进行摘要、根据长上下文的信息进行简单的推理判断
在这些传统的评测基准中
当下的前沿大模型表现都堪称优异
甚至在标准的NIAH测试中
前沿模型的检索准确率
已经普遍达到了近乎饱和的状态
而在LongBench v2这类侧重理解的评测中
模型的得分甚至能以两倍的水平远超人类
但是问题就在于
真实世界的Agent应用场景
和这些评测基准的场景完全不同
如果说传统评测是大海捞针
那么真实场景的上下文任务
就是拿着针去缝一件精密的衣服
前者只需要找到目标
后者则需要理解目标的价值、掌握使用的方法
并且精准地应用到具体的任务中
而CL-BENCH所要测试的
正是模型的这种真正学会并应用的能力
研究团队将它定义为上下文学习
context learning
这里我们必须要明确一点
它和我们更熟悉的In Context Learning有着本质的区别
ICL的核心
是模型通过提示词里的少量示例或指令
学会怎么解决问题
它学的是一种映射关系、一种题型套路、一种输出格式
本质上
ICL只是唤起了模型在预训练阶段已经见过的模式
并没有让模型真正学到新的知识
而研究团队定义的上下文学习
则完全相反
它要求模型在一个具体的任务中
必须从给定的复杂上下文中
吸收此前预训练中完全没有学过的新知识
这些知识可能是特定的领域知识、一套全新的规则系统、一个复杂的操作流程
甚至是从海量数据中归纳出的潜在规律
然后模型需要用这些刚学到的新知识
去精准地完成对应的任务
简单来说，ICL学的是怎么干
而CL-BENCH学的是干什么
为什么这么干，以及怎么干好
这种能力
才是真正从上下文中学习的能力
也是我们人类在日常工作和生活中
最常应用的一种核心能力
正如论文中所表述的
上下文学习代表了一种基础能力
它架起了静态参数化知识与真实世界应用动态需求之间的桥梁
为了精准测试模型的这种核心能力
研究团队在构建CL-BENCH时
做了一系列极其严谨且贴合真实场景的设计
首先是上下文的构建
团队联合领域专家
打造了500个高度复杂的上下文内容
这些上下文的平均token数量达到了10.4k
最长的甚至能达到65k
这个长度和复杂度
完全匹配了真实世界中专业文档、技术手册、规则体系的实际情况
而这些上下文所对应的任务
更是覆盖了四大主要类型
细分为19个子类
完整模拟了人类从新手入职到专家决策的所有上下文学习场景
这四大任务类型
每一种都对应着真实世界中不同的工作场景
也对模型的能力提出了不同的要求
第一种是领域知识推理
对应的是资深顾问的工作场景
比如给模型一部完全虚构的《火星商业法》，
模型不能只是简单地检索其中的某一个条款
而是要像专业的律师一样
在这套全新的逻辑体系里
建立完整的因果关系
进行利益的权衡与专业的决策
第二种是规则系统应用
对应的是硬核玩家或专业技术人员的场景
比如给模型一套反直觉的新数学定义
或者一套全新的游戏规则
模型必须洗脑自己
抑制住预训练阶段形成的肌肉记忆
在这个封闭的逻辑闭环里严丝合缝地进行推导和应用
第三种是流程任务执行
对应的是一线操作员的工作场景
比如给模型一份复杂的工业SOP手册
考验的是模型长链条的执行纪律
因为在真实的工业场景中
错一步就可能满盘皆输
模型必须严格按照流程执行
不能有任何遗漏或偏差
第四种是经验发现
这是四大类型中最高阶的挑战
对应的是科学家的研究场景
比如给模型一堆杂乱无章的实验数据
让模型自己归纳出背后隐藏的物理定律或客观规律
这是从应用知识到发现知识的跨越
也是对模型上下文学习能力的极致考验
为了保证这次评测的真实性
避免模型依靠预训练阶段的知识作弊
研究团队在构建上下文时
做了一个关键的设计
这些上下文中包含的知识
要么是完全虚构的
要么是对现有知识进行了修改
要么是极其小众的长尾内容
这些知识在模型的预训练数据中
几乎不可能出现
为了验证这一点
研究团队还做了一个对照实验
让当前最强的GPT-5.1 High 在不提供任何上下文的情况下
尝试解答1000个随机抽样的CL-BENCH任务
结果显示
模型的任务解决率只有0.9%，
这个数据充分说明
没有有效的上下文学习能力
这些任务对于模型来讲几乎是无解的
也从根本上保证了CL-BENCH评测结果的有效性
如果说上下文和任务的构建是CL-BENCH的骨架
那么它的评分检验模式
就是让这个骨架有了灵魂
也是CL-BENCH区别于其他所有评测基准的核心特色
为了精准判断模型是否真正掌握了上下文的知识
研究团队为每个上下文任务
都配备了平均16.6个验证规则
这些规则覆盖了事实正确性、计算准确性、程序完整性、格式遵循等多个维度
几乎考虑到了真实世界任务中所有的细节要求
而评分模式
则采用了极其严格的全有或全无的二元制评分体系
简单来说，每一道题
模型必须满足全部的验证规则
才算这道题做对
哪怕模型算出了完全正确的数值
只要漏了一个单位、漏了一个关键假设、漏了一个要求的输出结构
最终的评分就是0分
为了保证评分的客观性和一致性
研究团队还采用了大模型作为裁判的方法
也就是用大模型作为评分教师
并为评分模型制定了极其详细的系统提示和评分步骤
首先要逐项分析标准答案
列出所有明确和隐含的要求
然后将每一项要求与模型的答案逐一核对
最后还要进行自我反思
包括完整性检查、严格性检查、一致性检查和客观性检查
确保没有任何要求被遗漏
也没有任何主观放宽评分标准的情况
这种严格的检验模式
完美复刻了真实世界中对工作成果的要求
最后，研究团队在CL-BENCH上
评测了当前行业内的10款前沿大模型
所有模型均在推理模式下进行评估
结果取三次运行的平均值和标准差
最终的测试结果显示
这10款模型的平均解题率只有17.2%，
哪怕是表现最好的GPT-5.1 High
解题率也只有23.7%，这个数据意味着
即使是当前最强的大模型
在面对真实世界的上下文学习任务时
也只有不到四分之一的概率能完成任务
而在四大任务类型中
最难的经验发现与仿真任务类别
整体平均解题率更是只有11.8%，
尤其是其中的观测数据与模拟环境子类
模型的表现更是大幅下滑
充分说明
模型在从海量数据中归纳知识
发现规律的能力上
存在着巨大的短板
但比整体解题率更值得关注的
是模型失败的原因分析
研究团队通过对所有模型的错误答案进行梳理
将失败原因总结为三大类
第一类错误是忽略上下文
简单来说就是模型在解题时
该用到上下文知识的时候没有用
依旧依靠预训练的常识和模板来解答
GPT-5.1 High 的这类错误比例达到了55.3%。
第二类错误是误用上下文
这是比例最高的一类错误
模型虽然用到了上下文的知识
但用错了适用范围、遗漏了例外情况、拼错了约束关系
GPT-5.1 High 的这类错误比例高达61.5%。
第三类错误是不遵守格式或约束
也就是模型没有按照任务要求的输出结构、流程顺序来解答
这类错误的比例相对较低
但是也依然存在
除了这三类核心错误
实验还发现了一个普遍的规律
无论推理设置如何，所有模型的表现
都会随着上下文长度的增加而持续下降
上下文越长，模型的解题率越低
其中Claude Opus 4.5 Thinking的下降最为陡峭
在短上下文区间
其解题率还能保持在较高水平
但是到了长上下文区间
解题率直接下降超过20个百分点
几乎腰斩
这种表现和真实世界的需求形成了鲜明的对比
人类在处理工作时
虽然长文本也会增加理解难度
但并不会出现如此陡峭的能力下滑
而模型的这种表现
也直接导致了当前的Agent系统在面对长上下文任务时
根本无法完整交付任务
看到这里
相信很多人都会产生一个巨大的困惑
近些年来
上下文能力一直是各大模型厂商竞争的核心焦点
早在二零二三年
Kimi就凭借两百K的大上下文窗口在国内市场一炮而红
而到了现在
百万token级别的上下文窗口
已经成为了各大厂商旗舰模型的标配
各种大海捞针的测试也显示
前沿模型在超长文本中检索特定信息的准确率
可以达到98%以上
甚至GPT-5.2和Gemini 3还把指令遵循能力的提升作为核心卖点
在IF·Eval等评测基准上表现优异
那为什么这些模型
在CL-BENCH的上下文学习评测上
会集体栽跟头呢？
这个问题的答案
藏在近些年来大模型长上下文技术路线的发展逻辑里
简单来说，过去几年里
所有厂商在长上下文上的技术突破
本质上都是把模型训练成了更能读、更能跑的阅读机器
而不是能学、能用的思考机器
这些技术突破解决的都是能不能读长文本的问题
而不是能不能用长文本的知识解决问题的问题
我们可以把这些技术进展分为三波
每一波都解决了一个具体的问题
但都没有触及到上下文学习的核心
第一波进展
解决的是长上下文的计算效率问题
因为Transformer架构的原生自注意力机制
计算复杂度非常高
这就从根本上限制了模型的上下文长度
想要让模型处理更长的文本
首先要解决的就是算得起的问题
于是各类技术应运而生
比如FlashAttention
它并没有改变模型在数学层面的注意力计算逻辑
而是优化了它在GPU上的计算方式
把中间张量和显存的读写成本压下去
让同样的硬件能处理更长的文本
运行也更稳定
再比如MQA或GQA
通过对注意力头进行分组
把推理时成本最高的KV存储进行压缩
从而换取更大的吞吐和更长的上下文长度空间
后续的MLA、Kimi Liner等技术
本质上都是在做类似的优化
核心目标就是让模型在固定的算力和缓存下
能吃下更多的上下文
第二波进展
解决的是长上下文的准确性问题
因为自注意力机制本身缺乏精准的位置标记
当模型处理的文本长度超过训练长度后
就会出现位置感错乱、长距离依赖发散的问题
简单来说就是模型读了后面
忘了前面
为了解决这个问题
行业内主要做了两类工作
一是位置编码外推
比如旋转位置编码
通过数学方法让模型在更长的位置范围内
还能记住上下文的位置关系
二是用长上下文继续预训练
把更长的文档结构喂给模型
让它更擅长读长材料
学会跨段引用、连贯理解
这一波的技术进展
让模型读长文的能力大幅提升
读起来更连贯、更能理解文本的整体逻辑
但这仍然只是偏阅读的能力
读得懂，不等于做得到
第三波进展
是把长上下文问题改写成了检索问题
既然模型处理长上下文的能力有限
那就干脆不让模型处理完整的长文本
而是把长文本进行切块、向量化
通过向量检索、重排序等技术
把最相关的几段内容筛选出来
再塞回模型进行处理
同时还加上了引用约束
让模型的答案能追溯到具体的文本片段
看起来更可靠
这一波的技术进展
直接让大海捞针这类测试的准确率大幅提升
因为模型的训练目标
已经变成了把找针变成自己的强项
但问题在于
检索只是上下文学习的第一步
找到相关的知识
并不等于学会并应用这些知识
这三波技术进展
确实让模型的长上下文能力有了质的提升
解决了带宽与定位的问题
在CL-BENCH的评测中，我们也能看到
模型的上下文忽视率与模型的整体能力呈负相关
更强的模型确实能更好地关注上下文中的相关信息
这说明这些技术进展是有价值的
但问题的核心在于
我们要求模型按上下文的知识行动
需要的是模型能遵守硬约束、能按流程执行、能被验证
并且全程保持正确
模型需要把上下文的信息转化为自身的行为约束
并在后续的每一步推理中都严格遵守
而这正是当前技术进展所缺失的
在CL-BENCH的评测中
所有模型的上下文误用率都保持在高位
哪怕是表现最好的GPT-5.1 High
误用率也超过了60%，
这说明模型虽然能找到上下文的知识
但根本无法正确理解和应用这些知识
而这种能力
是无法靠更会读、更智能自动长出来的
甚至在CL-BENCH的实验中
我们还能发现一个反直觉的现象
那就是提升模型的推理强度
对上下文学习能力的提升效果微乎其微
以GPT-5.1 High 为例
高推理强度相对低推理强度
解题率平均只涨了2.5%，
这说明推理能力的提升
并不是解决上下文学习问题的灵丹妙药
因为模型的很多失败
并不是想不出来
而是约束没有执行到最后一公里
哪怕推理能力再强
没有准确的知识应用
最终的结果依然是错误的
而另一个更反直觉的现象是
GPT-5.2 High 的整体表现
反而比GPT-5.1 High 低了5.6个百分点
核心原因就是GPT-5.2在长上下文推理时
难以维持连贯的因果链
频繁违反上下文中明确说明的约束条件
这也从侧面说明，当前的大模型研发
在追求参数、算力、上下文长度的同时
反而忽略了真实应用中最核心的约束执行能力
面对这样的上下文学习困境
当前学术界的努力方向
大多选择了绕道而行
也就是不去直接解决模型的上下文学习能力问题
而是通过外部手段来缓解这个问题
比如谷歌的EvoMemory、康奈尔大学的Clawedbot等工作
都在探索如何通过反思、压缩、外部记忆系统
来缓解上下文过长带来的问题
它们的基本思路高度相似
既然模型处理不了这么长的上下文
那就想办法把它压缩成短文本
既然模型会遗忘
那就为模型建立一个外部的记忆系统
把重要的知识存起来
需要的时候再调取
既然模型一次性学习的效果差
那就让模型反复迭代、多次反思
逐步完善答案
这些绕道的方法
在一些特定的场景下确实有效果
能在一定程度上提升模型的任务完成率
但本质上都是在回避核心问题
而且还带来了新的问题
比如
这些方法引入了额外的系统复杂性
增加了推理的延迟
也提高了应用的成本
在工业级的生产环境中
每增加一层抽象
每增加一个外部系统
都意味着新的失败点和更高的维护负担
因此很难在大规模的商业应用中落地
而之所以学术界会选择绕道
核心原因是大家普遍认为
这是Transformer架构层面的根本限制
自注意力机制可能天生不适合深度学习上下文的知识
所以解决方案自然就是绕道而行
而研究团队在论文的讨论章节中
给出了四条工程上的发展方向
核心在于把上下文学习这门模型缺了的课
在训练中直接给补上
让模型从根源上拥有上下文学习的能力
这也是唯一能真正解决问题的路径
第一条方向
是构建强上下文依赖的训练数据
想要让模型学会从上下文学习
首先要让模型在训练中就不得不依靠上下文完成任务
而不是靠常识和模板混过去
因此
训练数据需要刻意构造那些预训练里几乎不可能学到的新知识
让模型在训练阶段就养成看上下文、学新知识、解新问题的习惯
而不是依赖预训练的知识进行套模板式的解答
第二条方向
是用课程学习的方法对训练难度进行分级
CL-BENCH的实验结果显示
把模型直接丢进六十k token的复杂材料里
很多时候模型不是不会
而是直接失稳
根本无法处理如此复杂的信息
因此，训练需要遵循由浅入深的原则
先训练模型的基础上下文理解与简单约束执行能力
再逐步提升难度
让模型接触多规则、多例外、多步骤的复杂任务
通过循序渐进的训练
让模型逐步掌握复杂的上下文学习能力
第三条方向
是让验证规则从评测工具
变成模型的训练信号
在CL-BENCH的构建中
人工编写的规则提供了极其细粒度的反馈
能精准指出模型到底漏了哪条约束、错了哪个步骤
但是人工编写这些规则的成本极高
论文中提到
每个上下文的规则编写都花费了20个小时
根本无法大规模应用
因此
研究团队提出了合成规则的方向
通过大模型迭代生成的方式
自动构建验证规则
把到底漏了哪条约束这样的细粒度反馈
变成模型能收到的强化学习信号
让模型在训练中能精准知道自己的错误
并不断修正
第四条方向
是进行面向上下文利用的架构创新
研究团队在论文中表现得非常克制
没有给出具体的架构设计
但指向的方向非常清晰
比如近期DeepSeek提出的显式记忆结构、让模型对上下文进行多轮处理、设计多通路处理不同类型的上下文信息等等
这里的关键词
不再是更长的上下文窗口
而是更能把上下文变成内部可调用的知识
让模型能把从上下文中学到的知识
真正转化为自身的推理约束
而不是简单的信息存储
这四条方向的共同点非常清晰
那就是通过训练数据、训练方法、训练信号、模型架构的全方位优化
直接训练模型获得上下文学习能力
在姚顺雨和团队看来
绕道终归是绕道
虽然能解一时之需
但无法实现AI的真正突破
想要让大模型真正走向真实世界的应用
必须正面攻坚这个核心问题
有时候，人工智能行业的进步
并不是跑得更快
而是先知道该往哪里跑
而姚顺雨和研究团队，用这篇论文
为行业指出了AI下半场的核心赛道
从能读到能学，从能检索到能应用
让大模型真正拥有上下文学习的能力
才是AI走向实用化的唯一路径
而这
也正是AI下半场最核心的竞争所在
感谢大家收看本期视频
我们下期再见
