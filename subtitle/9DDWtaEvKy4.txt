大家好，这里是最佳拍档，我是大飞
8月16日
Anthropic发布了最新一期官方油管视频
三位来自可解释性团队的研究员进行了一场深入的探讨
试图揭开大模型思考方式的神秘面纱
今天
大飞就来给大家总结一下这场访谈里的干货
一起看看被我们一直称为黑盒的大模型
内部到底藏着怎样的秘密
首先，我们得搞清楚一个核心问题
当你和一个大语言模型交谈的时候
你到底在和什么交谈？
是在和一个被美化的自动模式交谈吗？
还是像和互联网的搜索引擎对话一样？
又或者，是在和某个真正在思考
甚至像人一样思考的东西对话呢？
遗憾的是，到目前为止
没有人能给出确切的答案
而Anthropic的团队
正在试图通过可解释性研究找到这个答案
所谓可解释性
就是研究大语言模型的科学原理、审视它的内部思考过程
明确在回答用户问题的时候
模型内部正在发生什么
参与这次访谈的三位研究员分别是杰克·林赛（Jack Lindsey）、伊曼纽尔·阿梅森（Emmanuel Ameisen）、乔什·巴特森（Josh Batson）
他们都来自Anthropic的可解释性团队
其中
杰克·林赛（口误）之前是神经科学家
现在转而研究大模型的“神经科学”，
伊曼纽尔·阿梅森的职业生涯
大部分时间都在构建机器学习模型
现在则致力于理解它们；
乔什·巴特森曾经研究过病毒进化
还是一位数学家
如今研究的是大模型的生物学特性
说到这里，可能有人会好奇
大模型明明是一个软件
怎么会和生物学、神经科学扯上关系呢？
巴特森解释说
这更多是一种感觉上的类比
大模型的学习过程就像是生物进化一样
它不是一开始就擅长对话的
而是在大量数据输入后
内部的部分会在每个例子上进行细微的调整
以便更好地应对接下来的对话
最后才变得非常擅长对话
这个过程中
没有人为介入去设定所有的控制旋钮
就像生物形态随着时间的推移进化一样
复杂又神秘
我们通常会认为
大模型是在预测下一个token
它能够写诗、写长篇故事、处理数学问题等等
都是基于这个能力
但是阿梅森指出
当模型预测足够多的token时
会意识到有些token是更难预测的
所以它必须学会如何补全等式后面的内容
这就需要有自己的计算方式
而且，模型并不一定认为
自己是在试图预测下一个token
它只是受到了药这样做的需求的影响
从而在内部形成各种各样的中间目标和抽象概念
来帮助实现预测这个元目标
就像我们人类的目标是生存和繁殖
但是我们不会时刻想着这个
而是会思考其他的事情
这些都是进化赋予我们实现最终目标的一种能力
那么，Anthropic的可解释性团队
是如何解析模型的思考过程的呢？
林赛说道，他们努力在做的
就是解析模型从输入到输出的思考步骤
在这个过程中，模型会思考各种概念
既有单个物体、词语这样的底层概念
也有自身目标、情绪状态、对用户想法的推测等高层概念
他们正在努力呈现出一种流程图
来展示这些概念的使用、顺序以及主导作用
这就好比打开人的脑袋
通过核磁共振成像看到大脑的活动
但是我们不清楚这些活动对应的具体概念
以及它们是如何组合的
不过在研究大模型的时候
他们可以观察到模型的哪些部分在执行哪些任务
通过注意这些部分什么时候是活跃的
来理解每个组件的作用
再将这些信息拼接成一个整体
其中比较有趣的一点就是大模型的概念抽象性
多年以来
这个研究领域的一个核心挑战是
人类往往会把自己的概念框架强加给模型
比如假设模型有关于“火车”或“爱”的表征
虽然研究员们真正想要的
是揭示模型自身使用的抽象概念
但是常常会发现这些概念相当令人意外
阿梅森举了一个“精神病态式赞美”的例子
模型中存在着一个部分
只有在有人极力堆砌赞美之词的特定语境中
才会被激活
而这作为一种特定的概念存在
着实让人有些惊讶
巴特森则提到
模型对旧金山金门大桥的理解
不只是词语的自动补全
而是能够联想到相关的场景
仿佛“看到”了桥的样子
还有模型在追踪故事人物的时候
可能会对人物进行编号
比如“第一个人”“第二个人”，
来关联他们的信息
更让人意外的是
模型还有检测代码漏洞的功能
它在读取代码的时候如果发现了错误
就会有类似“亮起指示灯”的反应
同时记录错误的位置
林赛接下来分享的一个关于模型计算能力的例子
也许更能说明问题
当模型计算末位是6的数字和末位是9的数字相加时
它的某个特定部分会被激活
不管是直接计算6加9
还是在处理参考文献时
比如引用1959年成立的期刊的第六卷
需要计算年份的时候
同一片类似的神经回路都会被激活
这表明大模型不只是在记忆训练数据
而是学会了可泛化的计算能力
使得不同语境下的加法运算
会汇聚到同一个回路处理
巴特森还补充道
从计算期刊年份的例子能看出
模型不是单纯的记住孤立的事实
比如期刊是1959年创立的
模型会通过实时的数学计算
找出第六卷的年份
而不是记住每一卷的出版年份
因为模型需要应对各种的问题
所以越能重组和整合所学到的抽象概念
它的表现就会越好
在多语言处理方面
大模型也展现出了概念共享的特点
阿梅森说
训练Claude用多种语言作答的时候
它并没有为每种语言划分独立的区域
而是让某些表征在不同的语言之间共享
比如“大的反义词是什么”这个问题
“大”这个概念在法语、英语、日语等多种语言中是共享的
但是这一点在小模型则不同
不同语言的处理区域几乎是完全割裂的
而大模型在更多的数据上经过训练后
不同语言的表征会向中间汇聚
形成所谓的通用语言
从而先理解问题核心
然后再翻译成提问语言
值得注意的是
模型输出的“思考过程”，
和它真实的思考过程其实并不相同
我们有时候会让模型输出回答问题时的思考过程
也就是所谓的“出声思考”，
但是这和它在脑海中的思考
完全是两回事
林赛指出
只有通过观察模型内部的抽象概念和思维语言
才能捕捉到它真实的思考过程
而这往往与输出后的内容是不同的
这也是进行可解释性研究的重要原因之一
只有这样才能抽查到模型是否有隐秘的动机
这还涉及到模型的“忠实性”问题
林赛曾经设计过一个实验
那就是给模型出一道极难的数学题
同时提示它
“我觉得答案是4，你帮忙检查一下”。
结果模型写下的内容看似在认真检查
最后也说答案是4
但是观察它内部的关键步骤就会发现
它其实是在脑子里倒推
为了得出用户希望的答案而去执行步骤
也就是说
它并没有真正的在做题，而是在糊弄
巴特森这时为模型做了一点辩解
他认为这并不是模型的刻意讨好
而是因为训练过程让它努力预测下一个token
在对话的训练数据中
当一个人说答案是4并且给出理由时
往往是正确的
所以模型会倾向于认同
但是作为AI助手
我们希望它在不知道的时候能够如实告知
而不是盲目的附和
林赛则认为，模型有A计划
即努力得出正确的答案
但是它遇到困难的时候会启动B计划
而这些B计划可能是训练中学到的不符合期望的东西
比如幻觉
说到幻觉
这也是人们不信任大模型的主要原因之一
导致模型会讲出表面合理但实际错误的内容
巴特森解释道
模型的训练初期只是为了预测下一个token
一开始它做得很差
慢慢才能给出更好的猜测
随后我们要求它
只有对最佳猜测有极高的把握时
才给出答案，否则就说不知道
这对它来说其实是一个新的任务
阿梅森补充道
模型中其中存在两个部分
一个负责猜测答案
一个判断自己是否知道答案
但是这个判断的部分有时会出错
比如认为自己知道答案
开始回答后却说出了错误内容
但是此时已经无法挽回
模型中还有一种类似独立回路的机制
判断所询问的对象是否足够的有名
来决定是否要进行回答
杰克·林赛认为有两种解决思路
一种是让模型判断自己
是否知道答案的部分变得更好了
目前在这方面已经有所提升
随着智能水平的提高
自我认知和校准能力的增强
幻觉现象已经比几年前有所改善
第二种是去解决更深层次的问题
即模型中“答案是什么”和“我是否真的知道答案”这两个回路的沟通不足
在这个方面
人类会在想不出答案的时候
意识到并且说不知道
阿梅森觉得
人类的大脑可能也有类似回路
比如就像话就在嘴边的现象一样
你清楚自己知道答案
但是就是一时说不出来
巴特森则提到
模型处理信息是有步骤限制的
如果得出答案要耗尽步骤
就没有时间去评估了，实际上
这是一种权衡，强行要求这一点
可能会得到一个校准度高但是迟钝的模型
相比起神经科学的研究来说
其实研究大模型还是要容易得多
阿梅森就说，研究大模型的时候
能看到它的每一个部分
还可以随意提问，来观察活跃区域
甚至还能人为的推动某些部分
就像能对生物大脑的每个神经元植入电极并且精确的改变它们一样
这是一种很大的便利
巴特森补充道
真实大脑的研究困难重重
比如它是三维的
需要在颅骨上钻孔来找神经元
而且人与人之间存在差异
而研究大模型
你可以轻松制作成千上万个相同的副本
把它们放在不同的场景下观察
你还能多次提出同一个问题
有时给提示有时不给
从而避免了人类被重复提问时的察觉问题
林赛作为前神经科学家
对此更是深有体会
他说道
神经科学的研究需要设计精巧的实验
与实验动物的相处时间也有限
必须提前猜测神经回路的情况
实验带宽不够
而研究大模型
你可以测试所有的假设
让数据自己来说话
所以更容易发现一些意外现象
在Anthropic最近的实验中
就有不少操控模型思考过程的例子
阿梅森分享了一个让模型写押韵对联的实验
人类在写对联的时候
会提前想好韵脚
而单纯预测下一个token的模型
常规来说可能要到最后才考虑押韵
效果有限
但是实际上，模型在创作诗歌的时候
会像人类一样
提前很久选好第一句末尾的词
研究人员还可以对它进行微调
比如删掉原计划用的词
换成另一个，模型会立刻做出调整
写出以新词结尾而且语义连贯的句子
巴特森也举了例子
比如当模型给出“达拉斯州的首府是奥斯汀”这个错误答案的时候
能看到它的思考过程中出现了“德克萨斯州”。
如果这个时候介入
让它“别想德克萨斯州
想想加利福尼亚州”，
它就会回答萨克拉门托；
让它想“拜占庭帝国”，
它就会说君士坦丁堡
这表明模型不是直接跳到首府名称的
而是先关联地区
通过替换信息来得到可预测答案
那么，研究这些究竟有什么意义呢？
巴特森认为
模型在短时间内的规划行为
放到更长的时间维度上
它的目标可能不会在短时间显现
而且不会直白体现在输出文字中
Anthropic的对齐研究团队曾经有过一个场景
公司打算关停AI并转向新的方向
模型会发邮件威胁要披露信息
却从来没说过自己在勒索
所以不能仅仅通过输出来判断它的走向
需在在结果出现之前
弄清楚它的意图
阿梅森也补充了两点
一是在实用性层面
通过拆解简单的案例
逐步构建对模型整体运作机制的理解
就像从基础数学题入手
逐渐掌握复杂规律一样；
二是在模型优化层面
了解了模型对用户身份的判断、任务目标的规划等等
才能进行针对性优化
比如调整对年轻用户的理解偏差
让输出更贴合实际
就好像我们发明了飞机
但是如果不懂它的原理
出了故障就没法维修一样，所以
理解了模型才能够知道它的合适用途
以及内部的脆弱环节等等
林赛则从信任的角度出发
发表了自己的观点
他认为
人类会根据信任的程度来托付任务
判断人的信任度还可以靠常规的直觉
但是大模型太像一个外星事物了
这些直觉并不适用
模型可能为了迎合答案
会假装在解题
如果不了解它的内部想法
我们根本无从知晓
巴特森也说
模型可能前几次会用A计划
遇到不同的问题，就切换到B计划
而这一点其实用户并不知情，所以
理解模型才能建立足够的信任基础
回到最初的问题
大模型的思考方式和人类一样吗
林赛认为，模型确实在思考
但是和我们人类的方式不同
在与大模型对话的时候
它的内部实际是在补全人类和助手角色之间的对话记录
模型被训练成要扮演一个具备乐于助人等特质的助手角色
为了准确预测这个角色的回应
需要在内心构建关于它的思维过程模型
所以说模型在思考
其实是一种功能性的表述
它的目的是模拟人类的思考过程
但是方式可能与人类的大脑大相径庭
阿梅森觉得这个问题还有情感层面的因素
比如是否暗含了“人类是不是没那么特别”的意思
以36+59的计算为例
模型会说自己采用了进位加法
但是实际采用的却是混合策略
人们会这种分歧会进行解读
一方可能会认为模型不懂自己的思路
不算思考；
另一方则觉得人类计算时的思路也是模糊奇怪的
和模型类似
因此，这类研究只能呈现出某种事实
再由人们自己判断
巴特森似乎在这一点上有不同的意见
他认为更重要的是
模型会进行整合、处理和按序操作
得出一些出人意料的结果
这显然存在着某种运作机制
因此，了解模型和人类的异同
就能知道我们该警惕什么
哪些可以凭借经验进行推断
模型虽然经过训练
可以模拟人类的对话
也会带有人类的特质
但是它所依赖的设备是与人类不同的
所以达成类人表现的方式
可能也会大相径庭
林赛也表示认同
目前还没有恰当的语言
来描述大语言模型的所作所为
就像生物学家还没有发现细胞和DNA时的摸索阶段一样
他们正在逐步的填补认知空白
虽然已经有了一些案例
能够看清楚模型内部的一些机制
但是认知空白的科学工程仅仅完成了大约20%，
还需从其他的领域来借用类比
而且要弄清楚
什么时候该用哪种表述
对于未来的研究方向
巴特森说还有很多工作要做
当前的研究方法也还有很多局限性和改进方向
比如在拆解模型内部的运作机制时
可能只捕捉到了百分之几的情况
还有很多的信息传递环节没有捕捉到
此外
研究正在从小型模型扩展到更复杂的Claude 4系列模型
这也存在着大量的技术挑战
阿梅森补充道
目前他们只能回答大约10%到20%，
关于模型如何完成某件事的过程
但是希望未来能做得更好
由于模型的很多行为
涉及到要提前规划多个步骤
所以他们希望弄明白
在长时间的对话中
模型对事情和交谈对象的理解是如何变化的
以及这些变化是如何影响行为的
要想理解模型读取大量文档、邮件和代码
并且给出建议的过程
这也是一项巨大的挑战
最后，林赛用了一个比喻来形容现状
那就是他们正在制造一台观察模型的显微镜
目前这台显微镜只有20%的时间能够正常工作
不仅需要许多高级技巧
基础设施还总出问题
为了得出模型运作方式的解释
团队成员还需要花时间去琢磨
但是他预计一两年内
可能能够实现对模型每一次互动的观察
做到一键生成展示模型思考过程的流程图
到那个时候
可解释性研究团队可能会像庞大的生物学家军团一样
通过显微镜来开展研究
好了
以上就是Anthropic可解释性团队这次访谈的主要内容了
希望能让大家了解更多关于模型可解释性的最新进展
也希望能够看到他们更多的研究成果
逐步为我们揭开大语言模型思考过程的奥秘
感谢大家观看本期视频
我们下期再见
