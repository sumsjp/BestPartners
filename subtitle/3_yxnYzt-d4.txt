大家好这里是最佳拍档
我是大飞
临近过年
国内的大模型公司
是扎堆的发布新产品
可谓是神仙打架
DeepSeek-V3刚刚推出没多久
DeepSeek又推出了R1推理大模型
在多个推理基准测试中表现出了惊人的能力
甚至可以与OpenAI的o1系列模型相媲美
更重要的是
这是首次公开研究并且验证了
大语言模型的推理能力
可以通过纯粹的强化学习来激发
而无需监督微调
颠覆了长期以来人们对于大语言模型训练流程的认知
在开源模型权重的同时
DeepSeek还公开了R1相关的技术报告
今天大飞就来给大家解读一下
首先，在我们一般的认识中
训练大语言模型
监督微调SFT通常会被认为是不可或缺的一步
监督微调的逻辑是
先用大量人工标注的数据来让模型初步掌握某种能力
然后再利用强化学习来进一步地提升模型的性能
不过这一次
DeepSeek团队却打破了这个传统
他们选择直接选择了在基础模型
也就是DeepSeek-V3-Base上进行强化学习
而没有经过任何形式的SFT预训练
而DeepSeek团队之所以会选择这么做
很大程度上是因为抛弃了对大规模人工标注数据的依赖
众所周知
SFT需要耗费大量的人力物力
来构建和维护高质量的训练数据集
而DeepSeek团队则直接让模型在强化学习的环境中进行自我探索
通过与环境的互动
自主地发现和学习解决复杂问题的能力
这种自主学习的方式
不仅节省了大量的标注成本
更重要的是
它让模型能够更加自由地探索解决问题的路径
而不是被预先设定的模式所束缚
因此也增加了模型的泛化和适应能力
在这个思想的指导下
DeepSeek团队先是训练了一个DeepSeek-R1-Zero模型
主要有三点独特的设计
首先是模型采用了群组相对策略优化GRPO
来降低训练的成本
GRPO不需要使用与策略模型同样大小的评估模型
而是直接从群组分数中估算基线
对于每个输入问题q
GRPO算法会从旧的策略中
采样出一组输出，形成评估群组
然后通过最大化目标函数来优化策略模型
其中
优势值A_i是通过标准化每个输出的奖励来计算的
其次是奖励设计
我们都知道
奖励设计决定着强化学习的优化方向
所以这次DeepSeek给出的方法
是采用准确度和格式两种互补的奖励机制
准确度奖励用来评估回答的正确性
在数学题中
模型需要用特定格式给出答案
以便进行验证，而在编程题中
则需要通过编译器运行测试用例
来获得反馈
另一种是格式奖励
模型需要将思考过程放在‘<think>’和‘</think>’这两个特定的标签之间
从而提升输出的规范性
之所以DeepSeek团队没有使用常用的神经网络奖励模型
是因为在大规模的强化学习过程中
模型可能会出现“作弊”的问题
同时
为了避免重新训练奖励模型需要的额外资源
所以简化了训练流程
第三个特点是训练模板
在GRPO和奖励设计的基础上
开发团队设计了一个简单的模板来引导基础模型
这个模板要求DeepSeek-R1-Zero先给出推理过程
再提供最终的答案
不过
这种设计只是规范了基本的结构
不对内容施加任何的限制或者偏见
比如不强制要求使用反思性推理
或者特定的解题方法等等
这种最小干预的设计
能够清晰地观察模型在强化学习中的进步过程
经过这些优化
DeepSeek-R1-Zero的性能提升也非常显著
在2024年的AIME数学奥赛中
DeepSeek-R1-Zero的平均pass@1分数
从最初的15.6%大幅提升到了71.0%，
达到了与OpenAI-o1-0912相当的水平
在多数投票机制中
DeepSeek-R1-Zero在AIME中的成功率
进一步提升到了86.7%，
甚至超过了OpenAI-o1-0912的表现
此外，在训练过程中
DeepSeek-R1-Zero也展现出了显著的自我进化能力
比如学会了生成数百到数千个推理token
以及产生了反思和探索不同解题方法的能力
这些能力都不是团队预先设定的
而是模型在强化学习环境中自然产生的
特别值得一提的是
开发团队还观察到了一个有趣的「AhaMoment」，
中文可以翻译为顿悟时刻
指的是DeepSeek-R1-Zero模型在推理过程中
突然会意识到可以“重新评估”之前的步骤
并且尝试用一种新的方法来解题
不过
尽管DeepSeek-R1-Zero展示出了令人惊叹的推理能力
但是训练的过程也并非完美无缺
依然存在着一些需要解决的问题
比如，输出的内容可读性较差
经常会出现不同语言混合使用的情况等等
为了解决这些问题
并且进一步提升模型的推理性能
DeepSeek团队又在DeepSeek-R1-Zero的基础上
训练了DeepSeek-R1
并且在训练过程中引入了多阶段训练策略
其中最关键的一步就是“冷启动”。
所谓“冷启动”，
是指在进行强化学习训练之前
先收集少量高质量的CoT数据
然后利用这些数据来微调DeepSeek-V3-Base模型
这样做的目的
并不是要让模型直接学会复杂的推理能力
而是先给模型“打个底”，
让它在开始强化学习训练之前
先具备一定的推理基础和良好的语言表达能力
为了构建高质量的冷启动数据
DeepSeek团队尝试了多种方法
包括使用带有长CoT的少样本提示
直接提示模型生成带有反思和验证的详细解答
以及收集R1-Zero的输出
并且进行人工标注和格式化
冷启动可以带来几个好处，首先
它能够避免强化学习初期的不稳定
让模型能够更快地进入稳定的训练状态；
其次
它可以有效地加速强化学习的收敛过程
缩短训练时间；
此外
它还可以提高模型输出的可读性
减少不同语言混合使用的情况
在“冷启动”阶段之后
紧接着要进入第二个阶段
也就是推理导向的强化学习训练
在这个阶段
DeepSeek团队采用了与DeepSeek-R1-Zero相同的训练方法
但是更加专注在提升模型的推理能力
尤其是一些需要高强度推理的任务上
比如代码生成、数学问题求解、科学推理和逻辑推理等等
这些任务的特点是
具有明确的规则和清晰的解决方案
因此非常适合检验模型的推理能力
为了达到这个目标
DeepSeek团队还在训练过程中引入了一些新的技巧
比如对推理链的质量进行细致地评估
然后通过奖励机制来引导模型生成更加合理、准确的推理过程
他们还特意在强化学习过程中加入了语言一致性的奖励
也就是根据CoT中目标语言单词的比例来计算奖励
比方说
如果使用多种语言来输出就会受到惩罚
当面向推理的强化学习收敛后
R1会利用训练好的强化学习模型进行拒绝采样（RejectionSampling）
从而生成新的监督微调数据
与之前的冷启动数据不同
这个阶段的监督微调数据不仅包含推理任务
还涵盖了其他领域的数据
例如写作、角色扮演、问答等等
这个阶段的重点是让模型理解推理的本质
并且能够将推理能力
运用到各种复杂的问题之中
接下来
R1会进入到下个阶段的强化学习训练
通过引入多目标优化策略
将多种不同的目标整合到一起进行优化
此外
R1还会采用不同的奖励信号和提示分布
针对不同的任务类型来进行优化
例如
对于数学、代码和逻辑推理等任务
采用基于规则的奖励；
对于开放式问答、创意写作等任务
则采用基于模型的奖励
这种策略的好处在于
它可以让模型在提升推理能力的同时
还能够兼顾其他方面的性能
比如写作能力、角色扮演能力
以及在各种通用任务上的表现
在一定程度上弥补了纯粹强化学习的局限性
从最终性能上看
DeepSeek-R1在多个推理基准测试中
都取得了令人瞩目的成绩
充分展现了强大的推理能力和泛化能力
比方说
在主打数学推理的AIME2024基准测试中
DeepSeek-R1的pass@1得分为79.8%，
略高于OpenAI的o1-1217模型
在处理复杂数学问题的MATH-500基准测试中
DeepSeek-R1的得分高达97.3%，
与OpenAI的o1-1217持平
在Codeforces代码竞赛任务中
DeepSeek-R1的Elo评分为2029
超过了96.3%的人类参赛者
在MMLU、MMLU-Pro和GPQADiamond等知识密集型任务基准测试中
DeepSeek-R1分别取得了90.8%、84.0%和71.5%的优异成绩
性能显著超越了DeepSeek-V3
在长文本理解任务FRAMES中
R1的准确率达到了82.5%，
也优于DeepSeek-V3
在开放式问答任务AlpacaEval2.0和Arena-Hard基准测试中
R1也分别取得了87.6%和92.3%的最高分
在训练出DeepSeek-R1后
DeepSeek团队意识到
大型模型虽然性能强大
但是也存在着一些局限性
比如计算资源消耗过高
部署和使用门槛较高等等
为了让更多的人可以使用到DeepSeek-R1的强大推理能力
DeepSeek团队还尝试了模型蒸馏
将DeepSeek-R1的推理能力迁移到一系列小型模型中
包括基于Qwen和Llama的1.5B、7B、8B、14B、32B和70B模型
在经过蒸馏之后
这些小模型的推理能力也得到了显著的提升
在某些基准测试中甚至可以超越一些大型的开源模型
比如
R1-Distill-Qwen-7B在AIME2024上得分为55.5%，
远超QwQ-32B-Preview；
R1-Distill-Qwen-32B更是在AIME2024上得分72.6%，
在MATH-500上得分94.3%，
在LiveCodeBench上得分57.2%，
显著优于其他的开源模型
与o1-mini相当
这不仅充分展现了模型蒸馏的巨大潜力
也说明R1学到的推理模式具有通用性和可迁移性
可以通过蒸馏的方式传递给其他模型
在论文中
DeepSeek团队还分享了一些他们暂时没有尝试成功的方法
比如说过程奖励模型PRM
它的限制在于
首先在一般的推理过程中
明确定义细粒度的步骤比较困难
其次难以扩展对步骤的标注工作
采用自动标注的准确率较低
手动标注又难以规模化
第三就是在PRM过程中
不可避免地会遇到rewardhacking
重新训练奖励模型不仅需要额外的训练资源
还会让整个训练流程复杂化
另外
DeepSeek团队也尝试了使用蒙特卡洛树搜索MCTS
但是也遇到了一些问题
一是搜索的空间过大
容易陷入局部最优；
二是训练细粒度的价值模型难度较大
影响模型的迭代改进
在论文的最后
DeepSeek团队也探讨了R1模型的局限性
并提出了未来的研究方向
比如在通用能力方面
DeepSeek-R1的通用能力仍然不及DeepSeekV3
接下来
DeepSeek团队还计划探索如何利用长CoT
来提升这些领域的任务表现
在语言混合方面
DeepSeek-R1目前针对中文和英文进行了优化
但是在处理其他语言以及语言遵循方面
还是会有问题
此外，DeepSeek-R1对提示词非常敏感
少样本提示会持续降低性能
所以论文建议用户直接描述问题
并且指定输出格式来获得最佳的结果
在软件工程任务方面
由于长时间的评估会影响强化学习的效率
所以团队还没有在软件工程任务中广泛应用大规模的强化学习
因此
DeepSeek-R1在软件工程基准测试上
并没有显示出比DeepSeek-V3更大的改进
团队计划
R1的未来版本将通过在软件工程数据上实施拒绝采样
或者在强化学习过程中引入异步评估
来进一步提高效率
除了这些工作之外
DeepSeek团队这次又继续充分发扬了开源共享的精神
不仅发布了DeepSeek-R1模型的权重
还将DeepSeek-R1-Zero以及一系列蒸馏的小模型
都进行了开源
也难怪总理座谈会都要邀请DeepSeek的创始人梁文锋参加了呢
另外DeepSeek不愧是号称大模型界的拼多多
这次R1 API 服务的定价为每百万输入token
命中缓存的话是1元，没命中是4元
每百万输出tokens是16 元
相比之下
o1的API 定价为每百万输入 token 15 美元
每百万输出 token 60美元
目前
DeepSeek 已经在网页端、App 端和 API 端全面上线了 R1
在网页端对话界面
选择“深度思考”就能直接体验DeepSeek-R1
好了
以上就是本期视频的全部内容了
感谢大家的观看
我们下期再见
