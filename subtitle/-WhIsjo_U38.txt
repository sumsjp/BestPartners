大家好，这里是最佳拍档，我是大飞
之前有消息称
DeepSeek 或许将提前推出
原定于在五月份初发布的 R2 模型
虽然还无法确认消息的真实性
但是DeepSeek 刚刚和清华大学联合发布的一项关于推理时Scaling的新研究
或许能让我们提前窥探到R2的一角
这篇论文题目为《通用奖励模型的推理时间扩展（Inference-Time Scaling for Generalist Reward Modeling）》，
已经发表在了预印本平台arXiv上
今天我们就来解读一下这篇论文
看看DeepSeek又一次如何通过创新的架构设计和训练方法
来实现模型的性能突破
大家应该清楚的是
当前主流的 AI 模型大多数都采用了强化学习
尤其是基于人类反馈的强化学习RLHF
作为后训练的核心方法
这个方法的核心在于
会训练一个奖励模型RM来模拟人类的偏好
从而指导大语言模型的优化
但是传统 RLHF 依赖于大量的人工标注
成本高昂而且扩展性有限
尤其难以处理复杂和主观性强的任务
因此
如何构建一个更加强大、更加通用的奖励模型
就成为了突破瓶颈的关键
现有的奖励模型范式
比如Scalar RM或者是Pairwise RM
在通用性和灵活性上都存在着一些局限性
同时
随着推理时Scaling越来越成为提升模型性能的重要途径
如果奖励模型能够在推理的时候
通过更多计算变得更准确
那么将直接提升大语言模型的对齐效果
在这个背景下
DeepSeek 联合清华大学的研究团队
提出了一种名为 DeepSeek-GRM 的通用奖励模型
以及名为自我原则评价调优、简称SPCT的训练方法
目的就是为了解决通用奖励模型的构建难题
并且系统性的探索如何利用推理时Scaling来提升模型的性能
论文的核心也是围绕着这两个内容展开的
研究团队首先关注了奖励模型的结构范式
他们认为
为了实现通用性和充分利用推理时Scaling的潜力
需要一种更加灵活、表达能力更强的范式
最终，他们选择了生成式奖励建模GRM
并且采用了逐点式Pointwise的评分机制
Pointwise GRM 的工作方式与传统 的RM 不同
它不是直接输出分数或者排序
而是会针对输入的查询和一组待评价的回答
来生成一段结构化的评价文本
这段文本通常会包含两个主要的部分
首先，模型会根据当前的输入内容
自适应地生成一系列的评价原则
这些原则定义了评价的关注点和标准
有时还会附带各个原则的相对重要性
也就是权重；
其次，模型会基于这些生成的原则
对每一个回答进行详细的分析和评价
说明回答的优缺点
最后，通过预设的解析规则
从生成的评价文本中
提取出对每个回答的具体评分
Pointwise GRM 有两大关键的优势
一是输入灵活性
无论是评价单个回答，比较两个回答
还是需要对多个回答进行独立的评分和排序
都可以使用统一的框架和模型进行处理
这就极大地拓宽了模型的应用范围
二是推理时Scaling的潜力
由于模型的核心行为是生成文本
因此在推理时进行多次采样
就变得非常自然而且有意义了
每次采样
都可能产生不同的评价原则侧重和评价分析角度
通过综合这些多样化的评价结果
我们就能获得比单次生成更全面、更稳定、也更精细的最终评分
从而为利用推理计算来提升奖励的质量
提供了可能
那选择了合适的模型范式后
接下来的关键就在于如何进行有效的训练
才能让 GRM 具备强大的通用评价能力
并能真正地从推理时Scaling中受益
为此
团队设计了提出了一种名为自我原则评价调优
简称SPCT的学习框架
SPCT 的核心思想在于
对于通用的评价任务来说
要想预先定义一套固定的、普适的评价标准或者原则
是非常困难的
更为有效的方式
应该是让模型学会根据具体的输入内容
来动态地、自适应地生成最相关的评价原则
并且基于这些原则进行准确的评价
这就意味着模型需要从被动地应用规则
转变为主动地构建评价框架
研究团队通过初步实验
验证了评价原则的重要性
直接使用模型生成的评价原则
效果有限
但是如果提供经过筛选的高质量评价原则
那么奖励模型的准确性会得到显著提高
这表明，能否生成“好的评价原则”，
是实现高质量奖励的关键
因此
SPCT 的目标就是训练模型掌握这种生成高质量评价原则和准确评价的能力
简单来说
SPCT的训练过程包含有两个阶段
第一个阶段是拒绝式微调RFT
作为模型的冷启动阶段
需要使用预训练的大语言模型作为基础模型
研究人员利用了包含查询、回答和人类偏好标签的奖励模型数据集
让模型尝试生成“原则+评价”的文本
并且提取评分
这个阶段的关键在于“拒绝式”的采样策略
比方说
如果模型生成的评分结果
与已知的人类偏好不相符
那么这次生成的训练数据就应该被视为“不正确”，
而被拒绝；
另一方面，如果对于某个输入
模型连续多次生成的评分结果
都与人类的偏好完全一致
那么这可能表明这个任务过于简单
缺乏足够的学习信号
而这类数据也会被认为“太容易”而被拒绝
通过这种方式
模型得以专注于学习那些具有挑战性
而且能够帮助它更好理解人类偏好的那些样本
从而快速掌握生成指定格式文本
和初步区分回答好坏的能力
这个阶段还结合了“提示式采样”和“非提示式采样”，
来平衡学习过程
第二阶段是基于规则的在线强化学习
虽然之前的RFT 阶段提供了基础能力
但是要让模型的原则生成和评价能力持续提升
适应更广泛的场景
并且为推理时Scaling做好准备
就需要在线优化的介入
在这个阶段
GRM 模型作为 强化学习中的策略
会根据实时输入的查询和回答来生成评价原则、进行评价
并且提取评分
研究人员设计了一套简单的准确性规则作为奖励信号
如果模型给出的评分能够正确地将最优回答排在首位
那么就会获得+1的正奖励
否则将获得-1的负奖励
这个奖励信号用来更新 GRM 模型的参数
这个在线过程
会持续激励模型去探索和学习
如何生成那些能够更可靠地区分回答质量的原则和评价的逻辑
通过内化模型的评价能力
让模型在面对新情况的时候
也能够做出良好的判断
这对于推理时Scaling的有效性至关重要
研究团队还发现
通过适当调整 KL 散度的惩罚
可以有效保证生成文本格式的稳定性
从而避免模型产生不必要的行为偏差
这样一来
经过 SPCT 训练的 DeepSeek-GRM 模型
就具备了通过增加推理时计算量
来提升性能的潜力
研究团队重点研究并且实现了两种推理时Scaling的策略
第一种是基于投票的Scaling
这是一种相对直接的方法
对于给定的查询和一组待评价的回答
使用训练好的 DeepSeek-GRM 模型
设置一定的采样随机性
并行地进行 k 次独立的推理
每次推理都会生成一套可能不同的原则、评价和相应的评分
最后再将这 k 次推理得到的评分进行聚合
对于 Pointwise 评分
通常的做法是将每个回答在 k 次采样中获得的分数
相加或者取平均数
来得到最终的综合评分
这种方法的好处在于
它不仅可以通过聚合多个评价视角
来提高结果的稳定性
而且通过求和等方式
实际上增加了最终奖励值的范围和粒度
使得模型能够更好地区分质量相近的回答
同时，为了减少潜在的顺序影响
每次采样前还会对输入回答的顺序进行随机排列
第二种是更进一步的基于元奖励模型引导的投票
简单投票会假设每次采样的结果质量是相当的
但是在实际中
部分采样可能由于随机性或者模型的局限
而产生较低的质量
或者是有偏见的评价
为了解决这个问题
研究团队提出训练一个元奖励模型Meta RM的方法
这个 Meta RM 的作用是评估 DeepSeek-GRM 生成的、每一次“原则+评价”输出的质量或可靠性
Meta RM 本身通常也是一个简单的奖励模型
它会通过学习判断 GRM 的输出是否与基准一致
来进行训练
在推理时
首先会让 DeepSeek-GRM 生成 k 份的评价结果
然后使用Meta RM对这 k 份结果进行评分
筛选出评分最高的 k_meta 份结果
最后只基于这些被认为是高质量的评价结果
来进行投票聚合
这种方法通过引入一个“质量过滤器”，
可以有效地剔除噪声采样
使得最终的聚合结果更加准确
从而更充分地发挥推理时Scaling的优势
研究团队在多个主流的奖励模型评估基准
包括 Reward Bench、PPE、RMB、ReaLMistake上
对 基于不同尺寸的DeepSeek-GRM 模型及其推理时Scaling策略
进行了全面的实验评估
并且与多种公开的基线方法进行了对比
实验结果清晰地展示了这项研究方法的有效性
首先
即使在不进行推理时Scaling的基础设置下
经过 SPCT 训练的 DeepSeek-GRM 模型
在整体性能上已经优于同等规模的多种基线奖励模型
并且展现出与一些大型闭源模型
比如Nemotron-4-340B-Reward和GPT-4o相竞争的实力
其次
SPCT 训练方法的有效性也得到了证实
相比只进行 RFT 冷启动
完整的 SPCT 流程带来了显著的性能提升
消融实验也证实了其中关键步骤的贡献
除此以外
DeepSeek-GRM 还展现了优秀的推理时Scaling特性
随着采样次数 k 的增加
模型性能持续稳定的提高
尤其是在 元奖励模型的引导下
提升效果更为明显
更加惊人的是，实验数据表明
通过推理时Scaling
在DeepSeek-GRM-27B 模型上进行 32 次采样并使用元奖励模型引导
性能提升的幅度有时可以达到、甚至超过把模型参数增加几倍所带来的提升
这充分显示了出推理时Scaling
在提升奖励模型质量方面可能具有更高的计算效率
最后
相比一些偏科严重的Scalar或者Semi-Scalar模型
DeepSeek-GRM 在不同类型的任务和评价维度上的表现更为均衡
展现出了更好的通用性和更少的领域偏见
不过
虽然 SPCT 在提升 GRM 的性能和推理时Scaling方面
取得了显著的成功
但是目前也存在一些局限性
首先，GRM的效率
本质上落后于同等规模的Scalar RM
这限制了它在在线强化学习管道中的大规模使用
不过
由于采用了并行采样来进行推理时Scaling
使用合理数量
比如 8 个样本来进行奖励生成
延迟不会有显著的增加
其次
在一些可验证任务的特定领域上
DeepSeek-GRM 仍然落后于Scalar RM
这可能是因为Scalar RM捕获了推理查询和回答的隐藏特征
而 GRM 需要更强的推理能力来全面检查回答
不过
Scalar RM也存在严重的偏差和可扩展性的问题
而GRM则没有这些问题
最后，研究团队在论文中还指出
未来的研究方向包括工具集成、原则和评价生成范式的分解、以及在大模型离线评估中的应用和长视野推理的探索
他们相信
具有更强可扩展性和效率的 GRM
可以作为通用奖励系统的多功能接口
推动大语言模型后训练和推理的前沿发展
好了，以上就是对这篇论文的解读了
如果 DeepSeek-R2 真的能够整合这项技术
把推理时Scaling做到极致
那么它或许能够进一步以更低的训练成本
挑战 OpenAI 的o系列模型
继续实现“以小博大”的惊人逆袭
期待早日看到R2的发布
感谢大家观看本期视频
我们下期再见
