大家好，这里是最佳拍档，我是大飞
视频生成和多模态是今年发展最快的AI领域之一
Luma AI 也在上个月推出了视频生成模型 Dream Machine
惊艳程度让 Luma 在一夜之间
跻身视频生成的头部玩家行列
而在这之前，Luma AI从 NeRF 起家
产品和研究也主要集中在3D 领域
如今为什么又突然选择进入视频生成领域呢？
3D 生成与视频生成有什么关系？
Luma 的未来业务重心
会逐渐从 3D 转向视频
还是多个方向同时进行呢？
在Luma 的首席科学家 Jiaming Song （宋佳铭）看来
3D和视频其实是一回事
视频是通往3D的路径之一
而Luma 只是在探索 3D 的过程中
完成了对视频生成模型的训练
这位斯坦福毕业的大牛在最近的一次访谈中表示
在训练视频模型的过程中
AI自然涌现出了对物理世界的理解
对三维空间、深度、光的反射和折射
以及光在不同介质、不同材质中运行的效果等等
在加入 Luma 之前
宋佳铭曾经在 Nvidia 研究中心的 DIR（DeepImagination Research） 4470 LPR （Learning and PerceptionResearch）小组
进行 AI 领域的研究
他提出的DDIM (Denoising Diffusion Implicit Models）算法
在许多主流的生成模型中都得到了应用
今天大飞就来带大家回顾一下
这位Luma 首席科学家的最新访谈
这场访谈的核心
毫无疑问就是Luma AI的突然转向
Luma 之前一直做的是 3D 重建和 3D 生成相关的业务
为什么现在要开始做视频生成呢？
在宋佳铭看来，大家其实误会了
这属于是闹了个乌龙
Luma的团队并没有进行产品上的转型
他们研究视频生成
本质上是为了实现更好的3D、甚至是4D生成
他认为
视频生成和4D生成是息息相关的
甚至可以说是后者必备的前置技术
并且分享了团队在研发4D生成时遇到的一些麻烦
他们在做3D的时候
意识到当前的一些 3D方案
如果想把 3D 转成 4D
也就是加上时间维度，会比较困难
因为现在比较主流的3D 生成的方案
是用大量的图片训练一个基础模型
然后把它微调成一个多视角的 3D 模型
最终再变成一个真正的 3D 场景
如果Luma想实现最终的4D生成
可行的路线只有两条
一种就是像刚才说的那样
用图片生成 3D
再把 3D 动画变成 4D
另一种则是直接做一个视频模型
把视频模型再变成 4D
团队最终觉得
还是方案二更靠谱一点
因为即使不考虑视频生成本身的好处
只是为了在 3D 领域更进一步
也是需要做视频的
沿着这个思路
Luma AI 开始对一些 3D 场景的视频模型进行微调
早在Dream Machine之前
开发团队其实就已经搭建了一个视频转 3D 的 workflow
因此，Luma最初的动机
不是从做 3D 转向做视频
而是想要通过视频的方式去驱动更好的3D
一开始的时候
宋佳铭自己其实心里也没底
虽然3D生成和视频生成
在视觉上有一定的共通之处
但是毕竟不是同一个领域
团队一开始只是打算拿视频模型做一些测试
并没有太期待模型的 3D 生成能力
不试不知道，一试才发现
如今视频生成 3D 的能力已经很强了
关键在于
这个视频模型本身的 3D 一致性
以及和图形管线相关的、大家会关心的一些光学方面的东西
它都处理得不错
当然这个东西还并不完美
可能会在跳体操这种例子上出问题
但是视频生成在 3D的一致性、光学、深度、以及一些动态的物理现象上
已经表现得十分惊艳了
宋佳铭举了个例子
如果我们把一个图片丢进 dream machine
直接转成一个视频
再把视频丢到之前已经建立的视频转 3D 工作流里
二者可以做到直接就能交互
效果非常惊艳
这也回到了他们想做视频生成的初衷
因为视频可能是一条
能够更好地实现 3D 的路线
不过，宋佳铭也表示
3D 和视频生成技术的互通
也并不是每次都能成功
成功的时候很惊艳
但是也存在着大量的变量会导致失败
比如视频镜头的焦距
以及镜头的状态假设
所以这并不是一个 100% 能够成功的工作流
但是视频模型能做到这个地步
已经比他们用的传统方案好很多了
具体来说，以前在3D 领域
大家会比较关心深度
也就是预测这个物体距离镜头有多远
在文生图阶段
就已经有人尝试做这件事
比如有一篇很出名的论文 MVDream
内容就是直接将文生图模型作为一个起点
再用一些深度数据对它微调
结果发现，不需要加很多深度数据
模型的效果就已经很好
如今的视频模型
不需要我们再做特殊处理
也不需要加入深度等3D相关的数据
只需要通过学习视频数据
就能够学习到深度的知识、知道视频里面物体的远近
这也是深度的一种涌现方式
在这个过程中
甚至不仅真实的图片能产生不错的效果
即使是一些非常抽象的图片
比如毕加索风格的图片，效果也很好
如果把非常抽象的旋转木马图片
放进视频模型里
模型生成的视频依然能够模拟旋转木马的旋转状态
这说明即使是抽象的图片
模型也能够理解关于深度的信息
除了深度
视频模型也能很好地理解光的反射、折射
以及光是如何在不同介质中运行的
比如说，一个视频中
背景里有一个红色的霓虹灯
当镜头移动时
人背后光影颜色的深度和光影覆盖的区域大小
会随着镜头相应的改变
同时，当镜头移动的时候
这个人戴的眼镜与人脸的距离、镜片的厚度等等
都会随之调整
再者，塑料材质基于光的反射
在传统意义上也不是那么容易去模拟的
但是这个视频所表现出来的效果
包括一致性都很不错
与此同时
视频模型也比传统的3D生成模型更加省时省力
假设我们想做一个咖啡机的视频
如果用传统的NeRF 去做
就需要围绕着咖啡机采集上百张图片
才能得到比较好的 3D 重建效果
但是同样的咖啡机
今天只需要把一张图片放进视频模型中
它就能够很好地模拟咖啡机的钢铁材质
包括光在钢铁材质上的反射
这实际上就可以形成一个图片转视频再转成 3D 的工作流
这样的工作流比之前采集上百张图片、用NeRF进行重建的方式
要方便得多
不过
视频生成毕竟没有针对性的进行训练
所以在一些3D图像的处理上还是有瑕疵的
比如生成人类高速移动图像的时候
模型就会错误地把移动中的残影
识别为独立的脑袋
从而生成一个人有好几个头的错误场景
不过瑕不掩瑜，宋佳铭的思路是
通过收集更多视角的视频数据
也许可以把当前的视频模型
优化成一个多视角的视频模型
再利用多视角视频模型生成的数据
去生成3D甚至4D事件
因此
宋佳铭相信视频模型的3D图像能力
也是众多涌现例子中的一个
这种未知的惊喜给团队带来了极大的鼓舞
事实上
不只是宋佳铭为3D能力的涌现感到兴奋
围绕视频生成
整个社区也开始讨论 World Model、World Simulator 的概念
视频生成模型要实现 World Model、理解世界物理规则
究竟是一个会随着模型 scaling up 涌现的过程
还是需要我们对模型本身进行升级改造呢？
宋佳铭认为，前者的可能性更大
这其实也和 scaling law有关
宋佳铭更喜欢用它的另一个名称
“Bitter Lesson”。
Bitter Lesson 是 理查德·萨顿Richard Sutton 提出来的
他是强化学习领域的泰斗级人物
大部分时间都在研究算法
以及怎么用算法的方式提高模型训练效率
2019 年的时候他说过一句话
从历史的进程来看
一般来说简单但是能更好利用计算量的方法
在长期来讲
会优于加入人类的先验知识、但是计算量比较少的方法
它的核心是少用人类的先验知识
多用数据、多用计算
一定程度上
我们可以认为人类语言其实也是人类的某种先验知识
因为虽然不同的人说的语言都不太一样
但是并不影响大家去理解物理世界、在这个世界里面进行操作
Richard Sutton 当时举的例子是 AlphaGo
围棋 AI 在使用 deep learning 之前
还是下不过人类的
随着 AlphaGo、AlphaZero 的出现
大家才开始觉得围棋这个问题被解决了
以前基于人类大脑运算的一些算法
现在可以更好地利用计算机的算法去突破
一定程度上
语言本身也是对于这个世界所展示现象的一种压缩或者先验知识
因为语言可以利用更多先验知识的点
压缩率比较高
所以它利用计算的效率也会比较高
肯定会先起飞，因此
大家在语言领域先做出成果
就像大家之前用搜索在象棋中可以做出成果
但是在围棋做不出成果一样
之前大家都普遍认为
语言可以做出成果
视频不一定做的出成果
但是随着数据量的提高
对于世界的理解也会随着多模态 token 的数量提高
而超越之前语言模型所能够达到的能力
当然语言的重要性还是有的
并不是说要完全甩掉语言
让模型自己去开发新的语言
但是宋佳铭想用这个类比来说明
他比较相信未来是以多模态为主的发展趋势
这个模态具体是视频、4D、语言、action或者是其他等等
相对来说不那么重要
语言模型也是类似
大家花了这么多时间去研究语言理解
做了很多的语法、分词、情感分析、段落整理的任务
结果发现只要计算量上来
很多任务语言模型其实都能做
宋佳铭觉得
视频模型也会有同样的发展趋势
以前针对每个图像学的问题
我们都需要单独去设计一套方案来解决
基于先验经验去做
这样确实能做得不错
但也确实有它的上限
在这个过程中
也发现对于一些相对复杂的情况
有的时候用scaling up、用更大计算量的思路
对于它的长远发展可能会有更好的突破
这也同样适用于更复杂的物理模拟
随着计算资源的增多
大家可能也会发现
视频模型也存在类似的涌现现象
现在解决不了的问题到时候自然而然就解决了
视频模型现在还相当于大语言模型赛道
刚刚做出来 ChatGPT 的那个阶段
往后还有很长一段增长的空间
继续提升模型的能力肯定是很重要的
包括可控性以及生成的速度
因为生成速度会影响 商业模型
当然了
这也不是说宋佳铭就是个纯堆计算量的爱好者
他也提到，单纯提升速度可能还不够
因为总有人可以去堆更多的 GPU
达到更快的生成速度
但是这样并没有节省成本
所以最终的商业模型可能还不太一样
基于 scaling law
或者说Bitter Lesson
宋佳铭最终要实现的目标
就是在Luma模型中集合多模态的理解和生成
从而实现4D的突破
Dream Machine 更类似于这个目标的副产物
从数据的角度来讲
视频数据比文字数据的 token 数量大很多
多模态模型也都是现在最大的文本预训练模型的百倍以上
宋佳铭直言
原生的文本数据已经接近枯竭了
大家一方面在做合成数据
另一方面把 模型的规模不断提高
但是如果模型规模无限提高
也会有成本的问题
从多模态的角度来讲
因为多模态信号的数据量很多
scaling law 会更倾向于数据
所以可能不需要那么大的模型去 scale up
就可以达到不错的效果
谈完了眼下Luma的成果以及研究思路
宋佳铭呢
也分享了一些未来的研究目标
他最近比较感兴趣的方向
跟系统、或者说 transformer 本身的范式有关
大家现在基本不否认 scaling law 会创造更多的智能
但是 transformer 本身随着序列变长
它的二阶性能存在明显的限制
如果有更有效的算法能解决这个 问题
会是个非常有意思的方案
虽然现在有一些基于 RNN 的方案
像RWKV Mamba
或者是一些基于线性 transformer 的方案
但是好像最终在大规模实验中的效果
都还不如 transformer
所以他最好奇的问题就是
如何能在保证性能的情况下
让序列长度从现在的百万级
变成千万级或者亿级
宋佳铭自己的一个深刻体会是
任何问题乘以 10 或者乘以100
解决方案都会变得非常不一样
无论从算法还是系统上
都需要重新设计一遍
如果在这个领域能够实现大的突破
那对于训练的效率和多模态的理解
都会很有帮助
宋佳铭关心的第二个点是
现有的 transformer 或者模型本身它们在做什么
或者这些模型真的学到了什么东西
理解这些可以帮助我们更好地理解或者预测 scaling law
让我们能够以更加低的成本或者更高的效率去训练模型
第三点，也比较难的一个点
是扩散模型的 scale，相比于 自回归
它的主要问题是在一个连续空间里
在信息论的层面
连续空间跟离散空间的逻辑非常不一样
不知道这是不是一个因果关系
但是最后导致的一个情况是
你很难在扩散模型里去算困惑度
也无法去衡量 token 的效率
这不像在语言模型里面
你看到这个loss就知道它是什么样
宋佳铭觉得现在在扩散模型上
大家都知道扩展模型上有 scaling law 的存在
但是真正做到还处于一个非常模糊的阶段
找不到它这个系数
相当于说我们要推一个定理
现在认为常数是存在的
但是并不知道这个常数是多少
如果能够找到这个神秘的常数
研究人员也许就就能摆脱黑箱的束缚
在一定程度上定制自己想要的功能
用宋佳铭的话来说
训练大模型会更像科学而不是玄学
好了
以上就是宋佳铭这次访谈的基本内容
整个访谈几乎没什么水分，干货满满
讨论的也是我们平时不多见的3D生成领域
不知道对大家有没有一些启发
欢迎把自己的收获分享到评论区里
感谢大家的观看，我们下期视频再见
