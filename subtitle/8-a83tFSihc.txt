大家好，这里是最佳拍档，我是大飞
11月28日，被誉为AI教父的杨立昆
Yann LeCun接受了尼基尔·卡马斯Nikhil Kamath一个半小时的专访
围绕着他对人工智能的理解、发展历程、未来趋势
以及对年轻人的建议等等话题展开
是一次难得的精彩访谈
杨立昆应该算是我们频道的老熟人了
他是深度学习领域的三位奠基人之一
在2018年与杰弗里·辛顿（Geoffrey Hinton）和约书亚·本吉奥（Yoshua Bengio）共同获得了图灵奖
并且被誉为卷积神经网络（CNN）之父
他设计的LeNet-5卷积神经网络
是第一个成功应用在手写数字识别的深度学习模型
也为之后深度学习的发展奠定了基础
他现在是纽约大学的教授
同时也是Meta的首席人工智能科学家
好了，话不多说
我们正式进入这次访谈的内容
杨立昆先介绍了一下他的成长经历
他在巴黎郊区长大
父亲是一位工程师
在父亲的影响下
他从小就对科学和技术产生了浓厚的兴趣
最初他立志成为一名工程师
但是后来逐渐对人工智能领域的基础问题着了迷
他在求学过程中学习的是电气工程专业
不过在学习期间
他对数学、物理以及人工智能中的基本问题越来越感兴趣
尽管他没有系统地学习过计算机科学
但是他积极参与了与数学教授合作的人工智能独立项目
从此便全身心的投入到了人工智能的研究之中
当被问到工程师和科学家的区别时
杨立昆认为科学家致力于理解世界
而工程师则侧重于创造新的事物
但是这两者之间往往相互关联
科学的进步常常依赖于技术的发展
而技术的进步又为科学研究
提供了新的工具和数据
就像望远镜的发明让我们能够发现行星
显微镜为众多科学发现打开了大门
这些技术进步推动了科学的不断发展
而对于杨立昆来说
之所以选择从事人工智能
正是因为他既有科学研究
又有工程实用价值两方面的追求
另一点从事人工智能的原因
是他认为
我们现在遇到的几乎所有问题
都可以追溯到人类的知识或智力的匮乏
我们之所以犯错
是因为我们不够聪明
无法意识到我们有一个问题
并且因为我们不够聪明
所以也无法找到解决方案
而人工智能
能够增加整个人类的整体智能
这才是解决我们所面临的许多问题的关键
接下来
杨立昆深入探讨了人工智能的本质
到底什么是人工智能呢？
杨立昆用“盲人摸象”的故事
来比喻人工智能的发展历程
他指出智能具有多个不同的方面
但是在人工智能发展的历史中
人们往往只关注在其中的某一个方面
而忽略了其他方面
在20世纪50年代
人工智能发展的一个重要方向是逻辑推理
这个时期的人们致力于研究如何让机器进行逻辑推理
寻找解决问题的最佳方案
比如说，著名的旅行商问题
就是给定一堆城市
要求确定遍历每个城市的最短路径
这个问题成为当时人工智能研究的典型代表
当时人们认为
所有的推理问题都可以归结为寻找解决方案的过程
通过在可能的解决方案空间中搜索
利用某种评估标准
来判断是否找到了最优解
这在数学上被称为优化问题
这种基于逻辑推理的人工智能方法
在当时包括了搜索解决方案和规划等方面的研究
比如在机器人操作中
如何为机器人手臂规划抓取物体的轨迹
就是一个典型的规划问题
然而，这一时期的人工智能研究
主要集中在推理和规划上
却完全忽略了感知方面的问题
也就是机器如何理解世界、识别物体
以及将物体与其背景分离等问题
同时也没有考虑到人类思考的抽象层面
与此同时
另一个人工智能分支也在悄然兴起
这个分支的研究人员受到生物智能机制的启发
试图重现动物和人类大脑的学习能力
他们认为，智能是大量简单元素
也就是神经元网络相互连接所产生的一种涌现现象
大脑的学习过程是通过改变神经元之间连接的强度来实现的
早在20世纪40年代
麦卡洛克（McCulloch）、皮茨（Pitts）等数学家就提出了神经元的计算模型
为后续的研究奠定了理论基础
到了50年代和60年代初
人们设计出了简单的算法
来调整神经元之间的连接强度
从而使系统能够学习完成特定的任务
这也标志着神经网络研究的开端
1957年，第一台基于这种原理的机器
感知器被提出
感知器的工作原理基于对简单图像的识别
比如区分黑白图像中的C形和D形
图像在计算机中被表示为数字数组
每个像素对应一个数字
黑色像素为0，白色像素为1
感知器通过给输入像素值赋予权重
并计算加权和来进行判断
如果加权和大于阈值
则判断为C形，小于阈值则判断为D形
训练感知器的过程就是不断调整这些权重的值
当系统输出错误的时候
通过特定的规则来增加或减少相应权重
最终让系统能够准确区分C形和D形
然而，在50年代和60年代
感知器的应用范围相对有限
除了简单形状识别外
并没有太多实际的应用场景
在这个时期
人工智能领域形成了两个主要的研究方向
一个是以逻辑推理和搜索为核心的方法
另一个则是以神经网络和学习能力为重点的方向
这两个方向相互竞争
共同推动着人工智能的发展
在人工智能的早期发展中
有一位杰出的人物
马文·明斯基（Marvin Minsky）
他是麻省理工学院的教授
在人工智能领域具有重要影响力
他早期从事神经网络研究
但后来观点发生了转变
成为了基于逻辑和搜索方法的坚定支持者
在60年代后期或中期
他与西摩尔·派普特（Seymour Papert）合著了《感知器》一书
书中对感知器的能力进行了深入分析
并且论证了其能力的局限性
这本书的出版对当时的神经网络研究产生了重大影响
许多从事神经网络研究的人虽然继续坚持研究
但是也改变了研究工作的名称
将其称为统计模式识别
或者自适应滤波理论
这些研究方向在工程领域取得了广泛的应用
比如在金融行业中
对冲基金和基金经理常常会利用神经网络来分析大量数据
试图识别其中的模式
尽管金融市场的数据具有随机性
但是神经网络在一定程度上
能够帮助发现潜在的规律
随着时间的推移
人工智能领域的技术也在不断演进
其中机器学习就是解决人工智能问题的一种特定方法
而在机器学习领域中
又包含了多种不同的技术和方法
其中，“老式的人工智能”（GOFAI
Good Old - Fashioned AI）
是早期占据主导地位的人工智能方法
主要基于符号主义和逻辑推理
它使用符号来表示知识和概念
比如说用“猫”这个符号
来代表实际的猫这种动物；
还有就是通过逻辑规则进行推理
比如“所有猫都是动物
咪咪是一只猫，因此咪咪是动物”；
以及将知识存储在明确定义的知识库中
以便系统访问和使用；
GOFAI的主要应用之一就是专家系统
能够模拟人类专家的决策过程
例如医疗诊断系统等等
这些系统通常都需要依赖于清晰、明确而且易于理解的规则
而机器学习中的一个重要分支是深度学习
它也是当前人工智能发展的核心驱动力
深度学习的基础是神经网络
神经网络的发展经历了多个阶段
早期的神经网络比如感知器
虽然简单但是存在一定的局限性
它可以访问的输入-输出函数类型非常有限
因此无法采用自然图像，比如说照片
来训练系统学习到其中是否有狗、猫或桌子
直到20世纪80年代
神经网络研究取得了重大突破
也就是反向传播算法的出现
这个算法使得多层神经网络的训练成为可能
从而克服了感知器的一些限制
通过堆叠多层神经元
每个神经元计算加权和
并且通过阈值函数进行非线性转换
系统就能够学习到更复杂的函数关系
从而具备了处理更复杂任务的能力
例如，在图像识别领域
卷积神经网络（CNN）的发明就是一个重要的里程碑
CNN的架构受到了视觉皮层结构的启发
它通过卷积层、池化层等组件
能够有效地处理图像数据中相邻像素之间的关系
从中提取出图像的特征
从而实现对图像的准确识别
在机器学习中，除了深度学习
还有其他一些重要的类型
比如传统的机器学习
它源于统计估计
包括线性回归、提升分类树、支持向量机、核方法等等
这些方法的共同特点是
程序具有一些可调的参数
输入输出函数也由这些参数的值决定
通过向系统展示大量示例数据
并且根据输出与期望输出的差异来调整参数
让系统能够学习到输入与输出之间的关系
这就是监督学习的过程
与监督学习不同
强化学习是另一种学习方式
在强化学习中
系统不会被告知正确的答案
而是根据它产生的答案得到一个以数字形式表示好坏的反馈
例如在训练一个下国际象棋的AI系统时
系统每走一步
会根据这一步对最终胜负结果的影响
得到一个评价
如果这一步有助于获胜
就会得到一个正向的反馈
反之则得到负向的反馈
系统通过不断尝试不同的策略
根据反馈调整自己的行为
从而逐渐学会选择最优策略
最终实现自我训练
强化学习在游戏等领域表现非常出色
是因为在这些场景中可以进行大量的试验
系统能够通过不断试错来学习
近年来
自监督学习也成为了人工智能领域的一个关键进展
自监督学习与监督学习类似
但是又有所不同
以自然语言处理为例
在自监督学习中
系统不需要明确的输入输出对
而是通过对输入数据自身的处理
来学习它们的内在结构
例如，对于一段文本
可以通过删除部分单词
让系统预测缺失的单词
系统利用文本中其他可见单词的信息来进行预测
在训练过程中
系统会被告知正确的答案
以便调整参数
这种方法在大规模文本数据上取得了巨大成功
成为大语言模型成功的关键因素之一
大语言模型也是当前人工智能领域的热门话题
它们基于Transformer架构
通过在海量文本上进行训练
能够对下一个单词进行预测
从而实现文本生成、问答等功能
然而
大语言模型也存在着一些局限性
尽管它们在语言处理方面表现出色
能够捕捉语法、句法
甚至能够处理多种语言
但是它们对物理世界的理解非常有限
大语言模型主要适用于离散数据
比如文本
因为文本中的单词数量是有限的
系统可以通过学习单词之间的概率关系来生成合理的文本
但是对于像视频这样的连续高维数据
大语言模型就难以处理了
因为预测视频中下一帧的所有像素
是几乎不可能的任务
这也导致了大语言模型缺乏对物理世界的常识性理解
无法像人类或动物那样理解世界的运行方式
在我们之前的一期节目中也提到
杨立昆甚至认为最聪明的大语言模型
可能都不如一只家猫聪明
因为家猫能够理解基本的物理世界知识
比如物体的位置、运动等等
而大语言模型却缺乏这种能力
当然
神经网络的架构也在不断地演进
从而适应不同类型的数据处理需求
卷积神经网络（CNN）在处理图像和音频等具有局部结构的数据时
表现出色
它的设计灵感来源于视觉皮层结构
通过卷积层中的神经元对图像的局部区域进行特征提取
多个神经元在不同位置检测相同的特征
从而实现对图像的高效识别
而Transformer架构则擅长处理序列数据
其中数据的顺序相对不重要
Transformer层的特性是对输入的置换具有等变性
即如果对输入进行重新排列
输出也会相应地进行类似的排列
但是其他方面保持不变
这种特性使得Transformer在自然语言处理领域
取得了巨大的成功
它能够处理长序列的文本数据
通过多头注意力机制等技术
捕捉文本中单词之间的复杂关系
从而实现对文本的深入理解和生成
而杨立昆一直以来则提倡一种新的架构
联合嵌入预测架构，也就是JEPA
用来解决当前人工智能系统在理解物理世界方面的局限性
JEPA的基本思想是，对于视频数据
不再试图预测视频中的每个像素
而是将视频输入编码器
生成视频的抽象表示
然后在这个抽象表示空间中进行预测
比方说
对于一段包含人物动作的视频
JEPA可以学习到人物动作与视频后续部分的抽象关系
从而进行合理的预测
通过这种方式
系统能够更好地理解视频中所包含的信息
进而学习到世界的运行方式
为实现目标驱动的人工智能
也就是System 2奠定基础
与大语言模型所代表的反应式人工智能
也就是System 1不同
目标驱动的人工智能能够进行更为深思熟虑的计划和推理
比如根据目标规划一系列动作
预测这些动作的结果
并且根据结果调整策略
这更接近人类的思维和决策过程
在人工智能的未来发展方面
杨立昆呢也有着深刻的见解
他认为开源平台将在未来的人工智能发展中占据主导地位
就像Linux操作系统在嵌入式设备和操作系统领域的广泛应用一样
开源平台具有许多的优势
比如可移植性、灵活性、安全性和低成本等
这些优势将会让它发展成为人工智能的主流趋势
随着技术的发展
人工智能系统也将逐渐具备更强的理解物理世界和规划行动的能力
这将极大地改变人类与技术的交互方式
例如
智能眼镜等设备可能会成为人们与AI交互的新工具
人们可以随时随地获取人工智能的帮助
比如在餐馆中通过智能眼镜来翻译外文菜单等等
对于印度的年轻人
杨立昆也提出了一些建议
他鼓励年轻人要攻读博士学位或硕士学位
深入学习人工智能技术
在他看来
高等学位的学习过程能够训练年轻人发明新事物的能力
确保他们所使用的方法具有科学性和创新性
在人工智能这样复杂而且技术含量高的领域
深入学习可以让年轻人更好地理解现有技术、把握技术发展的可能性和局限性
这对于他们在这个领域的发展至关重要
而对于想要在人工智能领域创业的年轻人
杨立昆则建议选择利用开源基础模型
比如LLAMA等
然后针对特定垂直应用进行微调
从而成为这个领域的专家
在垂直领域的选择上
他提到了法律、会计、商业信息、教育辅助工具、医疗保健等领域
以法律领域为例
人工智能可以协助律师进行案例分析、合同审查等工作；
在医疗保健领域
特别是在一些医疗资源相对匮乏的地区
人工智能可以通过提供医疗辅助
帮助患者初步判断症状
决定是否需要就医
大大提高医疗服务的可及性；
在教育领域
人工智能可以作为教育辅助工具
为学生提供个性化的学习资源和辅导
提高学习效率
好了
以上就是杨立昆这次访谈的主要内容了
其实过程中
杨立昆也提到了很多趣事
比如说他自己并不喜欢AI教父这个称呼
因为“教父”这个词在新泽西州
意味着你属于黑手党
另外他也调侃了自己在社交网络上非常活跃
对技术问题甚至政策问题都有着强烈的意见
这也会放大他受欢迎或者不受欢迎程度
他也坦言，在某些圈子里
自己被认为是个十足的傻瓜
让人感觉他其实是个挺耿直可爱的小老头
希望杨立昆的这些观点能带给大家一些启发
也希望大家有时间去看一下原视频
相信会有更多收获
感谢大家的观看，我们下期再见
