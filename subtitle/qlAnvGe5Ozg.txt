大家好，这里是最佳拍档，我是大飞
围棋
一项拥有数千年历史的智力游戏
因为游戏流程过于大量的不确定性和决策需求
长期以来都被认为是只有人类才能玩得来的项目
直到2015年
由Google DeepMind主导开发的ALPHAGO横空出世
利用深度神经网和蒙特卡洛树搜索
ALPHAGO在前后不到两年的时间内先后击败了李世石
柯洁等顶尖的围棋选手
成为当之无愧的世界第一棋手
在感叹AI科技发展神速的同时
人们也意识到
随着AI能做的“人事”越来越多
人与人工智能之间
在智能这个概念上的差异也越发的模糊
如今的大模型已经能够按照人们的要求去绘画
创作音乐
写小说，拍电影
在智慧这条走不完的长路上
AI追赶的脚步声在人类的背后越发的清晰
我们距离第一个拥有和人类一样智能的AI的诞生
究竟还有多远？
曾经开发出ALPHAGO的公司
DeepMind的联合创始人谢恩
莱格表示，到2028年
我们就有很大的概率能够看到AGI的诞生了
谢恩受邀参加了德瓦克什·帕特尔主持的访谈
同主持人围绕着人工智能的未来发展、技术挑战和伦理等问题
讨论了 AGI 的定义、实现路径、潜在风险以及激励机制等话题
今天大飞就来带大家看看
DeepMind的首席人工智能科学家
对于AGI有何独到的见解
谢恩·莱格（ Shane Legg ）在访谈中首先澄清了他对AGI的定义
在他看来
AGI并不是科幻电影中那些拥有近乎超能力的人造机械
而是一种能够做到、人类所能够做到的事情的机器
这个标准听上去
好像比“能够毁灭世界的超级机械”低了不少
但是在科研工作者的眼中
这一要求却是实打实的高门槛
因为“人类所能够做到的事情”本身
就是一个还没有被研究透彻的课题
研究者需要很多不同类型的测量和测试
来涵盖人类能够完成的各种认知任务
因此
制定AGI的标准将会是一项非常庞大的任务
即便是那些最聪明的头脑
也很难完全涵盖人类智能的全部可能
并且一一进行检测
出于现实情况的考虑
谢恩做出了一些妥协
他表示AGI能否实现
取决于研究者能否建立起一系列
涵盖人类所做的各种常见认知任务的测试
如果有一个 AI 系统在所有这些任务上
都能够达到人类的表现
并且人们还无法轻易找到新的认知例子
来证明机器的表现低于人类
谢恩认为，在这种情况下
尽管概念上可能还存在着机器无法做到、而人类能做到的事情
但是从实践的角度来看
AGI就已经成立了
对情景记忆的测试
就是一个可以清晰地表现出
当下AI仍然有缺陷的例子
人类拥有不同类型的记忆
比如工作记忆
用来记录最近发生的事情；
还有皮质记忆
一种存储在大脑皮层中的记忆信息
但是在这两者之间还有一个由海马体负责的系统
那就是情境记忆
这个记忆涉及到快速学习和特定信息的存储
比方说，今天我对你说的一些话
如果你明天还记得
那就是你的情境记忆在起作用
谢恩认为，因为情景记忆的能力
人类智能的样本效率
也就是利用有限的交互次数
通过合理的策略与环境交互进行数据采样
并且利用采样数据进行学习的能力
是非常强大的
相对地
尽管大语言模型在某种程度上具有样本效率
可以在上下文允许的范围内实现快速学习
但是它依然没有情景记忆的能力
谢恩表示
目前的大模型只能通过延长上下文窗口
也就是加强工作记忆的方式来补偿这一点
但是谢恩并不认为情景记忆的缺失
是大模型技术中无法解决的缺陷
在他看来
现有模型的大多数缺点都有相对明确的思路来解决
无论是关于错觉、事实、它们的记忆和学习类型
还是理解视频内容等问题
谢恩将情景记忆归因于现有大模型的底层架构的问题
他认为
眼下AI的底层架构过于依赖权重
而这些权重会在学习的过程中非常缓慢地固化
就像大脑中的激活状态会固化脑皮层中的权重与突触一样
但是
大脑之所以没有出现和大模型一样的问题
是因为它的内部并非只有一套架构
在需要的时候
大脑可以唤醒一个独立的机制
也就是情景记忆
来快速学习特定的信息
并且这个特殊机制与缓慢学习深层次的普遍性记忆并不冲突
谢恩强调，一个能被称为AGI的系统
应该是一个从构建上就同时具备这两种能力
并且可以根据情况灵活切换的系统
这也是DeepMind当下努力的方向之一
尽管眼下的AI产品依然和AGI有着不小的差距
但是谢恩依然认为
AGI的实现近在咫尺
早在2001年
他就推断AI技术将会在2020左右迎来“大爆炸”。
以当下GPT等大模型的火热程度
可以说谢恩当年的预测相当准确
根据他所说
当年的判断基于两个猜测
一是计算能力将在未来几十年内呈指数级的增长
另一个是世界上的数据量也会在未来几十年内呈指数级的增长
他认为
当计算能力和数据量都呈指数级增长的时候
高度可扩展算法的价值也会越来越高
由于价值的增加
算法会逐渐“商品化”、“金融化”。
企业会有更大的动力
去开发更可扩展的算法
来利用这些计算和数据资源
资本市场也会更倾向于投资这些高价值项目
如果公司的算法提升了数据的利用效率
那么数据的价值就会上升
这会推动更多的投资者进入这些领域
如果模型的计算性能提高
那么数据的价值也会上升
因为你可以利用更多的数据
在这个基础上，谢恩表示
如果我们能够发现可扩展的算法
那么在 2020 年
人类应该能够开始在远超人类一生所经历的数据量上
训练模型
这一点已经在GPT等大语言模型上得到了证实
谢恩称之为重大突破开始出现的时刻
而下一个里程碑式的突破
将会是多模态技术的全面发展
他表示，回顾过去
人们会想到那些只能聊天、只能处理文本的老式模型
这显得非常狭隘
而现在的模型不仅能理解你对它们说的话
还能理解图像和视频
你可以向它们展示各种东西
它们会对正在发生的事情有更深入地理解
这将使系统以更强大的方式融入世界
如果下一个里程碑也如约而至
那么到了2028年
谢恩认为
人类就会有50%的可能性会实现 AGI
当然，这只是 50%的可能性
谢恩也直言
在研究和科学中经常会遇到意想不到的问题
有时事情会比预期的更久
所以要是AGI没有在2028出现
他也不会感到意外
不过，2028年可不是什么遥远的未来
面对近在咫尺的AGI
人类社会真的做好准备了吗？
谈到AGI可能造成的安全风险和伦理问题
谢恩不认为设立一套强大的监管系统会是靠谱的解决方案
如果这些AGI系统真的非常智能
并且强大到超出当下人类的想象
那从研发周期上来看
人类是无法赶在AGI出现之前
研制出一个更加强大的监管系统的
从底层逻辑上限制AI
在谢恩看来也是毫无意义的固步自封
姑且不论AI被限制后
还能否发展出AGI
大模型内部浩如烟海的节点数据
就足以让研究人员找不到需要加以限制的函数
谢恩提到
宪法式人工智能（Constitutional AI）
试图通过限制AI逻辑能力的方式来解决这个问题
但是由于需要进行调整的节点数量过于庞大
一旦出现遗漏就会导致整个方案的失败
因此这不太可能是一个非常稳健的解决方案
比起把AGI关进笼子里，谢恩更相信
只有让AGI理解伦理
并且让它明白遵循伦理
才能带来利益的最大化
才能保证AGI不会对人类产生危害
他的思路非常有趣
既然AGI是拥有人类智能水平的AI
那么让人类遵守伦理道德的方法论
对于AGI也应该会奏效
当人们面临一个非常困难的伦理决策时
除了各种在激素促进下爆发的情绪反应
人们还会产生基于理性的利弊权衡
人们要考虑的事情有很多
“我可以做什么？
如果我采取这些行动
每个行动会带来什么结果？
”还需要从伦理角度来评估每个不同的行动
以及可能带来的结果
只有当人们尽可能思考过所有不同的可能性、行动及其后果之后
人们才能将事件产生的后果
与自身的价值观和伦理观对齐
谢恩表示
AI 系统本质上需要做同样的事情
让AI在学习的过程中
通过理性推导的方式来实现结果上的伦理对齐
谢恩坚定地认为
只要让AGI系统拥有充分思考的自由
并且给予它们足够有说服力的思考动机
AGI自然而然地就会得出需要思考的结论
并且可以使用世界模型和善意模型
来理解每个选项可能带来的结果
分析面前的选项
然后从伦理角度进行推理
谢恩将这个过程与人类教育子女的过程类比
如果家长总是使用暴力手段体罚孩子
限制孩子的自由
那么孩子在成年后反而难以正常地认识社会伦理
甚至出现心理上的疾病；
相对的，循循善诱的教育方式
更容易让孩子拥有健全的社会认知和自我人格
当然了，谢恩也承认
诱导AGI进行伦理逻辑思考的前提
是AGI本身就拥有对人类伦理的良好理解
并且具有稳健而且非常可靠的推理能力
这本身就是一个容易引发争议的问题
比方说
究竟哪些论文和书籍可以帮助它深入理解人类伦理？
这些资料至少要达到顶尖伦理学家的水平吧？
研究人员还需要决定
在这种普遍的伦理理解中
我们希望系统实际重视哪些价值观
并且希望它应用哪些伦理原则
这已经远远超越了技术问题的范畴
而是一个需要社会和伦理学等领域共同解决的问题
谢恩也承认
他不确定是否存在所谓的真正正确的最优伦理
但是他确信
可以提出一套比"末日论者"所担心的、更好的伦理标准
然后，我们需要设计系统
让它能实际上遵循这些标准
每次它做出决策时
都需要基于对世界和伦理的深刻理解
以及稳健且精确的推理
来进行伦理分析
当然，我们还希望有很多其他的措施
我们希望有人来检查这些推理过程
验证它的行为是否符合预期
谢恩并没有排除AGI从一开始
就是出于反人类的种种理由而被制作出来的可能
而对于这种情况
谢恩也表示非常无奈
因为在GPT引发AI热潮之前
AI的安全管理行业实在是人才凋敝
谢恩很早就开始担心 AGI的安全问题
甚至在 DeepMind 成立之前就已经在担心了
但是在早期
他确实很难招聘到愿意从事 AGI 安全研究的人
就连DeepMind这种背靠Google的公司
也直到2013才招聘到第一个安全研究人员
甚至都不是全职的研究员
因为他觉得AI安全研究是个“没钱途”的行当
金钱的钱
甚至他还是一个之前已经在 AGI 安全领域发表过论文的人
连专业人员都如此看待AI安全研究
谢恩当时的窘迫可想而知
但是他也没有放弃AGI安全方面的研究和投入
他自豪地说
作为第一家 AGI 公司
DeepMind一直有一个 AGI 安全团队
而且多年来一直在这个领域发表论文
好了，以上就是谢恩
莱格这次访谈的主要内容
作为市面上最早的人工智能企业
DeepMind凭借AlphaGo的强大性能
引发了社会舆论场中的第一次“AI地震”。
但是他们并没有因自满而止步于此
多年以来
谢恩和他的团队依然走在AGI研究的最前沿
并且似乎探索出了与OpenAI略有差异的技术路线
当主持人向他打听有关DeepMind正在训练的大模型与GPT有何不同时
谢恩神秘兮兮地表示不方便透露太多
但是又忍不住强调 “我们有自己的见解和独特技巧
”那么大家如何看待他对AGI的观点呢？
欢迎在评论区发表自己的看法
感谢大家的观看，我们下期再见
