大家好，这里是最佳拍档，我是大飞
作为科技行业的绝对巨头
谷歌有很多没有被披露过的核心技术
都让外界非常好奇
尤其是最核心的搜索引擎技术
这十多年来
谷歌的搜索引擎已经占据超过90%的市场份额
成为了整个互联网上最具影响力的系统
它决定了绝大多数网站的生死存亡以及网络内容的呈现形态
但是谷歌究竟是如何对网站进行排名的呢
这些具体细节从来都是「黑匣子」。
显然，作为谷歌的财富密码
也是核心技术
想让官方披露技术细节是不可能的
虽然也一直有媒体、研究人员和从事搜索引擎优化工作的人士
进行过各种猜测
但也只能说大家都是在盲人摸象
不过，今年5月份27日
搜索引擎优化行业的一名资深从业者
埃尔凡·阿兹米Erfan Azimi
向SparkToro公司的CEO兰德·菲什金Rand Fishkin
提供了一份2500页的谷歌搜索API泄露文档
揭示了谷歌搜索引擎内部排名算法的部分详细信息
而在最近宣判的谷歌反垄断诉讼中
美国的各级检察官也搜罗了大约500万页的文件
如今已经变成公开的呈堂证供
里面有很多谷歌内部的Slides
内容也非常有趣
比如有一页Slide上赫然写着
我们不懂文档，我们伪造文档
除了一些基本的能力
我们几乎不看文档
我们看人，这就是谷歌魔力的来源
不过相对来说
这些内容的时间都相对较久
所以，这些谷歌内部泄露出来的文档
以及反垄断听证会的公开文件
都并没有真正告诉我们谷歌搜索引擎排名的具体工作原理是什么
甚至连负责排名算法的Google员工都表示
由于采用了机器学习
他们也无法解释为什么某个搜索结果
会排在第一或者第二的位置
不过，就在8月13日
专门报道搜索引擎行业的新闻网站Search Engine Land发表了一篇博客
作者马里奥·费舍尔经过对100多份文件的逆向工程
首次较为完整的揭秘了
谷歌搜索排名的核心技术原理
今天我们就来看看文章究竟都说了什么
首先
作者给出了一个经过他分析之后
较为完整的谷歌搜索引擎架构图
可以看到
这是一个庞大而复杂的工程
从爬虫系统Trawler、存储库Alexandria、粗排名系统Mustang
再到过滤和细排名系统Superroot
以及负责最终呈现页面的GWS
这些都会影响到网站页面最终的呈现和曝光
接下来
我们对整个系统逐步作个拆解
看看一个新网站是如何从被谷歌发现
到最终呈现在搜索结果中的完整过程
我们都知道
当一个新网站刚刚发布的时候
它并不会立刻就被谷歌索引
那么谷歌如何收集和更新网页信息的呢？
第一步就是通过爬虫和数据采集
谷歌首先需要知道这个网站的 URL
一般是通过更新网站地图
或者主动提交网站的URL
让谷歌可以抓取到新的网站
同时，频繁被访问的页面链接
也能够更快地吸引到谷歌的注意
随后
谷歌的爬虫系统Trawler会抓取网站上新的内容
并且记录要什么时候重新访问这个URL
从而检查网站是否有更新
这是由一个称为调度器Scheduler的组件来管理的
接着
存储服务器StoreServer会决定是否继续转发这个URL
还是要将它存放到沙箱Sandbox中
谷歌之前其实一直都在否认沙箱的存在
但是最近泄露的信息表明
一些垃圾网站和低价值网站也会被放到沙箱中
然后
文档中的外部链接会通过LinkExtrator被提取出来
根据内部或者外部链接进行排序
并且用来进行链接分析和PageRank计算
而图像链接会被传输到ImageBot中
供后续的搜索使用
而且ImageBot还有图像分类的功能
能够将相同或者相似的图片
放置在一个图像容器中
Trawler爬虫还似乎会基于PageRank来调整网站的抓取频率
如果一个网站的流量更大
那么抓取的频率就会更高
网站的内容被爬虫抓取并进行简单的提取之后
就会进入到谷歌的索引系统
这个系统被称为Alexandria
在这里会为每个网页的内容分配唯一的DocID
如果出现内容重复的情况
则不会创建新的ID
而是将URL 附加到已有的DocID上
谷歌会明确的区分URL和文档
简单来说
一个文档可以由多个包含相似内容的URL构成
包括不同的语言版本
而所有这些URL都由同一个DocID进行调用
对于重复的内容
谷歌会选择在搜索排名中显示规范URL的版本
也就是常说的Canonical URL，并且
这个所谓的规范URL也不是一直不变的
而是会随着时间发生变化
有了DocID之后
谷歌会在网站的各个部分搜索出关键词
并且汇总到搜索索引Search Index中
其中
热词列表Hitlist包含了页面中所有重要的关键词
这些关键词会被优先发送到直接索引Direct index中
并且继续形成关键词的倒排索引目录
以文章作者的网页为例
由于网页中多次出现了「pencil」铅笔一词
所以在词索引word index中
DocID就被列在了「pencil」条目下
随后这个DocID会被分配一个算法
根据各种文本特征
计算出文档中「pencil」一词的IR
也就是信息检索分数
这个分数稍后会用在发布列表Posting List中
比方说
文档中的「pencil」一词会被粗体标记
并且包含在一级标题中
这些都会增加它的IR得分
在存储方面
谷歌会将重要的文档都转移到HiveMind
也就是所谓的主存系统中
并且分别采用SSD来存储需要快速访问的信息
以及使用传统HDD硬盘的TeraGoogle
来长期存储不需要快速访问的信息
值得注意的是，专家估计
在最近的AI热潮之前
谷歌几乎掌握了全球大约半数的网络服务器
这样庞大的互联集群网络
能够让数百万个主存单元一起工作
一位谷歌工程师曾经在一次会议上指出
理论上
谷歌的主存储器可以存储下整个互联网
有趣的是，存储在HiveMind中的链接
包括反向链接，似乎有着更高的权重
而存储在HDD上的URL链接则权重较低
甚至可能根本不被考虑
每个DocID的附加信息和IR信号
都会以动态的方式存储在PerDocData存储库中
其中保存了每个文档最近的20个版本
供后续调整相关性的时候使用
同时
谷歌有能力随着时间变化来评估不同的版本
所以如果你想要完全更改文档的内容
或者是主题
理论上需要创建20个过渡版本
才能完全覆盖掉旧的版本
这就是为什么恢复一个过期的域名
不会带来任何排名优势的原因
如果一个域名的管理联系人和主题内容
同时发生了变化
那么机器可以轻松地识别出这一点
这个时候
谷歌就会将所有的信号置零
这个曾经具有流量价值的旧域名
就与全新注册的域名没有什么区别了
不再具有任何优势，因此
接手旧域名并不意味着能够接手原本的流量和排名
接下来
当有人在谷歌中输入搜索关键词「pencil」的时候
QBST（Query Based Salient Terms）系统就会开始工作了
QBST系统负责分析用户输入的搜索短语
如果其中包含多个单词
就会将相关单词发送给倒排索引来检索DocID
词汇加权的过程相当复杂
涉及RankBrain、DeepRank和RankEmbeddedBERT等等系统
但是，QBST的过程对于SEO很重要
因为它会直接影响Google对搜索结果的排名
从而影响网站可以获得多少流量和可见度
经过QBST系统的处理后
相关词汇比如「pencil」，
会被传递给Ascorer做进一步的处理
Ascorer会从倒排索引中提取「pencil」条目下的前1000个DocID
按照IR得分进行排名
根据内部文件显示
这个列表被称为「绿环」green ring
在业内，也被称为发布列表
posting list
而Ascorer其实还只是Mustang排名系统的一部分
Mustang排名系统会使用SimHash算法
进行重复数据删除、段落分析、识别原创和有用内容等进一步的筛选
目标是将1000个DocID精简为10个蓝色的链接
也称为蓝环 blue ring
而这个千里挑十的工作
则由Superroot系统来负责
任务具体由Twiddlers和NavBoost执行
但是由于信息模糊
具体的执行细节并不清楚
还拿作者的「pencil」为例
相应的文档在发布列表中排名为第132位
如果没有其他系统的介入
这就将是它的最终排名
已有的各种文件表明
谷歌使用了数百个Twiddler系统
我们可以将它视为类似于WordPress插件中的过滤器
每个Twiddler都有自己特定的过滤目标
比如调整IR分数或者排名位置
之所以用这种方式设计
是因为Twiddler相对容易创建
而且无需修改Ascorer中复杂的排名算法
要知道
修改排名算法是件非常有挑战的事
需要大量的规划和编程
相反
Twiddler却可以并行或者顺序操作
也并不需要知道其他Twiddler的活动
Twiddler基本可以分为两种类型
一种是PreDoc Twiddler
可以处理几百个DocID的集合
因为它们几乎不需要额外的信息
另一种是Lazy Twiddler
它们需要更多的信息
比如来自PerDocData数据库的信息
所以也需要更长的处理时间和更复杂的处理过程
因此
通常会先让PreDoc Twiddler将发布列表缩减到更少的条目
然后再使用较慢的Lazy Twiddler
两者结合使用
可以大大的节省算力和时间
除此以外
Twiddler还有许多其他的用途
比如调整 IR分数
甚至可以做到非常精准的操控
将一个特定的搜索结果
排名到另一个结果的前面或后面
谷歌一份泄露的内部文件显示
某些Twiddler功能
只能由专家和核心搜索团队协商后使用
更有意思是
在文件中还有这么一句话
如果你认为自己了解Twiddler的工作原理
请相信我们，你不了解
甚至我们也不确定自己是否了解
还有一些Twiddler只能用来创建注释
然后将这些注释添加到DocID中
比如在新冠疫情期间
为什么你所在国家的卫生部门在COVID-19搜索中
总是能排在第一位？
那正是因为Twiddler会根据语言和地区
使用queriesForWhichOfficial
提高官方资源的搜索量和精准匹配
在Superroot中
另一个核心系统就是 NavBoost
它在搜索结果的排名方面也发挥着重要作用
Navboost主要用来收集用户与搜索结果交互的数据
特别是他们对不同查询结果的点击量
尽管谷歌官方否认将用户点击数据用于排名
但是根据联邦贸易委员会FTC披露
一封来自谷歌内部的电子邮件证实
自从2012年8月起
点击数据就会影响排名
但是这种处理必须保密
原因有两方面
首先，站在用户的角度来看
谷歌作为搜索平台无时无刻不在监视用户的在线活动
这会引起媒体对于隐私问题的愤怒
而在谷歌反垄断案的审判过程中
NavBoost系统也被频繁的提到
仅在2023年4月18日的听证会上
就提到了54次
而后来的事实也证明
搜索结果页面上的各种用户行为
包括搜索、点击、重复搜索和重复点击
以及网站或网页的流量
都会影响排名
另一个原因是
通过点击数据和流量进行评估
可能会鼓励垃圾邮件的发送者和骗子
使用机器人系统来伪造流量
进而操纵排名
不过对于这种情况
谷歌也有自己反制的方法
比方说通过多方面的评估
将用户点击区分为不良点击和良好点击
评估指标包括在目标页面的停留时间、在什么时间段查看过网页、搜索的起始页面、用户搜索历史中最近一次「良好点击」的记录等等
简单来说
对于每个在搜索结果页面中的排名
都有一个平均预期点击率CTR作为基准线
根据约翰内斯·伯伊斯Johannes Beus在今年柏林CAMPIXX大会上的分析指出
自然搜索结果的第1位会平均获得26.2%的点击
而第2位会获得15.5%的点击
如果一个CTR显著低于预期的比率
NavBoost系统就会记录下这一差距
并相应地调低DocID的排名
反之如果CTR显著高于预期
则会调高搜索结果排名
可以说
用户的点击量基本上代表了用户对结果相关性的意见
包括标题、描述和域名
这也印证了谷歌在内部Slides中提到的那句话
我们几乎不看文档
我们看人，这就是谷歌魔力的来源
最后
我们来说一下谷歌网络服务器GWS系统
它负责呈现搜索结果页面，简称SERP
包括10个「蓝色链接」，
以及广告、图片、Google地图视图、「People also ask」和其他的元素
其中Tangram系统负责处理几何空间优化
计算每个元素需要多少空间
以及有多少结果适合进行布局
然后，Glue系统会将这些元素
排列到页面适当的位置上
在最后一刻
CookBook系统还可以干预排名
这个系统包括FreshnessNode、InstantGlue和InstantNavBoost
FreshnessNode可以实时监测用户搜索行为的变化
并且根据这些变化来调整排名
确保搜索结果与最新的搜索意图匹配
而InstantNavBoost和InstantGlue会在最终呈现搜索结果之前
对排名进行最后的调整
比如根据突发新闻和热门话题调整排名等等
因此，对于SEO来说
要想取得高排名
除了优秀的文档内容以外
还得加上正确的措施
尤其是要了解用户的搜索意图
通过优化搜索结果的标题和描述
来提高点击率
好了
以上就是对谷歌搜索引擎核心组件的基本介绍了
除了这些内容以外
还有一点出人意料的是
就是搜索结果实际上会受到人为评分的影响
我们一般都以为谷歌的搜索结果是系统自动评分的
但是实际上
谷歌在全球范围内还有数千名外包的质量评估员
负责为谷歌评估搜索结果
以及对新算法或过滤器进行上线前的测试
虽然谷歌表示，他们的评分仅供参考
不会直接影响排名
但是实际上
他们的评分和投票的确对排名产生了极大的间接影响
评估员们通常会在移动设备上进行评估
从系统接收网站的URL或者搜索短语
并且回答一些预设的问题
例如，他们会被问到
这篇内容作者和创作实践是否清晰？
作者是否拥有这方面主题的专业知识？
这些答案都会被存储起来并用于训练机器学习算法
让算法能够更好地识别高质量、值得信赖的页面
以及不太可靠的页面
也就是说
人类评估者提供的结果会成为深度学习算法的重要标准
而谷歌搜索团队创建的排名标准反而没那么重要
那么你可以想象一下
什么样的网页会让人类评估者觉得可信？
如果某个网页包含作者的照片、全名和LinkedIn链接
通常就会显得令人信服
反之
缺乏这些特征的网页会被判定为不那么可信
接着
神经网络会把这一特征识别为关键因素
经过至少30天的积极测试运行
模型可能开始自动将这个特征用作排名标准
因此
具有作者照片、全名和LinkedIn链接的页面可能会通过Twiddler机制获得排名提升
而缺乏这些特征的页面则会出现排名下降
另外谷歌指出
许多没有被点击的文档可能也很重要
当系统无法进行推断的时候
文档会被自动发送给评估员并生成评分
此外，泄露的文件显示
在评估员的相关术语中
可能对文档的评估存在一个黄金标准
而且有一个或者多个Twiddler系统
可能会将符合「黄金标准」的DocID
推进搜索结果的排名前十
同样，在Google泄露的搜索API中
也可以看到名为EWOK的质量评级平台参数
这也印证了人类质量评估员的评估结果
会对搜索结果带来一定的影响
在文章的最后
作者提出了一些SEO的关键要点
比如流量来源的多样化、建立品牌和域名知名度、评估隐藏内容、增强页面结构等等
但是我觉得最核心的
依然就是我们刚刚提到的
要充分了解用户的搜索意图
通过优化搜索结果的标题和描述
来提高链接的点击率
由此可见，谷歌搜索的SEO
也早不再是那些老掉牙的技术了
用户点击才是王道
谷歌关注的也不再只是网页内容的新旧
而是用户搜索行为的变化趋势
强烈建议对于搜索引擎和SEO感兴趣的观众
去阅读一下原文
相关的参考链接我也会放到视频简介里
那大家对于这次谷歌搜索引擎的分析有什么看法呢
欢迎在评论区留言
感谢大家的观看，我们下期再见
