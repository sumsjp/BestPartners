大家好，这里是最佳拍档，我是大飞
今天我们来聊一个非常硬核
但是对于一般人来说可能不一定有用的话题
那就是如何搭建一个10万个H100 GPU的集群
说实话，大飞我没机会
也没钱搭建这么大的集群
自然也不是什么经验的分享
这个视频的内容主要来源于SemiAnalysis的一篇文章
好在咱们频道的观众卧虎藏龙
正好有一位是英伟达搭建万卡集群的
所以内容提前给他看过
以求尽可能的真实和准确
话不多说，我们来看看
搭建一个10万张的H100 GPU集群
都需要考虑什么
首先，我们来做个简单的估算
一个10万卡集群
每年的耗电量大约为1.59太瓦时
按照美国电力的标准费率
每千瓦时0.78美元来计算
每年的用电成本就达到了1.24亿美元
那么10万卡的H100集群有多大能力呢
为了方便大家理解
我们拿OpenAI的GPT-4做个对比
GPT-4的训练使用了大约2.15乘以e的25次方 BF16 FLOPS
也就是21.5万亿的ExaFLOPS
在大约2万个A100上进行了90到100天的训练
峰值吞吐量只有6.28 ExaFLOPS
如果我们现在使用10万个H100来代替A100
那么峰值将飙升至198 FP8 或者99 FP16 ExaFLOPS
增加了31.5倍
如果使用FP8
那么一个10万卡H100的集群
只需要4天就能训练完GPT-4
我们都听说过AI的三大基础设施
数据、算法和算力
很多人会以为其中算力的门槛最低
只要有钱有资源，买到足够多的芯片
算力短缺就不是问题
但是事实绝非如此
构建一个超大的算力集群
绝对比一掷千金要复杂得多
自从GPT-4发布以来
下一代能力更强的大语言模型迟迟憋不出来
其中一个很重要的原因
就是几乎没有组织能够大规模地增加单个模型的计算量
像Gemini Ultra、Nemotron 340B和Llama 3这些、与GPT-4计算量相近甚至更高的模型
也是因为使用了较差的集群架构
导致无法进一步地释放能力
那么
在这些科技巨头搭建10万卡GPU集群的过程中
究竟会面临哪些挑战呢？
首先要面对的就是电力挑战
10万卡集群所需关键IT部件的总功率
大约为150兆瓦，相比之下
美国最大的国家实验室超算El Capitan的关键IT功率
只有30兆瓦
五分之一，可以说是相形见绌
而在这么庞大的功率中
GPU本身的耗电实际上只有不到一半
根据官方参数
每张H100的功率为700W
但是服务器上还有CPU、网卡（NIC）、供电单元等设备
加起来功率约为575W
除了H100服务器
集群中还需要部署一系列的存储服务器、网络交换机、CPU节点、光纤收发器和许多其他设备
约占IT功耗的10%。
由于目前还没有任何一栋数据中心的大楼
有能力部署150兆瓦功率的设备
因此，已建成的10万GPU集群
通常是分布在整个园区中
而非单座大楼
由于可用的数据中心有限
xAI甚至选择将田纳西州孟菲斯的一家旧工厂
改造为数据中心
又因为服务器分布在整个园区而非一栋大楼里
而光纤收发器的成本与传输距离成正比
所以联网的成本无形中就又会增高
像多模SR和AOC收发器
只支持最长大约50米的传输距离
长距离的单模DR和FR收发器
虽然能够可靠地在500米到2千米的范围内传输信号
但是成本是前者的2.5倍
而园区级别的800相干光收发器的传输距离
虽然可以超过2千米
但是价格更贵，要高出10倍以上
所以H100的小型集群
通常只敢使用多模收发器
通过一层或者两层的交换机
以400G的速度将每个GPU连接在一起
如果是大型集群
那么就需要增加更多层的交换机
光纤设备也会极其昂贵
因此，在大型集群的园区中
每栋大楼会包含一个或者多个pod
由多模收发器或者较为廉价的铜缆相连
形成一个「计算岛」。
每个计算岛之间再通过长距离收发器互连
这样的话
岛内的带宽较高，但是岛间带宽较低
其次就是并行化挑战
在较大参数的训练中
一般会有3种不同类型的并行化
分别是数据并行（data parallelism）、张量并行（tensor parallelism）和流水线并行（pipeline parallelism）
数据并行是其中最简单的并行方式
也就是每个GPU拥有模型权重的全部副本
并且分别保存一部分数据
前向计算过程中，每个GPU独自工作
梯度更新的时候再将所有GPU计算出的梯度相加
一起更新，因此在三种方式中
数据并行对GPU间通信的要求最低
然而，这种方案要求
每个GPU都有足够的内存来存储整个模型的权重、激活函数和优化器状态
像GPT-4这种级别的大语言模型
参数规模可以达到1.8万亿
需要占据10.8TB的内存
显然无法全部都塞到一个GPU里
为了克服内存的限制
就有人提出了张量并行
也就是让神经网络中每一层的权重和计算
都分布在多个GPU上
一般会覆盖掉全部的隐藏层
这样在每一层的自注意力、前馈网络和层归一化等操作中
都需要设备间进行多次归约
你可以把它想象成
在每一层的前向计算中
所有GPU都在协同工作
仿佛组成了一个巨型GPU
目前在NVLink上通常使用8个张量的并行等级
相当于每个GPU的内存消耗
降低到了原来的八分之一
不过
由于这种方式中设备间需要频繁通信
因此要求高带宽、低延迟的网络环境
除了张量并行
GPU内存不足的另一种解决方案
就是流水线并行
顾名思义
这种方案是将前向计算看成一个流水线
每个GPU负责其中的一环
也就是网络中的一层或者几层
完成计算后
再将结果传递给下一个GPU
虽然流水线并行对跨设备通信的要求也很高
但是没有张量并行那么苛刻
而在实际应用中
为了最大限度地提高模型的FLOP利用率
三种并行模式通常会结合使用
形成3D并行
其中，因为张量并行对通信要求最高
所以主要应用在同一服务器内的多个GPU上
再在同一计算岛内的节点之间
使用流水线并行
而由于数据并行的通信量最小
而且岛与岛之间的联网速度较慢
因此跨计算岛的时候使用数据并行
第三，就是网络方面的挑战
在网络拓扑上
由于存在我们刚才说的种种并行方式
所以数据中心在进行网络拓扑设计的时候
就需要同时考虑到使用的并行化方案
如果采用胖树拓扑结构（fat-tree topology）
那么每两个GPU之间都要用最大带宽相连
这就需要4层交换，成本十分高昂
因此
没有哪个大型GPU集群会部署全胖树架构
取而代之的方案是
搭建具有全胖树架构的计算岛
同时减少岛间的带宽
比如，Meta的上一代GPU集群架构
使用了3.2万张芯片
总共有8个计算岛
岛与岛之间部署了全速带宽
然后在顶部另外加置了一个收敛比为7比1的交换层
这样，岛与岛之间的联网速度
就是岛内的七分之一
在设备部署上
由于GPU的部署有多种网络方式
包括前端网络、后端网络和扩展网络（NVLink）
所以要针对不同的并行方案加以考虑
对于张量并行来说
NVLink网络可能是唯一足够快的网络
后端网络通常可以轻松处理大多数其他类型的并行
但是如果存在收敛比带宽的孤岛
通常只能采用数据并行
需要注意的是
谷歌在多TPU pod训练时只使用前端网络
他们被称为ICI的计算结构
最多只能扩展到8960个芯片
每个水冷机架之间包含64个TPU
需要使用昂贵的800G光纤和光路交换机进行连接
因此，谷歌必须让TPU的前端网络
比大多数GPU的前端网络更为强大
才能弥补这一不足
此外，前端网络的全局归约操作
必须能够依据各计算岛之间的网络拓扑结构
而且前端网络还负责加载数据
这在多模态数据的发展下
对前端网络的要求呈指数级增长
为了解决这个问题
另一种方法是使用4层InfiniBand网络
采用7:1的收敛比，4个pod
每个pod有24576个H100
采用无阻塞3层系统
这种方式为将来增加带宽提供了更大的灵活性
因为在两栋大楼的交换机之间增加光纤收发器
要比升级集群中每个机箱的前端网络网卡
容易得多
不过，这种方式的缺点在于
需要额外的交换机和收发器
而4层Infiniband的网络非常昂贵
为了提高全对全集体通信（all-to-all collective communication）的性能
英伟达推荐使用轨道优化设计（rail optimized design）
让每台H100服务器连接到8个不同的交换机
这样每个GPU只需要跳一次交换机
就能与更远的GPU通信
但是轨道优化设计的缺点则是
必须连接到不同距离的不同叶交换机
而不是将一个机架中间的交换机
靠近服务器中的所有8个GPU
此外
轨道优化设计的初始布线对于数据中心技术人员来说非常耗时
因为每个链路的两端距离长达50米
而且不在同一个机架上
于是
也有一些客户会放弃轨道优化设计
选择中间机架设计（Middle of Rack design）
将板上交换机直接放在机架中间
让每个GPU都能使用DAC铜缆
利用DAC铜缆运行温度更低、耗电更少、成本更低
而且可靠性更高的优势
减少网络链路的间歇性瘫痪和故障
十万卡GPU集群要面临的第四个挑战
就是可靠性与恢复
由于当前的模型训练都是同步进行
所以可靠性就成为巨型集群最重要的运行问题之一
最常见的可靠性问题包括GPU HBM ECC错误、GPU驱动器卡死、光纤收发器故障、网卡过热等等
为了保持较短的平均故障恢复时间
数据中心必须在现场保留热备用节点和冷备用组件
发生故障的时候
最好的办法不是直接停止训练
而是换上已经开启的备用节点
继续训练
事实上，大部分服务器宕机的情况
都可以通过重启修复
但是有些时候则需要技术人员对设备进行现场诊断和更换
在最好的情况下
数据中心技术人员只需要几个小时
就能修复损坏的GPU服务器
但是很多情况下
损坏的节点可能需要几天时间
才能够重新投入使用
在训练模型的时候
需要经常将检查点存储到CPU内存或者NAND SSD
以防出现HBM ECC等错误
发生错误时
必须重新加载模型和优化器的权重
再继续训练
也可以利用一些容错训练技术来处理GPU和网络故障
不过
频繁地备份检查点和容错训练技术
也会降低系统的整体FLOP利用率
因为集群需要不断暂停
将当前权重保存到持久内存或者CPU内存中
通常
每100次迭代才会保存一次检查点
这意味着每次重新加载
最多会丢失99步有用的工作
在一个10万卡集群上
如果每次迭代耗时2秒
那么如果在第99次迭代失败
最多会损失229个GPU日的工作
另一种恢复故障的方法就是内存重建
让备用节点通过后端结构
从其他GPU进行RDMA复制
后端GPU的速度约为400Gbps
每个GPU有80GB的HBM内存
因此复制权重大约需要1.6秒
采用这种方法，最多只能损失1个步骤
因此只需要2.3个GPU日的计算时间
再加上从其他GPU HBM内存RDMA复制权重的1.85个GPU日
大多数领先的人工智能实验室都采用了这种技术
但是许多小型公司仍然在坚持使用前者
也就是从检查点重启处理所有故障的方式
在网络故障方面
最常见问题之一是Infiniband/RoCE链路故障
由于收发器数量较多
即使每个网卡到最底层交换机链路的平均故障率为5年
在一个全新的、正常运行的集群上
发生第一次作业故障也只需要26.28分钟
如果不使用内存重建
那么在10万卡的GPU集群中
由于光纤故障而重新启动所花费的时间
将比模型实际进行计算所花费的时间更多
由于每个GPU都直接连接到了ConnectX-7网卡上
所以在网络架构层面没有容错能力
用户必须在训练代码中处理故障
从而增加了代码库的复杂性
这也是当前英伟达和AMD的GPU网络结构的主要挑战之一
即使是一个网卡故障
这个GPU就无法与其他的GPU通信
由于大语言模型在节点内使用的是张量并行
如果一个网卡、收发器或者是一个GPU故障
整个服务器就会宕机
目前呢
英伟达已经注意到了这个致命的问题
他们的应对策略呢
是增加一个专用的RES引擎
通过分析芯片级数据
比如温度、恢复的ECC重试次数、时钟速度、电压等指标
来预测芯片可能的故障
并且提醒数据中心技术人员进行主动维护
此外，在开始训练任务之前
每个芯片的RAS引擎会执行全面的自检
例如运行已知结果的矩阵乘法
从而来检测静默数据损坏SDC
第五个挑战在于成本优化方面
这里面有几个点
首先是用Cedar Fever-7网络模块来代替ConnectX-7网络卡
使用Cedar Fever模块的主要好处是
它只需要4个OSFP插槽
而非8个，并且允许在计算节点端
使用双端口2x400G收发器
这可以让每个H100节点
连接到叶交换机的收发器数量
从8个减少到4个
计算节点端连接GPU到叶交换机的收发器总数
从98304减少到49152
由于GPU到叶交换机的链接减少了一半
这也有助于延长首次作业失败的时间
根据估计
每个双端口2x400G链接的平均故障时间为4年
因此改用Cedar Fever模块
将使首次作业失败的估计时间
从26.28分钟延长至42.05分钟
其次是采用Spectrum-X来代替InfiniBand
InfiniBand的优势在于支持SHARP
网络内缩减
它能将每个GPU需要进行的发送和写入次数
减少2倍
因此将理论网络带宽增加2倍
但是InfiniBand NDR Quantum-2交换机的端口容量较低
只有64个400G端口
而每个Spectrum-X以太网的SN5600交换机
有128个400G端口
Spectrum-X的主要优势是得到了NVIDIA库
比如NCCL的一级支持
老黄也会将你推到他们新产品线的首批客户队列中
不过，如果采用Spectrum-X
就必须加价购买Nvidia LinkX产品线中的收发器
因为其他收发器可能无法正常工作
或者通不过英伟达的验证
第三
为了避免给英伟达支付高昂的费用
许多客户都会选择部署基于博通Broadcom Tomahawk 5的交换机
每个基于Tomahawk 5的交换机与Spectrum-X SN5600交换机一样
拥有128个400G端口
此外
你可以从任何供应商购买通用的收发器和铜缆
并且进行混合使用
基于交换机和通用收发器的成本
Tomahawk 5相比Nvidia InfiniBand便宜得多
相比Nvidia Spectrum-X也更具成本效益
坏消息是，你需要有足够的工程能力
为Tomahawk 5修补和优化NCCL通信集群
毕竟后者只针对Nvidia Spectrum-X和Nvidia InfiniBand进行了优化
好消息是
如果你已经为这个十万卡的集群花了40亿美元
那么也应该有足够的工程能力
来修补NCCL并进行优化
好了，最后我们总结一下
搭建一个10万个H100 GPU的集群
总资本支出大约为40亿美元
可以有以下四种方式
1、采用4层InfiniBand网络，包含32
768个GPU集群
轨道优化，7:1收敛比
2、采用3层Spectrum X网络，包含32
768个GPU集群
轨道优化，7:1收敛比
3、采用3层InfiniBand网络，包含24
576个GPU集群，非轨道优化
用于前端网络的集群间连接
4、3层Broadcom Tomahawk 5以太网网络
包含32
768个GPU集群
轨道优化，7:1收敛比
可以看到
方案1比其他方案贵了1.3到1.6倍
方案2虽然提供了更大的集群、更高的集群间带宽和相似的成本
但是需要更多的电力
而方案3则会严重降低并行方案的灵活性
因此，综合来看
基于博通Broadcom Tomahawk 5的32k集群
搭配7:1的收敛比
是最具成本效益的选项
这也是多家公司选择构建类似网络的原因
好了
现在大家应该基本了解如何搭建一个10万H100的集群了
剩下的就是怎么搞定40亿美元了
感谢大家观看本期视频
我们下期再见
