大家好，这里是最佳拍档，我是大飞
在过去的6年里
英伟达的市值翻了45倍
从2019年的大约1000亿美元
一路飙升到如今的4.5万亿美元
成为全球市值最高的公司之一
很多人都想知道
英伟达的爆发式增长到底靠的是什么呢？
是GPU芯片的算力优势？
还是AI浪潮的风口红利呢？
其实，有一个关键事件被很多人忽略
那就是2019年3月
英伟达以70亿美元收购了以色列公司迈络思（Mellanox）
10月28日
在红杉资本（Sequoia Capital）的欧洲100活动上
英伟达现任首席技术官（CTO）迈克尔·卡根（Michael Kagan）接受了专访
而他正是迈络思的联合创始人
也是当年推动两家公司整合的核心人物
在这次专访里
他阐述了英伟达的增长逻辑、AI算力的底层挑战
以及未来技术的发展方向
今天我们就来回顾一下这场专访
看看为什么说
没有迈络思，就没有今天的英伟达？
按照惯例
我们先介绍一下迈克尔·卡根（Michael Kagan）
他是一名拥有40年行业经验的老兵
早在1983年
他就加入了英特尔（Intel）
参与了i860、奔腾MMX（Pentium MMX）等经典微处理器的架构开发
其中，他主导设计的i860XP芯片
还实现了首次在初代硅片上成功启动Linux系统的里程碑
而且计算性能远超当时的i486芯片
2000年左右
他联合创立了迈络思（Mellanox）
专注于高性能网络技术
2019年迈络思被英伟达收购后
他在2020年5月正式出任英伟达CTO
一直以来，卡根都强调一个观点
没有软件支持的芯片只是昂贵的硅片
这也成了他推动英伟达从芯片制造商
向AI基础设施架构定义者转型的核心思路
好了，回到专访的内容
卡根在一开场就提到了英伟达的企业文化
双赢
他说
英伟达从不追求在存量市场里抢蛋糕
而是想和客户一起把蛋糕做大
换句话说，英伟达的成功
不是建立在对手的失败上
而是建立在客户的成功上
举个例子
英伟达早期靠着GPU切入游戏市场
后来发现GPU的并行计算能力
特别适合AI任务
于是推出了CUDA生态
这个生态不只是卖芯片
而是给客户提供从硬件到软件的完整解决方案
比如科研机构用CUDA训练AI模型
企业用CUDA部署推理服务
客户的AI项目成功了
自然会持续采购英伟达的产品
形成正向循环
这种客户成功即自身成功的逻辑
其实就是英伟达能在AI时代
站稳脚跟的底层文化支撑
而迈络思的收购
就是把这种双赢逻辑推向更大规模的关键一步
卡根在专访里反复强调
收购迈络思不是简单的补全产品线
而是解决了英伟达的一个致命短板
那就是GPU算力的扩展问题
在收购之前
英伟达的GPU扩展主要靠NVLink技术
但是这种技术只能实现纵向扩展（Scale-up）
也就是在单个节点内整合多块GPU
比如，一台服务器里放8块GPU
通过NVLink连接
让它们像一块超大GPU一样工作
但是如果客户需要训练千亿、万亿参数的AI模型
单节点的GPU数量根本不够用
比如训练GPT-4这样的模型
可能需要上万块GPU协同工作
这就需要横向扩展（Scale-out）
也就是把成百上千个节点连成一个大集群
而横向扩展的核心，就是网络技术
如果节点之间的网络速度慢、延迟高
哪怕每个节点的GPU性能再强
整个集群的效率也会被拉垮
迈络思的技术正好解决了这个问题
卡根举了个具体的例子
在收购迈络思之后
英伟达可以通过迈络思的网络技术
把36台双GPU计算机连接起来
让它们在软件层面呈现为一个单一的GPU
这是什么概念？
相当于把原本分散的小算力单元
整合成了一个超大算力单元
而且，这种整合不是硬凑
而是通过软硬件协同优化
比如迈络思的InfiniBand网络技术
能够让不同节点的GPU之间直接交换数据
不用经过CPU中转，延迟大大降低；
再配合英伟达的软件调度
整个集群的算力可以无缝扩展
客户不用修改太多代码
就能把原本在单节点上跑的任务
迁移到上千节点的集群上
为什么这种扩展能力在AI时代如此重要呢？
因为AI的性能需求增长实在太快了
卡根在专访里给出了一组惊人的数据
AI模型的规模和复杂性
每三个月就会翻一番
这意味着，要满足客户的需求
英伟达每年需要提供10到16倍的性能提升
而大家熟悉的摩尔定律
说的是芯片性能每两年翻一番
这个速度
其实早就跟不上AI的需求了
如果只靠升级单块GPU的性能
比如增加晶体管数量、提升制程工艺
根本不可能满足每年10倍的增长
所以，必须靠架构创新
也就是通过网络把海量GPU连接起来
用集群算力对抗AI的性能需求
而迈络思的网络技术
就是实现这种架构创新的核心工具
说到这里，咱们需要再深入聊一下
网络在AI集群里的作用
很多人觉得
网络不就是传数据的管道吗？
只要带宽够大就行
但是卡根告诉我们
AI集群的网络需要满足三个条件
高带宽、极低延迟、以及极小的延迟抖动（Jitter）
前两个好理解
带宽大才能快速传输大量数据
延迟低才能减少计算单元的等待时间
但是延迟抖动为什么重要呢？
卡根举了个反面的例子
如果一个网络的平均延迟是1毫秒
但是有时候是0.5毫秒
有时候是2毫秒
这种波动就会导致集群里的GPU步调不一致
有的GPU算完了数据
等着其他GPU传结果
有的GPU还在等数据
整个计算过程就会断断续续
这种情况下
哪怕你把任务拆分成1000份
分给1000块GPU
最后实际效率可能还不如分给10块GPU
而迈络思的网络技术
恰恰在低抖动上做了极致优化
卡根说
英伟达的网络不追求峰值性能的噱头
而是追求稳定的性能
无论多少个节点同时通信
延迟的波动范围都非常小
这样一来
客户才能把任务拆分成更多细小的部分
分给更多GPU并行处理
真正发挥集群的规模优势
比如，训练一个千亿参数的模型
用普通网络可能需要10天
用英伟达的网络可能只需要3天
效率提升非常明显
也正因为如此，卡根才说
本质上，网络决定了集群的性能
除了连接GPU的计算网络
迈络思还有一项关键技术
BlueField DPU（数据处理单元）
卡根在专访里把它称为数据中心的操作系统计算平台
我们大概可以这么理解
传统的数据中心里
服务器的CPU既要运行应用程序
比如AI推理，又要处理基础设施任务
比如说网络的流量调度存储管理等等
又要处理基础设施
比如网络流量调度、存储管理等等
这就像一个人既要干核心工作
又要干后勤工作
效率肯定不高，而且还不安全
而BlueField DPU的作用
就是把后勤工作从CPU里剥离出来
让CPU专注于核心工作
它可以单独处理网络流量、存储管理、安全防护等基础设施任务
相当于给数据中心配了一个专职后勤团队
这样一来，就有了两个好处，第一
CPU的资源不再被浪费
应用程序的运行效率更高；
第二
基础设施和应用程序在硬件层面被隔离
被攻击的风险大大降低
哪怕DPU层面有问题
也不会影响到CPU上的应用数据
这种隔离思路
其实是未来大型数据中心安全和效率的关键
当然，构建大规模GPU集群
不只是解决扩展和网络问题这么简单
还有很多现实挑战
卡根在专访里提到了两个核心难题
分别是组件失效和网络设计
我们先来说组件失效
大家可能都知道
电子设备的可靠性通常用几个九来形容
五个九，也就是99.999%，
就意味着每年的故障时间只有大约5分钟
但这是针对单个组件的
如果一个集群有100万个组件
比如GPU、服务器、网络设备
那么所有组件同时正常工作的概率几乎为零
卡根说，在大规模集群里
故障不是会不会发生的问题
而是一定会发生的问题
所以
必须从软硬件层面进行容错设计
比如
软件层面要有自动故障检测功能
一旦发现某个GPU失效了
能立刻把它的任务分配给其他GPU；
硬件层面要有冗余设计
比如关键的网络设备有备份
某个设备坏了，备份能立刻顶上
这些设计看起来不起眼
但却是大规模集群能稳定运行的基础
再来说网络设计
卡根强调
AI集群的网络和普通数据中心的网络
完全是两个概念
普通数据中心的网络
主要服务于松散耦合的微服务
比如一个电商平台
用户登录、商品搜索、下单支付是不同的微服务
它们之间的通信比较零散
对延迟的要求不高
而AI集群的网络
需要服务于紧密耦合的单应用
比如一个AI训练任务
要在10万台机器上同时运行
数据需要在这些机器之间频繁交换
任何一个节点的网络拥堵
都会影响整个任务的进度
所以
AI集群的网络需要更精细化的控制
比如
能够根据任务的优先级调度流量
给AI训练任务分配更多带宽
以及能够实时监控网络状态
一旦发现拥堵
立刻调整数据传输路径
这种定制化的网络设计
是普通数据中心网络无法满足的
聊完了集群的技术挑战
卡根和主持人又聊到了AI领域另一个热门话题
训练和推理的差异
很多人会觉得
AI的算力需求主要来自训练
毕竟训练一个大模型需要上万块GPU跑好几天
但是卡根在专访里纠正了这个观点
推理的算力需求
可能比训练还大
为什么这么说呢？
这就涉及到训练和推理之间的区别
训练是让AI学会东西的过程
比如给模型喂大量图片
让它学会识别猫和狗
这个过程包含两个阶段
前向传播和反向传播
最后还要把多个节点的参数合并
而推理是让AI用学会的东西解决问题的过程
比如把一张新图片输入模型
让它判断是不是猫
早期的AI推理，主要是感知类的任务
比如图像识别、语音识别
这些都是一次性的
也就是输入一个数据
得到一个结果，计算量不大
但是生成式AI出现之后
推理的性质就变了
比如ChatGPT，你输入一个提示词
它生成回答的时候，不是一次性算完
而是逐个token生成
每生成一个字或者一个词
都要把之前的内容重新输入模型
重新计算一次
这种递归式的推理
计算量自然就上去了
而且，推理还有两个不同的阶段
预填充（Pre-fill）和解码（Decode）
预填充阶段是处理提示词
模型需要先理解这个提示的含义
把相关的上下文加载到内存里
这个阶段需要大量计算
属于计算密集型；
解码阶段是生成回答，逐个token输出
这个阶段主要是把内存里的上下文数据读出来处理
属于内存密集型
针对这两个阶段的特点
英伟达还专门推出了不同的GPU SKU
有的SKU优化了计算性能，适合预填充；
有的SKU优化了内存带宽，适合解码
而且
这些SKU都保持了相同的编程接口
客户可以根据自己的负载灵活切换
不用修改代码
这就是可编程性带来的优势
更重要的是
一个模型通常只会训练一次
但是会被推理无数次
比如ChatGPT
训练一次可能需要几万块GPU跑几周
但是训练完成后
全球上亿用户每天都在使用它进行推理
哪怕每个用户每天只生成100字的回答
总的计算量也远远超过了训练时的计算量
卡根甚至开玩笑说
他妻子现在跟ChatGPT聊天的时间
都比跟他聊得还多
这虽然是玩笑
但也反映了推理需求的广泛性
所以，未来AI算力的竞争
不只是训练的竞争，更是推理的竞争
除了单个数据中心的挑战
卡根还提到了跨数据中心的问题
现在很多大型企业和科研机构
会把AI任务分散到多个数据中心
比如一个数据中心在北美
一个在欧洲，这样既能避免单点故障
又能靠近用户降低延迟
但是跨数据中心有一个天然的限制
那就是光速
光在光纤里的传播速度大约是每秒20万公里
跨大陆的数据中心之间
延迟差异可能达到几十毫秒
如果两个数据中心之间的网络
用大缓冲区来缓解延迟差异
又会导致延迟抖动
因为缓冲区里的数据多了
传输时间就会不稳定
影响AI任务的效率
为了解决这个问题
英伟达推出了Spectrum-X交换机
这种交换机的核心特点是提供遥测数据
它能够实时监控网络的拥堵情况、延迟变化
然后把这些数据反馈给终端节点
终端节点再根据这些数据
自动调整通信模式
比如
如果发现和某个远程数据中心的延迟很高
就会减少数据传输的频率
避免缓冲区堆积；
如果发现本地数据中心的网络很通畅
就会增加传输频率
通过这种动态调整
就能在跨数据中心的场景下
既保证低延迟，又避免抖动
当然
数据中心的规模也不是越大越好
它还受到两个关键因素的限制
能源和散热
卡根说
现在大型AI数据中心的功耗已经达到了GW级
差不多能满足100万户家庭的用电需求
未来如果要建十GW级的数据中心
能源供给就是一个大问题
而且，功耗越高，散热压力就越大
传统的风冷技术
已经无法满足高密度GPU集群的散热需求
因为GPU工作时会产生大量热量
如果散热不及时
芯片会降频，性能会下降
所以
英伟达现在已经全面转向液冷技术
液冷的散热效率要比风冷高很多
风冷只能带走芯片表面的热量
而液冷可以直接接触芯片
把热量快速带走
用液冷技术
一个机架的功率可以从传统风冷的20千瓦
提升到100千瓦以上
计算密度大大增加
卡根预测，未来的数据中心
会越来越依赖于液冷技术
甚至可能出现全液冷的数据中心
整个机房都用液体来散热
才能做到既高效又节能
在专访的后半部分
卡根还谈到了英伟达与英特尔（Intel）的合作
很多人觉得
英伟达和英特尔是竞争对手
毕竟英特尔做CPU，英伟达做GPU
都在争夺数据中心的市场
但是卡根不这么认为，他说
把加速计算和通用计算融合起来
能为两家公司开辟新的市场
具体来说
英特尔的x86架构在通用计算领域有很强的优势
比如运行操作系统、处理事务性数据
而英伟达的GPU在加速计算领域有优势
两者结合
就能服务于那些既需要通用计算
又需要加速计算的场景
比如，制造业的AI质检系统
一方面需要英特尔CPU来处理生产线上的实时数据
另一方面需要英伟达GPU运行AI模型
检测产品是否有缺陷
这种互补合作，不是争夺存量市场
而是开拓新市场
正好符合英伟达的双赢文化
说到文化
卡根还分享了迈络思并入英伟达后的文化整合过程
他说
迈络思和英伟达的文化相似又互补
两家公司都重视创新
都以客户为中心
但是迈络思更专注于网络技术的细节
英伟达更擅长构建生态和平台
为了确保整合成功
卡根当时的首要任务是让迈络思的员工感到安心
毕竟，一家以色列的本土公司
突然变成全球巨头的一部分
员工很容易有被冷落的感觉
黄仁勋（Jensen Huang）当时也明确表示
网络技术是英伟达未来的核心
迈络思的团队是关键
最终，整合的效果超出了预期
迈络思的员工留任率达到了85%-90%，
英伟达在以色列的员工总数甚至增长了两倍多
还计划建造新的园区
卡根说
这可能是科技史上最成功的合并之一
在专访的最后
卡根还聊了聊AI的长远价值
AI不只是会改变商业
还能够推动科学的进步
他以地球二号气候模拟器为例
这个模拟器用AI技术
能够精准模拟今天的人类活动对50年后全球气候的影响
这相当于把历史变成了一门实验科学
科学家可以调整不同的变量
观察未来的变化
而不是只能靠过去的数据推测
此外
AI还能帮助人类发现新的物理定律
卡根说，人类发现物理定律的过程
本质上是观察现象、归纳规律
而AI在处理海量数据、归纳规律方面有天然优势
比如，在粒子物理领域
AI可以分析粒子碰撞产生的海量数据
找出人类尚未发现的规律
甚至可能推翻现有的理论
至于大家关心的AI的增长速度会持续多久
卡根提出了一个自己的观点
他认为英伟达正在把产品发布周期
从两年缩短到一年
而且每一年都实现指数级的性能提升
关注点也在从单芯片性能
转向整个系统的性能
虽然这种增长无法预测具体能持续多久
但是他可以肯定的是
AI会像电力一样，彻底改变世界
卡根还做了一个很形象的比喻
史蒂夫·乔布斯（Steve Jobs）曾经把电脑比作思想的自行车
因为它能帮人类更高效地思考，而AI
就是思想的宇宙飞船
它能让人类的思考突破时间和空间的限制
涉足那些之前想都不敢想的领域
好了
以上就是英伟达CTO迈克尔·卡根（Michael Kagan）在红杉资本专访中的主要内容了
简单总结一下，在卡根看来
英伟达的成功
不只是靠着GPU芯片的硬实力
更是靠着双赢的文化、迈络思带来的网络技术突破
以及对AI算力需求的深刻理解
未来，AI的竞争一定会越来越激烈
但是无论如何
基础设施的重要性都只会越来越高
毕竟，没有强大的算力和网络
再先进的AI算法也无法落地
感谢大家收看本期视频，我们下期再见
