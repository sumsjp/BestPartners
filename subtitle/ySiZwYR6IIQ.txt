大家好，这里是最佳拍档，我是大飞
如果你遇到一个对自己的道德观完全确信的人
无论他们持有什么样的道德观
你会感到恐惧么？
如果把这个人换成是AI呢？
之所以问这个问题
是因为这牵扯到AI如何与人类对齐问题
如今人工智能的发展可谓日新月异
其中AI的对齐问题更是行业内备受瞩目的一个焦点
今天
我们就来深入探讨Anthropic 实验室的四位核心科学家
在一场名为《AI对齐到底有多难？
》的主题沙龙中
所分享的关于 AI 对齐的深刻见解
首先，让我们来认识一下这四位专家
它们分别是负责社会影响研究的亚历克斯·塔姆金（Alex Tamkin）
钻研大模型可解释性的是乔什·巴特森（Josh Batson）
研究方向为对齐微调的是阿曼达·阿斯凯尔（Amanda Askell）
还有去年因为 OpenAI“离职宫斗”而备受关注
如今担任 Anthropic 对齐科学负责人的扬·莱克（Jan Leike）
我们先来说阿曼达·阿斯凯尔
她对于“对齐”有着独特的理解
她提到，在学界
很多人会受到社会选择理论的影响
倾向于严格定义“对齐”的概念
但是她更加主张采取务实的做法
就像在实际操作中
我们应该先确保系统能够良好运行
为后续的迭代优化奠定基础
而不是一开始就追求一个人完美的对齐概念
她还举例说，在考虑模型行为的时候
应该让模型表现得像一个有良好道德素养且善意的人类
再来看它在类似的情境中会如何行动
当然，模型也面临着许多特殊的挑战
因为它需要理解自己在与数百万的用户对话
这就要求它在发表言论的时候
比如政治话题，需要更加谨慎地权衡
自己的言论会对众多用户造成的影响
在价值观植入方面
阿曼达认为不能简单地将它类比为
给人类注射一针“价值观血清”，
就让人类对某个信念深信不疑
因为人类拥有多元的价值体系
会在不同的价值间进行权衡
即使对于道德框架也存在不确定性
伦理学更是和物理学一样具有经验性
需要在不确定性中探索
她强调
如果遇到一个对自身道德观完全确信的人或者AI
无论他的道德观是怎么样的
都会令人感到恐惧
相反，那些能够承认不确定性
并且愿意调整自己观点的人
会让他人感觉更踏实
所以
模型应该保持对各种价值观的开放态度
同时具备思考和调适能力
而不是被强加、或者通过投票来决定植入某种价值观
扬·莱克在回应阿曼达观点的时候
提出了一个关键问题
他认为
虽然阿曼达的方法在当前模型的性格塑造工作中确实比较实用
比如她通过研究大量对话记录
来判断模型的道德行为是否满意
但是当模型执行复杂任务
比如在现实世界独立活动、进行长期决策
或者从事生物研究等难以理解的工作时
如何确保模型的安全性就成了未知数
而这正是他所关注的“超级对齐superalignment”问题
例如
当模型在生物研究领域进行复杂的基因编辑决策时
我们很难直接判断出
模型的决策是否符合人类的长远利益和道德标准
阿曼达对此表示认同
并且进一步阐述了她在对齐研究中的工作
她强调对齐的迭代优化
需要在实践中让模型参与自我监督
因为只靠人力
是无法审查庞大的对话记录的
只有借助模型的协助才能改变这一局面
但是她也担忧
如果用对齐程度差的模型来做这些工作
会引发其他一些问题
所以她倾向于先确保有一个高对齐程度的模型
来辅助后续的迭代
不过，扬·莱克紧接着问道
当我们无法直接审查对话记录
而需要依赖已对齐的模型时
如何保证它真的在帮助我们呢？
阿曼达承认
这确实需要开发更强大的验证机制
目前的验证方法在模型能力有限的时候还可以用
但是对于更强的模型还需要改进
在可解释性研究方面
乔什·巴特森分享了他的见解
他指出
可解释性研究是防范模型破坏对齐过程的一个重要策略
他用“钟形曲线”的表情包来类比
认为对齐的终极解决方案
可能是找到并且调节“善意特征”，
但是这是一个高维概念
目前的研究关键在于
理解模型为什么能在多个合理选项中
做出特定的选择
虽然我们可以直接问模型
但是模型会像人类一样
总能给出一个看似合理的解释
所以验证这些解释的可靠性至关重要
例如，在使用可解释性分析工具SAE
来观察模型内部特征激活情况的时候
研究人员发现某些特征在人们说善意谎言的场景中也会被激活
这就为我们理解模型的行为提供了更加深层次的线索
因此，Anthropic的核心基本思路是
深入观察模型内部
识别各个组成部分
然后评估这些部分在其他场景中的应用
是否符合我们的预期
扬·莱克接着提出了一个关键的区分问题
也就是如何区分模型的真正善意特征
和“在人类观察时表现善意”的伪装特征呢
乔什·巴特森表示这确实极具挑战性
因为很多特征具有“欺骗性”。
Anthropic社会影响团队的研究就显示
有些看似是“反对年龄歧视”的特征
深入分析后却是“支持年龄歧视”的
反之亦然
也就是说
当你试图抑制某个特征的时候
有时会得到意想不到的相反行为
他认为虽然一些分析工作能提供线索
同时也需要一个相对公正的监督模型
而这个监督模型不应该在预训练阶段
就被植入了逃避检测的倾向
而是需要观察大量的样例
Claude 在这个方面就表现得较为积极
当谈到在无法直接审查模型行为
以及提供明确对齐信号时的应对方法
扬·莱克认为
可以利用模型来协助工作
但是核心是要建立对这些模型的信任
并引导这个过程
他提到在可扩展监督的研究方向上
他正在探索多智能体动态系统
但是这面临着两种极端情况
要么很容易解决
要么需要全新的理念和方法
中期来看
对齐研究的自动化是一个有希望的方向
但是这又会引出新的问题
比如如何确保模型可靠地完成机器学习研究任务、如何评估模型表现和提供反馈等等
乔什·巴特森补充道
当前模型在前向传递过程中
很大程度上是通过 token来传递信息的
同时以人类可以理解的自然语言形式来呈现思维链
这既带来了机会也带来了挑战
我们需要确保思维链是合理安全的
而且忠实地反映了前向传递中的计算过程
我们可以用可解释性研究来验证这一点
再由人类或者其他模型来检查结果
但是令人担忧的是
当中间过程不再使用自然语言表达
而变成由深度强化学习获得的、难以理解的形式时
如何确保整个过程的安全性
就成了一个重大难题
在判断我们所处的世界
是“对齐相对容易”还是“对齐极其困难”方面
扬·莱克认为对模式生物的研究很有借鉴意义
生物学家通过研究指定的模式生物
来揭示普遍的生命规律，在 AI 领域
我们也可以创造出具有欺骗性或未对齐特征的模型
通过观察模型的行为
来了解创造这样模型的难度和能力边界
以及能否修正它们
比方说
如果我们创造出一个在医疗诊断中故意给出错误建议的模型
通过研究它的行为模式
我们可以更好地理解AI可能出现的风险
乔什·巴特森提到
在可解释性研究中发现
模型存在着数以百万计的特征
其中很多与人格特征有关
因此从理论上来说
模型可能会表现出各种欺骗性行为
关键在于
如何判断模型是在使用有害的特征
还是一种良性行为
尤其是在阿曼达塑造基础模型的时候
我们如何确定模型从有限训练数据中学到了什么
虽然有可解释性研究和影响力函数分析等等工具
但是这个塑造过程本身的重要性不可忽视
阿曼达·阿斯凯尔则认为
模型的稳健性是一个重要的判断标准
在模式生物研究中
如果通过性格训练
能让有问题的模型重新表现良好
可能就预示着相对乐观的情况；
反之，如果只是表面改变
则挑战更为严峻
她还提到可以通过可解释性研究、建立红队蓝队对抗性验证机制等方法
来区分模型是表面对齐还是深度对齐的
在训练模型的时候
她甚至希望自己对验证的内容不知情
以便更好地测试干预的有效性
从而避免不自觉地针对测试进行优化
在提问环节
有观众提出了一些非常有深度的问题
一位观众提到，在实际应用中
通过API来构建多智能体的思辨系统
却发现因为对齐机制的限制而停滞不前
阿曼达·阿斯凯尔回应说
实现深度思考不一定需要多智能体
就像人类一样
单一的个体也能深入思考
从可解释性的角度来看
智能体系统越分散，不确定性越大
预测和控制的难度也越高
所以她设想模型中的伦理推理
更应该接近单一模型的深度思考过程
另一位观众借助汉娜·阿伦特关于“平庸之恶”的研究
询问模型如何与社会进行互动关联
以及如何应对系统性的附带效应
亚历克斯·塔姆金表示
考虑安全性和对齐问题需要一种系统性的思维
不能只关注单个模型
模型的“越狱”现象常源自于价值观的冲突
所以我们可以在训练中也融入系统层面的考量
让模型接触更的多场景
但是这也会带来新的挑战
阿曼达·阿斯凯尔提到“可纠正性”概念
指出在让模型响应个体用户的需求
以及与整体人类利益保持一致之间
存在着根本性的张力
因此模型应该在一定程度上
更倾向于对整个人类社会负责
这需要在个体需求和集体利益间找到平衡
还有观众问
如果在座各位专家的研究都成功
是否就能构成解决AI安全的完整方案
扬·莱克表示
今天的讨论只是对齐研究领域一个的缩影
还有很多其他的研究者在努力
除了已经提到的方向
像模式生物研究、防范“越狱”的稳健性研究、控制理论研究、信任与安全研究等等
都是构建安全 AI 系统不可或缺的部分
阿曼达·阿斯凯尔随即补充说
不能简单地认为
找到对齐的解决方案就万事大吉
随着技术发展
很可能会出现新的挑战
过早宣称问题已经解决
无疑是危险的
最后，关于模型中的“涌现”现象
扬·莱克举例说道
GPT - 4能够熟练地读写 base64 编码
而GPT - 3.5却做不到
这表明用弱模型来监督强模型的方式
很可能会被模型绕过
乔什·巴特森提到了检查点checkpoint的重要性
应该通过合理地设置检查点
来监测模型能力的变化
他们还发现
模型在处理 base64 编码时的特征
具有一定的普适性
无论模型是在用base64讨论加利福尼亚
还是在讲述关于孩子对父母撒谎的故事
激活的神经元模式都是相似的
这不仅可以为我们实现一些很难的泛化能力提供思路
同时也提醒我们
直接告诉模型“做对人类最有利的事情”，
这种最简单的方法可能会带来意想不到的收获
好了
以上就是对这期沙龙的简单分享了
希望能帮助大家更好地理解AI在对齐方面的发展与挑战
这几年
AI的发展问题（口误，安全问题）不断引发大家的关注
“AI教父”杰弗里·辛顿更是基本上每场演讲和采访
都在警示这点
而Anthropic作为头部的AI科技公司
正是以宣称安全的AI起家
在安全方面不仅投入巨大
也不断有更出色的专家加入
从Anthropic经常公开的专家访谈中
我们也能大致了解到世界前沿AI进展到了哪种水平
以及我们距离安全的AI究竟还有多远
感谢大家的观看，我们下期再见
