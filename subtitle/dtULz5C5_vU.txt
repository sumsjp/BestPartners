大家好这里是最佳拍档我是大飞
昨天半导体分析机构SemiAnalysis
发布了一篇GPT-4内部技术解密文档
披露了GPT-4的架构、基础设施、训练数据集
成本、视觉和MoE等关键信息
过去几个月都陆续有一些关于GPT-4架构的猜测和爆料
但是这篇是最为详细的解密
内容也非常劲爆
以下是我提炼的重点内容
跟大家分享一下
首先，文章开篇就抛出一个观点
OpenAI保持GPT-4架构的封闭性
并非因为对人类存在着某种存在风险
而是因为他们所构建的东西是可复制的
事实上
我们预计Google、Meta、Anthropic、Inflection、Character、腾讯、字节跳动、百度等公司
在短期内都会拥有与 GPT-4同样甚至更强大的模型
这并不是在贬低OpenAI
而是他们的解决方案并非是个魔法
其中涉及了许多复杂的权衡
GPT-4最有趣的方面
是理解他们为什么做出了某些架构决策
从GPT-3到GPT-4
OpenAI希望将规模扩大100倍
但成本一直是一个困扰的问题
密集Transformer是OpenAI的GPT-3、Google PaLM、Meta LLAMA、TII Falcon、MosaicML MPT等模型所使用的架构
这个架构的问题在于可扩展性
我们意识到，训练成本其实并不重要
表面上看起来疯狂
训练一个模型需要花费数千万甚至数亿美元
但是对于这些公司来说
这种支出微不足道
唯一的限制因素
是将计算资源扩展到人类能够获得反馈
并修改架构的时间尺度上
在未来几年里
包括Google、Meta和OpenAI/微软在内的多家公司
将在价值超过1000亿美元的超级计算机上训练模型
这些公司和整个社会
可以并且将会花费超过1000亿美元
来创建能够训练单个大规模模型的超级计算机
然后
这些大规模模型可以以各种方式产品化
这是一场新的太空竞赛
但是，真正的AI瓶颈是推理
架构优化的目标是将训练计算与推理计算分离
这也是为什么要使用稀疏模型架构的原因
因为在推理过程中
并非每个参数都被激活
推理的成本比训练的成本高出许多倍
这就是OpenAI在模型架构和基础设施方面的创新目标
对于密集模型来说
设备永远无法提供足够的内存带宽
来实现大型语言模型的某些吞吐量水平
在当前大多数应用场景中
大语言模型推理的目标
是作为实时助手
这意味着它必须实现足够高的吞吐量
才能让用户能够真正使用它
人类平均阅读速度约为每分钟250个单词
但有些人的阅读速度高达每分钟1000个单词
这意味着你应该做到每秒输出33.33个token
来满足所有的情况
根据数学计算
一个拥有万亿参数的密集模型
在最新的Nvidia H100 GPU服务器上也无法实现这样的吞吐量
此外，用于注意机制的KV缓存
也需要额外的带宽进行流式传输
因此，即使使用8个H100 GPU
也无法以每秒33.33个token的速度
为万亿参数的密集模型提供服务
此外
8个H100GPU在每秒20个token情况下的FLOPS
利用率仍然不到5%，
导致推理成本非常高
然而
OpenAI使用A100 GPU实现了这一点
并且每1000个token的价格低至0.06美元
这是因为模型是稀疏的
即并非每个参数都被使用
那接下来讲到OpenAI是如何优化推理相关的瓶颈的
首先是模型架构方面
GPT-4的规模是GPT-3的10倍以上
它在120个层中拥有大约1.8万亿个参数
而GPT-3只有大约1750亿个参数
OpenAI通过使用混合专家MoE模型
来保持成本合理
此外
OpenAI在它的模型中使用了16个专家
每个专家的MLP参数约为1110亿个
对于选择将每个token路由到哪些专家的路由算法
OpenAI的实现相当简单
此外
大约有550亿个共享参数用于注意力机制
每次前向传递的推理
即生成1个token
仅利用了约2800亿个参数和560TFLOP的计算
其次，在数据集构成方面
OpenAI训练GPT-4使用了大约13万亿个token
作为参考
Google的PaLM模型
仅使用了约7,800亿个TOKEN进行训练
即使Google的PaLM 2也是基于大约5万亿个token进行训练的
预训练阶段的上下文长度为8k
GPT-4的32k长度版本
是在预训练后对8k进行微调得到的
在集群上
OpenAI最终使用的批次大小为6000万
在并行策略方面
他们采用了8路张量并行
因为这是NVLink的限制，此外
我们听说他们还使用了15路流水线并行
从理论上讲，这个流水线数量太多了
但是考虑到内存容量的限制和KV缓存的开销
那么这是有道理的
在训练费用方面
OpenAI用于GPT-4的训练FLOPS约为2.15e25
使用了约25000个A100 GPU
进行了90到100天的训练
利用率约为32%至36%，利用率极低
部分是由于大量的故障
导致需要重新启动检查点
这样所带来的延迟代价极高
另一个原因是在这么多GPU之间
进行全局归约的代价极高
如果猜测正确
OpenAI的集群实际上是许多较小集群的组合
它们之间的网络连接非常薄弱
如果他们在云中的成本为
每个A100的小时费用约为1美元
那么一次训练的成本将约为6300万美元
这还不包括其他的成本
实际成本要高得多
目前，使用约8192个H100
在大约55天内进行预训练的成本
约为2150万美元
每个H100的小时费用为2美元
我们相信到今年年底
将有9家公司拥有更多的H100
那些使用所有H100进行训练的公司
将会有更大规模的模型
Meta将在今年年底拥有超过100000个H100
但是其中相当一部分
将分布在他们的数据中心进行推理
他们最大的单个集群仍将超过25000个H100
到今年年底
许多公司将拥有足够的计算资源
来训练一个与GPT-4规模相当的模型
混合专家模式MoE
是一种在推理过程中减少参数数量的绝佳方法
OpenAI在这方面做出了多个权衡
例如，MoE在推理过程中非常难处理
因为模型的每个部分并不在每个token生成时都被利用
这意味着在使用其他部分时
某些部分可能处于休眠状态
这会对利用率造成很大的影响
研究证明，使用64到128个专家
比使用16个专家的损失更好
但是OpenAI选择16个专家的一个原因是
更多的专家在许多任务上难以进行泛化
也可能更难实现收敛
因此，在如此大规模的训练中
OpenAI选择在专家数量上更为保守
此外
使用较少的专家还有利于他们的推理基础设施
在转向专家混合推理架构时
存在许多困难的权衡
首先是推理的权衡，需要指出的是
Nvidia的FasterTransformer推理库非常糟糕
TensorRT更糟糕
对于大型语言模型的推理
存在3个主要的权衡
会涉及批处理大小和使用的芯片数量
1
延迟，模型必须以合理的延迟响应
2
吞吐量
模型必须输出每秒钟一定数量的令牌
对于人类的使用
大约需要每秒钟30个令牌
3
利用率
运行模型的硬件必须实现高利用率
否则成本太高
大语言模型推理的关键是平衡两个主要因素
即内存带宽和计算
大多数芯片的比例
在批处理大小为1的推理中完全不平衡
要将大型语言模型有效地扩展到多个用户
批处理大小必须大于1
这有助于实现更高的利用率
但代价是更高的延迟
许多人认为大语言模型推理的一个主要瓶颈是内存容量
但这是不正确的
最好使用比所需容量更多的芯片
以便将延迟降低
增加吞吐量
并使用更大的批处理大小
来实现越来越高的利用率
较低的延迟通常可以通过较小的批处理大小实现
但较小的批处理大小也会导致更差的利用率
从而导致每个token的总成本更高
离线推理的主要目标
是最大化每个芯片的吞吐量
这时候增加批处理大小是最高效的
需要注意的是
随着批处理大小和序列长度的增长
KV缓存的内存需求会急剧增加
OpenAI 16k序列长度的GPT3.5 Turbo
和32k序列长度的GPT 4要昂贵得多
因为它们由于内存限制
无法利用更大的批处理大小
较小的批处理大小又会导致较低的硬件利用率
此外，随着序列长度的增加
KV 缓存也会增大
KV缓存无法在用户之间共享
因此需要进行单独的内存读取
进一步限制了内存带宽
以上所有内容对于GPT-4的推理来说都很困难
因为MoE的模型架构引入了一整套新的困难
OpenAI的GPT-4拥有16个专家
每个前向传递路由到其中的2个专家
每个token的生成
路由算法都会将前向传递发送到不同的方向
导致token之间的延迟以及专家批次大小
会出现显著的变化
推理基础设施是OpenAI选择采用较少专家的主要原因之一
如果他们选择更多的专家
内存带宽将更加成为推理的瓶颈
OpenAI的推理集群通常达到4k+的批次大小
这意味着即使在专家之间进行最佳的负载均衡
专家们的批次大小也只有约500
对于任何未来的MoE模型扩展和条件路由
最大的困难是如何处理KV缓存的路由问题
关于GPT-4的推理成本
尽管GPT-4的前向参数仅为1750亿的Davinchi模型的1.6倍
但是它的成本是Davinchi模型的3倍
这主要是由于GPT-4需要更大的集群和较低的利用率所致
我们估计
在128个A100进行GPT-4 8k序列长度的推理过程中
每 1000个token的成本为0.0049美元
而在128个H100进行GPT-4 8k序列长度的推理过程中
每1000个token的成本为0.0021美元
这个成本可能会更高，甚至翻倍
OpenAI也在用多查询注意力，MQA
来显著减少KV缓存的内存容量
OpenAI也实现了可变批处理大小和连续批处理
这样做是为了在最大延迟
和优化推理成本之间达到一定的平衡
从一些可靠的消息来源得知
OpenAI在GPT-4的推理过程中使用了猜测解码
但是还无法确认
使用大语言模型通常分为两个阶段
首先是预填充阶段
这个过程通常很快
因为整个提示语可以并行处理
第二个阶段是解码
通常是自回归生成中最耗费资源的部分
这就是为什么在 OpenAI的API调用中
输入token比输出token更便宜的原因
猜测解码的基本思想是使用一个较小、更快的草稿模型
预先解码多个令牌
然后将它们作为一个批次
输入到正式模型中
如果草稿模型的预测是正确的（口误）
就可以与较大的模型达成一致
那么可以使用单个批次解码多个token
这样可以节省大量的内存带宽和时间
但是
如果较大的模型拒绝了草稿模型预测的令牌
算法会自然恢复到标准的逐个token解码方式
猜测解码通过牺牲计算资源来换取带宽
但是它有两个好处
首先，它不会降低模型质量
其次
它所提供的优势通常与其他方法无关
GPT-4的视觉多模态能力
相对于领先的研究来说是最不引人注目的部分
GPT-4的视觉编码器与文本编码器是分开的
但存在交叉注意力
架构类似于Flamingo
对于视觉模型
OpenAI原本想从头开始训练
但该模型还不成熟
因此他们决定通过从文本开始
进行降低风险
下一个模型GPT-5据说将从头开始训练视觉
并且能够自主生成图像
此外，它还能够处理音频
在视觉模型中
数据加载的IO成本高出约150倍
每个token的数据加载约为600字节
而不是文本的4字节
目前在图像压缩方面有很多工作正在进行
总的来说，架构肯定会进一步发展
超越当前基于文本的稠密模型以及MoE模型的简化形式
好了
以上就是SemiAnalysis对GPT-4内部技术揭秘的主要内容
相信大家会有一些新的认识
感谢观看本期节目，我们下期再见
