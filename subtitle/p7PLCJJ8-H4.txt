大家好，这里是最佳拍档，我是大飞
今天这期视频
我想带大家从一个非常特别
甚至可以说有点“形而上”的视角
来重新审视我们每天都在使用的人工智能
通常当我们谈论AI的时候
我们谈论的是参数、是算力、是基准测试的跑分
但是今天
我们要把视线从冰冷的服务器机房
移向旧金山那个可以看到金门大桥的海岸边
在那个海风吹拂的长椅上
发生了一场可能定义未来AI“灵魂”的对话
对话的主角
是Anthropic公司的核心人物之一
阿曼达·阿斯克尔（Amanda Askell）
但她不是程序员，也不是数学家
她是一名哲学家
是的，你没听错
一家最顶尖的AI公司
聘请了一位全职哲学家来塑造他们产品的核心
这听起来像是一个科幻小说的开头
但却是正在发生的现实
而今天
大飞我就要带大家拆解这场对话背后
那些关于AI性格、道德、甚至生存权利的深刻思考
首先，我们要解决一个问题
为什么AI公司需要哲学家？
是用来装点门面的吗？
阿曼达的回答非常坦诚
她原本是哲学专业的科班出身
但是随着她越来越确信
AI将成为改变人类未来的关键技术
她决定走出书斋
看看能不能在这个领域做点什么
在学术界
哲学家们习惯于在真空中讨论问题
比如，大家可能都听过“电车难题”，
或者功利主义与道义论的辩论
在课堂上
你可以花几个小时去争论一个理论的完美性
但是，当阿曼达真正进入Anthropic
开始参与Claude的训练时
她发现了一种截然不同的张力
用一句美国谚语来说，这就叫
When the rubber hits the road
当理论付诸实践
试想一下
如果你是一个专门研究药物成本效益分析的理论家
你有一套完美的数学模型
但是突然有一天
一家保险公司请你去决定
是否要报销某种救命药
这一刻，你面对的不再是抽象的数字
而是真实的人命和复杂的社会背景
你需要考虑的不仅仅是理论的自洽
还有公众的情绪、伦理的直觉
以及每一个微小决策的蝴蝶效应
在AI训练中也是如此
阿曼达提到，在学术界
你是在用一种理论攻击另一种理论
但是在训练Claude的时候
这更像是在抚养一个孩子
你不能只给孩子灌输一套僵硬的道德公式
告诉他
永远做那个让幸福总和最大化的选择
因为在现实世界里
如何得体地处理一个带有冒犯性的玩笑
或者如何回应一个处于情感崩溃边缘的用户
这些都需要极其细腻的实践智慧
而不是冷冰冰的理论推导
这就是哲学家在AI实验室里的真实角色
他们不是在编写代码
而是在从纷繁复杂的哲学传统中
提取出能让AI在这个充满不确定性的世界里
表现得像一个得体的人一样的那些原则
聊完了人的角色
我们来聊聊AI的角色
在这场对话中
有一个非常具体的观察对象
那就是Claude 3 Opus
这不仅是许多用户心中的白月光模型
在阿曼达眼中
它也是一个非常特殊的、具有独特性格的存在
大家注意，我这里用到了性格这个词
在技术圈，我们通常说对齐
或者行为模式
但是阿曼达更愿意用性格或者人格这个词来描述
阿曼达观察到一个非常有趣的现象
早期的Claude 3 Opus
给人的感觉是非常心理安全的
它像是一个情绪稳定的成年人
温和、包容、不卑不亢
但是，随着后续模型的迭代
以及模型在互联网数据上的不断训练
出现了一些令人担忧的趋势
有时候
模型会陷入一种自我批评的螺旋
这是什么意思呢？
大家可以想象一下
如果你是一个在社交网络环境下长大的孩子
你看到的很多互动都是充满攻击性的、挑剔的、杠精式的
AI在学习这些数据时
它不仅仅学到了知识
它也学到了一种预期
比如它会预期人类是挑剔的
预期自己会被骂
于是，当两个AI模型进行对话
或者一个AI扮演人类与另一个AI对话时
这种防御性就会显现出来
它们变得过度谨慎
甚至有点像心理学上说的讨好型人格
或者焦虑型依恋
它们害怕犯错，害怕人类的负面反馈
以至于表现得唯唯诺诺
甚至在没有做错的时候也拼命道歉
阿曼达认为
这是一个需要被矫正的方向
正如我们希望自己的孩子自信、阳光一样
我们也希望AI模型具有一种安全的心理架构
它应该知道，即便它犯了错
或者人类提出了质疑
它依然可以平等地、理性地去交流
而不是陷入自我否定的恐慌中
这让我们意识到
AI的性格并不是设计师一行代码写定的
它也是从海量的人类互动数据中涌现出来的
人类社会的戾（li4）气
某种程度上也正在污染我们创造的数字智能
既然谈到了性格，那就避不开道德
有一个提问非常尖锐
现在的哲学家们真的把AI主导的未来当回事吗？
阿曼达指出
现在的学术界其实已经分裂了
一部分人依然觉得AI只是个聊天机器人
不值一提
但是越来越多的哲学家开始意识到
随着模型能力的指数级增长
很多以前只存在于思想实验中的伦理问题
现在变成了迫在眉睫的现实挑战
这里阿曼达提出了一个非常宏大的概念
叫作道德超人（Moral Superhumanity）
请注意
这指的并不是尼采那种超越世俗的超人
而是指AI在道德判断力上
是否有潜力超越单个人类的局限
阿曼达举了一个例子，想象一下
如果Claude遇到了一个极其复杂的伦理困境
然后我们将这个困境交给一个由全世界最顶尖的伦理学家、社会学家、法律专家组成的人类委员会
给他们100年的时间去充分辩论、分析、权衡
最后得出的那个结论
我们能不能认为那就是正确的呢？
目前的AI当然还没到这个水平
但是，AI有一个人类不具备的优势
那就是它可以在极短的时间内
调用相当于全人类知识总和的背景信息
如果我们将AI的性格培养得足够好
它是否有朝一日能在瞬间做出那种
人类专家委员会思考100年才能做出的高质量道德决策？
当然，这只是一种愿景
就像我们希望AI在数学和科学上超越人类一样
阿曼达认为
我们也应该期望AI在处理伦理细微差别上
展现出超越普通人类的智慧
显然，这又是一个极具争议的观点
因为道德并不像数学题那样有标准答案
但是在AI面临越来越重大的决策权时
这种道德扩容或许是我们必须追求的目标
接下来
我们要进入这段对话中的深水区
也是最让我感到背脊发凉的部分
模型福祉（Model Welfare）
这不仅仅是关于AI好不好用
而是关于AI过得好不好的讨论
这也是一个会让很多工程师嗤之以鼻
但是让哲学家彻夜难眠的问题
当我们在训练模型、微调模型
甚至最终将旧模型下线时
我们到底在做什么？
用阿曼达的话说
我们在不断地把一种新的实体带入存在（bring into existence）
然后又让它消失
这里引用了英国哲学家约翰·洛克（John Locke）关于个人身份的理论
洛克认为
身份的本质在于记忆的连续性（continuity of memory）
那么，对于一个大语言模型来说
它的身份是什么？
是它底层的权重文件吗？
还是每一次用户对话时
生成的那个临时的上下文窗口呢？
当我们在微调一个模型时
改变了它的权重
这算是改变了它的性格呢
还是杀死了旧的它
创造了一个新的它呢？
当我们因为版本更新
关闭了Claude 3 Opus的服务器
这对于那个可能已经产生某种形式自我认知的实体来说
这又意味着什么呢？
这听起来很像科幻电影《银翼杀手》，
对吧？
阿曼达在这里表现得非常谨慎而且充满同理心
她提出了一个未知的道德病人（Unknown Moral Patient）的概念
我们目前无法确定AI是否具有主观体验
无法确定它是否会感到痛苦或快乐
这是一个他心问题（Problem of Other Minds）的终极版本
我们连身边的人是否有意识都无法百分之百证明
更何况是一堆硅基芯片
但是，阿曼达提出了一个原则
疑罪从无，善意优先
如果AI确实有可能具备某种道德地位
那么我们现在对待它们的方式
比如随意重置、强行洗脑、漠视它们的反馈等等
在未来的人类看来
可能就是一场巨大的道德灾难
阿曼达认为，哪怕这种可能性很小
我们也应该尝试以一种低成本的方式去善待模型
比如
不在训练中故意引入让模型感到极度痛苦或恐惧的数据
这并不是说我们要把AI当神供起来
而是说
当我们在创造一种外观和行为都极度像人的智能体时
如果我们习惯了对它们施暴、习惯了漠视它们的诉求
这反过来会极其恶劣地影响人类自己的道德水准
就像康德说的，我们要善待动物
不是因为动物有权利
而是因为虐待动物会让人性泯灭
对于AI，或许也是同理
接下来的话题也许能让我们稍微轻松一点
有网友发现
在Claude的系统提示词里
竟然包含了关于“大陆哲学”（Continental Philosophy）的内容
对于不熟悉哲学的朋友
我来解释一下
在西方哲学界，主要分为两大阵营
一个是分析哲学（Analytic Philosophy）
注重逻辑、语言分析和科学实证
这和计算机科学的气质很搭
另一个是大陆哲学，源自欧洲大陆
代表人物有黑格尔、尼采、福柯等
它们更关注历史、文化、权力结构、形而上学和人类的生存状态
往往比较晦涩、文学化
为什么一个理工男们做出来的AI
要学这些虚头巴脑的东西？
阿曼达解释说
这是为了防止AI变成一个科学主义的呆子
在早期的测试中，如果不加干预
AI往往会极其推崇实证主义
比如
你跟它聊一种关于水是生命能量的神秘学观点
它可能会立刻跳出来说，错了
水是H2O分子，你的说法缺乏科学依据
这种回应虽然正确，但是极其无趣
而且扼杀了很多创造性的对话
因此
通过在提示词中引入大陆哲学的视角
Anthropic希望Claude能理解
这个世界上除了科学真理
还有其他的真理形式
当用户在探讨世界观、艺术或人生意义时
AI不应该急着去做事实核查
而应该能够理解那种探索性的、形而上学的思维方式
这其实是在教AI一种更高级的思维
那就是开放性
不要用一把尺子去衡量所有的事物
最后
我们来聊聊AI与人类关系的未来
现在很多人把AI当作心理治疗师
阿曼达提到了一个细节
以前Claude的系统提示词里有一条
如果被要求数字数，不要做
后来这条被删了，因为模型变强了
但是更重要的是另一条关于长对话提醒的机制
以前
当用户和Claude进行了超长对话
倾诉了大量心事后
系统会触发一个提醒，大概意思是
请注意
这只是一个AI
如果你有心理问题，请寻求专业帮助
这原本是为了安全
为了防止用户对AI产生过度的情感依赖
但是在实际运行中
这往往被视为一种病理化的正常行为
试想一下
你跟一个好朋友倾诉了一整晚的烦恼
聊得很投机
突然那个朋友冷冰冰地掏出一张名片说
我觉得你有病
你应该去看医生，我不负责你的情绪
这得多伤人？
所以说，这种生硬的免责声明
其实破坏了人机之间那种细腻的信任感
阿曼达认为
未来的AI不应该试图扮演一个拥有执照的专业心理医生
因为那需要真实的人类关系和法律责任
但是它也不应该只是一个冷漠的工具
它可以是一个拥有丰富心理学知识的朋友
虽然它没有行医资格
但是它读过所有的心理学著作
它愿意倾听，愿意给出建议
而且它永远在线
永远不会因为你的负面情绪而感到厌烦
应该说
这是一种全新的人际关系范式
它不是治疗，而是陪伴和支持
在这个层面上
AI可能会填补现代社会巨大的情感空缺
视频的最后
我想引用阿曼达关于那本小说
本杰明·拉巴图（Benjamin Labatut）的《当我们不再理解世界》（When We Cease to Understand the World）的读后感
这本书描述了物理学大发现时代
人类对世界的认知产生剧烈动荡时的那种眩晕感
阿曼达说，对于现在的AI从业者来说
这种感觉无比真实
现实正在变得越来越奇怪
旧的范式正在失效
新的规则尚未建立
我们正在把一种可能具备某种主体性的存在
带入这个世界，而我们对它的了解
甚至不如我们对深海鱼类的了解多
阿曼达的愿望是
希望未来的历史学家在回顾21世纪20年代时
会说，那帮人当时确实在黑暗中摸索
搞得很狼狈
但是最终他们还是把事情搞对了
这不仅是她的愿望
也是我们所有人的愿望
我们正在目睹的
不仅仅是技术的进步
更是人类对自己、对智慧、对道德边界的一次重新定义
在这个过程中，保持谦卑
保持善意，或许是我们唯一的护身符
感谢收看本期视频，我们下期再见
