大家好，这里是最佳拍档，我是大飞
这两天
几乎整个AI圈的目光都被OpenAI发布的Sora模型吸引了去
但是其实还有件事也值得关注
那就是Google继上周官宣Gemini 1.0 Ultra后
火速推出了下一代人工智能模型Gemini 1.5
可惜的是
终究抵不过Sora的耀眼光芒
还是当了把AI界的汪峰
但是不管怎样
大飞我觉得还是有必要介绍一下Gemini的这个版本
让大家也都了解下行业的相关进展
Google的首席执行官桑达尔·皮查伊（Sundar Pichai）携首席科学家杰夫·迪恩（Jeff Dean）等一众高管
在推特同时宣布了Gemini的这一重大更新
其中最亮眼的就是它在跨模态超长文本理解能力上的大幅突破
简单来说
Gemini 1.5能够稳定处理的信息量高达100万个tokens
如果我们想更直观去感受一下这个长度
大概相当于1小时的视频、11小时的音频、超过3万行代码或者70万个单词
而在Gimini的这个版本发布之前
世界上公开可用的大语言模型中
最大的上下文窗口来自于Claude 2.1的20万个tokens
同时GPT-4是12.8万个tokens
Gemini 1.0 Pro是3.2万个tokens
因此这一次Gemini 1.5已经在窗口长度上成功碾压了所有其他大模型
成功的把上下文窗口长度的天花板提升了一个数量级
Google还表示
他们在内部研究中已经成功测试了高达1000万tokens
相当于一次将整个《指环王》三部曲放进去
桑达尔·皮查伊认为更大的查询窗口对企业来说会非常有用
比方说电影制作人可能会上传他们的整部电影
询问Gemini评论家是什么意见
公司还能使用Gemini来审查大量的财务记录等等
作为目前Google公开的最先进的大语言模型
Gemini 1.5采用了时下流行的混合专家（MoE）架构来提高效率
响应更快、质量更高
我们之前专门做过一期讲MoE的视频
与传统Transformer作为一个大型的神经网络运行不同
MoE模型被划分为较小的专家模块
执行任务时会根据信息类型
选择性地激活最相关的专家路径
从而大大提升模型的效率和准确性
不仅更适合处理大规模数据集的复杂任务
还有更强的可扩展性和灵活性
Google一直是MoE技术的早期采用者和先驱
提出了稀疏门控MoE、GShard-Transformer、Switch-Transformer、M4等研究
我们熟知的Mistral 8x7B、MiniMax abab6都是使用了Moe架构
之前更有爆料称GPT-4也是由8个或16个专家模型构成
根据Google的数据
这一次供早期测试的版本Gemini 1.5 Pro在使用更少计算资源的同时
对数学、科学、推理、多语言和视频等任务的执行水平已逼近1.0 Ultra
在官方演示和58页的技术论文中
Google还针对新模型的强大性能给出了以下几个用例
首先是大量信息的复杂推理和多模态分析
Gemini 1.5 Pro可以无缝分析、分类和总结给定的长篇复杂文档
例如
上传阿波罗11号登月任务的402页pdf记录
共32.6万tokens
让它根据要求列出3个有意思的瞬间
并引用原始对话细节
模型仅用了30秒给出了回应
其一是迈克尔·柯林斯的这句话“我敢打赌你一定要喝一杯咖啡”，
经查询文档中的确有记录
以及绘制一个靴子的图片
询问模型“这是什么时刻”，
模型可以正确地将其识别为这是Neil在月球上的第一步
同样
给出维克多·雨果的五卷本小说《悲惨世界》，
总共1382页，73.2万tokens
粗略地勾勒一个场景
并提问“看看这幅画中的事件是在哪一页上？
”，随后模型准确给出了页码
并且标识出了关键情节
同时
Gemini 1.5 PRO在超长视频理解上同样出色
能够快速准确地分析各种事件和情节点
比如以巴斯特·基顿自导的《福尔摩斯二世》为例
这是一部相当于69万tokens、时长为44分钟的无声电影
要求一句话总结电影情节
然后继续询问一个“纸张从口袋取出的关键信息和时间”。
Gemini 1.5 Pro用时57秒，指出12:
01的时候有个人从兜里掏出了一张纸
内容是高盛典当经纪公司的一张当票
并且还给出了当票上的时间、成本等详细信息
除了纯文字prompt，还有更多玩法
比如直接给模型一张抽象的“场景图”，
询问“发生这种情况时的时间点是多少？
”。
同样不到一分钟
模型准确给出了的电影对应的时间点15:
34
另外
Gemini 1.5 Pro超大的上下文窗口还能够深入分析整个代码库
以Three
js为例，这是一个3D Javascript库
包含约100000行代码、示例、文档等
相当于81.6万tokens
全部输入给模型并要求它“找到三个示例来学习角色动画”。
结果模型查看了数百个示例后筛选出了三个关于混合骨骼动画、姿势、面部动画的示例
这还只是开胃小菜
接下来用文字询问模型“动画Little Tokyo的demo是由什么控制的？
”，模型不仅找到了这个demo
并且解释了动画嵌入在gLTF模型中
并且它还能实现“定制代码”，
比如让模型给一些代码
添加一个滑块来控制动画的速度
同时使用其它演示所具有的那种GUI
Gemini 1.5也是分分钟给出了可以成功运行的代码
可以看到动画右上角出现了一个可控速的滑块
除此之外Gemini 1.5还可以做到“代码定位”。
仅靠一张demo的图片
Gemini 1.5就能在代码库中从数百个demo中
找到该图对应动画的代码
甚至还能修改代码，让地形变得平坦
并解释其中的工作原理
就连对文本几何体的修改也不在话下
除了以上演示的功能之外
Gemini 1.5 Pro另一项让人耳目一新的技能是“上下文学习（in-context learning）”，
意味着它能从一个长提示中给出的信息里学习新技能
而无需额外的微调步骤
为此
Google使用“对一本书进行机器翻译 (MTOB)”的任务进行测试
并且选用新几内亚西部不到200名使用者的卡拉芒（Kalamang）语
由于这门语言几乎没有任何网络信息
模型只能依赖于给定的上下文数据
而非训练权重中储存的知识来进行翻译
在测试中
工作人员向Gemini 1.5 Pro提供了500页参考语法、2000条双语词条和400个额外的平行句子
总计约25万tokens信息作为输入上下文
要求从中学习并完成卡拉芒语和英语的互译
从测试结果可见
Gemini 1.5 Pro对整本书的翻译得分接近人类学习者
相比之下
GPT-4 Turbo和Claude 2.1一次只能看完半本书
想获得这个技能就必须要微调或者使用外部工具了
对于一门在模型训练过程中几乎完全没接触过的语言来说
这个成就就显得尤为突出了
意味着不仅支持濒危语言的保护和复兴
也为其它低资源教育领域开辟了新的可能性
在模型评估方面
通过对文本、代码、图像、音频和视频的综合评估面板测试
在用于开发大语言模型的87%的基准测试中
Gemini 1.5 Pro都优于1.0 Pro
在相同的基准测试中
Gemini 1.5 Pro与1.0 Ultra相比
性能水平大致相似
即便上下文窗口增加了
Gemini 1.5 Pro也能保持高水平的性能
在NIAH评估中
评估人员将一小段包含特定事实或陈述的文本故意放置在长文本块中
而Gemini 1.5 Pro在长达100万个tokens的数据块中发现嵌入文本的概率为99%。
接下来
Google将通过AI Studio和Vertex AI向开发者和企业客户提供 Gemini 1.5 Pro的有限预览权限
最终在完成所有安全部署和测试后取代Gemini 1.0
免费使用的Gemini 1.5 Pro标准版将采用12.8万个tokens上下文窗口
普通用户需要额外支付费用获得100万tokens使用权
现在已经在业务中采用Gemini大模型的客户包括三星手机这样的大厂
也有像Jasper这种靠GPT起家的创业公司
甚至OpenAI董事Adam D‘Angelo旗下的Quora
虽然这次Gemini 1.5的发布时间很“不凑巧”，
前有OpenAI放话开发网络搜索产品和推出GPT记忆功能
后面紧跟着两小时后又横空杀出个Sora
可见OpenAI的武器库丰富且擅长针锋相对
每当Google有新动作
刚要炸起水花就被摁下去
但是
依然有不少声音站出来提醒大家切莫小看了Gemini 1.5 Pro
它对超长文本强大的分析推理能力是其它大模型做不到的
NVIDIA高级科学家Jim Fan更是表示
尽管Gemini-1.5 Pro被抢走了风头
被人们拿梗图来开玩笑
但是这仍然是大语言模型能力的巨大跃升
测试中达到的1000万tokens上下文、擅长检索、在零样本情况下对极长指令进行泛化、多模态工作能力都是惊人的
重要的不是声明中实现多少上下文长度的神话
而是模型实际上如何使用上下文来解决现实世界的问题
他认为1.5 Pro不通过微调而自主实现对Kalamang语的学习和应用
就展现出了这种神经激活中的复杂技能
超越了现有的技术水平
从这次Google Gemini 1.5和OpenAI Sora的发布可以看出
大厂之间对生成式AI底层技术的军备竞赛正在快速升温
以一种前所未见的速度不断突破技术壁垒
并且在多模态的前提下已经形成了全方面的竞争
形成一种奇幻的景象
大飞我也期待这些技术真正落地和普及的那一天
好了，本期视频内容就到这里
感谢大家的观看，我们下期再见
