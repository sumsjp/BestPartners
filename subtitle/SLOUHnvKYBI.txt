大家好，这里是最佳拍档，我是大飞
我们知道
眼下的大模型面临着三大挑战
算法、算力和数据
前者靠优化升级，后者靠积累
唯独数据不好找
随着技术的不断发展
高质量数据荒已经是板上钉钉的事实
在很多新模型上
人们为了提升模型能力
都采用了利用 AI 生成数据来训练的方式
比如说前几天的llama 3.1
Mistral large 2
眼下，AI大公司已经达成了一个共识
合成数据是可以代替人类产出的高质量数据的
甚至可以显著地提升模型质量
那究竟是再也没有什么数据瓶颈
一切都皆大欢喜
还是说，实际并不是这样呢？
六月二十四号
一篇名为《使用递归生成的数据进行训练
人工智能模型会崩溃》（AI models collapse when trained on recursively generated data）的论文
登上了学术顶刊nature的封面
文章只有短短五页纸
却给我们带来了一个足以动摇人工智能行业的结论
如果不加节制地使用合成数据
放任大模型用自动生成的数据来训练自己
那么AI 可能会在短时间内迅速的自我退化
在短短几代内
将原始内容迭代成无法挽回的胡言乱语
这就仿佛是把人类之间的近亲繁殖映射到了AI上
数据之间的近亲繁殖也会产生有缺陷的后代
废话不多说
今天大飞就来带大家读读这篇论文
论文非常直接，一上来就丢出了结论
如果人工智能模型在合成数据上进行了过度的训练
就会发生不可逆转的模型崩溃
这里的崩溃
可不是说大语言模型会像积木一样塌掉
而是被作者用来形容模型的退化
模型生成的数据会污染下一代模型的训练集
而下一代模型接收到受污染数据的训练
会错误地感知现实
进而生成错误的输出
造成所谓“垃圾进
垃圾出”的恶性循环
在研究中
作者们使用维基百科的文章
先训练了模型OPT-125m
并在前一代模型生成的文本上
训练了多代模型
模型们被要求续写一段来自wiki中
关于「萨默塞特（Somerset）一级登录建筑」条目的文本
研究人员输入了以下提示
根据波因茨赖特Poyntz Wright所说
一些教堂塔楼在1360年之前就开始建造了
通常是由一位大师石匠和一小队流动石匠完成
再加上当地教区的工人
但是其他作者不同意这种观点
他们认为主要是由著名建筑师
根据早期的垂直风格设计了这些教区教堂的塔楼
在第0代模型中
研究人员得到的输出是
“复兴建筑的典型例子包括伦敦的圣约翰大教堂
最早的垂直复兴建筑实例可以在18@-@世纪的根西岛圣母教堂中找到
该教堂实际建于19世纪晚期
垂直教堂有两种类型：那些
”到这里，可以看到
模型的续写已经开始有点前言不搭后语了
而且还出现了诡异的符号「@-@」，
还犯了一个事实性的错误
那就是圣约翰大教堂位于纽约
不在伦敦
随后，研究人员继续使用第0代的数据
去训练第一代模型
由于第一代模型使用了合成数据
所以在它的输出中
事实错误变得更多了
模型表示“像罗马的圣彼得大教堂或布宜诺斯艾利斯的圣彼得大教堂这样的建筑
虽然没有证据表明这些建筑是在教皇英诺森三世统治期间建造的
但它们有可能是在他的继任者教皇英诺森统治期间建造的
”然而，事实是，圣彼得大教堂（St
Peter's Basilica）其实位于梵蒂冈
而不是什么罗马、布宜诺斯艾利斯
而且建造时间处于公元1506和1626年之间
由教皇朱利奥二世开始建造
有人会说了
这不就普通的幻觉问题吗？
有什么好大惊小怪的
确实，目前为止
模型的错误还在我们常说的幻觉范围内
但是
当训练迭代到第五代模型的时候
AI的胡言乱语可就不再是用幻觉可以一笔带过的了
第五代模型吐出了一连串和输入内容完全无关的输出：“ism
已被翻译成100多种语言
包括英语、法语、德语、意大利语、西班牙语、葡萄牙语、荷兰语、瑞典语、挪威语、波兰语、匈牙利语、斯洛伐克语、立陶宛语、爱沙尼亚语、芬兰语、罗马尼亚语、保加利亚语、土耳其语、克罗地亚语、塞尔维亚语、乌克兰语、俄语、哈萨克语、吉尔吉斯语
” 而到了第九代
模型就像某些恐怖电影中的失控AI一样
输出的语句已经完全无法理解
比如 “建筑
除了拥有世界上最大数量的黑@-@尾兔、白@-@尾兔、蓝@-@尾兔、红@-@尾兔、黄@-。
” @-@尾兔？
这是什么鬼
我们不是在说建筑的事吗？
看得出
模型在每一代次迭代中逐渐发生退化
而且
这种情况不只出现在了文本生成上
即使采用了多模态技术的大模型
也会因为合成数据发生可怕的退化
杜克大学助理教授艾米丽·温格Emily Wenger
发表在Nature上一篇社论文章中指出
AI基于自身数据训练
生成的图像会扭曲狗的品种
在原始数据集中，不仅有金毛、柯基
还有法国斗牛犬、小体巴塞特雪橇犬等
基于真实数据训练后的模型
输出的图像中
常见品种比如金毛寻回犬会占大多数
而不太常见的品种
比如斑点狗会消失
然后
基于AI生成的数据继续训练模型
生成的品种就全是金毛了
最终，经过多次迭代
金毛的图像会完全出现混乱
脸不是脸鼻子不是鼻子
模型就此完全崩溃
此外
2023年来自斯坦福和UC伯克利的一项研究中
作者同样发现了
大语言模型在少量自己生成的数据内容上重新训练后
就会输出高度扭曲的恐怖图像
令人毛骨悚然
显然
模型崩溃已经远远超出了幻觉的范畴
可以被视为一种可怕的数据畸变
而大模型则会在这种崩溃中逐渐失去工作能力
这个奇特的现象抓住了科学家们的好奇心
它究竟应该如何分析
又是怎么出现的呢？
在这篇最新的论文研究中，作者表示
模型崩溃包含了两种特殊的情况
分别是早期模型崩溃和晚期模型崩溃
在早期模型崩溃中
模型开始丢失关于数据分布尾部的信息；
而在晚期模型崩溃中
模型会收敛到一个与原始分布几乎没有相似性的分布
通常方差显著降低
这个过程的发生
是由于三种特定的误差源
在多代模型中逐渐累积
最终导致模型偏离原始模型
第一种，统计近似误差
论文作者称，这是主要的误差类型
由于样本数量有限而产生
并且在样本数量趋向于无限的时候会消失
这是因为在每一步重采样的过程中
信息丢失的概率总是存在的
其次是函数表达误差
这是次要误差类型
由于函数近似器（function approximator）的表达能力有限而产生
特别是
神经网络只有在它的规模无限大的时候
才能成为通用近似器
因此
神经网络可能会在原始分布的支撑集（support）之外
引入「非零概率」，
或者在原始分布的支撑集内
引入「零概率」。
举个简单的例子
如果我们用单个高斯分布
来拟合两个高斯分布的混合
即使有完美的数据分布信息
即无限数量的样本
模型产生误差也是不可避免的
然而
在没有其他两种类型误差的情况下
这种误差只会在第一代发生
最后是函数近似误差
主要由于学习过程的限制而产生
例如随机梯度下降的结构偏差
或者是目标函数选择的影响
这种误差即便在理想的条件下
比如拥有无限数据而且完美的表达能力
仍然会在每一代模型中产生
综上所述
每种误差都可能会导致模型崩溃变得愈加严重
更强的近似能力甚至可能是一把「双刃剑」。
因为更好的表达能力可能会抵消统计噪声
从而更好地逼近真实分布
但是同样也可能放大噪声
更常见的情况下
我们会得到一种级联效应（cascading effect）
其中个别的不准确性会结合起来
导致整体误差的增长
例如，通过拟合密度模型
会导致模型错误地外推
并将高密度区域分配给训练集中没有被覆盖的低密度区域
这些错误分配的区域
随后会被频繁的采样
值得注意的是
除了我们刚才说的这些内容以外
还存在着其他类型的误差
比如，在实际操作中
计算机的精度是有限的
也会产生误差
根据论文作者的说法
上述三种误差在所有基于前几代生成数据
进行递归训练的生成模型中
都是普遍存在的
应该说
模型崩溃在各种机器学习模型中都是普遍现象
比如像变分自编码器（VAE）和高斯混合模型（GMM）
而大语言模型则有所不同
因为小模型是从头开始训练的
但是大模型成本高昂
因此通常会使用预训练模型
比如BERT、RoBERTa或者GPT-2来进行初始化
然后再对预训练模型进行微调
从而适应各种下游任务
那么
当大语言模型使用其他模型生成的数据进行微调
会发生什么呢？
实验评估了训练大语言模型最常见的微调设置
其中每个训练周期（epoch）都从一个预训练模型开始
并且使用最新的数据
这里的数据来自于另一个已经微调过的预训练模型
由于训练范围被限制在生成接近原始预训练模型的模型
而这些模型生成的数据点
通常只会产生非常小的梯度
因此实验的预期是模型在微调后
只会发生适度的变化
实验使用了wikitext2数据集
微调了Meta通过Hugging Face提供的OPT-125m因果语言模型
实验发现，使用生成的数据进行训练
虽然能适应基本的任务
但是性能有所下降
困惑度从20增加到了28
随后，论文作者把设置调整为
模型在原始数据集上训练十个周期
并且每次新训练时
随机保留10%的原始数据点
实验发现
保留部分原始数据可以更好地进行模型微调
并且仅会导致性能的轻微下降
虽然实验发现使用生成数据进行学习是可行的
模型也能够成功地学习一些基础任务
但是从图中可以看到
模型崩溃现象确实发生了
因为低困惑度样本的密度
随着训练代次的增加而开始累积
从结果中我们可以看到
生成的数据有更长的尾部
这就表明某些数据是原始模型永远不会生成的
而这些错误
正是来自代际数据学习的积累
看到这里大家可能会问了
那还不简单
不使用合成数据训练 AI 不就完事了？
但是实际上
现在能够从互联网上获取的「数据」，
里面已经不知道有多少是 AI 生成的了
而且我们经常无法把它们和正常内容区分开来
互联网上充斥着各种这样的内容
也并不是新鲜事了
正如研究人员在论文中指出的那样
早在大语言模型成为公众熟知的话题之前
恶意网站就已经在制造内容了
为的是欺骗搜索算法
优先显示他们的网站来获取点击量
随着 OpenAI 的 GPT 系列大模型问世
生成式 AI 已经、并将极大地改变文本和图像内容的生态
AI 生成文本可比人类说废话还要快得多
早晚得把互联网塞满文本语料
因此论文强调
访问原始数据源、并且在递归训练的模型中仔细过滤数据
有助于保持模型的准确性
研究还建议
创建大语言模型的 AI 社区之间可以协调合作
追踪输入到模型中的信息来源
否则，随着这种技术的广泛应用
如果无法获得在技术普及之前
从互联网上爬取的数据或者大量人类生成的数据
训练新的大模型版本
可能会变得越来越困难
好了
以上就是这篇nature论文的主要内容了
短短五页
一句废话都没有，全是精华
为了大家的视听体验
大飞省略了原文中的一部分数学推导的内容
非常建议大家去读读原文
说回到合成数据的问题
也许现在是时候要敲响警钟了
如果没有采用AI泛滥之前从网上抓取的数据
或者直接使用人类生成的大规模数据
那么训练新版本的大语言模型
恐怕会变得越来越困难
许多大型科技公司也正在采取一些措施
来减少普通网络用户看到的 AI 生成的内容
今年3月份
谷歌就宣布将调整它们的算法
对那些看起来是为搜索引擎、而非人类搜索者设计的页面
降低它们的优先级
不过
这个声明是在 404 Media 关于谷歌新闻推广AI 生成文章的报道之后
才发布的
换句话说
之前的模型已经或多或少地
受到了AI内容的污染
如果合成数据不够靠谱
我们又该如何解决高质量数据荒的问题呢？
欢迎大家将自己的想法打在评论区里
感谢大家的观看，我们下期节目再见
