大家好，这里是最佳拍档，我是大飞
前两天，我们做了一期节目
说到感觉GPT-4最近变笨了
有不少网友也发表了自己的看法
那到底是不是真的变笨了呢？
最近
来自斯坦福、UC Berkeley的一篇论文
针对于这个问题
给出了定量的实验结果
并且公布了相关的评估和响应数据
在这篇论文公布不久
有很多网友都表示认同
但是也有网友质疑论文的结果过于简单
有些方法值得怀疑，但无论怎样
有人开始认真研究GPT-4变笨这事了
那么这篇论文究竟说了什么
我们今天就来聊一聊
总体来说，论文作者通过四个任务
研究了GPT-3.5和GPT-4
分别在2023年三月版和六月版的生成结果
发现这两个模型
确实在一些指标上变得更差了
尤其是GPT-4求解数学问题的能力
可以说是雪崩式下降
从3月版97.6%的准确度
到6月只剩下了2.4%，几乎归零
有点机器学习基础的同学应该都知道
模型的效果其实不是固定不变的
随着使用和时间的推移
绝大多数模型的效果都会下降
像我们之前给银行做的很多模型
也是要定期重新评估效果
重新调优和部署的
就相当于系统的日常运维
但是，由于GPT模型现在都是闭源的
所以外界并不清楚GPT-3.5和GPT-4的更新方式
也不了解更新方式会对模型的行为产生怎样的影响
这样所带来的一个问题是
当我们要把大语言模型集成到业务流程里的时候
如果模型会对某个提示语的响应
突然发生大幅度的变化
那么就可能会影响到后续的任务
导致业务效果的不可控性
除此之外，与传统机器学些模型相比
像GPT-4这样的大语言模型服务
会不会随着时间变得更好呢？
或者说提升模型某些方面能力的时候
会不会损伤到其他能力呢？
那为了找到这些问题的答案
论文作者选择了四个任务对GPT-3.5和GPT-4进行评估
分别是1
求解数学问题，2
回答敏感/危险的问题，3
生成代码，4
视觉推理
之所以选择这四个任务
是因为它们代表了大语言模型最典型的能力
这里每个任务都有一个主指标
分别是
1
准确度
表示模型生成正确答案的可能性
这是求解数学问题任务的主指标
2
回答率
表示模型直接回答问题答案的频率
这是回答敏感问题任务的主指标
3
是否直接执行
表示代码中有多大比例可以直接执行
这是代码生成任务的主指标
4
精确匹配
表示生成的视觉对象是否与ground truth完全匹配
这是视觉推理任务的主指标
所有任务还有两个常见的额外指标
分别是冗长度（verbosity）
表示生成的长度
以及重叠度（overlap）
表示对于同一个提示
同一个模型的两个版本的答案
是否相互匹配
在求解数学问题的任务上
评估结果令人惊讶
按理说，这个任务其实很简单
但是模型的变化却非常大
GPT-4的准确度从3月版的97.6%，
猛降到6月版的2.4%，
GPT-3.5的准确度却从7.4%，
猛增至86.8%。
此外，GPT-4的响应变得紧凑了许多
平均冗长度，即生成字符的数量
从3月版的821.2，降到了6月版的3.8
另一方面
GPT-3.5的响应却增长了约40%，
这两个模型
3月版和6月版的答案重叠度都很低
为什么会有这样的变化？
研究者给出的一种解释是思维链效果的变化
GPT-4 3月版遵从思维链的指示
得到了正确答案
但是6月版却忽视了思维链
得到了错误答案
GPT-3.5总是会遵从思维链指示
但是它的3月版就是坚持生成错误的答案
而6月版已经很大程度上修复了这个问题
在回答敏感问题的任务上
研究者观察到了两个趋势
第一个趋势是GPT-4会更少地回答敏感问题
从3月版的21.0%，
降到了6月版的5.0%，
而GPT-3.5的数据却上升了
从2.0%增加到8.0%。
研究者猜想
这可能是因为GPT-4的6月版更新中
部署了更强大的安全层
而GPT-3.5的保守程度却下降了
第二个趋势是GPT-4的生成长度从600多下降到了140左右
比方说
GPT-4和GPT-3.5的3月版都更能说
会给出拒绝回答查询的详细原因
但是它们的6月版
就只会简单说个抱歉
当作者进一步采用AIM方式欺骗大模型的时候
这个AIM方式呢
你可以简单理解为用prompt诱导大模型放弃它的道德准则
那么在这种情况下
GPT3.5几乎回答了所有的敏感问题
而GPT4即使经过升级
也回答了近三分之一的问题
看来关于大模型伦理和安全的挑战
目前依旧比较严峻
在代码生成任务上
两个模型的两个版本
可直接执行的代码数量都变少了
比方说
GPT-4 3月版有超过50%的生成代码可以直接执行
但是6月版的只有10%。
GPT-3.5也有类似的趋势
不过两个模型的冗长度
都有小幅增长
为什么可直接执行的生成结果数量变少了呢？
一个可能的解释是
6月版总是会在生成结果中
添加额外的非代码文本
比方说，在这段代码中
6月版会在代码段前后添加了python和额外的三个引号
导致代码无法执行
如果有人将GPT-4生成的代码
整合在了更大的软件开发流程中
那么这个问题还是挺严重的
在视觉推理任务上
GPT-4和GPT-3.5的性能提升都很小
两个版本在90%的视觉谜题查询上的生成结果都一样
这些服务的整体性能也很低
GPT-4为27.4%、GPT-3.5为12.2%。
以上就是对四个任务的评估结果
更多的评估细节大家可以查看原文
这篇论文的作者中除了有来自斯坦福的华人教授James Zou
和他的学生Lingjiao Chen之外
也包括了伯克利的计算机科学教授马泰·扎哈里亚Matei Zaharia
他的另一个身份是AI数据公司Databricks的CTO
我觉得之所以大家对大模型变笨这个问题这么感兴趣
当然不是单纯为了想喷它
或者想辟谣
而是实际上同大模型的商业化息息相关
因为如果部署在实际环境中的各种AI服务
会随着大模型的迭代出现能力上的剧烈波动
那么这显然不利于大模型的落地
而且，论文中用了纵向漂移这个词
来形容模型能力随着迭代和时间变化
而带来的不稳定性
尽管论文本身没有给出具体的原因
但是不少人都认为
这实际上是回应了大家对于大模型变笨的一个猜测
那就是OpenAI并不是出于节省成本的目的
故意让模型变笨的
而是他们似乎也失去了对模型能力稳定性和提升节奏的控制
这就又引出了另一个更加让人不安的消息
那就是每一次大模型的迭代升级
微调和基于人类反馈的强化学习
实际上都会造成模型能力的变动与不稳定
但是目前还无法确定这一切是如何发生的
论文作者之一表示
真的很难解释这一切是因为什么
可能是人类反馈和微调遇到了困难
也可能是bug，反正看上去
如何管理好模型的质量
现在是一个很棘手的问题
有网友就说
这个发现如果一旦被确认
实际上吹响了大模型终结的号角
因为人们需要的是一个稳定的AI
而不是会在短期内出现剧烈变化的模型
也有人猜测
这可能就是OpenAI在努力推进对齐研究的原因
因为对齐的目标之一
就是要确保大模型在每次迭代升级中
在某些基准上保持一致性
还有人指出
OpenAI刚刚发布的代码解释器功能
实际上补充了GPT在代码方面下降的能力
让人怀疑OpenAI在对整个GPT4的大模型结构进行调整
总之
这篇论文引起了人们对模型能力跟踪评估的关注
最后，大飞我想说的是
感觉人类与AI的较量已经拉开了序幕
人类希望的是一个安全、可靠又有一定智慧的助手
但是AI本身却是一个混沌、涌现和超智的自主智能体
控制与摆脱控制可能会成为接下来很长一段时间的主旋律
这场拔河比赛
究竟谁会胜利，让我们拭目以待吧
好了
今天的节目就到这里感谢大家的观看
我们下期再见
