Celestial AI参与了台积电5nm和4nm的早期创新客户计划
已经完成了四次流片
技术成熟度很高
他们目前的重点是“带中介层的HBM”。
HBM是AI芯片的核心内存
用光连接来优化HBM的数据流
能够直接提升AI计算效率
他们的PFLink技术包含一个硅光子层
集成了无源和有源元件
关键是实现了“SerDes与通道匹配”，
从而达到超高能效
它们还在构建光MAC（OMAC）
确保光连接的可靠性（RAS功能）
避免光链路故障影响系统
Celestial AI还提到了一个关键的观点
那就是电fabric与光fabric的扩展定律不同”。
随着多芯片封装尺寸的增大
电fabric的带宽会受限于物理接口的数量
而光fabric的带宽能持续增长
因为光可以并行传输更多通道
且不受电磁干扰
这意味着未来更大规模的多芯片封装
光fabric会成为必然的选择
另外
他们还展示了CoWoS-L芯片组的光学多芯片互连桥OIMB
解决了光学接口的安全问题
由于光信号很容易被窃听
所以OIMB能确保光传输的安全性
这对金融、政务等敏感场景很重要
再看Ayar Labs的TeraPHY光I/O芯片
他们的目标是用光学技术实现AI系统的横向扩展
大规模AI系统需要把数百万个芯片连成一个集群
传输距离从机架内
大概3米的距离，到多机架的15米
如果用电I/O
每机架的功耗会暴涨，根本扛不住
Ayar Labs的解法是采用UCIe光I/O重定时器
UCIe是一种新的芯片互连标准
Ayar Labs把光I/O做成了UCIe Chiplet
这样就能够轻松集成到AI芯片的封装里
不用改现有芯片设计，兼容性很强
这款UCIe Chiplet的速率达到了8Tbps
能提供大量封装外带宽
解决AI芯片“对外传不动数据”的问题
它的核心创新是解耦光信号和电信号
因为在传统的光连接中
光信号和电信号的传输路径是绑定的
一旦其中一个出问题
整个链路就废了
而Ayar Labs的设计里
UCIe接收器会先把电信号重定时
再转换成光信号传输，这样光电解耦
各自的优化空间更大
也更容易排查故障
Ayar Labs的TeraPHY芯片已经进入设计验证测试阶段
即将量产
而且他们还做了长期链路的稳定性测试
比如热循环测试
由于芯片的加热和冷却
会导致材料膨胀收缩
改变光在通道中的传播方式
Ayar Labs通过优化封装材料
确保光链路在热循环下仍能稳定传输
他们还展示了一个共封装的500W设备
证明光I/O能适配高功率AI芯片的散热需求
不再是只能跑低功耗的场景
而Lightmatter公司则提出了“3D光中介层”的概念
推出了Passage M1000平台
它的核心是在光中介层上封装计算和内存芯片
用3D堆叠实现紧凑结构
同时提供超高带宽
现在芯片互连的一大痛点是
芯片外围的物理区域有限
I/O接口数量上不去
要想实现100倍以上的带宽
就必须改变现有范式
而3D光中介层就是这个新范式
Passage M1000的预期速率高达114Tbps
按照Lightmatter的说法
这是迈向200Tbps XPU和400Tbps交换机的第一步
而且已经做好了生产准备
设计上，他们解决了一个关键问题
也就是光学元件与电SerDes的物理尺寸匹配问题
光元件通常要比电SerDes小
直接集成会浪费空间
Lightmatter用硅微环谐振器来调节光信号
实现了非常紧凑的光I/O
让光学元件和电SerDes的尺寸刚好匹配
不浪费封装空间
为什么要选择硅微环谐振器呢？
Lightmatter解释了三个原因
一是尺寸小
能在有限的空间里集成更多的通道；
二是功耗低
调节光信号不需要太多能量；
三是响应速度快
能跟上AI芯片的高频数据传输需求
他们还打造了光引擎Lightmatter Guide
负责光信号的生成、传输和接收
Passage M1000还具有一定的可重构性
能够根据不同工作负载来调整光链路
灵活性很强
另外
他们的Tile设计有16条水平总线
通过十字形金属缝线实现电气连接
光路交换功能还能提供冗余
这样哪怕一条光链路坏了
其他链路还能工作，提升了可靠性
最后我们来看看英伟达的光I/O创新
他们的重点是“跨区域扩展”，
推出了Spectrum-XGS以太网技术
目标是把多个分布式数据中心组合成一个“十亿瓦级AI超级工厂”。
这意味着不仅需要硬件支持
还需要“距离感知算法”，
因为不同数据中心之间的距离可能有几十、上百公里
光信号传输会有延迟
所以这个算法需要根据距离
动态调整数据的传输策略
确保整体性能
根据英伟达的数据
用Spectrum-XGS技术
多站点NCCL的横向扩展性能
是传统OTS以太网的1.9倍
能大幅加速多GPU和多节点的通信
让AI训练不再受单个数据中心的资源限制
硬件方面
英伟达推出了“200G/SerDes共封装光学”技术
由于传统的可插拔光学引擎
需要额外的电力和空间
而共封装光学直接把光学元件和交换机芯片封在一起
不用额外供电，能够节省大量的电力
还有NVIDIA Photonics硅光CPO芯片
速率达到了1.6T
采用了新型微环调制器
进一步提升了能效
英伟达的Spectrum-6 102T集成硅光交换机也很有亮点
实现了翻倍的吞吐量、更高的可靠性和更低的功耗
搭配之前的Spectrum-X和Quantum-X交换机
形成了完整的光网络产品线
而且他们透露
即将推出CPO网络交换机
由于共封装技术是未来光网络的主流方向
所以英伟达的布局
显然是想抢占这个赛道的先机
CPU现在面临的最大挑战就是“摩尔定律触顶”，
晶体管密度的提升速度越来越慢
单芯片性能很难再像以前那样
“每18个月翻一番”。
本届Hot Chips上
CPU领域的分享有个共识
那就是要靠先进制程+Chiplet+3D堆叠来突破性能瓶颈”，
把不同功能的芯片裸片
用先进封装技术集成在一起
这样既能提升性能
又能控制成本和功耗，还能提高良率
我们先来看英特尔的下一代至强处理器Clearwater Forest
这款处理器有288核
用的是Intel 18A制程和3D封装技术
核心思路是用3D堆叠来提升缓存和内存带宽
它的Chiplet设计很精细
12个能效核CPU Chiplet采用了Intel 18A工艺
3个基础Chiplet采用Intel 3工艺
2个I/O Chiplet沿用了上一代Sierra Forest的Intel 7工艺
芯片间用EMIB连接，简单解释一下
EMIB，也叫嵌入式多芯片互连桥
是英特尔的一项成熟封装技术
能够提供高带宽、低延迟的Chiplet互连
缓存方面
Clearwater Forest的末级缓存LLC达到了1152MB
意味着每个插槽有576MB的LLC
具体到芯片上
每个144核的Tile有108MB的LLC
两个Tile就是216MB
再加上其他缓存，总缓存容量非常大
大缓存的好处是可以减少内存的访问次数
CPU不用频繁去内存里读数据
延迟会大幅降低
这对AI推理、数据库等场景很友好
前端设计上
Clearwater Forest通过3个3-wide指令解码器
把指令宽度提升了50%，
简单来说就是一次能读取更多指令
提升指令的执行效率
分支预测器也做了优化
能够更准确地预测程序下一步要执行的指令
减少“预测错误”带来的性能损失
后端的乱序执行引擎也升级了
从每时钟周期能调度5个操作
提升到8个操作
执行端口数量增加到26个
整数和向量执行能力都翻了一倍
这意味着CPU能够同时处理更多任务
多线程性能会有明显提升
内存子系统也有改进
L2未命中缓冲区的大小翻倍
能存储128个未命中数据
以前缓冲区满了，CPU就要等
现在能存更多了
等待的时间就相应减少了
单个Clearwater Forest模块有4个核心
共享4MB的统一L2缓存
L2缓存的带宽也比上一代翻倍
达到400GB/s；
双插槽系统中
每个芯片有12个DDR5-8000内存通道
总内存带宽达到1300GB/s
能够满足大规模数据处理的需求
根据英特尔的数据
Clearwater Forest机架的每瓦性能是上一代Sierra Forest的3.5倍
能效提升非常明显
我们再来看IBM的Power11处理器
这款处理器用的是三星7nm工艺
设计理念是“按需增加核心数”，
不盲目堆核心
而是根据实际的需求来优化架构
他们计划在后代Power处理器中聚焦几个重点
一是每个插槽上集成的硅片数是上一代的3倍；
二是利用良率协同效应；
三是保持跨Chiplet的高带宽连接；
四是优化OMI内存的效率；
五是减少延迟，提升拓扑协同性；
六是保证长期发展的效率和灵活性
Power11的核心升级在于内存子系统
IBM称之为“OMI内存架构”，
这是一种分层内存架构
一块芯片最多可以支持32个DDR5内存端口
传输速度最高可达38.4Gbps
最终会推出定制化的内存规格OMI D-DIMM
IBM对HBM并不是很看好
因为HBM的容量较低
满足不了大规模数据处理的需求
他们的目标是8TB DRAM和1TB/s以上的内存带宽
而OMI架构基于DDR5内存就能实现这个目标
成本比HBM要低很多
不过OMI也有个小缺点
那就是缓冲区会增加6到8ns的延迟
但是IBM认为这个延迟在可接受的范围内
而且换来的容量和带宽优势更值得
另外
Power11还优化了对外部PCIe加速器的支持
IBM有自己的Spyre加速器
能够和Power11配合提升AI计算性能
比如在机器学习场景中
Spyre加速器能够卸载Power11的计算任务
让CPU专注于数据调度
整体性能提升明显
晶心科技的子公司Condor Computing
他们展示了首款高性能RISC-V CPU IP
Cuzco
这款产品由一支仅有50名工程师的团队完成
却实现了很高的性能
据介绍与其他有相近功耗的高性能授权CPU相比
Cuzco的性能更加出色
Cuzco的优势很明确，一是降低了成本
RISC-V是开源架构，不用交授权费；
二是提高了能效
针对低功耗场景做了优化；
三是扩展性强
每个集群有8个高性能计算CPU核心
能按需扩展核心数量；
四是兼容性好
符合面向高性能RISC-V计算的最新RVA23规范
软件生态能复用；
五是灵活定制
完全支持指令集架构ISA的定制
能够根据客户需求添加特殊指令
Cuzco的设计和多数高性能处理器类似
提供了完整的IP方案
除了CPU核心以外
还有缓存和一致性管理功能
能直接接入内存和I/O总线
客户不用再做额外的集成开发
它的核心创新是基于时间的微架构
传统乱序执行CPU需要复杂的调度逻辑
晶体管多、功耗高；
而Cuzco用硬件编译来进行指令排序
通过“寄存器计分板”和“时间资源矩阵（TRM）”，
精确预测指令的执行时间
提前安排好执行顺序
这种设计的好处是
调度的确定性降低了逻辑复杂性
消除了复杂的运行时每周期调度
减少了动态功率
用更少的晶体管实现了更高的能效
Cuzco采用基于slice的CPU设计
最多支持8个核心
每个核心有私有的L2缓存
多个核心共享L3缓存
这种分层缓存设计能平衡延迟和容量
根据Condor Computing的数据
Cuzco在SPECint2006测试中
每时钟周期的性能几乎是晶心科技当前AX65核心的两倍
证明了这款RISC-V CPU的高性能潜力
最后看日本PEZY Computing公司的第四代MIMD多核处理器PEZY-SC4s
采用了多指令多数据MIMD的架构
特点是每个处理器核心能够执行不同的指令、处理不同的数据
适合具有高度独立线程的应用程序
比如科学计算、基因组分析等等
PEZY认为，对这类应用来说
MIMD比单指令多数据SIMD更加有效
因为能够充分利用线程的独立性
PEZY-SC4s采用的是台积电5nm FinFET工艺
芯片尺寸为18.4mm×30.2mm
总共大约556mm²
集成了48亿颗晶体管
SRAM容量为1.6Gb
内部总线的读带宽达到了12TB/s
写带宽达到6TB/s
能满足大规模数据传输需求
它的计算资源很丰富
有2048个处理单元PE
支持16384个线程
还有PE和缓存的分层缓存
能够减少数据访问延迟
提升线程并行效率
外部内存方面，PEZY-SC4s采用了HBM3
有4个设备
带宽达到3.2TB/s，容量达到96GB；
外部接口是PCIe Gen5，有16个lane
带宽达到64GB/s
能够快速连接主机和其他设备
系统部署上
PEZY设计了“主机CPU+PEZY-SC4s”的节点
每个节点包含1张AMD EPYC 9555P CPU、4张PEZY-SC4s和NDR InfiniBand网卡
规划的系统配置有90个节点
总共737280个PE
双精度下峰值算力达到8.6PFLOPS
这个算力能够满足大型科学计算的需求
比如气候模拟、量子化学等
PEZY还对PEZY-SC4s做了仿真测试
在执行双精度通用矩阵乘法DGEMM的工作负载时
功率效率是上一代的2倍以上
在基因组序列比对算法Smith-Waterman测试中
性能可以达到359GCUPS
是上一代PEZY-SC3的3.86倍
也侧面证明了MIMD架构在特定场景下的性能优势
另外，PEZY还在开发第五代PEZY-SC5
计划采用3nm或者更小工艺
预计2027年发布
他们还在开发一种新的硬件描述语言Veryl
作为SystemVerilog的开源替代方案
PEZY-SC5的核心组件也会用Veryl开发
目标是降低芯片设计门槛
图形领域现在的趋势很明确
那就是要用AI来提升渲染效率
不管是游戏、创作，还是AR/VR
都需要更逼真的画面
但是传统的渲染方式功耗高、速度慢
AI的加入能大幅优化这个过程
本届Hot Chips上
AMD和英伟达两大GPU巨头都分享了图形架构的优化
尤其强调对光线追踪、AI性能和神经渲染的支持
Meta则带来了AI眼镜专用芯片的设计
聚焦空间和功耗受限场景下的渲染优化
我们先看AMD的RDNA 4架构
这款架构专为下一代游戏和创作设计
核心升级是“AI算力+光线追踪优化”，
能够支持严苛的游戏应用、先进的视频编码和流媒体能力的生产力场景
RDNA 4的SoC架构设计很灵活
高度可扩展
能根据市场需求调整配置
打造从入门到旗舰的多种产品SKU
而且不用重新设计整个架构
降低开发成本
RDNA 4针对高端游戏工作负载做了大量优化
一是栅格化和计算效率
通过优化着色器引擎的执行逻辑
提升多边形渲染和通用计算的速度；
二是光线追踪性能
也是本次升级的重点
RDNA 4的光线求交性能比上一代翻了一倍
还新增了专用的硬件实例转换器
把“实例转换”这个任务
从着色器程序中转移了出来
让着色器专注于渲染
进一步提升光追速度；
边界体积层次结构BVH结构
也从4列加宽到了8列
能够更高效地组织场景数据
减少光线求交的计算量
新采用的节点压缩技术还能减少BVH的尺寸
降低内存占用
光线追踪的另一项优化是“定向边界框”。
传统的边界框是轴对齐的
对不规则物体的包裹性差
会导致很多无效的求交测试
而定向边界框能够根据物体形状调整方向
更精确地包裹物体
大幅提高光线相交测试的效率
另外，乱序内存访问也做了优化
某些高优先级的请求能优先处理
不用等其他延迟高的工作
这对光追这种“频繁访问内存”的场景很重要
着色器引擎方面
RDNA 4通过动态寄存器分配
增加了“传播波数”。
传播波是着色器中的并行执行单元
数量越多，并行处理能力越强
能够同时处理更多的像素或顶点数据
针对机器学习和AI的工作负载
RDNA 4还增加了FP8精度支持和稀疏化功能
FP8能在保证画质的前提下提升算力密度
稀疏化则能跳过无效数据的计算
减少功耗和延迟
AI在图形中的应用也很具体
比如用神经辐射缓存来存储场景的辐射信息
用神经超采样和去噪技术
填补因使用过少光线造成的画面空白
既能提升渲染速度，又能保证画质
存储方面，RDNA 4的SoC架构中
数据在着色器引擎、各种缓存和内存控制器之间的流动很高效
Infinity Fabric的带宽高达1KB每时钟频率
能够快速传输大量的渲染数据
RDNA 4的结构是模块化的
比如Navi 48 GPU能切成两半
制造出更小的GPU
这不仅减少了开发GPU变体的工作量
同时能够提升芯片的可靠性
哪怕某个模块出问题
其他模块还能工作
另外
RDNA 4还有新的内存压缩和解压缩功能
对软件完全透明，全部由硬件处理
根据AMD的数据
某些栅格工作负载的性能能提升大约15%，
fabric带宽占用率降低大约25%，
而且不用软件修改，兼容性很好
我们再来看英伟达的Blackwell架构图形产品
他们的重点是“神经渲染”，
称RTX Blackwell为“神经渲染的新时代奠定基础”。
神经渲染的核心是“融合传统图形与AI”，
也就是用AI来生成画面
而不是完全靠传统的光栅化或光追
这样既能提供更好的视觉保真度和沉浸式体验
又能节省电力
还能支持游戏中的AI agent
比如动态NPC
为了提升AI性能
英伟达在Blackwell架构中大量使用了FP4计算
FP4精度虽然比FP8更低
但是算力密度更高
对AI渲染这种“对精度要求不高”的场景来说
完全够用
而且能大幅降低内存占用和功耗
另外
英伟达还大量使用了“着色器执行重排序”技术
根据着色器的执行状态
动态调整任务顺序
保持GPU核心计算单元SM的满载
这样才能发挥最大性能
避免资源浪费
内存方面
Blackwell增加了对GDDR7的支持
GDDR7用的是PAM3调制技术
和GDDR6X的PAM4相比
PAM3每时钟周期的位数更少
但是信噪比（SNR）更高
能支持更高的时钟速度
整体带宽反而更高
还能支持更低的电压，减少功耗
英伟达还特别关注“首token执行时间”，
因为在混合图形/机器学习工作负载中
首token时间越短，AI响应越及时
交互体验越好
所以Blackwell通过优化AI计算的调度逻辑
大幅缩短了首token时间
另外
Blackwell图形GPU还集成了一整套AI管理处理器
专门协调图形和机器学习的交错工作
比如在游戏渲染的间隙
调度AI任务执行
确保数据传输和SM的高效运行
不会出现图形任务等AI
或者AI任务等图形的情况
帧生成技术也是一大亮点
用AI生成中间帧
能够在保证帧率的前提下降低GPU功耗
根据英伟达的数据
帧生成能将GPU功耗减半
这对移动设备和笔记本电脑来说非常实用
Blackwell还支持通用多实例GPU，MIG
也就是把一张GPU分成多个独立的虚拟GPU
同时运行多个工作负载
比如RTX Pro 6000
单个1080p客户端的工作负载太小
无法完全利用GPU的算力
把它拆成多个小的虚拟GPU后
能通过并行执行多个工作负载
保持GPU满载
4个MIG实例比传统的时间切片timeslicing
性能提升60%，资源利用率更高
最后我们看Meta的Orion AI眼镜专用芯片
Orion眼镜原型的特点是普通眼镜外观+AR沉浸式功能
正在突破AI眼镜在空间和功耗方面的极限
由于眼镜的体积小
功耗预算极其有限，通常只有几瓦
但是又需要实时处理眼动追踪、手势识别、世界锁定渲染等任务
传统芯片根本满足不了
所以Meta专门设计了一套芯片方案
Meta的核心挑战是世界锁定渲染（WRL）
这是AR/MR应用中的关键技术
指的是把虚拟物体固定在现实世界的特定位置上
让它与物理环境保持相对静止
这样在用户移动的时候
虚拟物体不会“飘”，体验更加沉浸
WRL对延迟和功耗的要求极高
延迟超过20ms
用户就会有“眩晕感”；
功耗太高，眼镜续航就会很短
所以Meta必须用专用芯片来加速WRL
为了平衡性能和功耗
Meta用了多种前沿技术
一是先进工艺节点
Orion构思之初就定了5nm工艺
能在小面积内集成大量晶体管
同时控制功耗；
二是减少DRAM的使用
DRAM功耗高、体积大
Meta尽量用片上SRAM存储数据
减少DRAM的访问；
三是Vmin Fmax优化
在保证性能的前提下
尽可能降低最小工作电压
减少静态功耗；
四是积极的电源管理与数据压缩
根据任务负载动态调整芯片电压和频率
同时压缩数据传输量，减少功耗；
五是创意封装和减线数
用更紧凑的封装技术
比如SiP来减少体积
同时优化引脚设计
减少线数，降低信号干扰和功耗
Orion的计算任务拆分也很巧妙
它把重负载任务，比如复杂AI推理
放到外部的Puck设备中
眼镜本地只处理低延迟任务
比如WRL、眼动追踪等等
这样既能降低眼镜的功耗和体积
又能保证实时性
Puck设备中有三个主要处理芯片
分别是显示处理器、眼镜处理器和计算协处理器
分工明确
眼镜处理器负责处理所有的眼动、手部追踪以及摄像头输入
采用系统级封装SiP
5nm工艺，集成了24亿颗晶体管
这个晶体管数量
在眼镜芯片里算很多的了
为了安全
Meta还在芯片中植入了“安全信任根”，
确保所有进出芯片的数据都经过加密
防止隐私泄露
来自Puck的图像是HEVC编码的
眼镜处理器需要先解码
再重新编码为显示处理器的专有格式
这个过程要在几毫秒内完成
对解码性能要求很高
显示处理器有两个
每只眼睛对应一个
负责重新投影，或者叫时间扭曲
这是由于摄像头采集图像和屏幕显示有延迟
显示处理器需要根据用户的实时位置
来调整图像的视角
确保虚拟物体和现实世界对齐
显示处理器没有外部存储器
所有数据都存在片上SRAM中
所以SRAM容量很大
能避免DRAM带来的延迟和功耗
计算协处理器是Orion中性能最强、功耗最高的芯片
同样采用5nm工艺，配备LPDDR4X内存
集成了57亿颗晶体管
负责计算机视觉处理、机器学习执行、音频渲染、HEVC编码等重负载任务
比如手势识别的AI模型推理
就由它来完成
它也有相对较大的片上SRAM缓存
减少内存访问延迟
确保任务实时执行
随着AI和云计算的发展
网络安全的挑战也是越来越大
微软在大会上亮出了一组触目惊心的数据
2024年网络犯罪的“GDP”高于9万亿美元
预计2025年将超过10万亿美元
排名介于中国和德国之间
已经成为“全球第三大GDP实体”。
面对这么严峻的安全形势
软件防护已经不够
必须从硬件层面构建安全屏障
本届Hot Chips上
微软分享了Azure的硬件安全方案
核心是把安全集成到每台服务器
而不是依赖中心化的HSM
那么什么是HSM呢
它的全称是硬件安全模组
这是一种专门用来保护加密密钥的硬件设备
能够提供高强度的加密运算和密钥管理
防止密钥被窃取
传统的HSM部署是“中心化模型”，
比如一个数据中心里放几台HSM服务器
所有需要加密的任务都要和这些服务器通信
这种方式的问题很明显
一是延迟高，所有请求都要排队；
二是单点故障，HSM服务器坏了
整个数据中心的加密任务都得停；
三是扩展性差，随着服务器数量增加
HSM会成为瓶颈
微软的解法是“Azure Integrated HSM”，
这是一款专用的安全ASIC芯片
直接集成到每台服务器中
不用再依赖中心化HSM
这种设计的好处，一是延迟低
加密任务在本地服务器就能完成
不用和中心化服务器进行TLS握手；
二是扩展性好，服务器越多
安全算力也越多，不会出现瓶颈；
三是可靠性高，单台服务器的HSM坏了
不影响其他服务器，避免单点故障
这款ASIC的设计也很有针对性
它集成了HSM优化硬件
包括AES和PKE操作的硬件加速引擎
这些都是最常用的加密算法
硬件加速能大幅提升效率；
还有用来控制逻辑的实时核心
确保加密任务的实时处理；
接口和安全标准也做了加固
能够检测入侵和篡改行为
比如有人试图物理拆解芯片
或者用侧信道攻击窃取密钥
ASIC能立即触发防护机制
销毁密钥或停止工作
微软还进入了“机密计算”领域
机密计算的目标是“保护正在使用的数据”，
尤其是在多租户云环境中
用户的数据在云服务器上运行时
即使是云服务商也看不到原始数据
Azure Integrated HSM能够为机密计算提供硬件根信任
确保数据在内存中运行时也是加密的
只有授权的应用程序才能解密
防止数据被窃取或篡改
为了证明设计的合理性
微软还详细分析了ASIC的门数分布
在ASIC的芯片面积中
硬件密码模块占了62%，
这足以说明加密功能是这款芯片的核心重心
而微软之所以决定将这款定制ASIC开源
背后有四个关键的考量
第一是安全透明度
开源能让全球的安全专家参与审计
找出潜在漏洞，相比闭源设计
“众包式”的安全验证更能抵御高级攻击；
第二是一致性
开源方案能确保微软全球数据中心的设施安全与操作安全标准统一
避免不同闭源组件间的兼容性问题；
第三是密码学标准化
加密算法本身就是高度标准化的技术
开源能更好地贴合行业标准
减少自定义闭源算法的风险；
第四是层层防御，开源并不是“裸奔”，
而是在硬件级防护的基础上
再叠加软件、协议层面的安全措施
形成多维度的安全屏障
让攻击者“突破一层还有一层”。
聊完安全
我们再看芯片领域的另一个“隐形杀手”，
散热
随着AI芯片的算力越来越强
功耗也跟着飙升
比如之前提到的AMD MI355X液冷版本
总板功耗达到1400W
谷歌Ironwood TPU的SuperPod更是要支撑百万级芯片的散热
传统的风冷、甚至普通液冷已经快扛不住了
本届Hot Chips上
散热领域的创新点也很明确
那就是要用更精细的结构设计+生成式AI优化
让散热效率追上算力的增长
其中Fabric8Labs的方案最具代表性
Fabric8Labs的核心技术是“电化学增材制造（ECAM）”，
听起来好像很复杂
但其实原理可以简单理解为
用电荷来替代光
以像素级精度来沉积铜
传统的3D打印是用激光或者喷嘴进行“堆料”，
而ECAM是利用电化学原理
让铜离子在电场作用下精准附着在基底上
从而制造出传统工艺做不到的复杂3D结构
而铜又是绝佳的导热材料
这就为散热设计打开了新的空间
他们展示的散热方案有三个关键方向
第一个是3D结构优化
通过ECAM制造出多孔、镂空的铜制散热结构
比如类似“蜂窝”或“树枝”的形态
这种结构的表面积比传统扁平式的散热片大几十倍
能够大幅提升热交换效率；
第二个是生成式AI驱动设计
用AI来模拟不同芯片的发热分布
比如GPU的SM单元、CPU的核心区域
这些都是发热的热点
然后自动生成最适配的散热结构
比如在发热最严重的区域
设计更密集的铜制通道
在低热区域减少材料用量
既保证散热效果，又避免浪费；
第三个是直接硅基沉积
这是一种更加激进的思路
把铜直接沉积在硅片上
让散热结构与芯片“零距离接触”，
热量不用经过封装层传导
直接通过铜结构导出
这种方式能够把热阻降到最低
尤其适合高功率密度的AI芯片
Fabric8Labs还展示了一款“两相液冷浸入式蒸发板”，
这款产品的核心是“优化流体蒸发过程”。
传统液冷是靠液体流动来带走热量
而两相液冷是让液体在散热板内蒸发
利用“蒸发吸热”的物理原理高效降温
比单相液冷的散热效率高3到5倍
他们通过ECAM
在蒸发板内部制造出复杂的微通道结构
增加液体与散热面的接触面积
同时引导蒸汽快速排出
避免“蒸汽堵塞”影响散热
这种设计能让散热板在相同体积下
比传统产品多带走40%的热量
更长远的设想是“封装级冷板”和“直接硅基散热”，
也就是说
未来的芯片封装不再是“先做芯片再套散热壳”，
而是把散热结构纳入封装设计的第一步
比如在Chiplet之间预留铜制散热通道
在HBM内存旁边集成微型蒸发管
甚至用ECAM在芯片裸片上直接制造散热鳍片
Fabric8Labs还提到
他们正在与EDA软件厂商合作
把ECAM散热设计工具集成到芯片的设计流程中
让芯片设计师在画电路的时候
就能同步优化散热结构
实现“算力与散热的协同设计”，
而不是事后补救
好了
以上就是本届Hot Chips大会七大技术领域的完整解析了
从AI计算的存储突破到散热的精细设计
每一个技术细节都在推动芯片行业向“更高算力、更低能耗、更安全可靠”迈进
在大会的尾声
主办方还预告了“2025全球AI芯片峰会”，
这场峰会将聚焦AI芯片的“落地挑战”，
比如如何降低AI芯片的部署成本、如何构建统一的软件生态、如何平衡算力与能耗等
预计会有更多企业展示量产级的AI芯片产品
而不只是实验室原型
对于关注AI芯片行业的朋友来说
这场峰会值得重点关注
后续我也会第一时间为大家带来报道
感谢大家收看本期视频
我们下期再见
大家好，这里是最佳拍档，我是大飞
上周
国际顶级芯片会议Hot Chips正式落下帷幕
作为芯片及系统设计领域的“年度风向标”，
今年的大会没有让人失望
从能把AI超算塞进桌面的芯片
到用光线替代电线的互连技术
再到能扛住十亿瓦级数据中心的散热方案
每一个技术细节都在悄悄改写未来科技的底层逻辑
今天这一期，我会用近一小时的时间
逐领域的拆解本届Hot Chips的核心突破
把那些藏在技术文档里的参数、架构和创新
用更加通俗易懂的方式讲透
也让大家看清楚接下来1-2年芯片行业的真实走向
这届Hot Chips的议程设计也很有逻辑
首日聚焦CPU、安全、图形与网络这四大“基础支柱”，
英特尔、IBM、AMD、英伟达这些巨头都带来了硬货
第二天则转向光学、散热与机器学习
话题更贴近AI时代的核心痛点
比如怎么解决AI芯片的“内存焦虑”，
怎么用更少的电撑更大的算力
这个视频我会沿着大会的技术脉络
从最核心的“AI计算”开始讲起
再逐步延伸到网络、光I/O、CPU等领域
确保大家能跟上技术的逻辑链条
AI计算现在最大的痛点是什么？
不是“算力不够”，而是“数据传不动”，
当模型参数涨到千亿、万亿级的时候
内存带宽和容量就成了瓶颈
很多时候芯片的算力没跑满
就卡在数据的读取上了
本届Hot Chips上
几乎所有AI计算相关的分享
都在围绕“如何突破存储瓶颈”做文章
而且方向非常清晰
要么优化内存架构
要么支持更低精度的数据格式
要么实现超大规模芯片互连
同时还要兼顾能效
毕竟现在数据中心的电费
已经成了很多企业的沉重负担
我们先来看Marvell的方案
他们提出了一个观点
存储是唯一重要的东西
并且拿出了三项针对性创新
分别是定制SRAM、定制HBM和CXL控制器
这三者层层配合
从“近内存”到“远内存”全面优化带宽和延迟
先说定制SRAM
SRAM是离AI加速器最近的内存
速度最快但是容量小
所以优化它的关键是
如何在有限的空间里榨出更多的带宽
Marvell展示了业界首款2nm定制SRAM设计
这款产品能够提供6Gb的高速内存
最关键的是它的“性价比”，
在相同工艺尺寸下
它的带宽密度是标准SRAM的17倍
所需面积减少50%，
待机功耗还能减少66%。
那它是怎么做到的呢？
核心是三个思路
一是让SRAM运行的速度更快
二是把SRAM的单元做得更宽
这样能一次传更多数据
三是增加更多端口
让同时读写的通道更多
这样一来，哪怕是1Mb的大型SRAM阵列
也能够保持高带宽密度
这对需要高频读取数据的AI推理场景来说
简直是一场“及时雨”。
然后是定制HBM
HBM，也就是高带宽内存
现在是高端AI芯片的“标配”，
但是它有个问题
就是接口会占用大量的片上空间
挤占计算单元的位置
Marvell的解法是和SK海力士、三星、美光这三大HBM供应商合作
优化HBM的基片和接口
具体来说，就是减少I/O接口的面积
腾出芯片边缘的空间来支持高速信号的传输
从而提升带宽
他们用的是标准DRAM芯片
但是搭配了为加速器定制的基片
还用上了速率达每秒每毫米30Tbps的下一代D2D IP
这样一来
不仅能缓解物理和散热限制
还能大幅减少功耗
省下来的空间就能装更多计算单元
或者增加新的功能
形成性能和功耗之间的正向循环
最后是CXL控制器
它针对的是更大规模的内存扩展
Marvell打造了Structera CXL产品线
核心就是“不绕路”。
传统的内存扩展要经过CPU和PCIe交换机
延迟高、带宽损耗大
而Marvell的高容量内存扩展设备能够直接连接
延迟更低、带宽更高
其中Structera A CXL近内存加速器很有代表性
它集成了16个Arm Neoverse v2 CPU核心、4通道DDR5
内存带宽能到200GB/s，容量达4TB
功耗却不到100W
刚好能够分担AI推理这类带宽密集型的任务
举个例子
一台64核的高端x86 CPU服务器
加一颗Structera A芯片
就能增加25%的核心数、50%的内存带宽
还能多4TB内存，但是总功耗只多100W
算下来每GB/s的传输功耗反而下降了
这对于需要扩容但是又不想换整机的企业来说
成本优势很明显
如果说Marvell是“优化现有内存的架构”，
那么AI芯片公司d-Matrix的思路
就是“重构内存与计算的关系”，
用“存内计算”来突破瓶颈
现在的AI推理有个新的趋势
那就是小参数模型的表现已经能够超过大语言模型了
但是生成更多token的时候还是会被内存卡住
像实时语音、AI agent这类场景
对延迟要求又极高
传统架构根本满足不了
所以d-Matrix的解法是把内存和计算紧密集成
重新设计内存结构
他们的AI推理芯片Corsair就是基于这个思路做的
Corsair采用数字存内计算架构
搭配自定义的矩阵乘法电路和块浮点数据格式
能效能够做到38TOPS/W
在FP8精度下算力达2400TOPS
FP4精度下更是能到9600TOPS
更关键的是延迟
用它跑Llama3-70B模型
单token生成时间仅用2ms
这对实时交互场景来说至关重要
我们再来看硬件设计
每张Corsair PCIe卡有2个封装
每个封装含4个芯粒chiplet
用的是台积电6nm工艺
总共提供2GB SRAM
内存带宽高达150TB/s
这比传统HBM的带宽高太多了
峰值功耗600W
在800MHz时功耗275W，1.2GHz时550W
属于“高性能且功耗可控”的水平
为了支持更大规模的扩展
Corsair的设计也很灵活
它的PCIe卡顶部有桥接连接器
两张卡能够通过DMX Bridge连成16个chiplet
实现“All-to-All”连接
标准服务器里能装8张卡
还能通过PCIe或者以太网
横向扩展到多台服务器
芯片内部的架构也很讲究
每个chiplet由4个Quad组成
每个Quad含有4个Slice、1个RISC-V控制核心和1个调度引擎
每个Slice又有DIMC核心、SIMD核心和数据重塑引擎
这种分层设计能够让计算和内存访问更加高效
另外
它支持MicroScaling的“块浮点格式”，
也就是一个块（Block）内所有的数据
会用相同的缩放因子来进行运算
这样既能利用整数运算的高效性
又能实现浮点的高动态范围
接下来我们来看华为的分享
他们的重点是“超节点网络”，
现在十亿瓦级AI数据中心越来越多
超节点就是把大量设备紧密连接成一个大型的计算系统
目标是把芯片数量扩展到100万
带宽提升到10Tbps
数据传输模式也从“异步DMA”变成“同步加载/存储”，
而且要能连接CPU、GPU、内存池、SSD、网卡、交换机等所有设备
华为提出的解决方案是“统一总线网格（UB-Mesh）”，
核心思路是“用统一协议+混合拓扑来平衡性能与成本”。
为什么传统网络不行呢？
因为随着节点规模扩大
传统网络的成本会“超线性增长”，
要实现100倍的带宽
成本可能要涨1000倍，这谁也扛不住
UB-Mesh的优势就是成本的“亚线性增长”，
节点越多
成本增长的越慢
在具体的视线方式上
华为研究了三种拓扑技术
一是CLOS拓扑
适合低带宽的顶级网络
比如100万个节点的规模
特点是多功能、高可靠；
二是nD mesh拓扑，适合机架
大约64个节点，或者大Pod
范围从128到8192个节点
特点是本地带宽高
远程带宽能够按需减少；
三是nD sparse mesh拓扑
适合更小的本地部署
比如16到128个节点
特点是成本低且带宽高
他们还发现一个关键规律
那就是大语言模型训练的流量是“两两分层”的
基于这个规律来设计拓扑
能进一步优化带宽的利用效率
另外
UB-Mesh还解决了“可靠性”的问题
传统光纤链路如果出故障
整个超节点都会受影响
华为的解法有两个，一是链路级重试
在同一个模块上对其他光纤链路重试
避免再次走故障路径；
二是MAC交叉连接
把MAC模块交叉连接到多个光学模块上
哪怕一个模块坏了，另一个还能工作
他们的目标是把平均无故障时间MTBF
提升100倍
具体做法是设置“热备机架”，
故障机架下线后，热备机架立刻接管
等故障机架修好后
再作为新的热备机架回归
如果机架本身带有额外芯片
还能作为“弱热备机架”来提供部分算力
进一步提升可靠性
而英伟达在AI计算领域的分享
走的是“小型化”的路线
他们把AI超算搬到了桌面
推出了GB10 SoC（系统级芯片）
这款芯片是英伟达DGX Spark小型工作站的“心脏”，
目标是让中小企业也能用上高性能AI计算
GB10的架构很有特点
它集成了英伟达Blackwell GPU和联发科（MediaTek）打造的20核Arm CPU
用的是台积电3nm工艺和2.5D先进封装
继承了Blackwell架构的所有核心功能
同时把功耗控制得很好
从参数上来看
GB10采用了128GB低功耗LPDDR5x高带宽统一内存
FP32精度下AI性能可以达到31TFLOPS
FP4精度下更是高达1000TFLOPS
额定热设计功耗TDP仅140W
这个功耗水平
放在桌面工作站里也完全可控
内存子系统是联发科做的
他们还实现了部分英伟达IP功能
比如显示控制器和C2C链接
特别值得一提的是GB10里的24MB L2缓存
它实现了CPU和GPU的缓存一致性
这能够大幅降低数据传输的性能开销
还能简化软件的开发
以前CPU和GPU要频繁同步数据
现在有了一致性缓存
开发难度直接下降一个档次
搭载GB10的DGX Spark工作站
现在开始支持更大规模的扩展
单机能跑2000亿参数的AI大模型
或者700亿参数的微调模型
如果用ConnectX-7网卡把两台DGX Spark连起来
还能支持更大的模型
比如参数超2000亿的场景
对于很多需要本地部署AI模型的企业来说
这种“小而强”的工作站
比动辄上千万的大型集群更为实用
我们再看AMD
他们带来了全新的MI350系列AI芯片
基于CDNA 4架构
核心亮点是“3D堆叠+低精度支持”，
专门为生成式AI设计
MI350系列的硬件架构很有诚意
用3D芯片堆叠技术
在两个6nm I/O基片上
堆叠了8个3nm XCD芯片
总共集成了1850亿颗晶体管
这个数量在AI芯片里可以算是第一梯队
封装方面
MI350系列支持标准OAM封装
分两个版本，MI350X采用风冷系统
MI355X采用液冷系统
液冷系统的总板功耗达到1400W
适合高密度部署
虽然风冷和液冷的内存容量、带宽相同
但是液冷版本的计算性能更高
因为它能承载更高的功率
相比上一代产品
MI350系列的两个I/O die提供了更宽、更低时钟频率的D2D连接
这样也能提升能效
毕竟高频运行虽然快，但是功耗也高
平衡频率和带宽才是关键
在内存和缓存方面
MI350系列的HBM带宽比上一代多2TB/s
内存容量也更大
这意味着跑同样的模型
需要的GPU数量会减少
间接降低了部署成本
本地数据共享LDS的容量比上一代MI300翻了一倍
XCD芯片的峰值引擎时钟频率达到了2.4GHz
每个XCD还有4MB L2缓存
这些都能提升数据的访问效率
精度支持上，MI350系列不仅支持FP8
还支持行业标准的MXFP6和MXFP4数据格式
更低的精度意味着更高的算力密度和更低的内存占用
对生成式AI推理场景非常友好
软件方面，AMD搭配了ROCm 7软件栈
根据官方数据
用MI355X跑DeepSeek R1模型
推理速度是上一代MI300X的3倍
在FP4精度下性能超过英伟达B200
预训练Llama 3 70B模型的时候
性能也能达到上一代的两三倍
这个提升幅度很可观
而且AMD还预告了明年发布的MI400系列
会搭载432 GB的HBM4
性能提升会更猛
看来呢
他们在AI芯片领域的追赶速度
已经越来越快了
最后呢我们来看谷歌的压轴分享
他们带来了代号为IRONWOOD的新一代TPU
这是谷歌首款专为大规模AI推理设计的TPU
突破点非常多
几乎覆盖了“性能、扩展、能效、可靠性”所有维度
我们先看核心参数
Ironwood单SuperPod最多可容纳9216颗芯片
采用光电路交换机OCS共享内存
可直接寻址的共享HBM内存容量达到1.77PB
FP8精度下单SuperPod性能可扩展到42.5EFLOPS
能效方面
每瓦性能是上一代谷歌TPU Trillium的2倍
还支持机密计算
集成了更多可靠性和安全性功能
硬件架构上
Ironwood是谷歌首款双计算die TPU
采用8层HBM3e内存
提供192GB容量和7.3TB/s带宽
能够满足超大规模模型的实时数据读取需求
值得一提的是
Ironwood的设计还用到了AI
谷歌和AlphaChip团队合作
用AI来设计算术逻辑单元ALU电路和优化芯片布局
大幅提升了设计效率和芯片性能
算是“用AI造AI芯片”的一个典型案例
互连方面
Ironwood支持单SuperPod扩展到9216个芯片
还能横向扩展到数十个SuperPod
满足不同规模的推理需求
硬件形态上
每个Ironwood Tray包含4个TPU
采用液冷设计；
16个Tray装入一个机架
每个机架有64个TPU
同时连接16个CPU主机机架
机架内所有互连采用铜缆
OCS负责连接其他机架
这样既能保证近距离传输的低延迟
又能实现远距离的高带宽扩展
和之前采用OCS的TPUv4相比
Ironwood把一个Pod内的芯片数量增加了1倍
而且OCS支持将Pod配置成不同大小的“矩形棱柱体”，
如果有节点失效，能直接丢弃
通过从检查点恢复
重新配置切片来使用其他的机架
这对于超大规模集群的可靠性至关重要
另外
Ironwood还搭配了谷歌第三代液冷系统
采用多重循环设计
确保进入冷却板的水足够干净
避免堵塞冷却板
它还集成了第四代SparseCore
用于嵌入和集体卸载任务
提升推理效率
电力方面
它可以通过软硬件功能平滑电力消耗波动
避免因为功耗骤升骤降影响系统稳定
根据谷歌的说法
Ironwood创下了“共享内存多处理器”的新纪录
1.77PB HBM
实现了低开销的高带宽数据共享
不仅能有效支持超大型模型
同时把每瓦性能提升到TPUv4的近6倍、Trillium的2倍
这对于需要长期运行推理任务的企业来说
能节省一大笔的电费
AI集群要想跑起来
网络可以说是一条“生命线”。
如果网络延迟高、丢包多
哪怕芯片算力再强
整体性能也会被拉垮
本届Hot Chips上
网络领域的分享主要围绕着“减负、提速、保可靠”三个方向
比如英特尔用IPU帮CPU卸载工作
AMD和英伟达推出新一代高速网卡
博通则聚焦高性能交换机芯片
目标都是让大规模数据传输“又快又稳”。
我们先看Intel的IPU E2200 400G
这款芯片用的是台积电N5工艺
核心定位是“卸载并且加速网络传输的基础设施工作负载”，
简单说就是“帮CPU干活”。
现在的数据中心里
CPU要处理计算、网络、存储等一堆的任务
很容易被网络传输所拖累
所以IPU的作用就是把网络相关的工作接过来
让CPU专注于计算
IPU E2200的网络子系统很全面
包含PCIe Gen5 x32域、400G以太网MAC、Arm Neoverse N2核心计算单元
还提供自定义的可编程卸载选项
支持P4可编程数据包处理、高性能内联加密等功能
它还支持三种工作模式
包括多主机模式、无头模式和融合模式
兼容性很强
能够适配不同的数据中心场景
实际应用方面
IPU E2200已经在数据中心里落地
比如在云环境中
它能够卸载虚拟机的网络转发任务
降低主机的CPU占用率
在存储场景中
能够加速分布式存储的数据流传输
提升存储访问速度，在AI集群中
还能优化跨节点的数据同步
减少训练延迟
可以说
IPU正在成为数据中心网络的“新基建”。
再看AMD的Pensando Pollara 400 AI网卡
这款网卡有个很特别的标签
那就是业界首款超以太网联盟（UEC）就绪的AI网卡”。
超以太网联盟UEC是干什么的呢？
就是用以太网来解决AI横向扩展网络的痛点
比如ECMP负载平衡的链路利用率低、网络和节点拥塞、丢包等问题
这些问题在AI训练集群里很常见
一旦出现丢包
训练任务可能要重新开始，损失很大
Pollara 400的核心优势是“可编程”，
它采用P4架构来构建数据包流程
P4是一种面向数据平面的编程语言
能够灵活定义数据包的处理逻辑
比传统固定功能的网卡更适应AI网络的动态需求
它能够根据AI训练的流量特点
动态调整路由策略
避免拥塞
还能够优化虚拟地址到物理地址的转换
减少数据传输的延迟
它的原子内存操作
比如对共享内存的读写
也设计在了SRAM相邻位置
进一步降低延迟，另外
它还增强了“管线缓存一致性”，
确保多节点之间的数据同步更高效
根据AMD的数据
搭配AMD的分布式深度学习通信库RCCL使用时
Pollara 400能将AI的训练性能提升40%，
这个提升很实在
因为对于AI训练来说
网络延迟每降低一点
整体训练时间就能缩短一截
英伟达的网络产品则更加激进
他们带来了ConnectX-8 SuperNIC
这是一款PCIe Gen6网卡
最高速率达到800Gb/s
有48个PCIe Gen6通道
是目前速率最高的网卡之一
ConnectX-8的设计很灵活
既支持Spectrum-X以太网
又支持Quantum-X Infiniband
以太网成本低、兼容性强
Infiniband延迟低、性能高
用户可以根据需求选，不用换网卡
为什么需要这么高的速率呢？
因为现在数据中心已经从“单服务器计算”变成了“集群计算”，
GPU需要和集群里的其他GPU、CPU、存储快速通信
速率低了根本满足不了
ConnectX-8的首个部署是GB300 NVL72
它的连接方式很讲究
因为英伟达Grace超级芯片以PCIe Gen5速度运行
所以用Gen5 x16链路来连接Grace CPU
然后用Gen6 x16链路来连接B300 GPU
确保高带宽传输
它还留了一个Gen5 x4链路连接SSD
满足存储访问需求
另外
英伟达的MGX PCIe交换机板卡也用了这款网卡
这样既能支持博通的交换机芯片
又能为未来的B300 PCIe GPU提供Gen6到NIC的高速连接
兼容性拉满
为了提升网络效率
ConnectX-8还集成了PSA数据包处理器和数据路径加速器DPA
DPA是一个RISC-V事件处理器
能够实时处理网络事件，减少CPU干预
Spectrum-X以太网的拥塞控制和路由功能
也能和DPA配合
进一步降低延迟
根据英伟达的数据
Spectrum-X以太网在训练时间步长和尾部延迟上的表现
比传统以太网好很多
能有效提升AI训练的稳定性
最后我们来看博通（Broadcom）的Tomahawk Ultra网络芯片
这款芯片是为高性能计算和AI扩展设计的
目标是改变“以太网不适合高性能工作负载”的偏见
博通的交换机阵容里
Tomahawk 6是102.4Tbps吞吐量的专用芯片
而Tomahawk Ultra则更加侧重“平衡性能与成本”。
它的packet转发管线设计很有特点
支持多项关键功能
一是链路层重传（Link Layer Retry）
作为以太网前向纠错FEC的补充
能够提高突发错误或者次优链路的健壮性
减少对高延迟的端到端重传的需求
这对AI训练来说很重要
因为端到端重传会大幅增加延迟
二是基于信用的流量控制（CBFC）
确保交换机缓冲区不会溢出
避免丢包
三是AI Fabric Header
它覆盖在以太网MAC header上
只保留最有用的字段
既能优化传输效率
又能保持完整的以太网MAC兼容性
不用改现有设备
四是网络计算支持集体操作
比如AI训练中的All-Reduce操作
能在交换机层面加速
减少数据在节点间的传输次数
另外
Tomahawk Ultra还支持拓扑感知自适应路由
能够根据网络拓扑动态调整路由
避免某个链路过载
拥塞控制功能也能确保流量均匀分布
不会出现“一条链路堵死
其他链路空闲”的情况
延迟方面
Tomahawk Ultra所有接口在以64B数据包大小测试的时候
延迟不到250ns，这个延迟水平
已经很接近Infiniband了
再加上以太网的成本优势
对很多企业来说是“性价比之选”。
随着芯片性能越来越强
电I/O的瓶颈也越来越明显
传输速率到一定程度后
功耗会急剧上升
而且传输距离有限，还容易受干扰
光I/O的优势就在于此
速率更高、功耗更低、抗干扰能力强
还能传更远的距离
本届Hot Chips上
光I/O领域的分享都在探索“如何把光技术落地”，
从共封装光学到3D光中介层
方向很明确
那就是逐步用光连接来替代电连接
尤其是在AI芯片和集群互连场景上
我们先看Celestial AI的Beach Front光结构模组
他们的思路不是“传统共封装光学”，
而是把光连接引入大型GPU
同时解决封装和散热问题
简单来说
就是不想把光学元件和GPU硬塞在一个封装里
而是用更灵活的模组设计
让光连接能适配现有GPU的形态
降低部署成本
