大家好，这里是最佳拍档，我是大飞
最近OpenAI遭遇了重大的人事变动
联合创始人之一、OpenAI总裁格雷格·布罗克曼（Greg Brockman）
宣布将延长休假时间
另一位联合创始人兼关键领导约翰·舒尔曼（John Schulman）已经跳槽到了竞争对手Anthropic
同时
产品负责人彼得·邓（Peter Deng）也选择离开
这让大飞不禁又想到
再早之前离开OpenAI的首席科学家
Ilya Sutskever
在成立了名为SSI的新公司后
一直还没有什么消息
最近翻看Ilya早期的一些视频
又有很多的感触
Ilya曾经在伯克利大学做过一次演讲
由于内容比较晦涩
知道的人也不多
但是我觉得它却是人工智能历史上的最重要的演讲之一
不仅揭示了大模型的核心原理
并且生动展示了Ilya对无监督序列学习的痴迷
今天我就来带大家回顾一下这场演讲的内容
在开始之前
我们先来复习一下机器学习的基本概念
大家都知道
机器学习就像让计算机当学生
人类当老师
通过给计算机大量的“练习题”和“答案”，
让它慢慢学会解题的能力
这就是所谓的监督学习（supervised learning）
但是，计算机真的能学到知识
而不是死记硬背吗？
Ilya告诉我们，是的
如果把模型的训练数据比作题海
模型训练就是疯狂刷题
那么只要刷的足够多
把题尽可能的作对
那么模型如果上了考场
表现就不会太差
但是如果只是死记硬背
缺少真正的应变能力
那肯定是不行的，只有去总结规律
提炼精华，才能从题海中学到真本事
接下来，我们正式进入Ilya的演讲
第一页，他先从监督学习讲起
给出了理论上的保证
所谓的理论就是统计学理论中著名的霍夫丁不等式（Hoeffding's inequality）
它的主要含义是
当训练误差足够低
而且训练样本数远大于“模型自由度”，
也就是模型规模的时候
测试误差也能保证足够低
具体来说
就是模型规模一定要小于数据规模
否则
它根本就不用进行“压缩”或者抽象
也不用去寻找规律
直接就全部死记硬背了
而这样是没有泛化能力的
所以在这个公式里面，模型的复杂度
也就是模型的参数大小
是个关键的变量
只要低训练误差加上大训练集
就能确保模型的泛化能力
这就是监督学习背后的理论保证
对于这一点，我们早已经知道的是
第一
“万能近似定理”（Universal Approaximation Theorem）早就已经论证了
深层神经网络可以逼近任意函数
第二
12年前的深度学习革命就不断地证明
只要有足够带有标注的数据
神经网络就可以学到任何知识
Ilya接着说
虽然无监督学习似乎缺乏类似的理论支撑
但是
他发现了一种叫做“分布匹配”（distribution matching）的范式
似乎能够让无监督学习也获得数学上的保障
问题的关键在于
我们要透过现象看本质
像GPT这样的语言模型
表面上是在学习预测下一个词（next token prediction）
实际上，它是在匹配语言的分布
学习语言中的隐含规律
比如，在“我爱吃苹果”这句话里
“我爱吃”后面更可能出现“苹果”，
而不是“砖头”，
这就反映了语言的内在知识
Ilya认为
这种分布匹配是一种特殊的模式规律的匹配
不同的是
它匹配的不是具体的字符串或者token序列
而是词与词之间的关系
也就是语言的规律性
类似于语义结构
而这种分布匹配
才是无监督学习获得智能的本质
不管是文本、图像还是语音
它们都有内在的分布规律性
而无监督学习就是要发现、匹配和对齐这些分布规律
所以，训练用的数据集不能过于随机
得有一定的规律性
无监督学习才能抓住它们内部的隐藏共性
而至于学到的知识对于其他任务有没有用
那就要看这些任务的数据分布是不是相似
比如从猫狗身上学到的特征
可能还能迁移到其他动物身上
但是如果是完全不同的领域
比如说医学影像
那可能就没什么参考价值了
总之
Ilya这里给了我们一个新的视角
那就是无监督学习的本质是分布匹配
是一种规律性的模式匹配
那如果我们以机器翻译为例
无监督学习又该如何实现呢？
现在我们当然已经知道
大模型可以轻松实现翻译功能
那背后又是什么原理呢？
Ilya解释道
还是前面提过的分布匹配
如果训练的数据集足够大
包含了两种语言种的各种句型和语法
那么它们的语言规律性就会显现
就可以被无监督学习到
比如，英语里出现"I"的上下文分布
和汉语里出现"我"的分布
应该有某种对应的规律性
因此Ilya指出
只要两种语言原生的数据足够丰富
以一种语言的输入作为条件
就能几乎唯一确定另一种语言的翻译等价物
而且，这个原理不仅适用于机器翻译
还适用于语音识别、图像转换等各种AI任务
自从2015年Ilya发现这个思路之后
就被它背后的数学原理给迷住了
也就是柯尔莫戈洛夫复杂度
简称柯氏复杂度
Ilya把它称为压缩原理
我们还拿翻译举例
如果我们能找到一个方法
既能最大限度地压缩英语数据
又能最大限度地压缩汉语数据
那这个方法就能抓住两种语言之间的共同规律
而这些规律就是翻译的基础
所以Ilya提出
无监督学习其实就是在寻找最优的数据压缩方法
这也为无监督学习的有效性
给出了数学上的解释
不过他也指出
现实中的机器学习任务
与这种理想化的分布匹配还是有些差距的
接下来
Ilya提出了他主要想说的观点
那就是如果把无监督学习
看作是一个数据压缩问题
那么压缩和预测之间其实有一一对应的关系
每个压缩算法都对应着一个预测模型
反之亦然
我们可以这样理解
压缩的逆操作就是解压缩
而解压缩的同义词就是预测
所以，Ilya认为这都是一回事
机器学习中的过程与逆过程
只是推理阶段的指向不同而已
从模型角度都看是一回事
在这张幻灯片中
Ilya提出了一个从压缩视角
来形式化无监督学习的思路
考虑一个机器学习算法A
它试图去压缩数据集Y
同时可以利用另一个无标注数据集X
如果我们的目标是让A尽可能好地压缩Y
那么怎么去衡量算法A的性能呢？
Ilya引入了“遗憾(regret)”这个概念
如果A的“遗憾”很低，就意味着
我们已经充分利用了无标注数据X中的所有信息
来帮助压缩Y
换句话说
没有人能比我们做得更好了
对于X中存在的任何对Y有用的模式
我们已经尽力去挖掘和利用了
这样就提供了一个评估无监督学习算法的角度
好的算法应该能够最小化这种“遗憾”，
充分挖掘无标注数据的价值
榨干海绵中的最后一滴水
接下来，Ilya要谈计算理论了
他先给了个警告
说这个理论有点晦涩
Ilya先是讨论了柯氏复杂度作为“终极压缩器”的性质
以及它与无监督学习的关联
简单来说
我们可以把一个文件的柯氏复杂度
看作是能够完整描述这份文件的最短指令
其他人如果知道这个指令
就能够完全还原出文件
在Ilya看来
一个好的无监督学习算法
就应该能够找到数据的最简洁表示
即柯氏复杂度
同时又能够最大限度地利用这种表示
来找到输入和输出之间的映射关系
不过，从数学上讲
真正的柯氏复杂度是不可计算的
但是Ilya认为
我们可以训练一个大型神经网络来近似这个过程
因为理论上
神经网络可以拟合任何函数
包括“生成文件最短指令”的这个函数
通过不断调整网络的参数
我们就可以一步步逼近最优的压缩方案
为了证明柯氏复杂度是最佳的压缩器
Ilya还给出了一个数学论证
大意就是，任何一个文件的压缩长度
加上解压算法的长度
以及制定和理解这套算法所需的额外信息
一定会大于文件最短的“压缩长度”，
即柯氏复杂度
所以
柯氏复杂度代表了压缩的理论极限
任何实际的压缩算法都不可能超越它
虽然这个极限不可达
但是它为我们评判无监督学习算法提供了一个基准
Ilya认为
GPT这些大语言模型之所以有效
正是因为它们能通过梯度下降等优化算法
不断逼近这个基准
学习到数据的高度压缩表示
并且运用到下游任务上
这就是柯氏复杂度和无监督学习之间的联系
虽然有点抽象
但是Ilya的核心思想是清晰的
那就是压缩是无监督学习的本质
而追求最简洁的压缩
就是追求最优的无监督学习
我们跳过复杂的数学证明过程
直接来看Ilya演讲的最后一页幻灯片
也是Ilya理论最精彩的部分
值得我们好好的学习和品味
前面我们已经讲过
Ilya提出从数据压缩的角度来理解无监督学习
也就是说，一个好的无监督学习算法
应该能最大限度地压缩数据
最简洁地表示数据的内容
而一个数据对象的柯氏复杂度
就是能够完整描述这个对象的、最短计算机程序的长度
可以想象
这个最短程序就像一个“压缩包”，
里面包含了重构原始数据所需的全部信息
从这个角度看
无监督学习的目标就是寻找数据的最优压缩表示
也就是柯氏复杂度
但是在实践中
我们往往需要处理多个相关的数据集
比如在机器翻译中
我们有源语言数据集X和目标语言数据集Y
如果我们希望能够把X中的句子翻译成Y中的句子
那么按照传统的思路
这就是一个条件概率问题
用柯氏复杂度来表示，就是求K(Y|X)，
即给定X的条件下，Y的最短描述长度
然而，Ilya提出了一个不同的思路
他说
与其像监督学习那样将X和Y视为条件与结果
不如将它们视为一个整体
在一个巨大的模型里面一起进行压缩
也就是说
我们要寻找一个联合的柯氏复杂度K(X
Y)，即同时压缩X和Y的最短程序长度
这就是无监督学习出来的预训练大模型
这个联合压缩程序必须能够充分利用X和Y之间的相关性
用X中的信息去自动对齐Y
就像我们学习外语的时候
会自然利用母语的知识
去理解和记忆外语单词一样
Ilya认为，这种联合压缩的思想
才是无监督学习的真正威力所在
因为现实世界的数据往往都是相互关联的
存在大量深层次的共同模式和规律
如果我们能够用无监督学习去发现和利用这些规律
就能极大地提高学习的效率和泛化能力
这也是GPT等大语言模型
能够在各种任务上展现惊人性能的原因
因为它们通过海量数据的无监督预训练
学会了训练数据集的种种内在规律性
而这种规律性在相关的数据上有通用性
可以对齐
虽然在现实中
通过梯度下降等优化算法
神经网络得到的最优压缩表示
可能不是严格意义上的柯氏复杂度
但是也足以捕捉到数据的本质特征
和它的对齐规律了
所以
Ilya的理论可以看作是一个无监督学习的新范式
它将传统的独立建模
提升到了统一的关联建模的高度
在这个范式下，无监督学习的目标
不再是单纯地压缩单一群体的数据
而是寻找数据之间的联系
而这种跨模式、跨模态的学习
才是通用人工智能的高级形态
Ilya还在幻灯片中指出
Conditioning on a dataset
not an example
也就是压缩的对象是数据集
而不是数据点，这一点非常重要
也是形式压缩与内容压缩的分水岭
形式压缩只是一个机械过程
产生不了智能
只有内容压缩才能成就人工智能
那么什么是形式压缩
什么是内容压缩呢？
举个例子
我们把一首特定的歌曲做无损压缩
保证压缩后可以100%还原成原来的音乐
这就是传统意义上的形式压缩
对象是数据个体，也就是那首音乐
而如果我们对音乐的集合进行压缩
无论是用GPT还是用Diffusion
对象就变成了一个群体
结果就是大模型了
虽然群体是由个体组成的
但是群体压缩勾勒的是群体的统计性形象
因为大模型压缩的本意
就是要找出数据集的特征和规律性
总之，Ilya想强调的核心观点是
条件柯氏复杂度 K(Y|X) ，
提供了一种理论上最优的无监督学习解决方案
它代表了从X中
提取所有对预测Y有价值的信息的理论上限
一个能够达到K(Y|X)的算法
就是利用无标签数据X对Y进行预测的最佳算法
然而，K(Y|X)在实际中是不可计算的
于是Ilya提出了一种可行的替代方案
也就是使用普通的柯氏复杂度 K(X
Y)来联合压缩X和Y
他认为，在实际的机器学习任务中
K(X,Y)可以达到与K(Y|X)相当的效果
最后总结成一句话
那就是条件建模被Ilya换成了序列建模
从而论证了GPT的大一统
好了 以上就是Ilya这次演讲的核心内容了
他似乎真的如辛顿所言
靠着自己天才般的直觉
发现了无监督学习的天机
背后呢更是隐含着深层的哲理
用实际中GPT的遗憾
来逼近理想中无遗憾的目标
如今他再度出山
建立安全超级智能公司SSI
希望这次
真的能够给人类
带来一个足够安全的超级人工智能
感谢大家收看本期节目
我们下期再见
