大家好，这里是最佳拍档，我是大飞
最近
前OpenAI成员安德烈·卡帕西Andrej Karpathy录制了一个长达 3 个小时的最新视频
深入浅出地讲解了从神经网络的起源、GPT-2、ChatGPT到最近的 DeepSeek-R1等一系列AI大模型的进化过程
内容十分通俗易懂
即使没有技术背景的观众也能够轻松地观看
不过，为了方便大家更好的理解
大飞我这里还是做了一个预习的视频
主要集中在关于大语言模型的技术性内容部分
大家可以在这个视频的基础上
再找时间去看原视频中
很多关于GPT-2、Llama 3和DeepSeek R1的案例分析
相信会有更多的收获
卡帕西先从大语言模型的训练讲起
其实大语言模型有如今的本事
很大程度上取决于它们“吃”了多少东西
这里所谓的“吃”，
其实指的就是预训练
你可以把预训练想象成让模型疯狂“读书”的过程
它读的书越多
知识就越丰富
那么显然，预训练的第一步
就是要收集和处理这些“书”。
这些“书”从哪里来呢？
主要就是互联网上的各种公开信息
这些数据
就是大语言模型的“粮食”。
像Common Crawl 自 2007 年开始就在抓取各种互联网信息
截止到 2024 年
已经索引了 27 亿个网页
而互联网上的信息鱼龙混杂
比如有恶意网站、垃圾邮件、少儿不宜的内容等等
所以通常还会维护一个黑名单
把这些网站排除在外
那网页上除了文字
还有很多图片、视频、广告之类的
所以会用到一些文本提取工具和技术
把文字从网页的 HTML 代码里“抠”出来
然后根据需要
我们可以只保留特定语言的文本
比如中文或者英文
比如FineWeb 数据集主要关注的就是英文
会使用语言分类器过滤掉非英语的网页
最后，为了保护隐私
我们还得把文本里的个人信息
比如身份证号、电话号码给去掉
通常会需要用到一些自然语言处理技术
比如命名实体识别NER
那有了数据之后，接下来要干什么呢？
因为大语言模型不识字
它们只认识数字
所以我们得把文本变成数字
这个过程就叫做Tokenization
也就是分词
我们可以把它想象成把一句话“打碎”成一个一个的小块
每个小块就是一个token
不过有时候
对于一些经常会一起出现的单词
也会把它们组合起来
变成一个新的 token
比如
“hello”和“world”经常一起出现
那么就会把它们合并成一个 token “helloworld”，
这样可以减少 token 的数量
让模型更容易“消化”。
在视频中
卡帕西利用了tiktokenizer这个工具
可以让我们更加直观地看到
GPT-4是怎么把一句话“打碎”成 token 的
比如，你可以输入一句话
看看它会被切分成几个 token
以及每个 token 对应的 ID 是什么
在“打碎”之后
我们就有了一串 token 序列
这就是所谓大语言模型的“输入”了
通常来说
大语言模型可以处理不同长度的句子
但是通常会有一个上限
比如最多 8000 个 token
这个上限就叫做“上下文窗口长度”（context window length）
由于模型的目的
是预测下一个 token 是什么
所以它会给词汇表里的每个 token 都打个分
也就是计算一个概率值，分数越高的
表示它觉得这个 token 越有可能是下一个
接下来
模型会把自己预测的下一个 token 和实际的下一个 token 进行比较
如果预测错了
就说明自己学得还不够好
需要调整一下自己的内部参数
这个调整的过程，就叫做反向传播
具体来说，模型首先要通过损失函数
计算预测结果和实际结果之间的差距
然后根据损失函数的值
计算每个参数对误差的贡献
也就是梯度
随后按照梯度的反方向
微调每个参数的值
这就是梯度下降的过程
最后重复以上步骤
直到模型的预测结果足够准确
对于大语言模型来说
Transformer就是它的大脑
这是一种很厉害的神经网络结构
特别擅长处理像文本这样的“序列数据”。
在Transformer 里面有很多的“参数”，
这些参数决定了大语言模型的“思考方式”。
实际上
这些参数会通过一套复杂的数学公式
把输入的 token 序列变成输出的概率分布
这些公式主要包括
注意力机制（Attention Mechanism）
它让模型关注到输入序列中最重要的部分；
多头注意力（Multi-Head Attention）
它会从多个角度关注输入序列
捕捉更丰富的信息；
前馈神经网络（Feed-Forward Network）
它会对每个 token 进行非线性变换
从而增强模型的表达能力；
残差连接（Residual Connection）
它用来缓解梯度消失的问题
让模型更加容易训练；
以及层归一化（Layer Normalization）
它可以加速模型收敛
提高模型的稳定性
在视频中
卡帕西借助于Transformer神经网络的3D可视化工具
向大家演示了token 是怎么一层一层地被处理的
以及每一层都发生了什么变化
训练好了大语言模型之后
就可以用它来生成文本了
这个过程就叫做推理
首先大模型会根据自己预测的概率分布
随机挑选一个 token 出来
这个挑选的过程，就叫做“采样”。
然后
模型会给词汇表里的每个 token 都打个分
也就是计算一个概率值
这个分数就代表了它觉得这个 token 有多大的可能性是下一个
因为是随机挑选的
所以即使输入同样的句子
模型每次生成的文本也可能不一样
我们可以通过调整一个叫做“温度系数”的参数
来控制生成文本的“创造程度”。
温度系数越高
生成的文本越随机、越有创意；
温度系数越低
生成的文本越保守、越可以被预测
不过
虽然现在模型能够写出像模像样的句子了
但是这时候它更像是一只模仿人说话的鹦鹉
要想让它变成一个真正的“助手”，
我们还需要对它进行调教
这个过程就叫做后训练
如果把预训练比作是通识教育
那么后训练就可以说是专科教育了
后训练的第一步
是收集和处理“对话数据”，
用来教模型怎么跟人对话
这些对话通常是你一句我一句
有很多轮
每一轮都包括用户说的话和助手说的话
那怎么去收集呢？
一般来说，AI公司会请一些人来
按照公司制定的“规矩”，
写一些对话数据
就像专门编写教材的老师一样
其中的规矩就是标注指南
它会告诉标注员
什么样的回答是好的
什么样的回答是不好的
通常会强调“有用性”、“真实性”、“无害性”等等原则
像在InstructGPT 论文中
就介绍了OpenAI 是怎么通过雇佣40名标注员
让他们根据标注指南
来构建高质量的对话数据的
在对话数据中
为了区分对话里的不同角色
可能还会加一些特殊的 token
比方说
用 <|user|> 表示用户说的话
用 <|assistant|> 表示助手说的话
不同的 模型使用的特殊 token 可能也不一样
由于模型是鹦鹉学舌
所以模型有时候会“胡说八道”，
这就是“幻觉”的问题
模型既不真正理解自己说的话是什么意思
也没办法自己去查证说的是不是真的
为了缓解这个问题
我们可以通过提问和测试
看看模型到底知道些什么
不知道些什么
这就像是学校里的“摸底考试”一样
在真正考试前看看学生的真实水平如何
另外
我们也可以让模型使用一些外部工具
比如搜索引擎、计算器等等
来获取更准确的信息
那么大语言模型到底有没有“自我意识”呢？
这是一个很有争议的问题
在安德烈·卡帕西看来
模型的“自我认知”并不是一个内在属性
而是通过训练数据或者系统提示塑造出来的
我们可以通过在训练数据里加一些关于模型自己的信息
或者在对话开始的时候告诉模型“你是谁”，
来让它知道自己是谁
但是这就像给一个机器人“设定程序”，
让它按照程序说的去做
而不是让它真正地认识自己
这背后其实反映出来的是
大语言模型的思考方式和我们人类是不太一样的
我们人类的思考是连续的、复杂的
而大语言模型的思考是离散的、一步一步的
简单来说
模型的思考能力是分散在每一个 token 上的
每个 token 就像一个小脑袋
负责处理一部分的信息
显然
每个小脑袋上能做的思考也是有限的
这也就是为什么
模型很难一步就算出一个很难的数学题
但是如果让它一步一步地算
把每一步的计算结果都写出来
它就更容易算对
另外一点要知道的是
模型与我们人类最大的不同
在于它看到的不是字，而是token
所以如果你让模型把一个单词的每个字母都写出来
或者数一数一个单词里有几个字母
它可能会搞错
因为它看不到字母，只能看到 token
这里卡帕西又提到了瑞士奶酪模型的概念
那就是大语言模型的智力
其实是参差不齐的
有时候很聪明，有时候又很笨
就像一块瑞士奶酪，上面有很多洞
所以模型有时候可能能回答出很难的历史问题
但是却算不对简单的加减法
想要解决这些问题
就需要进行后训练的第二步
也就是调教模型
其中最重要的两个方法就是微调和强化学习
相当于进阶教育
对于监督微调来说
就是继续使用预训练模型
但是在新的数据集上进行训练
这些数据集通常是针对特定任务的
比如对话、翻译、摘要等等
其中对话数据可以由人类标注员来编写
也可以通过模型来自动生成
而强化学习就像是人类训练小狗一样
我们让模型不断地尝试
如果它做得好
就给它奖励；
如果它做得不好，就惩罚它
就这样
模型就能慢慢学会怎么做才能得到更多的奖励
强化学习的关键在于设计一个好的奖励函数
告诉模型什么样的行为是好的
什么样的行为是不好的
这里卡帕西就详细分析了DeepSeek R1的例子
是如何通过强化学习训练出来的
并且指出，强化学习的优势在于
它不会受到人类表现的限制
对于像数学题和写代码这样有标准答案的领域
强化学习很适合
但是有些事情很难说清楚什么是好
什么是不好，比如写诗、写笑话
这时候
我们就可以让人来给模型生成的诗或者笑话打分
然后用这些分数来训练模型
这也就是所谓的基于人类反馈的强化学习RLHF
RLHF的优点是
我们可以用比较少的人力
来训练出来一个很厉害的模型
但是缺点是奖励模型可能会被玩坏
产生误导和欺骗，生成一些看起来好
但是实际上不好的东西
就像应试教育一样
并不是真正掌握了知识
最后
卡帕西还展望了一下大语言模型的未来发展
首先是多模态方面
未来的模型不仅能处理文字
还能处理图片、声音、视频等等
比如你可以上传一张照片
让模型给你讲一个关于这张照片的故事
或者对着模型说话
让它帮你把语音转换成文字
其次，Agent将会大发展
模型将不再只是一个助手
还能自己完成一些复杂的任务
比如你可以让Agent来帮你写一份商业计划书
或者让它帮你设计一个网站
最后
模型将会融入到我们生活的方方面面
就像如今的水和电一样
成为我们生活中不可或缺的一部分
好了
以上就是卡帕西这次最新视频的主要内容了
不仅对于专业人员来说是一次很好的补充学习
对于一些小白或者普通人来说
也是一次很好了解大模型运行原理的机会
希望大家都能有所收获
感谢大家观看本期视频
我们下期再见
