大家好，这里是最佳拍档，我是大飞
前两天
北京刚刚开完2024年的智源大会
大咖云集，发表了很多精彩的演讲
其中，Meta研究科学家
Llama 2、Llama 3的作者
托马斯·夏洛姆Thomas Scialom博士
也做了一场报告演讲
题目为大语言模型的昨天、今天和明天
在这个主题演讲中
托马斯通过对OpenAI、DeepMind、Meta等公司明星产品的分析
完整梳理了大语言模型近年来从萌芽到爆发的发展脉络
重点剖析了Llama 2等模型背后
SFT、RLHF等技术的细节和作用
同时从多模态、Agent、机器人等角度
分享了他对大语言模型未来发展的看法
今天大飞我就来跟大家分享一下
首先
托马斯从大模型的科幻时刻讲起
如果我们回顾一下现在所处的历史时刻
就会发现大语言模型的发展可谓是一日千里
就在一年半以前，ChatGPT才刚刚问世
一年前，Llama 2刚刚发布
在这之前
我们从来没有遇到过发展速度如此之快的技术
牛津大学哲学家尼克·博斯特罗姆Nick Bostrom曾经说过
当技术起作用的时候
它就不再是AI了
托马斯认为这句话说的很对
他觉得AI的影响力
可以用让多少科幻概念变得过时来衡量
GPT-3的出现是一个里程碑事件
它标志着AI具备了功能性
回顾大语言模型的历史，本质上
大语言模型可以看做是一组基于Transformer架构的权重
也就是我们通过自监督的方式
根据收集到的数据
训练了一个基于Transformer的架构
然后计算并预测下一个Token的最小损失
有两种方式可以来扩展模型的规模
分别是增加权重的参数量或者是增加训练的数据量
在GPT-3的论文中
OpenAI分别测试了扩大模型规模、增大每个训练步骤上的Batch Size、以及增加训练步数所带来的影响
他们认为蓝色的部分
也就是模型参数规模的影响最大
于是研究者们开始全力投入到模型参数规模的扩展上
从GPT-2的不到10亿参数
提升到了GPT-3的1750亿参数
他们发现
在不改变训练数据、训练步数等设置的情况下
仅仅通过增大模型参数的规模
就可以提升模型预测的准确性
这就是我们如今所熟知的Scaling Law
于是，扩大模型权重的参数量
就成为了业界的一种普遍做法
然而
DeepMind在论文“训练计算最优的大型语言模型「Training Compute-Optimal Large Language Models」”中
提出了Chinchilla
指出了OpenAI的分析存在错误和实验缺陷
他们发现
OpenAI其实忽略了训练过程中调度策略的重要性
没有为较小模型设置合适的学习率
此外
扩大训练数据的规模也对模型性能有着巨大的影响
这个我们在前两天做的一期Mistral CEO 亚瑟
曼彻的节目中，也有提到
Scaling Law实际告诉我们
当我们在扩大模型权重参数规模的时候
也要适当去扩大训练数据的规模
DeepMind之前发布的Gopher
模型参数量高达2000亿
训练消耗了大量计算资源
DeepMind认为，给定相同的计算成本
最优的训练方法应该是使用更多数据
来训练参数量更小的模型
比如说Chinchilla就只有大约700亿参数
相较于之前的计算资源分配策略
Chinchilla的计算资源分配方式
能够显著的提升性能
说明DeepMind在模型参数量和数据量之间
实现了最优的资源配置
最大程度上提升了模型的性能
接下来，在训练Llama的时候
研究人员重新思考了如何最优化计算资源
结果就是随着参数量不断增大
训练的损失函数值在不断下降
对于Llama而言
如果要想让数十亿的用户实际使用
那么推理阶段的效率
就要与训练阶段的效率同样重要
于是
托马斯他们就开始从数据和权重参数两个方面
来思考如何解决这个问题
虽然在训练阶段
研究人员可以在两者之间实现理想的平衡
但是在推理阶段，越多的权重参数
意味着需要越大的计算量
相反
训练中使用的数据量对于推理时间来说
是没有影响的
因此理论上
我们可以使用无限的数据来训练模型
但是不影响推理时间
通过对模型某种程度上的“过度训练”，
托马斯他们获得了更加小巧而且高效的模型
在这样的理念指导下
Llama系列模型可以在树莓派这样的小型终端设备上
实现媲美GPT-3的性能
而在这个基础上衍生出来的Alpaca、vLlama 等模型的累计下载量
已经超过了5000万次
Llama 1发布后人们要求公开它的权重
那时候除了GPT以外
还没有其他的开源基础模型
托马斯不禁感叹
这个领域发展是多么迅速啊
接下来
Llama 2的预训练参数规模虽然与Llama 1接近
但是增加了更多的训练数据token
使用了两倍的上下文长度
同时，Llama 2在后训练阶段
增加了指令跟随对齐
其中使用到了SFT和RLHF技术
托马斯对二者重点进行了说明
SFT，监督微调
是一种用来训练模型对齐指令的方法
简单来说
就是向标注人员展示一个提示
然后让他们编写相应的内容
托马斯他们投入了大量的人力物力
让标注人员根据各种有趣的提示生成内容
比方说，其中一个提示是创作一首
帮助记忆周期表前十个元素的诗歌
还要每个元素占一行
设计这样的任务是极具挑战性的
此外，标注人员还需要根据提示
编写出理想情况下期望模型给出的回答
然后训练人员用这些数据来微调模型
并且收集大量的指令
需要注意的是
要求标注人员编写提示和答案
远比要求他们只比较不同的回答要更费事
同时，前者的成本也比后者高十倍
而另一种方法RLHF
标注人员只需编写提示
并且比较模型生成的两个答案
从中选择出更好的答案即可
起初，托马斯他们认为SFT是黄金标准
但是考虑到可行性，不得不选择RLHF
在训练过程中
他们通过扩大数据集和调整模型大小
来完善奖励模型
不断提高模型的准确性
本质上
奖励模型就是为此而专门设计的模型
奖励模型的输入是提示及其答案
输出是一个标量分数
通过比较分数来执行分类任务
利用奖励模型
托马斯他们改进了答案
并且使用强化学习来训练模型
同时引入了回归采样技术
从若干样本中采样提示的回答
可以看到，奖励的中位数
也就是橙色曲线是稳定的
但是最大奖励会随着每次采样而增加
这表明获得最高奖励的可能性也在增加
图中橙色区域代表通过强化学习
循环利用奖励分数改进回答的潜力
托马斯他们的做法
是用奖励模型给回答打分
在当前的样本量级上取得最高奖励后
进一步将下一个量级的奖励从中位数推向最大值
从而提高平均的奖励分数
尽管一开始模型的错误较多
但是在持续进步
最后，无论是使用自己的元奖励模型
还是使用GPT-4的奖励模型
模型的表现都优于GPT-4
甚至超过了50%。
在每轮迭代优化中，可以看到
通过调整数据的分布
减少低分样本的数量
能够让得分向分布的右侧偏移
在这个过程中
托马斯他们意外发现了模型具备时间感知的能力
也就是通过设定一个模型学习的终止时间
可以让模型按照时间顺序
动态的调整答案内容
比方说
告诉模型训练知识截止到1940年
模型就不会给出涉及1940年之后知识的答案
例如它不知道谁赢得了二次世界大战
除此之外
如果训练知识截止到2023年
模型会根据GPS卫星认为地球是圆的
而如果训练知识截止到852年
模型则不知道地球是圆的还是扁平的
而这背后，正是RHLF的强大魔力
一开始，托马斯他们认为
人类的写作水平会明显高于模型
但是如果现在让你写一首关于大模型的俳句
你可能都要想半天
而模型瞬间就能生成一首
内容翻译过来是，硅基圣殿之中
沉睡着语言的巨兽
他们是智慧的产物
这首诗远远超过了大多数人的创作水平
早在项目初期
托马斯就发现只需要极少量的监督微调
模型就可以超越普通标注者的平均水平
RLHF真正的魔力在于
能够让模型达到超越人类的水平
与模型相比
人类的强项并不在于自己创作出好的答案
而在于判别答案好坏的能力
就像并非人人都能像毕加索那样画出杰作
但是我们能够区分艺术的好坏
不过
RLHF背后并不仅仅是强化学习或者人类反馈
要想创造出超越人类水平的模型
还需要结合人类和AI的能力
也就是所谓的RLAIF
最后
托马斯谈了谈自己对于大语言模型未来的看法
如今
GPT-4o已经指出多模态是未来的趋势
通过预训练和后训练的融合
语言建模技术已经能够达到较高的水平
接下来，我们要整合更多样化的信息
比如图片、声音、视频等等
让模型能够自如的处理它们
Agent也是当下火热的研究话题
得益于语言建模技术的成果
以及多模态技术
我们可以构建一个包含规划、记忆模块以及协调这些模块机制的Agent系统
在过去
语言模型仅仅能实现文本交流的功能
而有了Agent以后
就可以完成数学、执行代码、观测环境反馈等任务
机器人的相关研究也是未来的趋势
相关研究的开源库正在发展壮大
研究成本也正在逐年呈指数级下降
下一步我们会将Agent实体化
让它融入到物理世界
提供更为坚实的实践基础
一个重要的教训是
计算能力十分重要
规模化的效果是明显的
更多的算力意味着更好的性能
最近十年AI的发展突飞猛进
从ImageNet竞赛
到AlphaGo攻克围棋难关
如今的模型已经具备、接近乃至超越人类的理解力
在数学难题和逻辑推理上远超大多数普通人
AI领域虽然年轻，但是发展迅速
大家应该期待看到跟多意想不到的突破
演讲结尾，托马斯说了这么一段话
现在的AGI可能正类似于当年的哥白尼时刻
就像当初哥白尼发现地球并没有什么特别的一样
我们也可能会发现
AI其实也没有什么特别的
它只是复杂系统的自然产物
好了
以上就是托马斯这次演讲的主要内容了
大家有兴趣可以去看一下原视频回放
感谢大家的观看，我们下期再见
