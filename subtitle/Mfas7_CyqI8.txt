大家好，这里是最佳拍档，我是大飞
在我们的日常生活中
常常会遇到这样的场景
当面临一个任务的时候
有人会深思熟虑
把每一步都规划得井井有条才行动；
而有人则会先上手尝试
在实践中逐步摸索
其实
大型的推理模型在执行任务的时候
也面临着类似的“纠结”，是直接行动
还是先在“脑海”里把所有步骤都推演清楚再动手呢？
近日
加州大学伯克利分校、伊利诺伊大学厄巴纳-香槟分校（UIUC）、苏黎世联邦理工学院（ETH Zurich）、卡内基梅隆大学（CMU）等机构的研究者们
针对这个有趣的现象展开了深入的研究
他们发现，大型推理模型就像人一样
在“用脑过度”的时候也会出现问题
进而影响模型的行动能力
这项研究成果发表在了一篇名为《过度思考的危险：
审视Agentic任务中的推理-行动困境》（*The Danger of Overthinking:
Examining the Reasoning - Action Dilemma in Agentic Tasks*）的论文中
今天我们就简单来解读一下
感兴趣的观众可以通过视频简介中的链接去阅读原文
随着人工智能技术的不断发展
大型推理模型在许多领域都展现出了强大的潜力
不过，当它们处于“单机模式”的时候
在实时互动环境中的表现却往往不尽如人意
尽管它们在理论推理方面可能非常出色
但是在实际行动中
却像是“思想上的巨人
行动中的矮子”。
想象一下
一个Agent被赋予了在现实世界中完成复杂任务的使命
它不仅要实时获取信息、保持记忆
还要迅速做出反应
在这样的复杂环境下
如何平衡“思考”和“行动”的关系
成为了大型推理模型要面临的一大挑战
这也是这个研究想要深入探讨并且解决的关键问题
为了揭开大型推理模型在推理-行动过程中的奥秘
研究者们采用了一套严谨的研究方法
他们选择了现实世界中的软件工程任务作为实验框架
因为这个领域充满了各种复杂的问题和多变的环境
非常适合用来测试推理模型的能力
同时
他们使用了SWE-bench Verified基准
以及OpenHands框架内的CodeActAgent架构
通过这些工具
研究者创建了一个受控的环境
在这个环境中
推理模型必须在信息收集和推理链之间找到一个平衡点
而且还要在多次交互中保持上下文信息
这就好比让一个人在不断变化的环境中
既要收集各种信息
又要根据这些信息做出合理的决策
同时还要记住之前发生的事情
难度可想而知
在这个过程中
一旦推理模型的内部推理链过度延伸
就很可能会对环境做出错误的假设
从而影响任务的完成
在研究过程中
研究者们观察到了一个有趣的现象
那就是推理模型在面对推理-行动的困境时
表现出了一种一致的行为模式
即倾向于内部模拟
而不是与环境进行交互
简单来说
它们会花费大量的时间和精力
在自己的“内部世界”里构建复杂的预测行动链
却不愿意花费时间去适应实际的系统响应
研究者们把这种现象称为“过度思考”。
打个比方
就好像一个人在做一件事情之前
不停地在脑海里想象各种可能的情况
制定了无数的计划
但就是迟迟不付诸行动
为了更深入地了解过度思考
研究者们需要对这个过程进行量化分析
为此，他们利用LLM-as-a-judge
也就是以大语言模型作为评判者的方法
开发并且验证了一个系统评估框架
通过这个框架
他们确定了大型推理模型过度思考的三种关键模式
第一种模式是“分析瘫痪”（Analysis Paralysis）
在这种情况下
Agent就像陷入了一个思考的漩涡
花费过多的时间规划未来的步骤
却始终无法真正地采取行动
比如说，在解决一个软件问题的时候
Agent可能会花费大量时间去分析问题的各个方面
制定出一长串详细的解决方案
但就是不执行这些方案
它一直在思考如何做得更完美
却忽略了实际动手去做
才是解决问题的关键
这种行为导致Agent在环境中几乎没有任何实质性的进展
就像一个人站在原地不停地思考路线
却始终没有迈出第一步
第二种模式是“恶意行为”（Rogue Actions）
当Agent遇到错误的时候
它不是按照正常的逻辑一步一步去解决问题
而是试图同时执行多个动作
这就破坏了环境的顺序约束
这就好比在组装一件家具的时候
正常的流程是按照说明书一步一步来
但是Agent却不管不顾
同时拿起多个零件试图一次性安装好
结果可想而知，不仅无法解决问题
还可能让情况变得更糟
尽管之前Agent可能已经表现出了对逐步交互需求的认识
但是在这种情况下
它还是会继续构建复杂的动作序列
并且假定每个前一步骤都会成功
这实际上是用内部模拟代替了真实的环境反馈
第三种模式是“过早放弃”（Premature Disengagement）
大型推理模型有时会仅仅基于它们对问题空间的内部模拟
就决定终止任务
这个结果可能会表现为直接放弃
或者委托一个假设的动作序列来实现
而不是根据实际的环境反馈来做出决策
比如在开发一个软件功能的时候
Agent可能在没有充分尝试和验证的情况下
仅仅因为自己的内部模拟显示可能无法成功
就放弃了继续努力
这显然不利于任务的完成
通过这个评估框架
研究者们对推理模型的过度思考行为进行了量化分析
他们发现
推理模型的过度思考得分明显高于非推理模型
这意味着推理模型更容易受到过度思考的影响
这可能是因为推理模型经过了明确的推理训练
它们习惯于通过模拟环境互动
来产生扩展的思维链
所以在面对任务的时候
更加容易陷入过度思考的陷阱
为了进一步探究过度思考对模型性能的影响
研究者们还进行了大量的实验和分析
他们使用SWE-bench Verified对推理在Agent环境中的性能进行评估
通过生成并且评估3908条轨迹
得到了一系列有价值的发现
首先
过度思考与模型性能之间存在着很强的负相关关系
无论是推理模型还是非推理模型
随着过度思考程度的增加
模型在解决问题时的性能都会下降
只不过下降的模式有所不同
这就像是一个人在做事情时
如果想得太多，反而会变得犹豫不决
最终影响工作效率
比如说
运行具有强推理能力的o1模型
虽然它的推理能力很强
但是由于过度思考
它的问题解决率可能并不理想
而且运行成本还很高；
相比之下
运行较低推理能力的o1变体
虽然推理能力稍弱
但是由于过度思考较少
在一定程度上反而能以较低的成本达到一定的问题解决率
其次，在不同类型的模型中
过度思考的表现也有所不同
非推理模型也会出现过度思考的情况
这可能是因为它们也具有潜在的推理能力
而推理模型由于训练的方式和特点
更加容易出现过度思考
并且一旦过度思考
对它的性能的影响也更大
从回归分析的结果来看
非推理模型的beta系数更低
这意味着过度思考对非推理模型的性能影响更为严重
研究者推测
这是因为非推理模型没有经过专门的推理训练
在处理推理链时的能力有限
所以当出现过度思考的时候
表现会更差
此外
研究者们还对模型的规模、token使用以及上下文窗口与过度思考的关系
进行了研究
在模型规模方面
他们发现模型规模与过度思考行为之间存在负相关关系
以两个模型系列为例
非推理的Qwen2.5-Instruct和推理的R1-Distill-Qwen
随着模型规模从32B减小到7B
过度思考得分会逐渐增加
这可能是因为较小的模型在理解复杂环境方面存在困难
所以更为依赖内部推理链
从而增加了过度思考的倾向
不过
这种关系在不同类型的模型中表现也有所不同
随着模型规模进一步缩小
推理模型与非推理模型之间的过度思考得分差距明显缩小
这可能是因为较小的模型在处理环境复杂性时
都面临着较大的挑战
所以当面对环境互动中的失败时
它们都会更倾向于依赖内部推理链
而忽视外部反馈
在token使用方面
研究结果与一些以往的观点不同
分析表明
低推理努力程度的o1模型的过度思考得分
比高推理尝试程度的模型高出35%，
两种配置的平均过度思考得分差异具有统计学意义
这说明增加token分配可能会减少Agent上下文中的过度思考
而不是像之前的一些研究认为的那样
推理token使用量的增加会导致过度思考
这个发现突出了结构化推理过程在模型表现中的重要性
在上下文窗口方面
研究者们比较了架构和大小相似
但是上下文窗口大小从8K到32K的模型
发现上下文窗口大小与过度思考得分之间没有明显的相关性
由此推测
过度思考行为更多地是受到模型的架构设计和训练方法的影响
而不是模型的上下文能力
针对大型推理模型的过度思考现象
研究者们提出了两种潜在的缓解方法
分别是原生函数调用和选择性强化学习
这两种方法都能够显著减少模型的过度思考
同时提高模型的性能
尤其是函数调用模型
在实验中显示出了很有潜力的结果
比如说，在解决实际问题的时候
通过合理运用原生函数调用
模型可以更加高效地利用已有的资源
避免过度的内部推理
从而更快地找到解决方案
而在实际应用中
解决大型推理模型的过度思考问题
也能够带来巨大的经济效益
以o1模型为例
运行具有强推理能力的o1
可以实现29.1%的问题解决率
但是成本高达1400美元；
而运行较低推理能力的o1变体
虽然问题解决率为21.0%，
但是成本只有400美元，降低了3.5倍
另外
通过生成两个较少推理量、成本总计800美元的解决方案
并且选择其中过度思考分数较低的一个
竟然可以实现27.3%的问题解决率
这个结果几乎与强推理配置的表现相当
却将计算成本降低了43%。
这充分说明了
合理解决过度思考问题
可以在保证一定性能的前提下
大大降低计算的成本
好了
以上就是这篇论文的主要内容了
通过研究我们可以看到
推理模型在执行Agentic任务的时候
确实面临着推理-行动的困境
过度思考会对模型的性能产生负面影响
不同类型和规模的模型在过度思考方面
也会表现出不同的特点
不过
研究者们提出的缓解方法为我们提供了有效的解决方案
不仅可以减少过度思考
还能提高模型性能，降低计算成本
未来随着技术的不断进步
推理模型也必然将在更多的领域发挥重要作用
希望能看到更多针对推理模型的相关研究成果
感谢大家观看本期视频
我们下期再见
