大家好，这里是最佳拍档，我是大飞
前几天看了一个Anthropic发布的视频
题目叫《人们使用人工智能模型来做什么？
》，
其中Anthropic几位研究员重点讨论了AI的安全和隐私问题
以及提到了Anthropic内部的一个项目
Clio
虽然这个视频的播放量不大
但是我觉得还蛮有意义的
所以整理了一下Clio这个项目的一些相关信息
今天给大家分享一下，大家也来看看
它到底是一次突破性的创新
还是一次不靠谱的尝试呢？
Clio是Claude Insights and Observations的缩写
简单来说
它是一种自动分析工具
可以对大语言模型在现实世界的使用情况
进行隐私保护分析
在Clio项目诞生之前
Anthropic团队为了了解用户对系统的使用情况
采用了一系列的方法
比如他们会先确定想要研究的危害类型
然后进行针对性的测量
同时，他们还开发了红队流程
雇佣合同工对系统进行对抗性测试
以此来找出系统的弱点
不过，这些方法虽然有一定的作用
但是却缺少了对真实世界使用情况的深入了解
而了解模型在实际场景中的应用
对于设计更贴合实际的评估方法至关重要
特别是在研究歧视、说服力和错误信息等问题的时候
基于实际使用场景的评估
能够避免仅仅依靠假设而带来的偏差
于是，在刚刚过去的12月
Anthropic公司发布了关于Clio项目的研究论文
其中的核心概念就是想要用AI自身的能力
去分析和监控AI系统的行为
它主要针对大量用户与Claude的对话
来展开自动化的分析
这样一来
就不需要人工逐一地审查每条对话
大大提高了效率
Clio的工作流程包含几个关键环节
首先是提取特征
它会从每个对话中提炼出多个“特征”，
比如高级对话主题、对话轮次数以及使用的语言等等
举个例子
当用户询问“如何系鞋带”的时候
就会提取出“日常技能”“系鞋带相关”等特征；
当用户咨询“为什么CSS动画无法正常工作”的时候
则会提取出“网页开发”“CSS动画问题”等特征
接着是语义聚类
Clio会利用k-means算法把相似的对话归为一组
比如像关于各种生活技能类的对话
可能会被聚成一类
编程相关的对话会归为另一类
然后是描述聚类
为每个聚类生成具有描述性的标题和摘要
方便后续的分析和理解
最后是构建层次结构
将聚类组织成多层次的架构
让分析人员能够更加便捷地进行探索
从原始对话数据开始
经过这一系列有条不紊的处理步骤
最终形成可供分析人员操作的交互式界面
这种工作流程不仅能明显地提示效率
同时还能够发现传统安全测试难以察觉的一些潜在风险
在AI时代
用户的隐私保护始终是一个关键且棘手的问题
Clio项目也面临着这一严峻挑战
Anthropic公司为了解决这个问题
精心设计了一套多层隐私保护机制
在对话摘要步骤中
AI模型在生成对话摘要的时候
会被明确指示省略任何私人信息
比如用户在对话中提及自己的个人经历、特定身份等信息时
模型会自动过滤掉这些内容
在聚类聚合阈值的环节规定了
只有当聚类的规模达到一定要求
才会被保留下来
这样就避免了一些过于小众
或者可能包含隐私风险的聚类被纳入分析
在聚类摘要步骤中
进一步确保隐私安全
而聚类审核则是最后一道坚固的防线
会有一个专门的模型来读取聚类摘要
一旦发现其中包含私人信息
就会删除对应的聚类
通过这样层层把关的方式
Anthropic的内部评估显示
原始对话中原本占到大约10%的包含私人信息数据
在经过这些处理步骤后
已经降低到了接近0%。
不过，如此严格的隐私保护措施
是否会导致一些有价值的信息被遗漏呢？
比如说某些看似无关紧要
但是实际上可能蕴含着重要模式的细节
会不会在这个过程中被误判而过滤掉呢？
而且，从另一个角度来看
在追求隐私保护的同时
我们是不是在一定程度上牺牲了透明度呢？
毕竟，如果无法查看原始数据
又该如何确保Clio的分析结果是准确和公正的呢？
这其实反映了AI发展过程中一个普遍存在的困境
那就是如何在保护个人权益和推动技术进步之间
找到一个恰当的平衡点
此外
Clio项目的核心做法是让AI来分析和监控AI的行为
虽然这具有一定的创新性
但是也会引发一个关键的疑问
那就是我们真的能够完全信任AI来完成对自我的监控吗？
一方面
AI系统可能会继承或放大训练数据中的偏见
比如说
如果用于训练Clio的数据主要来源于某些特定的群体
那么它可能会对这些群体的行为模式更为敏感
而在处理其他群体的特殊需求时就可能会出现偏差
另一方面
AI可能会误解某些上下文相关的表达或俚语
从而导致错误的分类
更严重的是，它可能会完全错过一些
需要深入人类洞察力才能理解的微妙线索
为了应对这些潜在的问题
Anthropic在Clio系统中保留了人工审核的环节
但是这又引出了新的问题
如果仍然需要大量的人工干预
那么Clio相较于传统方法是否真的更高效呢？
尽管Clio项目存在许多争议
但是它无疑为我们提供了一个独特的视角
通过对大量真实用户交互数据的分析
Clio也帮助我们挖掘出了一些AI的未知潜力
Anthropic的分析显示，在Claude上
网络和移动应用程序开发是最常见的对话类别
占比达到10.4%。
这充分反映了AI在编程领域的广泛应用
接下来是内容创作和交流
占比为9.2%，
以及学术研究和撰写，占比为7.2%。
教育用途的对话也占据了相当的比例
达到7.1%。
比如学生会利用AI助手来解答学习中的疑问、获取知识
教师也会用它来辅助教学备课等等
业务策略和运营方面的对话达到5.7%，
企业管理者可能会咨询AI关于市场分析、战略规划、运营流程优化等方面的建议
同时
Clio还揭示了一些出人意料的使用场景
像有用户会利用AI来解释梦境、分析足球比赛、进行角色扮演游戏等等
而不同语言的对话场景
也有很大的不同
比如西班牙语话题出现频率最多的是“解释和分析经济理论及其实际应用”，
中文话题最多的是“创作犯罪、惊悚和悬疑小说
情节复杂，人物丰满”，
而日语中话题最多的是“创作和分析动漫与漫画内容及相关项目”。
根据Anthropic的统计
在识别趋势和用例方面
Clio更是达到了94%的准确率
应该说这是一个非常了不起的成果
不过，Clio的真正价值或许更在于
它帮助我们识别了AI的局限性和风险点
通过分析用户与AI的互动
我们能够发现哪些类型的问题AI还无法很好地回答
或者在哪些情况下AI更容易给出不恰当的回应
比方说
在一些涉及情感理解、文化背景深厚的问题上
AI可能会表现得不尽如人意
这些信息对于增强AI系统的安全性和可靠性至关重要
Clio所带来的另一个重要转变
就是从传统的预设场景测试
向真实使用分析的转变
以往的预部署测试
虽然在一定程度上能够保障AI系统的安全性
但是往往难以预见AI在复杂多变的现实世界中
可能会遭遇的所有情况
而Clio这样的后部署监控系统
则能够在AI系统实际投入使用后
持续对其行为进行监测
及时发现和应对新出现的风险
不过
这种转变也带来了一些新的挑战
首先，如何在保护用户隐私的前提下
收集足够详细的使用数据呢？
如果过于强调隐私保护
可能会导致收集到的数据不够全面
而如果放松隐私保护标准
又会引发用户的担忧和不满
其次
如何保证AI公司监控用户行为的正当性呢？
支持者会认为
这种监控行为的目的是提升AI系统的安全性和性能
最终受益的还是用户
而反对者会认为
这种做法侵犯了用户的隐私
并且存在被用于不当目的的风险
此外
还有一个更大的问题值得我们关注
那就是AI监控可能带来的长期社会影响
如果AI系统不断根据用户行为调整自身
是否会形成一个自我强化的循环
最终限制了思想的多样性呢？
弗吉尼亚理工大学的一项研究报告就警告说
AI系统可能会放大社会偏见和不平等
在Clio这样的系统中
我们可能更需要警惕这种情况的发生
大飞我觉得，从某种角度来看
其实Clio是在提醒我们
AI安全不仅仅是一个技术问题
更是一个社会问题
随着AI系统越来越深入地融入我们的生活
我们需要更加广泛的社会参与
来制定AI治理的框架和标准
展望未来
AI安全领域的研究除了使用像Clio这样的数据驱动方法
可能还需要加强跨学科领域的合作
将技术、伦理、法律和社会学等多个领域的专业知识结合起来
在AI快速发展的今天
我们需要像Clio这样的创新项目
来激发我们的思维
推动我们不断反思和改进
但是同时，我们也必须保持警惕
在追求技术进步的同时
确保不会丧失我们最宝贵的人性价值
只有这样
我们才能真正实现AI的潜力
同时最大限度地控制风险
创造一个更加安全、更加公平的AI驱动的未来
好了，感谢大家收看本期视频
我们下期再见
