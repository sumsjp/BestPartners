大家好，这里是最佳拍档，我是大飞
假如在一到两年内
一款真正意义上的AGI能够进入市场
你会期待它能做哪些事情呢？
不久前
GhatGPT首席架构师兼OpenAI联合创始人约翰·舒尔曼John Schulman做客Dwarkesh的访谈
分享了他对AI模型未来发展的看法
在他的眼中，AGI的实现近在咫尺
人们将在三到五年内见证人工智能的飞跃式发展
真正意义上的AGI将在这个期间问世
围绕这一预测
Schulman深入探讨了关于大模型训练的现状
相关技术的进化方向
以及未来AI需要面对的监管问题
这100分钟的访谈应该说是干货满满
今天大飞就来给大家分享一下
Schulman眼中的AI模型会走向怎样的未来
先说说AI在未来人类社会中的定位
这也是时下人们关注的焦点
在 Schulman看来
AI始终是人类的助手
而相关的技术也会朝着让AI成为更优秀的助手的方向发展
在访谈开始时，他解释道
所谓的AI训练
就是对已经从互联网中抓取了足够内容的模型
进行针对性的行为优化
来满足人类的需求
经过训练的模型可以非常精确
它不仅可以生成网上所有的内容
还可以按照要求排列内容的分布
从而实现特定的功能
Schulman期望在不久的将来
AI能够更像一个乐于助人的同事
而不仅仅是一个执行一次性查询的工具
他期待AI能够更加主动
能够理解并且参与到用户的整个项目中
甚至能够主动提出建议和帮助
它既可以充当你的私人秘书
也可以做你的程序员
但是
AI所能实现的功能都是建立在人的要求上
因此它始终都是人类的仆人
而非人类的主人
Schulman提到
目前的模型只是试图产生一些人们会喜欢
并且判断为正确的东西
而不关心产出的到底是什么
在此基础上，Schulman预测
在未来的五年内
AI模型将向着“更好的助手”进化
他希望AI能够独立完成整个编程项目
根据人类的要求自行编写代码
而不是仅仅是提供编写函数的建议
同时
AI在执行多线程与长时间任务上的能力
也将得到明显的提高
并最终做到能够长时间的编写多个代码文件
在一系列的进步下
模型将变得更有效率
可以更快地从错误中恢复
更好地处理临界情况
此外
Schulman还希望能够为AI模型添加新的任务模式
通过预训练和后训练的结合
不断改进功能，开拓新的用例
他预计随着时间的推移
人工智能将成为经济的重要组成部分
人们将更好地理解如何将其集成到不同的社会活动中
Schulman对于未来的畅想绝非空穴来风
实际上
他认为自己已经找到了AI技术通往未来的钥匙
那就是长期地进行强化学习训练
通过投入更多的多模态数据与训练成本
强化学习训练计划就会释放AI在更长时间里保持连贯的能力
一旦这种能力被解锁
我们就可以期待AI对任务的处理能够达到人类的水平
Schulman将这一质的变化称为“相变”。
他相信
一旦模型的规模与学习训练达到某个水平
就能够处理更长的任务
通过使用特定的提示语
人类可以向AI描述任务所需要的时间尺度
然后它就可以开始制定学习计划
尝试朝着目标前进
无论这个目标是一个月还是十年后
OpenAI当下的研究正是专注于AI的学习训练
也就是基于人类反馈的强化学习（RLHF）的学习系统
在RLHF中
AI表现出了类似人类的心理驱动力
或者说对于目标的渴望
以人类的行为为例
当你有了某个特定的目标
比如说吃饭
你就会试图转向一个“寻找食物”的状态
而不是其他状态
这个过程在AI上
则表现为对人类正反馈的追求
模型“希望”自己的产出得到人类的认可
Schulman认为
AI的驱动力或者目标概念
还包括实现目标后的满足感
这些因素可能与学习算法有着强关联关系
因此，他表示，在某种程度上
模型确实通过RLHF
以某种有意义的方式
实现了和人类相同的心理模式
在具体训练方案的选择上
Schulman则关注两种学习方式
一种是上下文学习
这种方式虽然样本效率高
但是会随着每个实例的变化而被破坏；
另一种是大规模训练
虽然不会随着实例变化而破坏
但是有可能过于浅薄
两种方案各有优劣
但是Schulman却不满足现有的技术路线
他探讨了第三种可能：
是否存在一种折中的学习方式
既不会随实例变化而破坏
也不会过于浅薄
而是有着更强的主观能动性
这种中间路径可能涉及模型的中期记忆能力
即对于一百万左右token的记忆调度
如果能够实现兼具前两者优点的第三种学习模式
模型就能做到既能适应上下文
又不需要大量的预训练资料
Schulman表示，在OpenAI之前
人们并没有真正努力在大规模训练和上下文学习之间找到平衡
他和他的团队希望填补这一技术空白
构建一个能进行在线学习的系统
从而使得模型具有一些类似人类的认知技能
比如自我反思的能力
以及自推理的能力
模型将能够通过自推理实现如同人类一般“举一反三”的能力
同时使用内省和自主知识来确定需要学习什么
这些能力是当前大模型所缺少的
但是一旦实现
我们就离真正的AGI不远了
当然了
任何尖端科技的发展都不可能是一帆风顺的
Schulman也坦率地承认了技术瓶颈的存在
但是他本人对于这些难题抱有乐观的态度
目前大模型面对的问题主要有四个
一是难以预测的杂项缺陷
Schulman表示
一旦开始进行长期的强化学习训练
模型将能够在更长时间里保持连贯
然而不同的模型有着不同的训练承受能力
这会导致它们经受过强的学习训练
从而陷入瓶颈
二是当下AI模型的局限性
我们依然没有完全摸清AI这个“灰盒子”，
这导致模型可能存在会幻觉等意料之外的问题
它们会错误地认为自己可以执行、甚至已经执行了某些它做不到的任务
比如
AI会表示自己已经帮助用户发送了电子邮件或叫了辆出租车
但是实际并没有这么做
不过
Schulman将以上两点称之为“鸡毛蒜皮的小事”，
这些事情也许会在初期减缓开发速度
但是不会持续太久
他相信，通过强学习训练
以上问题都可以在两到三年内获得解决
届时我们就会见证人工智能发展的又一高峰
第三点则是训练数据不足的问题
这被Schulman称为“一个对科学研究的挑战”，
足可见它受重视的程度
从人类的视角看
互联网上的知识浩如烟海
即使穷尽某个人的一生也不能完全了然
但是对于每秒吞吐上亿数据的语言模型来说
互联网中可供学习的资料的产出
远远赶不上它所消耗的速度
终有一天
大模型会陷入“辍学”的状态
也就是说再也没有足够的训练数据供它学习了
Schulman承认数据量的有限性会带来一些挑战
但是这个问题还是被夸大了
首先，即使在数据有限的情况下
通过少量示例也可以改善AI的性能
AI的“泛化”就是一个例子
通过对西班牙语资料的学习
AI的英语能力也能够得到提升
其次
只要模型的规模达到必要的程度
它就能从少量的数据中提取需要生成的功能
因此，对于GPT4级别的大模型而言
准确地识别需要的功能、并收集相关的数据
比单纯的训练资料更为重要
因此
Schulman认为数据不足还不是OpenAI眼下最要紧的问题
他们有足够时间应对挑战
最后一个难关落在了AI的聊天功能上
体验过AI的朋友都有类似的感受
AI的语言风格还是太过僵硬与公式化
Schulman相当关注聊天模型的发展
以及如何通过混合数据集
比如指令和聊天数据来优化模型
他认为，聊天模型更易于使用
并且展示出了一定程度的自省行为
这是AI智能的一种体现
眼下OpenAI正在努力改进AI的写作体验
让它更有“人情味”。
Schulman提到
他们不仅改进了ChatGPT的个性
也在探索语言模型如何影响语言的使用
在收集数据的过程中
研究者发现人们喜欢结构化的回应和大量信息
也注意到模型可能比人们需要的更加冗长
这可能是因为在标记阶段
评分者更喜欢冗长的答案
又或者是因为预训练的方式
导致模型倾向于继续生成文本
无论如何
针对AI聊天的功能优化已经被 Schulman提上了日程
从Schulman对于科研瓶颈的态度不难看出
对于AI模型在未来发展
以及它能实现的功能
这位首席架构师都充满了信心
他甚至在访谈中预言
真正意义上的AGI会在2027年到来
但是我们的社会真的能在三到五年内做好迎接更高性能的AI
甚至AGI的准备吗？
如果人工智能变得足够强大
比如能够自己经营一家成功的企业
那么人类是否还需要参与其中？
谈及这一点，Schulman认为
整个社会都需要拿出一套可以安全地处理AI的解决方案
政府要如何处理潜在的失业潮？
AI在道德伦理上是否仍然具有风险？
AI在企业中的应用是否会增加信息安全隐患？
他甚至半开玩笑地说
如果一切开发顺利
作为架构师的自己就会在一年之内丢掉饭碗
对于这些问题的解决方案
在当下的社会中还没有达成有效的共识
Schulman也承认
他不知道要如何在现在的社会中
长期保持AI与社会秩序的平衡
我们必须要找到一种方案来确保AI始终隶属于人的意志
与此同时
AI还得属于秉持社会正义的团体
一旦被心怀恶意的人滥用
AI就会导致难以想象的灾难
Schulman直言
在人们享受科技带来的繁荣和进步之前
先要考虑好如何确保这些系统不会被坏人滥用
甚至于颠覆现有的社会秩序
面对AI可能造成的问题
作为信息工程领域的大拿
Schulman提出了一个技术性很强的解决方案
那就是一套强大的监管系统与一场稳定的社会实验
他表示，既然步子迈大了容易扯到蛋
那还不如一步一走
先在社会中部署一个性能较弱的模型
再逐步对它的性能进行升级
一旦出现任何意料之外的恶性事件
整个部署流程可以随时被终止
与此同时
Schulman也寄希望于监管技术的进步
以便在系统开始出现问题的时候能够立即发现
他举了AI在公司管理中的应用作为例子
对于公司的运营者而言
海量的数据处理以及繁杂的管理问题始终是一座大山
压得人喘不过气来
如果管理者能够得到AI的辅助
那么整个公司的运营效率都将得到提高
但是如果AI表现出远超人类的管理效率
那么公司的运营还需要人类的参与吗？
至少在Schulman看来
即使人工智能展现出了超越人类的效率
人仍然是人工智能最终行动的驱动力
如果有人参与的公司
在市场竞争中败给了没有人参与的公司
那么这本身就是对社会秩序的动摇
在他看来
资本市场需要一套监察系统来维持AI与人之间的平衡
并且在必要的时刻禁止AI公司参与正常的市场竞争
Schulman还表示
虽然AI运营的公司在许多方面可能会表现得更加高效
但是它们也存在更高的尾部风险
这是因为AI在处理非常少见的情况时
仍然缺乏足够的样本
所以更有可能出现大规模故障
哪怕是出于风险管理的考虑
他也不建议把AI大规模投入到公司管理
但是他也没有就此否定AI公司的可能
Schulman认为，假以时日
当社会做好准备
AI也证明自身在管理方面比人类更善于对人类负责
那么让AI管理公司也许是可以的
在访谈的最后
Schulman从AI模型在未来社会中的畅想中回到了当代
并给予了中小企业和个人开发者一些技术路线上的建议
他认为与社会科学等其他领域相比
对于机器学习的研究是一个相对健康的领域
因为它足够“务实”，
整个领域的事物在很大程度上基于技术的实用性和实证主义
只要你能做出成果，就能获得回报
因此
他很推荐当代的有志者们投入这个领域来施展自己的才华
但是他也指出
训练创建一个真正具有人们关心的所有功能的模型
是相当复杂的
需要大量的专业人士和大量的研发积累
这个特点使得大模型的开发容易形成技术壁垒
就像企业的“护城河”一样
入行的新人难以复制前人的成功
对此
Schulman建议这些缺乏足够经验与人手的企业
可以考虑使用“蒸馏”模型来构建自己的AI系统
“蒸馏”模型的原理有点类似AI老师给AI孩子上课
开发者需要将一个复杂而且规模较大的教师模型中的知识提取出来
并传递给一个相对较小而且结构简单的学生模型
这样做的目的是在保留模型性能的前提下
减少模型的复杂度
从而使得模型易于部署和运行
当然
这么做可能会违反部分模型的服务条款
但是
舒尔曼还是肯定了这个方法的实用性
好了
以上就是舒尔曼这次访谈的主要内容
其实大飞我觉得
大家对于舒尔曼
关于AGI 2027年到来的预言呢
并不用太认真
他自己也说这个时间点呢
并不是很绝对的
但是讨论这个是很重要的
甚至他还预言了ASI超级人工智能
我理解呢
就是超越通用人工智能的智能
可能会在2029年到来
我认为呢
它想表达的更多是在未来3到5年中
我们将看到
人工智能
在科学研究
和复杂任务上的自主性飞跃
另外呢访谈中
舒尔曼还介绍了很多
关于模型训练的话题
也非常有价值
大家感兴趣的话呢
可以去看一下原视频
感谢大家观看本期视频
我们下期再见
