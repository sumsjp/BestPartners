大家好，这里是最佳拍档，我是大飞
前几天做了一期视频
OpenAI要开始组件一个新团队
做超级人工智能的超级对齐
今天我们就来讲讲，这个超级对齐
superalignment究竟是什么
我们先去掉Super
看看Alignment是什么
有一些基础的同学应该都知道
Alignment一般翻译成”对齐“，
稍微正式的说法是”让AI能够对齐人类的价值观或者偏好“，
通俗来讲
就是”做人类真正想要AI做的事情“。
其实对齐这个概念很早就被提出来
主要是不想让AI系统理解错人类指令
产生不想要结果
这在各种科幻作品中经常出现
比如说在电影《流浪地球》中
人类告诉MOSS
你的任务是延续人类文明
结果MOSS理解出来的是
延续人类文明的最优解就是毁灭人类
又或者说人类让AI来保护自己的人身安全
AI可能会认为把你关在屋子里
不让你出去，就是最安全的做法
所以，这些作品中
AI翻车一般都是翻在对齐上
就是AI无法真正理解人类文字背后的意图
这其实是文字或者语言本身会带来的问题
我们人类自己也经常犯这样的错误
词不达意，误解曲解
这些是较为广义的对齐
具体到OpenAI做的对齐
在他们之前的博客中有提到过
总体来说，我们做对齐研究
主要集中在如何设计出一个易扩展的训练信号
那然后呢
通过这个信号
让一个非常聪明的AI系统对齐于人的意图
之后提到OpenAI做对齐研究的三个重点
第一点
怎么用人类反馈训练一个AI系统
第二点
怎么训练AI系统，来辅助人类评估
第三点呢
怎么训练AI系统来做对齐相关的研究
第一点
可理解为主要通过基于人类反馈的强化学习RLHF来做
第二点，可以训练专门的AI系统
比如说摘要系统，或者提示工程
让GPT4这样的通用模型来辅助评估标注
关于第三点，前段时间
OpenAI已经开始尝试用GPT4来解释GPT2的工作
既然OpenAI已经在做对齐的工作了
那为什么还要提出一个超级对齐呢
难道是因为对齐工作已经做的太好
导致要重新提出一个目标吗？
在尝试理解这个问题之前
我们先来看看对齐的十种境界
首先
克里斯·奥拉曾经提出了一个整体框架
将不同阶段的安全研究都标记出位置
并给出了对各自的概率信心
其中对齐就是安全中最重要的部分
现在所处的AI安全研究前沿
比Constitutional AI稍微靠前一些
随后，萨米·马丁根据根据这张图启发
把对齐按难度分成了10个级别
第一级，默认的强对齐
可以理解为当前的预训练加监督微调
在扩大AI模型规模的时候
不指导或者训练它们进行特定的风险行为
也不会强加有问题的目标
所以不会构成太大风险
即便一个超人类的AI系统
如果只遵循简单的外部奖励函数和语言指令引导
就能够满足大部分的常用场景
第二级，RLHF
加入人类反馈偏好的强化学习阶段
通过在各种情况下用人类反馈
来更仔细的指导AI
确保AI在边界情况下也能表现良好
而不仅仅是在粗略指令、或者手动指定的奖励函数上
这种情况下
AI会倾向于学习简单的诚实和服从策略
即使这不是最大化奖励的最佳策略
第三级，宪法AI
Constitutional AI
这个阶段呢
加入规则指导AI来负责监督
因为人类反馈信号不够清晰和丰富
无法用来直接精调AI
所以可通过AI增强来模拟人类的反馈
从而覆盖一些边界情况
这也是基于AI反馈的强化学习，RLAIF
第四级，可扩展的监督
Scalable Oversight
这个阶段需要方法来确保
AI能够持续提供类似人类的监督信号
特别是对那种人类没有辅助无法监督的任务
所以就需要比宪法AI更有效的方法
能让AI比人类效率更高地提供类似人类的监督信号
第五级，AI辅助的可扩展监督
这个阶段，我们用1-4阶段的技巧
先对齐好一系列AI模型
然后用它们来辅助我们
研究更好的监督方法
以及加深对问题理解
之后用研究成果来改进整个监督流程
或者提高用于监督的AI对训练中AI行为的理解
第六级
用先进的可解释性技术进行监督
这个阶段是将概念或机械可解释性工具
用作监督流程的一部分
于是可以检测到AI内部可能和欺骗相关的内部状态
从而能用4和5阶段开发的AI
或者AI+人的监督
直接惩罚这种欺骗行为
第七级，用潜在的灾难性系统做实验
来了解未对齐的部分
到这个阶段，即使用2-6级中的技术
AI系统也会偶尔无视监督
进行某些不想要行为
主要会通过改变内部状态
来避免被可解释性工具检测到
并通过“操控训练指标数据”来显得行为是安全的
但是实际上模型中仍然包含不安全部分
因此
可以对这些AI系统进行潜在危险的实验
来了解模型部署后可能会如何进行泛化
这里会使用4-6级中基于AI的可解释性和监督工具
来测试系统，奖励它们进行黑客行为
尝试触发错误泛化
但是发现漏洞后
并不会尝试用基于监督的训练来消除这些行为
相反，会尝试用一些技巧
比如Red-Teaming来理解触发的基础原理
学习到关于未对齐的部分
怎么以及为什么会发生的实用技巧和新的理论见解
从而有助开发出新的风险缓解措施
第八级，理解未对齐的理论研究
所有有问题的行为要么出现得太快无法响应
要么只在系统部署时才出现
因此像前一级中的实验
是无用或者无法安全进行的
我们需要提前理解到
AI模型是如何泛化它的学习内容的
希望这些底层的见解能够激发新的对齐技术
第九级，放弃深度学习
提出一个全新的范式
如果改变深度学习系统底层的东西
也还是无法对齐
那么就已经达到深度学习范式的天花板了
第十级，不可能区域
原则上来说
对齐超智能系统是不可能的
以上就是对齐的十级分类，可以看到
现在大部分的人和机构都还处于第1阶段
少数挣扎在第2、3阶段
第2阶段往上难度会越来越大
而且会依赖于之前研究
这块做得好的头部就几个
OpenAI和Anthropic是比较早跨过2、3阶段
开始探索3以上路径的两家
其中Anthropic本身就是OpenAI安全团队独立出来的
需要注意的是
上面这些级别并非说只有做了一个才能做下一个
难度级别只是指每个级别做好的难度
可以同时探索多级
那我们回过来再说说OpenAI说的超级对齐是什么
先说为什么要加上超级、Super这个词
因为OpenAI觉得通用人工智能AGI已经满足不了自己野心
所以得以超级智能Super Intelligence为目标了
AGI和Super Intelligence的区别在于
一个是达到和人差不多智能水平
一个是超过人的智能水平
但是现在显然还没有达到AGI的水平
那为什么要以超级智能为目标呢？
OpenAI给的解释是对未来几年科技的发展速度不确定
所以先定一个更困难的目标
来对齐更强大系统
同时相信超级智能会在未来十年内到来
Altman在一些场合说过类似观点
并以此为由让大家合作建立监督组织
当然我觉得也可能是为了开始区别跟其他AI产品的对比
突出差距
所以超级对齐这个词
实际上是根据超级智能这个词来的
简单粗暴
不过，我们现在用于对齐AI的技术
比如RLHF
更多还是要依靠人类能力来监督AI
但人类是无法有效监督比我们聪明的AI系统的
所以当前技术是无法扩展到超级智能的
因此需要新的科学和技术突破
OpenAI也提到了
主要方向就是搭建一个近乎人类级别的“自动对齐AI研究员
然后用大量计算来扩展它的能力
所以准备拿出20%的算力
从而希望它能不断迭代到
能够对齐超级智能的程度
不得不说，打败AI的，终究只能是AI
这个过程会分为三步，第一步
通过用AI系统来辅助评估其他AI系统
给一些人类难评估的任务提供训练信号
同时理解和控制模型
如何将给到的监督信号
泛化到不能监督的任务上
第二步，为了验证系统的对齐性能
自动搜索有问题的行为
以及可能有问题的内部状态
第三步，故意训练错误对齐的模型
来测试整个流程
然后确认流程能检测出一些较恶劣的错误对齐情况
这个有点类似于软件工程里的混沌工程
Choas Engineering
如果我们照着对齐级别表来看
OpenAI的目标基本上把4-7级都给覆盖了
但是没有对超级智能进行独特处理
按照级别表作者的观点
认为直接认为对齐超智能系统是不可能的
所以我个人认为
OpenAI提出的超级对齐
目前更多还是噱头大过事实
主要做的还是对齐工作的延续
不过既然定下来了一个宏大的目标
又有IIya的加入和算力投入
还是很期待会做出什么样的东西
好了，以上就是本期的分享
感谢大家的观看，我们下期再见
