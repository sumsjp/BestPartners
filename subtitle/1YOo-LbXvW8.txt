大家好，这里是最佳拍档，我是大飞
最近半年
类人形机器人的新闻几乎承包了科技板块的半壁江山
特斯拉的Optimus机器人据说能创造30万亿美元的收入
而Figure AI的CEO更是放话
要打造一百万个人形机器人
最终接入劳动力市场
VC们也跟着疯狂
上百亿美金砸进这个赛道
甚至有投资人在面对做仓储机器人的团队时
直接质疑道，你们为什么还要做这个？
两年内双足双臂的类人机器人
就能接管大部分的人类工作了
但是，今天我想跟大家分享的
是一篇也许能给这个狂热市场降一降温的文章
作者是机器人领域的活化石
罗德尼·布鲁克斯（Rodney Brooks）
他不仅是MIT计算机科学与人工智能实验室（CSAIL）的前主任
还是iRobot和Rethink Robotics公司的创始人
一辈子都在跟机器人打交道
他在最新的博客里直言
当前的类人机器人，就算砸再多的钱
也学不会人类的灵活性
那些说两年内落地、五年内产生经济影响的预测
全是幻想
为什么这位行业权威会如此的笃定呢？
今天我们就从历史、技术、生理三个维度
来解读一下类人机器人的灵活性难题
以及布鲁克斯对未来15年的预判
要想回答今天的问题
我们得先回顾一下历史
很多人会觉得
类人机器人是一种新事物
但是实际上
人类对机器人操控物体的尝试
只比人工智能这个词的历史
短了一点点
1956年
约翰·麦卡锡（John McCarthy）等四位科学家提出了达特茅斯夏季人工智能研究计划
这是人工智能概念第一次被正式提出
仅仅5年后，1961年
MIT的博士生海因里希·恩斯特（Heinrich Ernst）就完成了一项里程碑式的工作
他把一个计算机控制的机械臂和手
连接到了MIT的TX-0计算机上
让它实现了捡积木、叠积木的动作
甚至还留下了当时的视频记录
有意思的是
恩斯特的导师是信息论之父克劳德·香农（Claude Shannon）
他还感谢了马文·明斯基（Marvin Minsky）的指导
而这两位
也正是1956年达特茅斯计划四位发起人中的两位
这个早期机械臂
直接奠定了后来工业机器人的基础
从那以后
工业机器人就以计算机控制的机械臂
加上末端执行器的形态
在工厂里用了60年
这里的末端执行器
就是我们常说的机器人手
但是直到今天
主流的设计依然是并行夹爪
也就是两个可以开合的平行手指
靠机械力来夹紧物体
比如布鲁克斯在斯坦福大学70年代用的并行夹爪
和他后来创办的Rethink Robotics
在2010年代卖的产品
核心原理几乎一样
唯一的区别就是后者加了摄像头
能够通过视觉定位目标
而70年代的计算机算力
还支撑不起这种视觉服务能力
德国有个叫Schunk的公司
专门做机器人抓手
现在已经能够提供1000多种并行夹爪
有电动的也有气动的
偶尔也卖一些三指径向对称的抓手
但是我们几乎看不到多关节仿人手指的工业产品
为什么？
因为直到今天
还没人能做出既有足够健壮性、又有足够力量、还能保证使用寿命的仿人手指
要么太脆弱，一碰就坏；
要么是力量不够，拿不起重物；
要么用不了多久关节就磨损了
根本满足不了工厂24小时运转的需求
那类人机器人又是怎么火起来的？
其实早在上世纪70年代
日本早稻田大学的类人机器人研究所就开始搞了
1970年代初做出了WABOT-1
这是世界上最早的全尺寸类人机器人之一
之前还花了好几年研究双足行走机构
到80年代又出了WABOT-2
之后就一直没停过
本田在80年代末
开始研发双足机器人
2000年推出的ASIMO
应该是很多人第一次在电视上看到类人机器人的样子；
索尼先做了Aibo机器狗
2003年又搞了个小型类人机器人QRIO
但是没量产；
法国Aldebaran公司2007年推出的NAO机器人
还成了国际机器人足球联赛的标准平台
后来又出了更大的Pepper
但是商业化不算成功；
还有大家熟悉的波士顿动力
1986年从MIT分出来，先做四足机器人
2013年才推出类人机器人ATLAS
布鲁克斯自己也在这个领域深耕了几十年
1992年
他在MIT的研究团队开始做类人机器人Cog
前后开发了7个不同的版本；
2008年创办Rethink Robotics
推出的Baxter和Sawyer两款类人机器人
卖了几千台，部署在全球的工厂里；
他的学生回到意大利后
还发起了RoboCub开源类人机器人项目
让全世界很多AI实验室都能做出自己的类人机器人原型
甚至在2004年
《国际类人机器人期刊》（International Journal of Humanoid Robotics）就创刊了
现在已经出到第22个年头，也就是说
类人机器人的研究
已经是一个持续了半个多世纪的老赛道了
但是为什么最近突然火了呢？
因为很多人觉得类人机器人能适配人类环境
可以用一种机型解决所有的问题
这种想法才是关键
就像Figure CEO说的
要么做几百万种特种机器人
要么做一种类人机器人
然后用通用接口解决几百万个任务
这种万能机器人的想象
加上AI热潮的加持
才让类人机器人成为了新的风口
但是布鲁克斯说
这种想象忽略了一个最核心的问题
那就是类人机器人要想替代人类
首先得有人类级的灵活性
而这恰恰是当前技术迈不过去的坎
布鲁克斯指出
现在很多类人机器人公司
包括Figure和特斯拉
都把教会机器人灵活的宝
押在了端到端学习上
他们的思路是
既然端到端学习在语音转文字、图像标注、大语言模型上都成了
那让机器人看人类做动作的视频
或者让人类远程操控机器人记录数据
再用这些数据来训练模型
不就能学会灵活操作了吗？
但是布鲁克斯说
这个思路从根上就错了
因为他们没搞懂，端到端学习的成功
从来不是纯靠数据堆出来的
而是依赖领域专属的预处理
我们先拆解三个端到端学习的成功案例
也许就能明白问题出在哪了
第一个案例是语音转文字
我们现在用的Alexa、手机语音输入
都是靠端到端学习
但是它不是直接把麦克风的原始信号喂给模型
首先要采样
然后用高通滤波器增强高频信号
因为高频是识别辅音的关键；
接着把信号切成25毫秒的帧
每帧之间重叠10毫秒
还要做加长窗口处理
避免后续傅里叶变换因为窗口太短出问题；
之后可能还要降噪
再用快速傅里叶变换FFT、Mel滤波器组这些手段
把信号分成不同的频段
最后做对数变换和余弦变换
这些预处理步骤不是随便来的
它们源自20世纪电话网络的技术
当时为了让一根线路传更多通话
工程师研究出了既压缩语音信号、但是又不影响人类理解的方法
这些预处理本质上是模拟人类听觉系统对语音的处理方式
没有这些预处理，直接喂原始信号
端到端学习根本不可能成功
第二个案例是图像标注
2012年后，深度学习
尤其是卷积神经网络CNN成为了图像识别的主流
但是CNN也不是直接用相机输出的原始像素流
相机输出的是线性的像素序列
但是CNN会先把这些像素
重构成保留空间邻接关系的数据结构
就像人类视网膜的信号会传递到大脑皮层的拓扑对应区域
相邻的像素在大脑里依然相邻一样
然后
CNN的前几层会用共享权重来实现平移不变性
比如不管猫在图像的左下角还是右上角
都能认出来
这其实是模仿了大卫·休布尔（David Hubel）和托斯坦·威塞尔（Torsten Wiesel）发现的
猫和猴子的视觉皮层柱结构
这两位科学家因为发现视觉皮层的神经元分工
获得了1981年的诺贝尔奖
而日本科学家福岛邦彦（Kunihiko Fukushima）受此启发
在1979年提出了神经感知机（Neocognitron）
这才是CNN的雏形
后来杨立昆（Yann LeCun）、杰弗里·辛顿（Geoffrey Hinton）他们在这个基础上做了改进
才有了今天的CNN
所以，图像标注的端到端学习
本质是用工程手段复现了生物视觉的核心结构
没有这个前提，纯靠像素堆数据
根本做不到高精度的识别
第三个案例是大语言模型
比如ChatGPT、Gemini
很多人觉得大语言模型是纯靠读文本学出来的
但是它也有关键的预处理步骤
首先是token化
把文本拆成一个个基本的单元
英文大概用5万个token
可能是完整单词
也可能是词根，或者前缀后缀
token不是随便拆的
是通过无监督学习
根据文本中出现的频率和组合规律选出来的
然后是向量嵌入
把每个token转换成高维向量
这个嵌入过程要学习token之间的语义相似性
比如orange和red在颜色子空间里更近
orange和fruit在物体类别子空间里更近
这些预处理
本质是把人类语言的结构
转化成机器能理解的数学形式
没有token化和嵌入
直接把字母串喂给模型
大语言模型也根本不可能学会语言
这三个案例的共性很明显
那就是端到端学习的成功
必须先有对领域核心信息的提取
语音要提取人类能听懂的频段
图像要提取生物视觉能处理的空间结构
语言要提取人类语言的语义单元
而类人机器人的灵活性学习
恰恰缺了这个关键前提
因为人类的灵活性，核心依赖触觉
但是我们现在既没有捕捉触觉数据的技术
也没有预处理触觉信号的方法
现在的公司怎么做的呢？
Figure说要通过人类视频直接迁移知识
特斯拉则完全转向纯视觉训练
让工人利用头盔和背包上的五个摄像头
录下叠衬衫、捡东西的动作
再喂给Optimus
但是这些方法
连本吉·霍尔森（Benjie Holson）提出的类人机器人奥运会任务都过不了
霍尔森是布鲁克斯在Robust
AI公司的同事
他提出了15个8岁小孩都能做
但是机器人做不到的任务，比如
把一件袖子反穿的男士衬衫叠好
并且至少扣上一颗扣子；
把自己指尖的花生酱擦干净；
用勺子把酸奶舀进杯子里，不洒出来
等等等等
这些任务难在哪呢？
难就难在需要触觉反馈
比如叠衬衫的时候
你得知道哪块布料是袖子
用多大力才不会扯破；
擦花生酱的时候
你得知道指尖哪个位置还有酱
摩擦力够不够把酱擦掉
而纯视觉数据
根本无法提供这些信息
你能看到手在动
但是看不到手用了多大劲
有没有摸到东西
摸到的东西是软的还是硬的
布鲁克斯呢
还引用了霍尔森
对于当前演示学习的批评
第一
没有腕部力反馈
人类远程操控机器人的时候
根本不知道机器人腕部受了多大力
动作精度自然上不去；
第二，手指的控制粗糙
不管是人类操控还是AI模型
都只能做到手指开合
做不到精细的关节控制；
第三，没有触觉感知
人类手上有十几万传感器
但是机器人手的触觉传感器要么没有
要么精度极差；
第四，精度只有1-3厘米
像叠衬衫这种任务
差1厘米就可能叠歪
这个精度根本不够
更关键的是
人类的灵活性不只是靠手
比如我们叠衬衫时会用肘部顶一下桌子
系鞋带的时候会用腿支撑身体
开车时会用脚踩踏板
这些全身协同的动作
加上触觉、力反馈的配合
才是灵活性的完整定义
而现在的类人机器人
连手部触觉都没搞定
更别说全身协同了
要明白触觉对灵活性的重要性
我们需要先看一个实验
这是来自瑞典乌普萨拉大学（Umeå University）罗兰·约翰松（Roland Johansson）教授的研究
他一辈子都在研究人类触觉
约翰松的团队拍了两段视频，第一段
一个人用正常的手
从火柴盒里拿出一根火柴
点燃，整个过程用了7秒；
第二段，还是同一个人
但是指尖被麻醉了
他不是完全没感觉
只是指尖的触觉没了
其他部位的感觉还在
结果呢？
这个人尝试了好几次才从火柴盒里拿出一根火柴
掉在桌子上的火柴捡不起来
好不容易拿起一根
又花了半天调整姿势
最后用了28秒才点燃
时间是原来的4倍
而且过程充满了失误
为什么会这样呢？
因为人类的手，尤其是指尖
藏着极其复杂的触觉系统
约翰松早在1979年的研究中就发现
人类手部的无毛皮肤里
有大约1.7万个低阈值的机械感受器
相当于每根指尖就有1000个左右
其他部位的密度低一些
后来
哈佛大学大卫·金蒂（David Ginty）教授的团队进一步发现
人类手部至少有15类触觉神经元
每类负责不同的触觉信号
比如
靠近皮肤表面的默克尔细胞复合体（Merkel cell complex）
对轻微按压敏感
让你能分辨物体的纹理
比如能够摸出纸是光滑的还是粗糙的；
指尖里的迈斯纳小体（Meissner corpuscles）
能够感知到微小振动
比如你拿杯子时，杯子稍微滑动一下
它就能立刻察觉到
让你及时调整握力；
皮肤深处的帕西尼小体（Pacinian corpuscles）
能够感知低频振动
比如地面的震动；
还有鲁菲尼末梢（Ruffini endings）
能感知皮肤的拉伸
比如你握拳的时候
能知道手被撑到什么程度
除了皮肤里的感受器
我们的肌肉和肌腱里还有肌肉纺锤体（muscle spindles）和高尔基腱器官（Golgi tendon organs）
前者能感知肌肉的长度变化
后者能感知肌肉的张力
比如你拿一个苹果
肌肉纺锤体会告诉你手指弯曲了多少
高尔基腱器官会告诉你用了多大劲
两者配合
你才能既抓牢苹果，又不把它捏烂
更复杂的是
人类的灵活性还依赖于预判
约翰松后来的研究发现
我们看到一个物体的时候
会根据它的外观来预判它的重量和硬度
然后提前调整握力和姿势
如果预判错了
我们会立刻通过触觉反馈进行修正
这个过程快到你根本意识不到
而当前的类人机器人，有什么呢？
最多在手上装几个压力传感器
能感知到有没有碰到东西
压力多大
但是距离人类1.7万个感受器、15类神经元的触觉系统
差了不止一个数量级
就像布鲁克斯说的
我们连触觉到底包含哪些信息都没搞清楚
更别说怎么用工程手段来捕捉、存储、复现这些信息了
而要靠纯视觉数据教会机器人灵活
就像闭着眼睛学钢琴一样
连琴键在哪都摸不到，怎么可能弹好？
当然，学术界也在尝试突破
比如MIT普利特·阿格拉瓦尔（Pulkit Agrawal）教授的团队
在2025年类人机器人灵活性研讨会上发表的最佳论文里
提出了一种手套和机器人的联动系统
相当于人类戴上一副手套
手套上固定一个机器人手
距离人类的手10厘米左右
动作同步，人类控制自己的手
带动机器人手去操作物体；
机器人手上装了触觉传感器
既能够记录触觉数据
又能给人类手提供关节级力反馈
这个系统的进步在于
它第一次把人类的动作和机器人的触觉关联起来了
但是离人类的触觉系统还差得远
它只能提供关节的力反馈
不能提供指尖的纹理感知、振动感知
更没有全身肌肉的张力感知
但是即便如此
这已经是当前最前沿的探索了
相比之下，企业们靠视频训练的方法
连这个级别的数据都没有
聊完灵活性
布鲁克斯还在博客里提了一个很少有人关注的问题
那就是类人机器人学会的灵活性
就算类人机器人学会了灵活
它的行走安全性也没解决
尤其是全尺寸的双足类人机器人
根本不适合和人类在同一个空间里活动
首先，我们得搞懂人类行走的原理
人类走路
其实是一个被动动力学和能量回收结合的过程
你可能见过一些机械模型
它们没有电源
只有几个连杆和关节，放在缓坡上
就能自己走下来，这就是被动动力学
靠重力和惯性就能维持行走节奏
人类也是如此，我们走路的时候
大部分动作是被动的
不需要大脑刻意控制；
同时，我们的肌腱会像弹簧一样
走路时储存能量
下一步蹬地时释放能量
这就是为什么我们走很久也不会觉得累
而当前的类人机器人
走路的原理完全不同
它们靠的是零力矩点算法（Zero-Moment Point
ZMP）
这个算法是米奥米尔·武科布拉托维奇（Miomir Vukobratović）在1950年代提出的
距今已经快70年了
ZMP的核心思路是
通过计算地面反作用力的力矩为零的点
来调整机器人的关节角度，维持平衡
为了快速响应失衡
机器人必须用强力电机
一旦检测到身体要歪
电机就会立刻输出大扭矩
把身体拉回来
这种设计的问题在于
机器人的结构必须非常僵硬
如果像人类一样软
电机的扭矩就无法及时传递
平衡就会被打破
而僵硬的结构，加上强力电机
意味着机器人一旦摔倒
会产生巨大的动能
这里涉及一个物理缩放规律
如果机器人的尺寸按比例放大
它的质量会按立方增长
而结构强度只能按平方增长
也就是说
全尺寸机器人的质量是半尺寸的8倍
但强度只有4倍，一旦摔倒
它的腿会像重锤一样砸向周围的物体
所以，布鲁克斯有个建议
不要靠近全尺寸行走的类人机器人3米以内
你可以去看现在所有类人机器人的宣传视频
里面要么没有人类
要么人类离机器人很远
根本没有人类和机器人并肩行走
或者人类靠在机器人身上的场景
而如果类人机器人要进入家庭、工厂
和人类近距离互动
这种安全性是必须解决的
总不能让老人或者小孩
时刻担心被机器人砸到吧？
更关键的是
学术界虽然研究被动动力学的是行走机器人很多年了
但至今没有一个能达到工业级可靠性的产品
波士顿动力的ATLAS能做后空翻
但是它的行走依然依赖强力电机和ZMP算法
摔倒时一样危险；
其他公司的产品也大同小异
也就是说，类人机器人的行走
还停留在能走的阶段
离安全行走还差得很远
最后
布鲁克斯给出了一个相对客观的预判
15年后，我们会看到很多类人机器人
但是它们的形态既不会像现在的类人机器人
也不会完全模仿人类
为什么？
因为当前的类人机器人
本质是为了模仿人类
忽略了功能优先
比如，人类有两条腿
是因为我们的祖先需要在草原上奔跑、跨越障碍；
但是在工厂里
轮子比腿更高效、更安全
布鲁克斯自己公司做的仓储机器人
用的就是新型的轮式移动系统
两年前还不存在
现在已经大规模部署了
未来的类人机器人
可能会保留能适应人类环境的核心
比如能够通过人类的门
或者操作人类的工具
但是在形态上会做优化
比如可能用轮腿结合的移动方式
用多自由度抓手来代替仿人手指
这样既灵活又安全
布鲁克斯还呼吁
行业应该把更多资源投入到基础研究上
比如搞清楚触觉的生理机制、开发更好的触觉传感器、研究被动动力学行走的工程实现
而不是把钱砸在靠视频训练机器人上
他甚至说，如果那些公司和VC
把现在1/5的钱给学术界
可能离目标更近
其实回顾机器人发展的历史
我们会发现一个规律
真正落地的机器人，都是特种化的
工业机械臂专门做装配
Roomba专门扫地
波士顿动力的Spot专门做巡检
类人机器人要想成功
可能也必须先解决某一个具体场景的核心需求
而不是追求万能
比如先做能在养老院帮老人递东西、开门的机器人
解决行走安全性和基础抓取问题
再逐步迭代灵活性
而不是一上来就想替代所有人类工作
当然，这不是说类人机器人没有未来
而是说我们需要理性看待它的发展节奏
就像布鲁克斯说的
机器人领域的进步，从来不是靠炒作
而是靠一个个技术瓶颈的突破
灵活性和安全性
就是当前类人机器人最需要突破的两个瓶颈
而这需要时间、耐心和扎实的研究
好了
以上就是布鲁克斯这篇文章的主要内容了
大家是如何看待的呢
欢迎在评论区留言
感谢观看本期视频，我们下期再见
