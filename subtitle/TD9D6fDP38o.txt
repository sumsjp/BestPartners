大家好，这里是最佳拍档，我是大飞
最近人工智能领域可谓是波澜起伏
其中最引人瞩目的
当属OpenAI下一代旗舰模型Orion的相关进展
不过
前几天爆出的Scaling Laws遇到瓶颈的消息
给Orion、Gemini和Claude等先进模型蒙上了一层阴影
也引发了AI行业的激烈争论
今天我们就来聊聊这件事
看看传闻究竟是怎么说的
正反双方都抛出了哪些观点
以及可能会拯救Scaling Laws的技术
测试时训练Test - Time Training
TTT
前几天
Sam Altman在YC的独家专访中
满怀信心地预言
2025年AGI将会降临世间
这个言论无疑让人们对人工智能的未来充满了遐想
然而，世事难料
The Information紧接着扔出一份独家爆料
直接打脸Altman
爆料称
根据OpenAI内部很多员工的消息
下一代旗舰模型Orion
并不像前代那样能实现巨大的飞跃
虽然性能仍然会超越OpenAI的现有模型
但是相较于从GPT-3到GPT-4的升级
改进幅度要小得多
不仅如此
这种提升也基本局限在了语言能力上
Orion的代码能力甚至还不如旧的模型
反而成本还更高
文章还直击OpenAI的痛点
提出Scaling Law正在逐渐放缓
如同撞上了一面墙一般
原因之一就是高质量的文本数据越来越少
为此
OpenAI不得不成立了一个专门的基础团队
由之前负责预训练的尼克·赖德Nick Ryder领导
去研究如何解决训练数据匮乏的问题
以及Scaling Law究竟还能适用多久
常看我们频道节目的朋友们
应该对Scaling Law不会陌生
OpenAI团队在2020年的一篇论文中
最先提出了Scaling Law这个概念
如今已经成为人工智能领域的一个核心假设
论文明确指出
随着大模型在数据量、计算能力以及模型大小
这三个关键要素上的不断增加
模型性能也应该会相应地持续提升
直白地讲，也就是业界一直宣称的
随着大模型参数不断增加
超级智能终有一天会实现
虽然Information后来又发了一篇文章出来灭火
但是对Scaling Law和Orion的质疑
已经在AI圈引发了一场激烈争论
一直对大模型唱反调的加里·马库斯Gary Marcus表示
这篇文章宣告了自己的胜利
数据科学家亚姆·佩勒Yam Peleg 也表示
某前沿实验室的 scaling laws 出现了巨大的收益递减问题
就连OpenAI的前首席科学家Ilya
也在路透社的采访中表示
现在重要的是「扩大正确的规模」，
他的原话是
2010年代是scaling的时代，现在
我们再次回到了奇迹和发现的时代
每个人都在寻找下一个奇迹
而Ilya的新公司SSI
也在尝试一种与OpenAI不同的Scaling方法
但是没有透露更多细节信息
而就在上个月的TED AI大会上
OpenAI 的研究人员、德扑AI之父诺姆·布朗Noam Brown 也表示
更先进的模型可能在经济上不可行
现在简单粗暴的扩展方式
会导致Scaling范式的崩溃
有意思的是，在质疑文章出来之后
很多OpenAI的员工第一时间站出来反驳
其中也有诺姆·布朗
当彭博社的经济专栏作家诺亚·史密斯Noah Smith表示对Scaling的质疑时
诺姆·布朗直接反问道
那o1-preview呢
更指出o1-preview就是推理计算的scaling
OpenAI的研究员Adam也支持道
o1系列模型的scaling有两个重要的维度
分别是训练时间和推理时间
后者也称测试时间
传统我们说的Scaling Law
专注在用更长的时间来训练更大的模型
而现在发现了Scaling的另一套齿轮
也就是测试时计算
Test Time Compute
这个新的维度
Noam在o1发布的时候其实已经解释过
就是o1在给出回答之前
会通过一个私有的CoT进行“思考”。
模型思考的时间越长
在推理任务上的表现越好
这样一来
模型的性能就不再仅仅受限于预训练阶段
而是可以通过增加推理时的计算资源
来提升模型的表现
OpenAI的产品副总彼得·韦林德Peter Welinder也表示
人们低估了测试时计算的强大能力
它可以持续更长时间的运算
进行并行处理
甚至能够任意地fork和branch
这就像是将你的思维复制1000份
然后从中挑选出最好的想法
今天Sam Altman更是站出来
直言并没有所谓的墙
好了
关于大佬们的争论我们就先介绍到这里
显然现在也没有定论
至少目前看来
Scaling Law即使效果放缓
但还远没有到撞墙的程度
这个在我们之前做的一期视频中
Anthropic的CEO也给出了同样的说法
但是Orion的性能提升受限
也绝对不是空穴来风
包括虽然OpenAI一直在强调o1推理性能的提升
但是对大模型不懂推理的论证也一直层出不穷
所以我们暂且把这个争论放置一边
来看看有别于o1的测试时计算、可能解决Scaling Law瓶颈的一个方法
测试时训练
Test - Time Training，TTT
其实我们频道在三个月之前
就介绍过TTT
它最初是由UC伯克利、UCSD机构的研究人员
于2020年在视觉模型中首次提出的
并且在2022年发表的序列模型中得到应用
大家有兴趣可以去回看我们那期视频
只不过这次的争论又把TTT推上了风口浪尖
为什么会这样呢
是因为最近MIT发表了一篇论文
证明了测试时训练TTT能够让模型性能暴涨
论文指出
TTT能够将1B微调模型的性能
提升高达6倍
并且，TTT与8B参数模型结合后
在ARC评估中可以取得53%准确率
比纯神经网络模型的SOTA提升近25%。
不仅如此，将TTT与程序生成方法集成
更是创下61.9%的最优性能
超越了人类的平均得分
那么研究人员究竟是怎样做的呢？
首先，在数据生成环节
研究团队采用了一种名为“留一法”（leave - one - out）的任务生成策略
大家可以想象这样一个场景
假设有一个装有K个样本物品的箱子
研究团队的做法是
每次从箱子中取出一个物品
将它作为特殊的测试样本
而剩下的K - 1个物品
则组成一个新的训练样本集
通过这样的方式
就可以构造出K个不同的训练任务
简单来说
就是从箱子中每次拿出一个不同的物品作为测试对象
然后用其余的物品来训练模型
从而让模型从不同的角度学习和理解数据的特征
随后
研究团队对这些任务应用了基于规则的可逆转换
比方说
他们会对数据进行旋转、翻转、颜色置换
以及打乱训练样本对的顺序等等操作
通过这种方式
研究团队成功地获得了增强后的数据集
经过这一步
TTT训练集的规模可以得到显著扩大
而且整个TTT数据构造过程可以高度自动化
不依赖人工标注
利用构造好的TTT数据集
就可以对预训练好的语言模型进行测试时训练
考虑到测试时的资源限制
研究团队采用了参数高效的LoRA手段
为每个测试任务学习一组独立的adapter参数
再附加在预训练模型的每一层之上
通过一个低秩矩阵与原始权重相乘
来起到调节作用
在这个过程中
研究人员还额外加入了对所有前缀序列的预测
目的是通过在各种长度的演示样本上都计算损失
来鼓励模型尽早地从少量信息中
总结出抽象规律
从而提高模型整体的鲁棒性
最后，为了实现TTT效果的最大化
研究人员还在推理阶段应用了数据增强和集成学习的策略
首先在推理过程中
先利用一系列预定义的几何变换算子
来扩充原始输入
生成若干个等价视角下的输入变体
然后将每个变体输入
并行地送入经过LoRA微调的模型
独立完成预测
然后再对齐和还原到原始输入空间
由此得到一组成对的预测
接着在成对预测的基础上
通过两层投票的方式完成集成融合
第一层在每种变换内部进行投票
选出置信度最高的Top 3个预测
第二层在不同变换的Top 3预测之间进行全局投票
选出最终的Top 2作为输出
经过这样的推理策略
既通过数据增强
引入了输入的多样性
又用分层投票的方式
对不同来源的预测进行了结构化的组合
进一步提升了TTT方法的效果
为了评估TTT方法的效果
研究团队以8B参数的GPT-3作为基础模型进行了测试
如果不使用TTT，只是进行微调
模型在ARC数据集上的准确率只有18.3%，
但是在加入TTT后提升到了47.1%，
增长率达到了157%。
另外，研究团队还从ARC数据集中
随机选择了80个任务作为子集进行了测试
测试发现
TTT方法对于1B模型的提升效果更加明显
调整后模型的准确率
接近调整前的6倍
并且在调整前后
1B和8B两个规模的模型之间的相对差距也在缩小
随后，团队进一步地将TTT方法
与之前在ARC任务上取得优异成绩的BARC方法
进行了比较和结合
具体来说
他们先独立运行了这两个系统
得到它们在每个测试任务上的输出
如果两者输出完全一致
那么直接可以认为推理结果是正确的；
如果输出不一致
就要看BARC是否能够生成确定的、唯一覆盖所有测试样本的解题程序
如果是
那么就认为BARC的输出更可靠；
反之，如果BARC生成了多个候选程序
但是无法确定最优解
或者干脆无法生成任何满足约束的程序
那么就认为TTT的输出更可靠
经过两种方式配合使用后
研究团队取得了61.9%的SOTA成绩
已经超过了人类的平均水平
好了
以上就是TTT这篇论文的主要内容了
其实之所以TTT会引起如此广泛的关注
还是因为它可以让模型
通过基于测试时输入的显式梯度步骤来进行更新
它与标准微调不同之处在于
TTT在极少的数据条件下也可以运行
通常可以通过单个输入进行无监督学习
或者从一两个上下文的token示例中
进行监督学习
在论文中
研究人员指出TTT在推理过程中
会通过动态的参数更新来进行自适应
这是如今大模型时代还没有被人深入探索的一个领域
直白地讲，TTT应该属于一种迁移学习
让模型利用测试数据的结构来改善自身的预测
MIT的研究人员也解释了
测试时训练就是指在测试时调整模型本身
另一位论文作者也表示
对于已经采用了CoT和搜索的测试时计算领域
TTT可以说是一个强大的补充
Keras之父同样也表示，测试时微调
是一种对DL模型中包含的向量函数
进行动态重组来适应新任务的方法
还有网友解释了o1和TTT的区别就在于梯度更新
TTT是通过改变模型的参数来适应数据
而o1则是使用内部对话来实现适应
总而言之，TTT所带来的范式革命
就在于模型推理时的即时适应能力
这很可能在推动下一代大语言模型的发展过程中
起到关键作用
好了，以上就是近期Scaling Law撞墙和Orion不及预期的传言
以及关于可能的解决方案TTT的介绍了
希望能对大家有所帮助
由于论文内容比较专业
如果大飞我有任何解读错误或者不准确的地方
欢迎大家指出
感谢大家收看本期节目
我们下期再见
