大家好，这里是最佳拍档，我是大飞
当我们每天使用GPT、Claude这些大语言模型时
我们真的知道它们是怎么思考的吗？
我们输入一个查询
模型给出精准的回答
但是中间的决策过程
却像一个完全封闭的黑盒一样
我们只能看到输入和输出
对内部的计算逻辑却一无所知
这种黑盒困境不仅让我们无法真正的信任AI
更成为了AI安全和对齐研究的最大障碍之一
而今天我们要解读的这篇来自OpenAI的论文
恰恰为打破这个黑盒提供了一种全新的思路
通过训练权重稀疏的Transformer模型
让大模型的内部计算电路变得人类可理解
这篇论文的作者团队包含了多位OpenAI的研究员
他们的核心目标是实现大模型的机制可解释性
搞清楚模型内部的算法和计算流程
在深入论文的细节之前
我们需要先搞明白一个关键问题
为什么大模型的可解释性这么难？
其实
大模型难以解释的核心原因之一
被研究者们称为叠加态（superposition）
简单来说，稠密模型
也就是我们现在常用的大多数大模型
为了追求效率
会把多个不同的概念压缩到同一个神经元或者残留通道中进行表示
这就好比一个房间里挤满了人
每个人都在同时说话
你根本听不清任何一个人的声音
导致神经元的激活模式变得毫无规律
无法对应到具体的人类可理解概念
之前的研究尝试过各种方法来解决这个问题
比如通过稀疏自编码器（SAE）学习一个稀疏的表示基
再在这个基础上解读模型计算
但是这些方法都有一个共同的局限
它们需要先对复杂的计算进行抽象
而这些抽象本身可能会偏离模型的真实机制
最终得到的可解释电路往往掺杂了人为假设
不够纯粹
而这篇论文提出的思路完全不同
既然稠密模型的叠加态是问题根源
那我们何不直接训练一个不叠加的模型呢？
论文的核心创新的就是
训练一个绝大多数权重都为零的Transformer模型
也就是权重稀疏模型
在这种模型中
每个神经元只能与少数几个残留通道建立连接
这就从根本上限制了模型
将多个概念压缩到同一组件中的可能
迫使模型用独立的、紧凑的电路来实现不同的任务
这种从源头设计可解释性的思路
彻底改变了之前事后解读黑盒的研究范式
接下来
我们详细聊聊论文中的关键技术细节
如何构建这样一个权重稀疏模型
首先是模型架构
作者团队基于GPT-2的解码器架构进行了改造
为了确保残留流中的零值具有特殊意义
他们用RMSNorm替代了传统的LayerNorm
这种选择不仅能让归一化权重
融入MLP和注意力权重中
还不会改变权重的L0范数
也就是非零参数的数量
这样一来
模型的嵌入层和非嵌入层是解耦的
部分模型还引入了attention sinks
这是一种每个注意力头可学习的分母偏置
能让注意力电路更加清晰
同时不会显著影响模型的性能
在稀疏性约束上
模型实现了双重稀疏
分别是权重稀疏和激活稀疏
权重方面，最极端的模型
每1000个权重中只有1个非零值；
激活方面
通过在模型各个节点位置插入AbsTopK激活函数
确保每个位置只有四分之一的激活值非零
这里的AbsTopK函数
会保留绝对值最大的k个激活值
将其余置零
这种设计能进一步强化模型的稀疏表示
让每个节点的功能更加明确
模型的优化过程同样充满了巧思
作者使用了AdamW优化器
核心挑战是如何在训练中严格执行L0权重稀疏约束
他们的解决方案是
在每个训练步骤的AdamW更新后
将每个权重矩阵中
除了最大幅值之外的所有元素置零
确保每个矩阵的非零元素比例一致
为了让模型适应稀疏性
他们还采用了L0退火策略
在训练的前50%，
模型从完全稠密逐渐过渡到目标稀疏度
学习率调度则采用了鲨鱼鳍sharkfin模式
包含1%的热身阶段
并且随着L0范数的减小适当增大学习率
因为稀疏度越高的模型
就需要更大的学习率才能稳定训练
此外，梯度裁剪
也就是将梯度的均方根限制为1
被证明是确保训练稳定性的一个关键步骤
没有梯度裁剪的模型
会出现严重的训练震荡
训练好权重稀疏模型后
下一步就是提取每个任务对应的可解释电路
这就需要论文提出的全新修剪算法
很多人可能会问
传统的模型修剪不也是去掉不重要的参数吗？
但是传统修剪有两个致命问题
一是修剪的单位太粗糙
比如整个注意力头
二是修剪后的电路依然庞大
无法直接解释
而这篇论文的修剪算法
实现了颗粒度最细的电路提取
首先
作者对节点和电路给出了精准定义
节点包括单个神经元、注意力通道、残留通道的读取或写入操作
而电路则是由非零权重连接起来的节点集合
这种定义让修剪能够精确到模型的最小计算单元
修剪的目标是找到完成特定任务所需的最小电路
具体过程是通过训练一组掩码
来控制每个节点的启用或禁用
掩码参数被钳位在[-1,1]之间
通过Heaviside阶跃函数转换为布尔值
决定节点是否被纳入电路
为了让掩码训练可微分
作者使用了sigmoid替代梯度
来近似阶跃函数的梯度
损失函数则是任务交叉熵和电路大小的线性组合
这样既可以保证电路能完成任务
又能最小化电路规模
训练完成后
还需要通过二分法确定刚好达到目标损失的电路大小
并且对最终的logits进行缩放和平移校准
为了优化修剪的超参数
作者还使用了CARBS超参数搜索框架
每个模型和任务组合都会经过32轮迭代搜索
确保找到最优的修剪方案
与传统的基于归因的修剪相比
这种学习式修剪的优势非常明显
在相同的任务损失下
学习式修剪找到的电路大小要小得多
论文中给出的数据显示
在多个任务上
学习式修剪的电路边缘数量
比归因修剪要少一个数量级
这为电路的人工解读奠定了基础
接下来就是这篇论文最精彩的部分
通过三大任务的定性分析
验证稀疏模型电路的可解释性
这三个任务分别是字符串闭合、嵌套深度计数和变量类型跟踪
覆盖了从简单到复杂的不同计算场景
我们逐一来看
第一个任务是字符串闭合（single double quote）
任务目标很简单
根据字符串开头的引号类型
比如单引号或者双引号
来预测对应的闭合引号
作者发现，模型完成这个任务的电路
只包含12个节点和9条边
完全可以被人类完整理解
具体来说，这个电路分为两个步骤
第一步是在最早期的MLP层（0
mlp）
模型将输入的引号标记嵌入转换为两个关键通道
分别是引号检测器（channel 985）和引号类型分类器（channel 460）
其中
引号检测器对单引号和双引号的激活都是正向的
作用是识别出引号的存在；
而引号类型分类器对双引号激活正向、对单引号激活负向
负责区分引号类型
第二步是在第10层的注意力头（10
attn
head82）
这个注意力头用引号检测器作为键（channel 1）
用引号类型分类器作为值（channel 0）
而最后一个标记的查询是恒定正值
因此注意力头会通过关注开头的引号
复制对应的类型信息
最终预测出正确的闭合引号
更令人惊喜的是
这个电路中的节点在整个预训练分布上都具有较好的单义性
比如引号类型分类器通道
在预训练数据中
只要是双引号字符串
激活值就为正，单引号字符串则为负
在非字符串区域则接近零
这意味着模型的节点确实学到了人类可理解的自然概念
而不是任务特定的虚假特征
第二个任务是嵌套深度计数（bracket counting）
难度比字符串闭合更高
需要模型根据列表的嵌套层数
预测用一个右括号（]）还是两个右括号（]]）闭合
作者提取的最小电路包含7个节点和4条边
计算逻辑可以分为三个步骤
首先是嵌入阶段
左括号（[）的标记嵌入
会写入三个残留通道
分别是759、826和1711
这些通道成为括号检测器
专门响应左括号标记
接下来是计数阶段
第2层的一个注意力头head 125
将这些括号检测器的激活值求和
得到一个开放括号检测器
head 125 channel 12
由于这个注意力头的查询几乎为零、键在序列中恒定
所以softmax注意力操作
就相当于对上下文所有左括号的激活值求平均
然后将结果写入残留通道1249
这个通道的激活幅值就编码了列表深度
最后是阈值阶段
第4层的另一个注意力头head 80
利用attention sinks
将列表深度作为查询通道激活（channel 4）
当查询与键的乘积大于attention sinks时
说明是嵌套列表
注意力头会输出正向激活到残留通道1079
最终预测两个右括号]]，
否则预测一个右括号]。
这个电路的美妙之处在于
我们可以基于对它的理解
成功设计出对抗样本
比如
在列表前加入大量不匹配的左括号作为干扰项
模型会因为平均激活值被干扰
而误判嵌套深度
或者将列表做得非常长
由于激活值是上下文的平均值
会随着上下文长度增加而稀释
模型会错误地将嵌套列表判断为扁平列表
这些对抗样本的成功
恰恰证明了我们对电路的解读是正确的
模型确实是按照我们发现的逻辑在进行计算
第三个任务是变量类型跟踪（set or string）
要求模型跟踪变量current的类型
是集合set()还是字符串""，
并且预测后续应该使用
add方法，还是+=运算符
这个任务的电路相对复杂一些
采用了两跳算法
涉及两个注意力头的协作
首先，第4层的注意力头（head 73）
会将变量名current的嵌入
复制到set()或者字符串""标记中
然后
当模型需要在序列末尾回忆变量的类型时
第6层的注意力头（head 26）
会以current的嵌入作为查询和键
将之前存储在set()或字符串""标记中的类型信息
复制到最终的标记位置
从而选择正确的运算符
这个电路虽然包含的查询、键通道和值通道更多
但是每个通道的功能依然清晰
计算流程符合人类的逻辑推理方式
通过这三个任务的分析
我们可以看到权重稀疏模型的核心优势
它的电路是紧凑的、模块化的
节点对应自然概念
连接逻辑直观易懂
而且这些电路是完成任务的必要且充分条件
如果我们将电路中的节点进行均值消融
也就是固定为预训练分布的平均激活值
那么模型的任务性能会急剧下降
而消融电路之外的节点
任务性能几乎不受影响
这说明我们找到的
确实是模型完成任务的核心计算路径
除了直接训练稀疏模型
论文还提出了一种桥接（bridges）方法
让稀疏模型能够解释已经训练好的稠密模型
这一点非常重要
因为现有的稠密模型在性能上远超稀疏模型
而且训练成本极低
我们不可能为了可解释性而放弃所有现有的稠密模型
桥接方法的核心思路是
在稠密模型和稀疏模型的每个子层之间
训练一组编码器和解码器
编码器将稠密模型的激活
映射到稀疏模型的激活空间
解码器则反之
为了让桥接后的稀疏模型与稠密模型的计算保持一致
作者设计了三种损失函数
首先是归一化MSE损失
确保编码器和解码器的映射精度
其次是KL散度损失（d→s方向）
训练稀疏模型能够接受稠密模型的激活
并且输出与稠密模型一致的结果
第三是KL散度损失（s→d方向）
训练稠密模型能够接受稀疏模型的激活
并保持输出一致
通过这种方式
稀疏模型就成为了稠密模型的可解释替身
我们可以在稀疏模型上找到任务对应的电路
然后通过桥接将对稀疏模型的干预映射到稠密模型上
从而验证电路的一致性
论文中给出了两个非常有说服力的实验
第一个实验针对字符串闭合任务
在4层稠密模型和对应的桥接稀疏模型上
当我们在稀疏模型的引号类型分类器通道施加干预
让它的激活值
从双引号模式转向单引号模式的时候
通过桥接映射到稠密模型后
稠密模型输出单引号的概率会显著上升
第二个实验针对while return true任务
模型需要判断行尾应该输出冒号还是换行
当我们在稀疏模型的对应通道施加干预
让它的激活值从return true模式转向while true模式的时候
稠密模型输出冒号的概率也会明显增加
这两个实验证明
稀疏模型的电路不仅自身可解释
还能反映稠密模型的内部计算机制
为解释现有大模型提供了全新的途径
当然，任何研究都有一定的局限性
这篇论文也不例外
首先是训练和部署的效率问题
权重稀疏模型的非零参数是无结构的
这与现代GPU的张量核心优化不兼容
导致它的训练和推理效率
比稠密模型低100到1000倍
而且训练过程中会出现大量的死亡神经元
进一步降低效率
其次是多义特征的问题
虽然稀疏模型的节点比稠密模型更单义
但是对于复杂任务
概念依然会在少数几个节点上轻微叠加
完全的单义性还没有实现
第三是特征的非二值化
很多节点的激活幅值包含重要信息
比如嵌套深度计数中的平均激活值
这意味着我们不仅要解释节点是否激活
还要解释激活的强度
增加了解读难度
此外，论文中使用的均值消融
虽然能够验证电路的必要性和充分性
但是与更严格的causal scrubbing相比
在忠实性验证上还有差距
论文作者也提出了未来的研究方向
第一个方向是训练可解释模型的有机体
将稀疏模型的规模扩展到GPT-3级别
探索不同规模模型中
是否存在通用的电路基序，如果存在
我们就可以通过研究这些基序
来指导前沿大模型的可解释性研究
第二个方向是针对特定任务训练稀疏桥接模型
比如欺骗、拒绝、目标寻求等等
与AI安全密切相关的任务
虽然无法对前沿模型进行全面的逆向工程
但是可以为AI安全提供有价值的工具
第三个方向是结合自动化可解释性
稀疏电路提供了一种更简单的计算表达语言
可能成为自动化解读大模型的关键原语
突破当前自动化可解释性的瓶颈
总结一下
这篇论文提出了一种可解释性优先的大模型训练范式
通过权重稀疏约束
迫使模型使用紧凑、模块化的电路完成任务
这些电路中的节点对应人类可理解的自然概念
连接逻辑直观易懂
同时
桥接方法让稀疏模型能够解释现有稠密模型
解决了可解释性与模型性能之间的矛盾
虽然这种方法目前存在训练效率低等局限性
但是它为大模型可解释性研究打开了一扇全新的大门
让我们看到了完全理解大模型内部计算机制的可能性
好了，感谢大家收看本期视频
我们下期再见
