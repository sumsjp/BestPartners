大家好，这里是最佳拍档，我是大飞
在近期举办的GTC 2025上
机器学习大佬彼得·阿比尔Pieter Abbeel
进行了一场关于「机器人训练数据」的主题演讲
主要探讨了如何解决人形机器人的数据困境问题
今天就来给大家总结一下其中的核心内容
我们先简单介绍一下彼得·阿比尔
他不仅是加州大学伯克利分校电气工程与计算机科学系的教授
同时也是伯克利机器人学习实验室主任
以及伯克利人工智能研究实验室BAIR的联合主任
在当今一线AI企业的知名创始人中
至少有12位都是他的学生
包括OpenAI创始团队成员约翰·舒尔曼John Schulman
Perplexity的CEO阿拉温德·斯里尼瓦斯Aravind Srinivas
以及Inception Al 的阿迪亚·格罗弗Aditya Grover
好了，话不多说
我们来看看彼得·阿比尔这次演讲都讲了哪些内容
首先，阿比尔介绍到
随着硬件的不断进步
如今机器人缺少的就是大脑
而大脑的关键驱动力就是AI
如今
大家似乎都感受到了基于Transformer的大语言模型的能力
就是因为基于海量的互联网数据
能够训练出非常庞大的神经网络
而这些数据中有一些经过了有效的筛选
同时又有大量的算力来支持训练
于是就形成了有效的方法
那么，人形机器人呢？
我们能用哪些数据来训练它们呢？
阿比尔指出
目前世界上还没有真正的人形机器人
也就没有大量的行为数据
因此，找到有效的数据源
是机器人学中的一大挑战
也是一大机遇
其中一个天然的数据源
就是通过远程操作
来直接获取到关节角度、操作力度等数据
在很多方面
这类似于大语言模型的数据获取方式
也就是直接获取与目标任务相匹配的数据
但是远程操作是非常耗时和昂贵的
目前通过这种方式采集到的数据量仍然很小
还有人提出
只需要在视频中追踪人类的手部动作就好了
毕竟
手部动作在物理世界产生了关键性的影响
但是阿比尔认为
好的计算机视觉技术确实可以追踪手部运动
但是这些视频并不都是完全符合需求的
还有一种方式
就是通过大规模的仿真来获取数据
这样也能够确切地知道机器人在做什么
但是仿真并不总是与现实完全吻合的
因为我们无法将所有的现实世界元素以及场景
都融入到模拟器中
那么
为什么不直接让机器人在现实世界中学习呢？
尽管这种方法原则上是可以的
但是如何在现实世界中
让机器人安全地进行强化学习、试错学习呢？
要在哪里设置它们呢？
这些都还无法实现
或许也有人打算采用更随意的方式
那就是只使用互联网视频
进行下一帧、下一个token的预测
并且通过这种方式了解世界
但是这样还是无法接触到实际的行为
也无法了解机器人是如何通过手、脚、手臂等身体部位
来影响世界的
阿比尔表示
虽然互联网数据可以用来构建背景知识
包括世界的运作、以及人类交流的方式
但是实际上
我们需要的是一个能在物理世界中完成各种任务的机器人
所以
我们需要在仿真环境和真实环境中
对机器人进行一些强化学习
来了解它是如何与物体进行交互的
最后，还需要让人类参与到其中
就像在大语言模型的RLHF中那样
对机器人的行为给予好、坏、更好、更坏的反馈
这样
机器人才能了解到你不希望它做什么
不过，这样的方式非常复杂
这不仅仅是下载数据、运行训练方案那么简单
而是一个复杂的「拼图游戏」，
由许多碎片组成
每个碎片都有其自身的挑战
阿比尔这时展示了一张幻灯片
数据金字塔
金字塔的基础是网络数据
然后是合成、仿真数据
而顶端则是现实世界的数据
可能需要人类参与收集
与大语言模型相比
目前业界在机器人领域的高信号数据方面
还没有达成共识
同时，如何最佳地组合数据源
也没有达成一致
但是
有许多令人兴奋的研究成果已经开始涌现
比如
人们可能不再需要1000万美元的预算来训练模型
只需要一个GPU就能做出惊人的成果
阿比尔指出，对于远程操作来说
获取符合需求数据的最直接方法
就是输入摄像头捕捉的画面
打印出机器人的关节角度
然后输出下一个关节角度
如果有遥控操作设置
那么就可以长时间的收集这类数据
虽然他自己和许多人都认为
大规模实现这种数据集是完全不可能的
然而
斯坦福大学的切尔西·芬恩Chelsea Finn团队证明
只要设置得当
也可以非常快速地收集数据
虽然这些数据还没有达到互联网的规模
但是收集速度显著加快了
阿比尔还介绍道，在这个案例中
通过第二代Mobile ALOHA操作系统
实验人员可以对机器人进行远程操作
然后再用得到的数据训练一个神经网络
自主执行任务
后来
切尔西·芬恩Chelsea Finn、塞尔吉奥·莱文Sergio Levine、卡罗尔·豪斯曼一起创办了一家公司
Physical Intelligence，也就是PI
并且成功地建立了大规模的数据收集系统
阿比尔展示了一个PI机器人整理衣物的视频
他提醒观众注意
其中机器人自我纠错的行为
这显然不是一次按照脚本进行的衣物折叠过程
而是通过神经网络控制完成的任务
它学会了如何应对大量变化
以及当计划出现微小偏差的时候
如何进行纠正
而对于跟踪手部动作来说
加州大学圣地亚哥分校（UCSD）的王小龙教授团队
与麻省理工学院（MIT）研究人员的合作
已经证明可以使用Apple Vision Pro从MIT进行遥控操作
来跟踪手部的动作
其中
MIT的手部动作由Apple Vision Pro进行跟踪
并且通过普通的互联网发送
允许操作者远程控制这个机器人
当然，由于是远程控制
会存在一定的延时
因此需要缓慢地操作，如果动作太快
就很难与机器人保持同步
不过，在这个过程中
机器人已经可以完成一些非常有趣的事情
其中一个例子
是让机器人将一个很小的耳塞放入盒子中
展示了在当前机器人硬件和正确的控制下
机器人可能具备的灵巧性
尽管对人形机器人很感兴趣
但是阿比尔还是认为
不应低估四足机器人未来的普及程度
因为四足机器人更容易操作
不容易摔倒
而且更加稳定
而且，四足机器人也可以装上手臂
而不仅仅局限于四条腿
阿比尔展示了一段视频
在王小龙教授的家里
装有手臂的四足机器人可以自主打扫孩子的玩具房
从另一方面来看
既然人类每天都在用手进行烹饪、打扫、整理、建造等各种各样的事情
为什么不直接记录下来呢？
卡内基梅隆大学的迪帕克·帕塔克Deepak Pathak正在进行类似的研究工作
这里的关键思想是
如果让神经网络观看人类动作的视频
那么或许应该利用在训练神经网络权重时使用的损失函数
来鼓励和优化神经网络
通过这种方法
机器人以后在被要求完成任务的时候
就不必从头开始学习了
因为它已经对物理世界的交互有了认知
比方说，它会知道门把手、抽屉把手
是它应该首先操作打开的位置
这样做的好处是
它教会了机器人一些先验知识
比如哪些东西是有趣的
在自主行走方面，目前
阿比尔在伯克利的一些同事取得了一些成果
他们通过神经网络控制下的仿真机器人动作
收集了大量关于行走的数据集
其中包括了机器人的所有关节角度、对这些关节施加的指令、机器人的质心
以及它的姿态
阿比尔指出，在某些数据集中
动作被忽略了
通常这只是机器人状态的一系列序列
再加上人类的数据作为参照
包括互联网上的低质量视频
而现在得到的是一个看起来更像大语言模型的训练集
对于后三个数据集其实都是状态序列
而对于第一个数据集
状态和动作是交替的
研究团队通过训练一个大型的Transformer模型
来预测下一个token
随后再运行了一些额外的强化学习
进行奖励反馈优化
随后
研究团队在更崎岖的地形上进行了训练
然后在现实世界中测试
目前
这个系统已完成了超过4英里的现实徒步
在自主性上
这个机器人实际上是在没有摄像头输入的状态下
实现多种地形的行走
它只是知道自己的身体姿态
并且能够有效地感知脚下发生的情况
不过
同时也会有人使用操纵杆来控制机器人的移动方向
所以
高层次的导航是由人来指示方向的
但是所有低层次的控制都是自主的
另一个Abbeel团队正在研究的课题
是如何让机器人跑得更快
目前，人类跑完100米大概需要十几秒
机器人需要20几秒
而伯克利的团队通过强化学习来训练控制器
试图将机器人的速度最大化
现场展示的机器人没有上半身
它是Digit机器人的一个早期版本
可以看到
这个机器人能以非常自然的方式运动
可以快速跑完100米
同样的强化学习也让它学会了跳跃
阿比尔强调，跳跃其实更难
因为这需要落地时保持稳定
也需要在后续做出大量踉跄动作
来弥补落地的不协调
通过使用类似的技术
也可以训练一个四足机器人
让它成为足球守门员
从视频来看
这个守门员机器人可以通过摄像头来捕捉足球
并且做出快速的反应
跳跃、扑救，或者阻挡足球
在价格方面，阿比尔表示
伯克利正在以非常低的预算建造机器人
他认为机器人的价格真的会快速下降
使得硬件的价格将不再是构建机器人的障碍
对于近期很火的宇树G1机器人表演功夫的视频
阿比尔表示
宇树很可能是先对人类的相同动作进行了动作捕捉
然后重新定位到机器人可以执行的动作上
考虑到机器人的物理限制
这需要在模拟环境中运行强化学习
学习执行这些动作
最后再转移到真实世界的机器人上
让它得以执行
其实如今很多人形机器人的全身控制
都要依赖于仿真，而且事实证明
仿真数据的质量非常高
这是因为足式机器人与世界的接触的点
主要集中在脚部
所以只需要关注这部分的训练即可
但是在上肢操作中
机器人会面临更多细节上的问题
比如物体可能变形、破裂
以及大量的随机事件
那么，接下来的重要问题是
要如何将更多现实世界的信息融入到仿真器中
这也是阿比尔的一名学生亚瑟Arthur在做的一个项目
他们认为，在收集人类行为的视频时
不仅可以进行动作捕捉
还应该要捕捉周围的环境
在一个实验案例中
他们将楼梯融入到了仿真环境中
然后通过强化学习
在模拟中执行一些复杂的动作
这其中的关键在于
如今通过类似NeRF和3D高斯泼溅等技术
神经网络正在真正理解世界的三维结构
同样，也可以通过这种方式
将世界的三维信息融入到机器人的仿真环境中
随后阿比尔介绍了一项最近让他感到非常兴奋的工作
那就是Body Transformer
阿比尔指出，如果观察人类和动物
会发现它们之间存在一种空间连接性
比如
当人类感受到指尖被灼烧的时候
反应路径并不一定要一路传递到大脑
而是存在一条更短的路径
这条短路径能让人们快速做出反应
而通过这样的方式来构建机器人的身体
会更加容易
因为能同时具备处理事情的短路径和长路径
这实际上是一种归纳偏置
也就是对模型进行偏好选择的先验假设或限制
这种局部性
反而可能实现更快地学习
不过，比起单一的神经网络
现在阿比尔团队对模块化的架构更感兴趣
在这样的架构中
Transformer中的连接不是全连接的
而是局部连接的
这样可以更加有效地查看机器人的骨架
并且利用它作为归纳偏置
让模型变得更加高效
同时，由于采用了局部连接
当应用注意力时
就能够实现多频率推理
并且在强化学习时提供了局部化的信用分配
阿比尔介绍道
强化学习中的一个常见挑战是
当机器人完成一个任务后
庞大的黑箱Transformer无法告诉你具体原因
而Body Transformer可以分析出导致任务成功或失败的原因
甚至可以具体到手和脚都做了什么
这样，基于机器人的局部性
本质上其实是拥有了一个稀疏的注意力
称为Masked Attention
图中的灰色曲线就表明
与Masked Attention相比
Full Attention每次迭代需要更多的时间
计算成本也高得多
而具有Masked Attention的Body Transformer
不仅提高了模仿学习的效率和可扩展性
而且在数据较少的情况下也能很好地工作
比如在示例中
机器人只需要三次演示
就能够学会完成一项任务
这就是一种很好的归纳偏置
因此
阿比尔认为只需要一块不错的GPU
人们甚至在家里就能够训练出一个机器人
对于机器人的未来发展
Abbeel认为要想让机器人取得进步
首先要确保为它设定了正确的目标
他提到，许多人会问
为什么不将机器人设计得更强
甚至超越物理极限呢？
这是因为
这样的目标需要将机器人设计得很重
移动起来也很困难
更理想的状态应该是让它们既安全
又尽可能的轻便
也就是让它在接近物理极限的情况下工作
将能力最大化
而这个目标
已被整合到了humanoid bench基准测试中
它不仅包含了问题的设置
还配备了触觉感知功能
阿比尔指出
原先的传统算法在处理大规模动作空间时
存在着一定难度
比如如果只控制身体，而不控制手
虽然学习速度会快很多
但是这样会欠缺全身控制的能力
而通过采用分层的方法，将技能解耦
能取得更好的进展
比如从一个伸手策略开始
通过强化学习来教机器人达到一个目标
然后在这个基础模型上再学习做其他事情
阿比尔还介绍了与Google DeepMind合作开发的MuJoCo Playground
MuJoCo Playground是一个模拟了各种各样任务的开源仿真平台
支持批量GPU渲染
只需要一行代码即可安装
对比来看
DeepMind的Control Suite是一个长期的机器人控制测试环境
用来测试多种算法
它可以在仿真环境中运行各种机器人
然后将它转移到现实世界中
而MuJoCo Playground则会先定义任务
通常也意味着定义奖励
而且是多个奖励
来告诉系统真正想要什么
然后开始训练模拟
它通常会使用PPO算法
并且根据需要来调整奖励
如果没有得到想要的结果
使用者还可以逐步构建更为困难的任务
然后通过域随机化
为仿真过程添加一点随机性
在演讲的最后
Abbeel和他的学生展示了一个基于MuJoCo Playground训练的机器人
即便在有人拉动遥控手柄进行干扰的时候
机器人也能向前、向后或侧向行走
并且在出现错误的时候能够进行恢复
好了
以上就是阿比尔这次演讲的主要内容了
希望能对想了解机器人领域发展的观众朋友们所有帮助
感谢观看本期视频，我们下期再见
