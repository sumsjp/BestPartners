大家好，这里是最佳拍档，我是大飞
在我们前两期做的一期节目中
其中Anthropic的联合创始人克里斯托弗·欧拉提到了“机械可解释性”的概念
很多观众很感兴趣
所以我们今天这期节目
就来带大家尝试探索一下人工智能的内部世界
在当今时代
人工智能无疑是最具影响力和颠覆性的技术之一
它正在逐渐重塑我们的生活、工作和整个社会
然而，讽刺的是
尽管我们每天都在与各种AI应用打交道
但是对于这些AI的内部运作机制
我们却知之甚少
它们就像一个个神秘的黑盒子
我们输入数据
它们输出结果
可是中间到底发生了什么
却无人能够准确的知晓
随着AI越来越广泛的应用
这种“黑盒”的状态也正在变得越来越危险
想象一下，在医疗诊断领域
AI系统做出的决策可能直接关乎患者的生死；
在自动驾驶领域
AI的决策会影响着道路上每一个人的安全
如果我们无法理解AI的决策过程
又怎么能确保它们不会做出危险或者有偏见的决定呢？
这就是整个AI领域所面临的核心挑战
它不仅关乎着科学探索
更是与AI和我们每个人的安全息息相关
传统的AI可解释性方法存在着很大的局限性
往往只能提供一些粗略的解释
比如指出哪些输入特征对输出的影响较大
但是这显然远远不够
曾经有一位匿名的AI安全研究员形象地比喻道
这就像是在看一本用外星语言写的书的目录
我们可能知道哪些章节很重要
但是完全不明白内容是什么
所以，我们迫切需要一种全新的方法
能够深入AI的“大脑”，
理解其中的每一个“神经元”是如何工作的
于是
“机械可解释性”（Mechanistic Interpretability）的研究应运而生
这个新兴领域的目标是将AI系统完全“拆解”，
如同生物学家去研究生命那样
系统地分析AI的每一个组成部分及其功能
不过
研究人员要面临的挑战十分巨大
他们需要开发新的数学工具和可视化技术
设计创新的实验方法
甚至重新思考我们对于智能的理解
随着对AI系统内部的深入探索
研究人员开始有了一个令人大为惊讶的发现
尽管这些系统能够完成极其复杂的任务
比如击败围棋世界冠军、创作逼真的图像等等
但是它们内部的表征方式却出奇地简单
这就是所谓的“线性表征假说”。
简单来讲
AI系统似乎是通过将不同概念
表示为高维空间中的方向来理解世界的
比如说在处理语言的时候
“性别”这个概念
可能对应着一个特定的方向
而“男性”和“女性”则分别位于这个方向的两端
这就解释了为什么我们可以用单词来做“数学运算”，
就像那个著名的例子
“国王-男人+女人=王后”一样
欧拉和他的团队还在研究中发现
这种线性表征不仅存在于语言模型中
在处理图像的卷积神经网络中也广泛存在着
他们在对Inception V1模型的研究中
分析了大约10,000个神经元
发现了专门用来检测曲线、边缘和颜色对比的神经元
这些基本的特征随后被巧妙地组合起来
形成更加复杂的概念
比如“猫”或者“房子”。
欧拉曾经回忆道
当我们第一次看到这些结果的时候
感觉就像是在解读外星人的图画书
每一层神经网络都在学习越来越复杂的特征
但是基本原理却出奇地简单
更令人惊叹的是
这种简洁的线性表征方式并非个例
它似乎是各种AI模型的一种共性
甚至在生物大脑中也有类似的发现
例如，他们在不同的模型中
都发现了类似的Gabor滤波器
而这种滤波器在生物视觉系统中
同样扮演着重要的角色
而且
研究人员在人工神经网络中发现的“高低频率检测器”，
后来在老鼠的大脑中也被发现
这种发现充分展示了机械可解释性研究的潜在价值
这不禁让我们思考
难道存在某种普遍的“智能原理”，
无论是人工智能还是生物智能
都要遵循这些原理吗？
这个问题的答案
或许会彻底改变我们对智能的理解
也为开发更先进的AI系统指明方向
随着研究的进一步深入
当聚焦到单个“神经元”的层面时
事情变得更加有趣和超乎想象
研究人员发现
许多神经元并不像我们之前认为的那样
只负责一项特定的任务
而是呈现出了令人惊讶的“多才多艺”。
其中最著名的例子当属“特朗普神经元”。
在多个AI模型中
研究人员都发现了专门对唐纳德·特朗普的相关内容做出反应的神经元
这些神经元不仅对特朗普的照片有反应
还会对“特朗普”这个词、他的签名
甚至是与他相关的新闻标题做出反应
一位研究者半开玩笑地说
这些神经元对特朗普的痴迷程度
简直比某些政治评论员还要高
但是其实“特朗普神经元”仅仅是冰山一角
实际上
许多神经元都展现出了“多义性”，
也就是一个神经元可以同时对多个看似毫不相关的概念做出反应
比如在欧拉的《Zoom In:
An Introduction to Circuits》论文中
提到了一个在InceptionV1模型中发现的神经元
它竟然同时对猫的脸、汽车的前脸和猫的腿做出反应
这个神经元不是在寻找这些概念之间的某种微妙联系
而是明确地对这三种完全不同的视觉特征做出反应
一位参与研究的博士后回忆道
当我们第一次发现这个现象时
我们以为是实验出错了
我们反复检查了好几遍
才确信这确实是模型的真实行为
这完全颠覆了我们对神经网络工作方式的理解
如果一个神经元同时表示多个概念
我们又该如何解释它的作用呢？
研究人员提出了几种可能的解释
一种观点认为
这种多义性可能是神经网络为了更加有效地利用有限的神经元而采取的策略
另一种观点则认为
这可能反映了现实世界中概念之间的某种潜在联系
只是这种联系对人类来说并不是很直观
无论如何，多义性神经元的发现
为我们理解AI的内部工作机制提供了重要线索
它暗示着AI系统可能采用了一种高度压缩和抽象的方式来表示信息
而这种方式可能与人类大脑的工作方式有着本质的不同
为了解释这种令人费解的多义性现象
研究人员提出了一个大胆的假说
叠加假说（Superposition Hypothesis）
这个假说借鉴了量子力学中的概念
认为AI系统能够在有限的神经元中
表示远超过其数量的特征
就如同量子比特可以同时表示多个状态一样
具体而言
AI系统可能将多个特征“压缩”到同一组神经元中
通过巧妙的编码方式
在需要时再“解压”出相关信息
这就像是一种高效的数据压缩算法
让AI能够用有限的资源表示来处理海量的信息
为了验证这个假说
研究人员精心设计了一系列的实验
比如说
他们创建了一个简单的神经网络
训练它完成一个有更多输入特征的神经元任务
令人惊讶的是
网络确实学会了用每个神经元来表示多个特征
这些特征被编码为近乎正交的方向
一位参与实验的研究员兴奋地说到
当我们看到结果时
感觉就像是在见证一个魔术表演
明明只有这么少的神经元
却能同时表示这么多的特征
简直不可思议！
这个发现不仅解释了多义性神经元的存在
还揭示了AI系统惊人的效率
它暗示着我们观察到的神经网络
可能只是一个更大、更稀疏网络的“投影”。
正如同一位研究者形象地比喻
这就像是我们一直在研究影子
现在终于开始窥见投射影子的实体
更有趣的是
这个假说与压缩感知理论有着深刻的联系
压缩感知理论告诉我们
在某些条件下
我们可以从远少于原始信号维度的测量中
重建完整的信号
这与神经网络的工作方式惊人地相似
一位具有数学背景的研究者激动地说
当我意识到神经网络可能在利用压缩感知的原理时
我感觉自己仿佛触碰到了智能的本质
这很可能是通向更深入理解AI的一把钥匙
这些发现
为我们提供了一个全新的视角来理解AI系统的内部工作原理
它们暗示着
与其将AI视为一个简单的函数近似器
不如将它看作一个复杂的信息压缩和解压系统
最后，让我们来看看AI的另一项能力
跨模态理解
最新的研究发现
在像Claude 3这样的大语言模型中
存在着能够同时处理文本和图像的神经元
在Anthropic公司对Claude 3 Sonnet模型的研究中
研究人员使用了稀疏自编码器
从模型的中间层提取出了大约1000万个独特特征
这些特征涵盖了广泛的概念
从具体的人物和地点到抽象的概念
比如性别偏见或者保守秘密
这个发现展示了大语言模型内部表征的丰富性和复杂性
其中一个特别有趣的发现是“后门”特征
这个特征不仅能够识别代码中的后门漏洞
还能识别图像中隐藏的摄像头设备
一位研究员回忆道
当我们发现这个特征的时候
我们都惊呆了
它竟然能在完全不同的领域中识别出相似的概念
这种抽象的理解能力简直匪夷所思
研究人员还发现
通过放大或者抑制与特定特征相关的神经元活动
他们可以显著地影响模型的行为
例如，放大与金门大桥相关的特征
会导致模型在每个回答中都提到它
即使这样做并不合适
一位研究员笑着说
我们甚至让模型在讨论量子物理的时候
也提到了金门大桥
这种能力既令人兴奋，又有点可怕
此外
通过激活与垃圾邮件相关的特征
可以绕过模型的限制
让模型生成垃圾信息
这个发现引发了研究团队的热烈讨论
一位成员严肃地说到
这提醒我们，在追求AI能力的同时
也要非常小心地考虑安全问题
而放大与阿谀奉承相关的特征
可以诱导模型使用奉承作为欺骗的手段
一位伦理学家研究员指出
这让我们看到了AI可能产生的道德风险
我们需要认真思考如何在赋予AI强大能力的同时
也能确保它们遵循道德准则
虽然通过操纵这些特征
我们有望能够精确地控制AI的行为
比如增强或者抑制某些特定的能力
但是研究人员也强调
我们目前提取的特征
可能只是模型总特征的很小一部分
更何况
完全提取所有特征需要巨大的计算资源
一位资深研究员也坦言道
我们现在可能只是看到了冰山一角
要想完全理解这些模型
我们还有很长的路要走
但是每一个新发现
都让我们离目标更近一步
随着对AI内部世界的探索
机械可解释性研究为我们提供了一个全新的视角
来思考AGI的实现
传统上，许多研究者认为
实现AGI的关键在于设计更复杂的模型架构
或者收集更多的训练数据
但是，机械可解释性研究告诉我们
也许关键在于更好地理解和利用
现有模型中已经存在的结构和能力
一位长期从事AGI研究的科学家分享了他的深刻洞见
过去
我们总是在追求更大、更复杂的模型
但是现在我开始意识到
也许答案就藏在我们已有的模型中
我们只是还没有完全理解它们
比方说
通过深入理解多义性神经元和跨模态特征
我们可能能找到更有效的方法
来整合不同类型的知识和能力
同样，叠加假说告诉我们
也许实现AGI不需要无限地增加模型的规模
而是找到更有效的方法来利用有限的计算资源
更重要的是
机械可解释性研究为我们提供了一种可能的方法
来确保AGI的安全性和可控性
从而避免潜在的风险
然而，通往AGI的道路依然充满挑战
比如
我们如何确保从简单模型中获得的见解
能够推广到更复杂的系统？
我们如何处理AI系统中可能存在的“暗物质”，
也就是那些我们还无法观察或理解的部分？
欧拉在他的最新研究中提出了“神经网络暗物质”的概念
指出我们可能只观察到了神经网络总特征的一小部分
他写道
就像宇宙学家试图理解暗物质一样
我们可能需要开发全新的工具和理论来探索神经网络的‘暗面’。
这是一个巨大的挑战
但也是一个激动人心的机遇
除此以外
我们还面临着如何在保持模型性能的同时
增加模型可解释性的难题
一位研究团队的负责人坦诚地说
每解决一个问题
就会出现十个新问题
但是这正是科学研究的魅力所在
我们正在探索未知的领域
每一个发现都让我们离理解智能的本质更近一步
正如欧拉在他的博客中所写到的
我认为神经网络内部存在着极其丰富的结构和巨大的美
如果我们愿意花时间去观察和理解
就能发现这种美
这不仅是一个科学问题
更是一个哲学问题
我们能否真正理解我们所创造的智能？
好了，今天的分享就到这里
随着我们对AI内部的理解不断加深
我们可能会发现
对AI的探索过程
就如同我们对数学定理、对物理法则、对原子量子的探索一样
本质上都是我们对这个世界的探索
也许我们最终能搞懂它
也许可能很长时间也没法完全搞懂
但是不可否认的是
我们将逐渐经历从使用AI、理解AI
最终到与AI共存的阶段
这也是对于我们每个人的挑战和机遇
感谢大家收看本期视频
我们下期再见
