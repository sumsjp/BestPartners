大家好，这里是最佳拍档
二零一七年
Transformer架构横空出世
以自注意力机制打破了RNN在序列建模领域的垄断
成为驱动ChatGPT、GPT 4、Claude等所有现代大语言模型的核心动力
它不仅重塑了自然语言处理领域
更掀起了一场席卷全球的AI革命
让人工智能从实验室走向了产业落地的快车道
然而
就在Transformer生态日益繁荣、相关研究论文呈现爆炸式增长的今天
它的共同发明者之一、Sakana AI的创始人Llion Jones
却发出了振聋发聩的警告，他说
当前的AI已经陷入了死胡同
无数微调的研究纯属浪费时间
Transformer绝不是AGI的终点
作为在Transformer架构上深耕最久的研究者之一
Llion Jones的发声背后
是对AI行业发展现状的深刻洞察
以及对技术演进规律的清醒认知
从RNN的兴衰到Transformer的统治
从硬件彩票到架构彩票
从锯齿状智能到生物启发的全新探索
AI行业正站在一个关键的十字路口上
是继续在现有的架构上修修补补
还是勇敢跳出局部最优的陷阱
奔赴可能孕育着下一场革命的全新赛道呢？
今天我们就来聊聊Llion Jones的观点
Transformer的诞生
可以堪称是AI发展史上的一个里程碑事件
在它出现之前
RNN以及它的变体LSTM和GRU
是序列数据处理的绝对主流
RNN通过迭代式的信息传递方式
来处理时序数据
曾经被认为是解决语言建模、机器翻译等任务的最优路径
当时，整个AI学术界都围绕着RNN
展开了密集的研究
研究者们致力于通过微调架构细节
比如调整门控单元的位置、优化激活函数、改进训练策略等等
来提升模型的性能
然而
这些努力始终没能够突破RNN的固有局限
由于存在梯度消失或者梯度爆炸的问题
RNN难以处理长序列数据
而且并行计算的能力十分薄弱
训练效率低下
即便经过了多年的优化
RNN在语言建模任务上的性能
也仅仅能够达到每个字符一点二五到一点二六比特的水平
而Transformer架构的出现
彻底改变了这个局面
它以自注意力机制为核心
实现了对序列数据的并行化处理
不仅解决了长序列的依赖问题
更是在性能上实现了跨越式的提升
将仅解码器结构的Transformer
应用在相同的语言建模任务的时候
瞬间可以达到每个字符一点一比特的优异成绩
这个差距是决定性的
Transformer凭借着它强大的特征捕捉能力、高效的训练方式和卓越的泛化性能
迅速取代了RNN 成为AI领域的新宠
从自然语言处理到计算机视觉
从语音识别到多模态交互
Transformer架构可以说是无处不在
构建起了现代AI技术的核心骨架
各大科技公司也纷纷基于Transformer推出大模型
通过不断扩大参数数量、增加训练的数据规模
来推动模型性能的持续突破
OpenAI的GPT系列、谷歌的Palm、Meta的Llama等等
无一不是Transformer架构的延伸与扩展
随着Transformer的巨大成功
一个危险的趋势正在AI行业蔓延
研究者们逐渐陷入了路径依赖
将所有的精力集中在对现有架构的微小调整上
而不是探索全新的技术方向
如今的AI学术圈，充斥着大量类似于
调整归一化层的位置
改良注意力机制的变体
优化训练调度策略的研究论文
这些研究本质上都是对Transformer架构的局部优化
而不是颠覆性创新
在Llion Jones看来
Transformer架构已经被研究得水泄不通了
即使继续在这个框架内深耕
也很难再诞生出真正有价值的突破
Jones 在采访中强调
某个突破终将发生
到那时我们会再次清楚的意识到
我们现在其实浪费了大量的时间
这种担忧并不是空穴来风
回顾AI的发展史
类似的局部优化陷阱曾经多次出现过
在深度学习兴起之前
符号主义是人工智能的主流范式
研究者们致力于通过手动设计规则
来模拟人类智能
但是这种方法始终难以应对复杂的现实场景
直到神经网络在图像识别等任务中
展现出了压倒性的优势
符号主义才逐渐被边缘化
而RNN的兴衰
更是和当前Transformer的处境惊人的相似
在Transformer出现之前
RNN的微调研究也曾经如火如荼
看似每一篇论文都在推动着技术的进步
但是最终都随着新架构的诞生
而变得毫无意义
更值得警惕的是，当前的研究生态
正在被Scaling至上的理念所绑架
由于扩大模型的规模和训练数据量
能够稳定提升模型的性能
各大公司和研究机构纷纷投入巨额资源来进行参数竞赛
却忽视了对模型架构本身的反思和创新
OpenAI的前首席科学家Ilya Sutskever曾经直言
Scaling时代的一个后果是
Scaling吸走了房间里所有的氧气
正因为如此
所有人都开始做同样的事
于是，我们走到了今天这个
公司数量远远多于创新点子的局面
二零二零年
当时在谷歌DeepMind任职研究员的Sarah Hooker
提出了硬件彩票的理论
为AI行业的技术选择提供了一种全新的视角
这个理论指出
通往AGI的道路不止有一条
深度神经网络之所以能够成为当前的主流
并不是因为它在所有技术路径中
具有普遍的优越性
而是因为它恰好契合了GPU等硬件的计算特性
从而在和其他技术路径的竞争中脱颖而出
在硬件彩票的基础上
Llion Jones进一步提出了架构彩票的观点
他认为，Transformer架构的成功
在很大程度上也是一种偶然的胜利
因为它恰好适配了当前的软件生态、数据形态和硬件架构
而不是一条必然的、通往AGI的最优路径
正如真正的突破
其实很少来自于反复打磨的同一块石头
而当前AI行业的现状
正是在 Transformer这块石头上反复的打磨
却忽视了其他可能更有潜力的技术方向
事实上，已经有部分的研究论文证明
有一些新架构在特定任务上的性能
是优于Transformer的
但是这些新架构始终无法取代Transformer的主流地位
核心的原因在于
行业对Transformer的依赖已经形成了强大的生态壁垒
经过多年的发展
研究者们对Transformer的理解已经非常成熟
围绕它构建的训练框架、微调工具、部署方案等配套生态
也日趋完善
所以
要让整个行业放弃成熟的技术体系
转向一个还没有经过充分验证、配套设施不完善的新架构
除非能够实现碾压式的胜出
否则几乎是不可能的
这种生态依赖
本质上是技术演进中的一种路径锁定现象
就像键盘的布局一样
尽管存在着更高效的替代方案
但是由于早期形成的用户习惯和产业生态
至今QWERTY仍然占据着主导地位
Transformer的处境和它有些类似
它的成功带来了完善的生态
而完善的生态又进一步巩固了它的主导地位
从而形成了一个难以打破的闭环
Llion Jones用重力井来形容Transformer对AI行业的强大的吸引力
这就像有一个巨大的重力井
所有尝试离开的新方法
都会被拉回来
哪怕你真的做出了一个效果更好的新架构
只要OpenAI再把Transformer模型的规模扩大十倍
那你的成果就被比下去了
这种重力井效应的根源
在于Scaling路径的有效性和便利性
扩大模型的规模和训练的数据量
能够以相对较低的创新成本
实现模型性能的稳定提升
这种简单直接的进步方式
自然也会成为各大公司的优先选择
相比之下
探索全新的架构需要承担极高的风险
可能投入了大量资源后毫无收获
甚至面临着被现有技术路径降维打击的风险
在商业利益的驱动下
大多数的机构自然会选择稳妥的Scaling之路
而不是冒险探索未知的新方向
重力井效应的另一个表现
是创新资源的过度集中
由于Transformer生态已经形成
顶尖的人才、巨额的资金、优质的数据资源
都在向这个领域聚集
导致其他潜在的创新方向
因为缺乏资源的支持而难以发展
AI行业的研究资源
被严重内卷在了Transformer的微调与Scaling上
而那些真正具有探索性的、颠覆性的研究
反而被边缘化了
这种资源分配的失衡
进一步加剧了行业的路径依赖
让AI行业陷入到越成功越保守
越保守越难以突破的恶性循环中
Llion Jones指出
当前的大语言模型并不是通用智能
而是呈现出了锯齿状智能的特性
所谓的锯齿状智能
是指大语言模型在不同任务上的能力表现极不均衡
它们可能在某些复杂任务上
展现出远超人类的水平
比如解出博士级别的数学难题、写出逻辑严谨的学术论文等等
但是在另一些看似简单的任务上
却会犯连小学生都不会犯的低级错误
比如计算简单的加减法出错、混淆基本的常识概念等等
这种天才与白痴的巨大反差
正是当前大语言模型的核心缺陷
比如
GPT 4能够在律师资格考试中取得优异的成绩
却可能在回答
一加一乘以二等于几的时候
给出错误的答案
虽然它能够生成流畅优美的诗歌
却也可能混淆太阳从东方升起这个基本常识
这种看似矛盾的表现
并不是因为模型的训练不足
或者调参不当所导致的
而是源于Transformer架构的根本性局限
Llion Jones认为，大语言模型的本质
还是一个统计语言模型
它们通过学习海量文本数据中的统计规律来生成内容
而不是真正理解语言的含义和世界的本质
尽管Scaling能够提升模型的统计拟合能力
让它在更多的任务上
表现出看似智能的行为
但是这种智能是表面的、缺乏根基的
当遇到统计规律难以覆盖的场景时
模型就会暴露它无知的本质
从而做出荒谬的判断
当前大语言模型的锯齿状智能
根源还在于Transformer架构的万金油式设计
Transformer是一种通用性极强的架构
能够处理文本、图像、语音等多种的数据类型
从而适配语言建模、分类、生成等多种任务
这种通用性让它能够快速适配不同的场景
成为产业落地的利器
但是也导致它在核心能力上
存在着先天的不足
为了实现通用性
Transformer采用了一刀切的设计思路
通过自注意力机制对输入数据进行无差别的处理
依靠海量的参数和训练数据
来覆盖不同任务的需求
这种设计的问题在于
它缺乏对知识表示和推理过程的专门设计
大语言模型无法像人类一样
构建结构化的知识体系
也无法进行严谨的逻辑推理
它们的所有输出
本质上都是基于统计规律的猜测
更严重的是
当前行业为了弥补Transformer的缺陷
采取了外挂模块的权宜之计
例如，为了解决不确定性建模的问题
研究者们在Transformer了中加入概率模型模块
为了提升自适应计算的能力
又增加了动态路由机制
这些外挂模块虽然在一定程度上提升了模型的性能
但是也让架构变得越来越复杂、臃肿
不仅增加了训练和部署的成本
更无法从根本上解决架构本身的缺陷
Llion Jones对此批评道
我们明明知道要有不确定性的建模、要有自适应计算的能力
但是我们却选择把这些特性外挂上去
而不是从架构本身去重新思考
这种头痛医头、脚痛医脚的改进方式
注定只能带来局部的性能提升
而无法让AI真正的走向通用智能
面对 Transformer 架构的局限性
Llion Jones选择了一条截然不同的道路
大幅减少Transformer相关研究的投入
转向生物启发的全新方向
他认为
人类大脑是迄今为止最强大的通用智能系统
从大脑的运作机制中汲取灵感
或许是突破当前AI困境的关键
大脑和当前AI模型的核心差异在于
大脑的神经元并不是一个个静态的计算单元
而是通过同步振荡、动态连接等方式
进行信息传递和处理的
大脑能够根据任务的需求
动态调整神经元之间的连接强度和信息的流动路径
实现高效的知识表示和推理
而Transformer的神经元
也就是模型参数
在训练完成后是固定的
只能通过静态的计算方式来处理输入数据
缺乏动态的适应能力
基于这个洞察
Llion Jones和Sakana AI的同事Luke Darlow等人
设计了一个名为连续思维机CTM的全新架构
CTM并不是对大脑的完全模拟
而是对大脑核心运作机制的一种简化和抽象
它以神经动态为核心表示
让模型在内部思考维度上逐步展开计算
从而模拟大脑的动态信息处理过程
和Transformer 相比
CTM的核心创新就在于动态性和连续性
在 Transformer 中
信息处理是一次性的
模型通过自注意力机制
对输入序列进行全局的计算和一次性输出的结果
整个过程缺乏中间的思考环节
而CTM则引入了内部思考维度
让模型能够像人类一样
对问题进行逐步分析、持续思考
最终得出结论
具体来说
CTM的运作机制可分为三个核心步骤
首先，模型将输入的信息
转化为神经动态的表示
模拟大脑神经元的激活状态
其次，通过动态耦合机制
让神经动态在内部的思考维度上持续的演化
模拟大脑的推理过程
最后，根据演化后的神经动态
生成最终的输出结果
这种设计让模型能够动态调整信息的处理路径
根据任务的复杂程度来自适应分配计算资源
从而避免了Transformer一刀切的处理方式
不过
这种方式并没有追求完全生物学的可行性
因为大脑并不是靠有线的方式
让所有神经元同步的
Jones 解释道
但是这种思路带来了全新的研究可能
CTM的优势在于
它能够自然融入不确定性的建模、自适应计算等关键特性
而不需要像Transformer那样依赖外挂模块
在初步实验中
CTM在部分需要持续推理的任务上
已经展现出了优于Transformer的性能
为AI的发展提供了全新的可能性
除了架构本身的创新以外
CTM的研究过程
也为AI行业提供了重要的示范意义
在当前的学术评价体系下
抢发论文已经成为常态
研究者们为了抢占研究的热点
往往仓促的投稿
导致论文质量参差不齐
缺乏扎实的实验验证和深度分析
而CTM的研究团队则完全摆脱了这种压力
由于生物启发架构是当时的冷门方向
没有其他团队的竞争
所以他们有充分的时间来打磨论文、完善实验
以及做足对照研究
这种慢下来的研究方式
反而让CTM的研究更加扎实、更具说服力
Jones希望
CTM能够成为一个示范案例
鼓励更多的研究者们跳出热门的赛道
尝试那些看似风险高、但是更加可能通向下一个重大突破的研究方向
真正的创新
往往来自于无人问津的领域
他想强调的是
如果所有人都挤在同一条路上
就永远无法发现新的大陆
Llion Jones 的警告和CTM的探索
揭示了一个残酷但是重要的真相
AI行业的发展始终遵循着范式转移的规律
从符号主义到深度学习
从RNN到Transformer
每一次重大的突破
都意味着前一代技术的边缘化
当前的Transformer架构无论多么的成功
终将会被更先进的范式所取代
这是技术演进的必然结果
然而，范式转移的过程往往是痛苦的
对于那些埋头于Transformer微调和Scaling的研究者和企业来说
Jones的言论无疑是一种冒犯
他们的工作可能会在未来
被证明是浪费时间的
但是历史已经多次证明
局部最优解是永远无法替代全局最优解的
即使是在现有架构上的修修补补
也终究无法实现AGI的终极目标
要想推动AI行业走出局部最优的陷阱
关键还在于要重构研究的评价体系和资源的分配机制
当前的学术评价体系
过度关注论文的数量、引用率和短期性能提升
导致研究者们被迫涌向这些热门赛道
不敢涉足冷门但是有潜力的方向
而资源分配也严重的向Scaling倾斜
巨额资金和顶尖人才
集中在少数几家大公司里
进一步挤压了创新的空间
因此，Llion Jones建议
AI行业需要做出两方面的改变
一方面
要建立更加多元化的学术评价标准
鼓励原创性、探索性的研究
而不是仅仅以论文的引用率和短期性能为导向
另一方面，优化资源的分配机制
通过政府资助、公益基金等方式
为冷门的创新方向提供支持
降低研究者的探索风险
只有让敢吃螃蟹的人得到认可和回报
才能激发整个行业的创新动力
当然
我们也不能完全否定当前 Transformer 相关研究的价值
范式转移是一个渐进的过程
而不是一蹴而就的革命
在新的范式出现之前
我们也可以对现有的架构进行优化和完善
这样不仅能够推动产业的落地
为社会创造价值
也能为未来的创新积累经验、数据和技术基础
总的来说，AI行业的健康发展
需要在探索和沉淀之间寻找平衡
一方面
要鼓励像CTM这样的探索性研究
为行业寻找新的出路
另一方面
也要重视现有技术的产业落地和经验积累
为新范式的诞生奠定基础
这种平衡并不是静态的
而是一种动态调整的
随着现有架构潜力的逐渐耗尽
行业应该逐步将资源和注意力
向的新方向倾斜
推动技术平稳的过渡到下一个范式
Llion Jones 的痛斥
其实是对AI行业注入的一次清醒剂
它提醒我们
在Transformer带来的繁荣背后
隐藏着创新停滞的危机
在Scaling带来的性能提升面前
也不能忽视架构本身的根本性缺陷
AI的终极目标是实现通用智能
而要达成这一目标
就必须跳出局部最优的陷阱
勇敢探索全新的技术路径
感谢收看本期视频，我们下期再见
