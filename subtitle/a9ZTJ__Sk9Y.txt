大家好，这里是最佳拍档，我是大飞
我们前几天已经做了Anthropic和OpenAI的报告
今天再放出第三弹
谷歌DeepMind委托Epoch AI完成的119页量化研究《AI in 2030》。
这份报告勾勒出了2030年AI发展的一条“保守基线”，
那就是单次训练量将达到10的29次方浮点运算、算力规模是现在的1000倍左右、硬件投入接近2000亿美元、全球用电占比超过2%，
最前沿训练的峰值功率将直逼GW级
更关键的是
这份报告还回答了行业最关心的问题
算力怎么涨、电力够不够、钱要花在哪、哪些能力会先落地
如果我们把《AI in 2030》和OpenAI、Anthropic的用户报告叠在一起
也许就能看到一个更清晰的“默认未来”，
AI会先重塑我们的案头工作
之后再慢慢渗透到物理世界
哪怕只是让10%的远程任务产出翻倍
都可能为全球经济带来1%到2%的GDP增量
这个影响不可谓不大
今天大飞提炼报告中的10个核心要点
为大家逐一拆解一下
让大家能清楚2030年的AI到底会发展到什么程度
又会给我们的生活和行业带来哪些变化
首先第一个关键点
也是整个报告的核心
算力趋势
自从2010年深度学习开始快速发展以来
AI训练算力的增长就保持着每年4到5倍的速度
按照这个趋势外推到2030年
最大模型的训练算力会是现在的1000倍左右
单次训练规模能达到10的29次方FLOP
这个数字是什么概念呢？
2020年全球最大的AI集群
要连续运行3000多年才能完成一次这样的训练
而且大家要注意
训练算力和推理算力的增长并不是“零和游戏”，
不是说重视一个就要放弃另一个
更强的训练能力
反而能让同样的推理预算完成更多有效工作
比如现在的大模型
经过更好的训练后
在处理同样的推理任务时
效率会更高，成本也会更低
这是一个正向循环的过程
接下来是投资量级
要支撑这样的算力扩张
需要多少钱呢？
报告给出的答案是
前沿AI集群的资本开支会走向2000亿美元量级
单个大模型的摊销开发成本能达到数十亿美元
可能有朋友会觉得这个数字太夸张
但是我们看看现在的趋势就知道
头部AI实验室的收入最近几年保持着“每年翻3倍”的速度
如果这个轨迹能延续到2030年
它们的年收入会达到数千亿美元规模
这就形成了一个“高投入—高产出”的自洽闭环
因为能产生足够的收入
所以愿意投入更多资金到算力和研发上；
而更多的投入又会提升AI能力
进一步扩大收入
这个逻辑在现在的科技行业里已经得到了验证
比如英伟达的AI芯片业务
就是因为AI算力需求的激增
收入和估值都在快速增长
第三个关键点是数据格局
这也是很多人关心的问题
现在高质量的人类文本是不是快用完了？
报告里提到，确实
高质量人类文本的增长已经见顶
但是这并不意味着数据会成为瓶颈
因为增长动能已经转向了多模态数据和合成数据
多模态数据很好理解
就是图片、音频、视频这些非文本数据
现在的大模型已经开始整合这些数据进行训练
比如GPT-4V、 Gemini这些模型
都能处理图像信息
而合成数据是一个更重要的方向
比如通过强化学习让模型自己生成训练数据
再结合推理时的算力进行筛选
就能不断补充高质量的数据
不过报告也强调
真正稀缺且有价值的数据
是那些可验证、和经济价值强耦合的专业数据
比如医药研发里的临床试验数据、工业生产里的设备故障数据
这些数据不是随便能生成的
未来谁掌握了这类数据
谁就可能在特定领域的AI竞争中占据优势
第四个是硬件与集群形态
AI能力的提升主要来自哪里呢？
不是靠显著拉长训练的时长
而是靠更大的加速器集群和更强的芯片
现在的前沿AI模型
训练时长大多在几个月的时间
不会无限的延长
因为如果训练时间太长，等你训练完
别人可能已经用更先进的算法或芯片训练出更好的模型了
所以现在的趋势是扩大集群规模
比如英伟达的DGX集群
已经有超过10万个H100 GPU的配置
而且更大的集群还在建设中
但是这里有个问题
单个数据中心的电力和供电能力是有限的
所以为了缓解这个瓶颈
多数据中心、跨站点的分布式训练会成为常态
比如DeepMind在开发Gemini Ultra的时候
就用了多数据中心训练的方式
这也意味着
未来训练和推理在地理和架构层面会进一步解耦
训练可能分布在多个地方
而推理则会靠近用户，减少延迟
第五个要点是能源与排放
这也是AI发展绕不开的话题
报告预测，到2030年
AI数据中心的用电可能会达到全球用电的2%以上
最前沿模型的单次训练峰值功率能达到吉瓦级
这个功率相当于一个中等城市的平均用电需求
比如中国一些县级市的年用电量大概就在这个量级
而排放占比则取决于电源结构
如果用的是火电
排放会高一些；
如果用的是风电、光伏等可再生能源
排放就会低很多
保守估算下来
AI的排放占比可能在0.03%-0.3%之间
这个比例虽然不低
但是远低于商业航班2.5%的预计排放量
不过报告也提到
AI本身也能够帮助减排
比如优化电网调度、改进交通和工业流程
这些应用带来的减排潜力可能比AI自身的排放还要大
关键就看我们怎么去部署和利用
第六个要点是AI的能力外推
也就是基于现在的趋势
2030年AI能在哪些任务上实现突破
报告的观点是
一旦某项任务在基准测试中出现了“能做”的迹象
继续扩大规模
性能通常就会可预测地提升
基于这个逻辑
我们可以看到几个明确的方向
首先是软件工程
会从现在的助手角色
走向能自主完成定义明确的功能实现和代码修复
现在的Copilot已经能帮程序员补全代码
未来可能连整个模块的开发都能交给AI；
其次是数学，会从简单的计算
迈向证明草图的形式化和研究助理的角色
比如帮数学家把非正式的证明思路转化为严谨的数学语言
甚至提出新的猜想；
然后是分子生物学
会从蛋白质结构预测
拓展到蛋白质相互作用和性质预测
不过端到端的新药研发会慢一些
因为涉及到很多湿实验和监管流程；
最后是天气预测，在一些关键变量
比如温度、降水和时段上
AI的表现会优于传统的数值方法
或者能和数值方法集成
提升预报精度
第七个要点是R&D的落地节奏
简单来说就是“数字世界先快、物理世界更慢”。
像软件、数学这些“案头研究”的领域
AI的提效会非常明显
因为它们不需要太多物理实验
主要靠数据和计算
AI能直接发挥作用
而像药物研发、新材料这些需要湿实验和监管的领域
工具层会先受益
比如用AI筛选药物分子
但是产品层的兑现会更晚
因为一款新药从研发到上市
需要经过临床试验、审批等多个环节
这个周期通常要8-10年
所以即使AI现在能加速早期研发
到2030年
我们可能也只能看到少数AI辅助研发的药物上市
第八个要点部署的三大门槛
这三个问题会影响AI的规模化应用
第一个是可靠性
AI现在还有“幻觉”的问题
比如生成错误的信息
而且对输入的微小变化很敏感
性能容易波动
未来需要持续降低这些风险；
第二个是工作流集成
很多AI工具现在还停留在演示阶段
没有真正嵌入到实际的工作流程里
比如企业的财务、生产流程
怎么把AI整合进去
而不是让员工额外多一个操作步骤
这是关键；
第三个是成本
虽然单次推理成本在下降
但是总的调用量在上升
怎么在这两者之间找到平衡
维持单位价值
也是需要解决的一个问题
而专业数据的可得性
会贯穿影响这三个门槛
没有高质量的专业数据
AI的可靠性就上不去
也很难和具体的工作流结合
成本自然也降不下来
第九个要点是自动化的宏观回报曲线
报告做了一个测算
如果只让10%的远程任务产出翻倍
大约能带来1%-2%的GDP增量；
如果让一半的远程任务产出翻倍
对应的GDP增量就会达到6%-10%。
即使2030年不能完全兑现这个目标
资本也会基于这个清晰的能力路径提前布局
因为大家能看到AI带来的经济价值是确定的
就像互联网早期，虽然还没盈利
但是资本会提前投入
因为知道未来的潜力
最后一个要点，也是报告反复强调的
这是一个“基线baseline”而非AGI的时间表
报告没有承诺2030年能达到AGI
也就是AI能像人类一样完成任何的认知任务
因为最大的不确定性来自三个方面
第一个方面是潜在的算法突破
如果出现颠覆性的算法
可能会加速或者改变趋势
第二个是社会与监管选择
比如严格的监管可能会放缓发展速度
以及第三个方面供应与能源瓶颈
比如芯片短缺、电力不足
所以报告给出的是一个“默认的未来”，
在没有意外情况的前提下
高能力AI会被大规模部署
优先改写知识劳动
为物理世界的变革打下基础
讲完了这10个核心要点
我们再深入看看报告的引言部分
其中重点解释了一个关键问题
为什么说算力是AI进步的核心
而不是算法或数据呢？
很多人会觉得，算法创新才是关键
但是报告的观点是
算法创新和算力规模化是紧密结合的
最重要的算法创新
比如Transformer架构
其实都是能支持算力规模化的通用方法
而且算法创新本身也依赖于算力规模化进行开发
比如只有当算力足够大的时候
研究人员才能尝试更复杂的算法
发现其中的规律
而数据虽然重要
但是对于现在的通用大模型来说
算力才是更大的瓶颈
用现有的公共文本和多模态数据
至少还能支撑几年的规模化
而且推理规模化还能生成更多的数据
所以报告认为
算力规模化是驱动AI进步的根本原因
其他因素都是为这个核心服务的
不过报告也承认
算力规模化不能预测所有的事情
尤其是AGI的时间线
这里有两个大的不确定性
一是AI基准的差距
现在的基准可能无法覆盖人类最难的任务
比如“自主证明一个新的实质性的数学定理”，
AI目前还没显示出太多的进展；
二是当前AI能力的差距
比如AI容易产生幻觉、推理能力不强、可靠性不够
这些问题能不能通过规模化解决
现在还不确定
所以报告的预测是一个“最小的基线”，
也就是我们能确定AI通过规模化会改进的任务
而不是去猜测AGI什么时候到来
接下来呢
我们来聊聊报告的核心章节之一
规模化与能力的关系
首先呢报告认为
规模化算力确实能够提升性能
不管是训练算力还是推理算力
训练算力的规模化已经被广泛研究
即使不同的模型用不同的架构、训练方法和数据
基准性能和算力的相关性也很强
而推理算力的规模化是最近的突破
以前的推理方法效率不高
但现在的大模型推理技术
能够有效地扩大推理算力
比如通过模型压缩、量化、分布式推理等方式
让推理的效率更高
很多人会问
规模化是不是已经“撞墙”了呢？
比如是不是算力增加了
但是性能提升越来越慢？
报告里做了澄清
有几种“撞墙”的说法需要区分
最激进的说法是“scaling laws失效了”，
也就是算力增加，性能不再提升
但是目前没有公开证据支持这一点；
第二种说法是下游任务的改进不如预期
比如虽然模型在预训练时算力增加了
但是在实际任务上的提升没那么明显
不过从GPT-4.5、Grok-3这些模型的表现来看
基准性能还是和算力规模化大体一致的；
第三种说法是规模化更难了
比如需要更多的投资、数据、电力
芯片生产也更复杂，这确实是事实
现在的AI训练数据中心已经到了数十万个GPU的规模
接近单个数据中心的供电极限
所以才会有跨数据中心训练
但是报告认为，这些困难在2030年前
还不足以阻止规模化的趋势
还有一个关键点是推理规模化和训练规模化的关系
很多人觉得现在重视推理
就会忽视训练
但是实际上两者是互补的
扩大训练规模能产生更强的模型
这些模型在同样的推理预算下能做更多事情；
而推理规模化能生成更多高质量的数据
反过来又能提升训练的效果
而且现在的前沿模型越来越依赖后训练
有报告说后训练的算力可能很快会和预训练算力相当
即使预训练因为数据的问题放缓
转向合成数据的后训练也能继续推动AI发展
最后，我们再回到宏观层面
看看2030年AI会给社会带来什么影响
以及我们需要面对的挑战
首先是经济影响，报告预测到2030年
AI会成为整个经济的关键技术
会像现在的互联网一样
渗透到各个行业
如果AI能实现预期的生产力提升
就能创造数万亿美元的经济价值
这会吸引更多的投资
推动技术进一步发展
但是同时也会有劳动力市场的颠覆
一些重复性的案头工作可能会被AI替代
比如数据录入、简单的文案撰写、基础的代码开发
这需要我们提前做好劳动力培训和转型
让人们适应和AI协作的工作模式
然后是环境影响
AI的能源消耗确实在增长
但是我们也可以用AI来减排
报告的附录里提到了很多AI减排的应用
比如优化数据中心的能源管理
已经实现9%-13%的节能；
减少飞机凝结尾迹
一项试点研究显示排放减少了50%，
能降低航空业0.4%的排放；
优化交通路线
减少了10%的交叉口排放
对应全球0.15%-1.5%的排放减少；
以及优化电网，提升风电价值20%，
可能减少1.6%的排放
这些应用如果能大规模部署
AI带来的减排可能会超过其自身的排放
但是这需要政策支持和资金投入
不是靠技术本身就能实现的
还有监管和伦理挑战
AI的滥用风险是显而易见的
比如用AI进行网络攻击、制造生物武器
或者生成虚假信息影响舆论
而且AI的决策过程不透明
可能会带来公平性问题
比如在招聘、贷款中歧视特定群体
所以我们需要建立完善的监管框架
平衡AI的创新和风险
同时制定伦理准则
确保AI的发展符合人类的共同利益
不同国家和地区的监管政策可能会有差异
这也需要国际协调
避免出现“监管套利”的情况
OK，现在我们来总结一下今天的内容
《AI in 2030》报告给我们描绘了一个看得见的未来
到2030年
AI会在算力、投资、能力上实现巨大突破
先重塑案头工作
再渗透到物理世界
带来显著的经济价值
但是同时也会面临能源、监管、劳动力转型等挑战
其实
我们也许不需要去纠结AGI什么时候来
而是要关注那些确定的、能落地的能力
思考如何利用这些能力提升效率、解决问题
对于我们每个人来说
了解AI的发展趋势，学会和AI协作
才能掌握未来的核心竞争力
感谢大家收看本期视频
我们下期再见
