大家好，这里是最佳拍档，我是大飞
最近这两天
DeepSeek可以说是霸占了AI行业的绝对热点
甚至同时登上了国内国外App Store免费下载榜的第一名
以往，大家都会觉得大模型的竞争
往往是一场算力的较量
背后是动辄千万亿美元的投入
只有像OpenAI、Meta、谷歌、英伟达这些大公司
凭借着强大的技术优势和巨额的资金投入
才能参与其中
它们的高估值也让不少人望尘莫及
不过，DeepSeek R1的出现
让这一切开始有了变化的迹象
甚至有人认为
DeepSeek或许将威胁到美国的AI霸权
那些大公司引以为傲的技术优势和高估值也可能因此而瓦解
据传Meta内部破了大防
开始组建专门的部门针对DeepSeek研究
甚至连带英伟达、Arm、博通等相关企业
也跌去了一万亿美元的市值
a16z的合伙人安尼·米德哈（Anjney Midha）也说道
一夜之间，从斯坦福到MIT
DeepSeek R1已经成为美国顶尖高校研究人员的「首选模型」。
应该说
DeepSeek R1的出现不禁让人思考
在大模型的发展过程中
数百亿美元的支出真的是必要的吗
Scaling Laws真的只能靠疯狂地烧钱才能实现么？
由于DeepSeek R1开源了相关的论文和Pipeline
随即
全球范围内开始掀起了一股复现R1模型的狂潮
今天，大飞就来给大家简单盘点一下
全球各团队对DeepSeek R1的复现情况
也许正如杨立昆所说
这一次是开源对闭源的胜利
首先是HuggingFace团队的Open R1项目
HuggingFace团队官宣要复刻DeepSeek R1所有的pipeline
并且在复刻完成后
会将所有的训练数据、训练脚本等等全部开源
目前这个项目已经收获了5.1k个star
399个fork
在项目主页中
可以看到HuggingFace团队写道
项目的目的是构建R1 pipeline中缺失的部分
以便所有人都能在此基础上复制和构建R1
为了实现这个目标
他们以DeepSeek-R1的技术报告为指导
分为3个步骤来完成这个项目
第一步
团队通过用DeepSeek-R1蒸馏高质量的语料库
来复制R1-Distill模型
之所以要这样
是因为DeepSeek开源了6个用R1蒸馏的小模型
这些小模型的表现也十分惊人
就拿蒸馏版的Qwen-1.5来说
它在部分任务上的表现甚至超过了GPT-4o
从一些具体的数据对比中
我们能更为直观地感受到这一点
在AIME 2024 pass@1这项指标上
GPT-4o-0513的成绩是9.3
而DeepSeek-R1-Distill-Qwen-1.5B的成绩达到了28.9；
在MATH - 500 pass@1上
GPT-4o-0513是74.6
而DeepSeek-R1-Distill-Qwen-1.5B则是83.9
这些数据充分展示了DeepSeek蒸馏小模型的强大实力
第二步
团队会复制DeepSeek用来构建R1-Zero的纯强化学习（RL）pipeline
这一步可能涉及到为数学、推理和代码
整理新的大规模数据集
通过这一步
团队希望能重现DeepSeek-R1-Zero的训练过程
进一步探索强化学习在模型训练中的应用
第三步，通过多阶段训练
从基础模型过渡到RL版本
这里的多阶段训练
是指DeepSeek-R1训练过程中引入的一个多阶段训练流程
具体包括4个阶段
第一个阶段是冷启动
用数千个长思维链（CoT）样本
对基础模型进行监督微调（SFT）
为模型提供初始的推理能力
第二个阶段是面向推理的强化学习
在第一个SFT阶段的基础之上
用和训练R1-Zero相同的大规模强化学习方法
进一步提升模型的推理能力
特别是应对编程、数学、科学和逻辑推理任务的能力
第三个阶段是拒绝采样和监督微调
再次使用监督微调
提升模型的非推理能力
比如事实知识、对话能力等
最后一个阶段是针对所有场景的强化学习
这次强化学习的重点是让模型行为与人类偏好保持一致
提升模型的可用性和安全性
目前，在这个项目的GitHub仓库中
我们已经可以看到HuggingFace团队准备的一些文件
比如GRPO实现、训练和评估代码、合成数据生成器等等
大家对项目的进展可以说是充满期待
除了HuggingFace团队以外
伯克利的团队也在DeepSeek的复现上取得了令人瞩目的成果
来自UC伯克利的博士生潘家怡和另外两位研究人员开展了TinyZero项目
他们在CountDown游戏中复现了DeepSeek R1-Zero
而且实验结果相当出色
CountDown这个游戏会要求玩家使用基础的算术运算
将数字进行组合
从而达到目标数字
让人惊讶的是
整个实验的成本不到30美金
换算成人民币大约217元
在这个项目中
他们采用了R1-Zero算法
给定一个基础语言模型、提示和真实的奖励信号
运行强化学习
然后模型应用在CountDown游戏中
在实验过程中
模型展现出了令人惊叹的能力
它从最初的简单输出开始
逐步进化出自我纠正和搜索的策略
比如说，在面对“使用数字 [19, 36
55, 7]，
创建一个结果为65 的等式”这样的问题时
模型会不断尝试不同的数字组合
自我验证计算结果，反复纠正错误
直到找到正确的答案“55 + 36 -7 - 19”。
为了进一步探究模型的性能
研究人员还进行了消融实验
他们运行了Qwen-2.5-Base模型
包括0.5B、1.5B、3B、7B四种参数规模
实验结果发现
0.5B模型仅仅是猜测一个解决方案之后就停止了
而从1.5B开始
模型学会了搜索、自我验证和修正自己的解决方案
从而能够获得更高的分数
这充分说明了在这个过程中
基础模型的性能起着关键的作用
此外，他们还验证了一个重要的结论
那就是额外的指令微调（SFT）并非是必要的
相比基础模型
指令模型的运行速度更快
输出的模型更具结构性和可读性
不过最终的表现与基础模型基本相当
这也进一步印证了R1-Zero的设计决策
同时，他们在研究中还发现
具体的强化学习算法并不重要
像在PPO、GRPO、PRIME这些算法中
长思维链（Long CoT）都能够表现出涌现
而且带来不错的性能表现
而且
模型在推理行为中非常依赖于具体的任务
对于Countdow任务
模型能学习到进行搜索和自我验证
而对于数字乘法任务
模型反而会学习到使用分布规则来分解问题
并且逐步解决
比如说在计算“129 * 14”的时候
模型会运用乘法分配律
将14分解为10 + 4
然后分别计算129 * 10和129 * 4
最后将结果相加得到1806
苹果的机器学习科学家张逸哲（Yizhe Zhang）表示
小到1.5B的模型
竟然也能通过强化学习涌现出自我验证的能力
这真的太酷了
接下来我们再来看看港科大团队的成果
港科大助理教授何俊贤的团队开展了simpleRL-reason的项目
他们只用了8K个样本
就在7B模型上复刻出了DeepSeek-R1-Zero和DeepSeek-R1的训练
项目的结果也非常令人惊喜
模型在复杂的数学推理上表现十分强劲
具体来说
他们以Qwen2.5-Math-7B作为基础模型
直接对模型进行强化学习
整个过程中，没有进行监督微调
也没有使用奖励模型
最终
模型在AIME基准上实现了33.3%的准确率
在AMC上为62.5%，在MATH上为77.2%。
这个表现不仅超越了Qwen2.5-Math-7B-Instruct
并且还可以和使用超过50倍数据量和更复杂组件的PRIME和rStar-MATH 模型相媲美
在训练过程中
团队还发现了一些有趣的现象
大概在第40步的时候
出现了「Aha moment」，
或者说是顿悟时刻
模型在响应中出现了自我反思
比如在解决一道关于狗狗散步付费比例的数学问题时
模型一开始列出的方程有误
但是它很快意识到了错误
并且重新检查设置，重新求解
而且，在这个过程中
模型还显现了更长的CoT推理能力和自我反思的能力
实际上
团队采用的强化学习方案极其简单
没有使用奖励模型或者蒙特卡洛树搜索等技术
他们使用的是PPO算法
并且采用基于规则的奖励函数
根据生成输出的格式和正确性分配奖励
如果输出是以指定格式提供的最终答案
而且答案正确
那么将获得 +1的奖励；
如果输出提供了最终答案但不正确
奖励则为 -0.5；
如果输出未能提供最终的答案
那么奖励设为 -1
这个实现基于的是OpenRLHF
初步试验表明
这个奖励函数有助于策略模型的快速收敛
以及产生符合期望格式的输出
团队还对训练过程进行了动态分析
在训练过程中
所有基准测试的准确率都在稳步提高
而输出长度则呈现出了先减少
然后逐渐增加的趋势
经过进一步的调查，他们发现
Qwen2.5-Math-7B基础模型
在初始阶段倾向于生成大量的代码
这可能源自于模型原始训练数据的分布特征
而输出长度的首次下降
是因为强化学习训练逐渐消除了这种代码生成的模式
转而学会使用自然语言进行推理
随后，生成长度再次开始增加
此时出现了自我反思的机制
另外
团队还尝试了在进行强化学习之前
先进行长CoT 的监督微调预热
使用了8000个从QwQ-32B-Preview中提取的MATH示例响应
作为监督微调数据集
这种冷启动的潜在优势在于
模型在开始强化学习的时候
就已经具备了长CoT思维模式和自我反思的能力
从而可能在强化学习阶段
实现更快更好的学习效果
实验结果也表明
与使用强化学习训练前的模型
比如Qwen2.5-Math-7B-Base + 8K QwQ知识蒸馏的版本相比
Qwen2.5-7B-SimpleRL的平均性能
显著提升了6.9个百分点
此外
Qwen2.5-7B-SimpleRL不仅持续优于Eurus-2-7B-PRIME
还在5个基准测试中的其中3个上
超越了Qwen2.5-7B-SimpleRL-Zero
应该说
全世界对于DeepSeek R1的复现肯定还远不止这些
陆陆续续还会有很多人尝试
不过从目前的反馈来看
基本对DeepSeek R1的训练方法都表示了广泛认可
而且可以很轻松复现效果
大飞我对DeepSeek团队的努力也表示由衷的敬佩
但是客观一点说
DeepSeek R1也并不是全面的超越
在很多方面还有不足
现在的热度稍微有点过了
又进入了无脑的狂欢时刻
容易让人迷失
更何况，一次小小的弯道超车
并不能代表中国整体AI实力超越美国
越是这个时候
可能越会激起美国的斗志和警惕
马克安德森也说这是AI的斯普特尼克时刻
毕竟我们要认识到
在先进制程芯片、基础设施和突破性算法研究方面
中国还有很大的落后
我更希望在这个时候
DeepSeek团队能够踏实下心来
继续好好做研究和实验
不要盲目激进，也不要自我骄傲
继续突破更多的困难
在AI领域持续不断地向前发展
好了，感谢大家收看本期视频
我们下期再见
