大家好，这里是最佳拍档
前几天
全球知名风投机构a16z发布了一张时间线图
将智谱AI的最新力作GLM-5与Anthropic公司的闭源旗舰模型Claude Opus 4.6
并列标注在人工智能分析指数的赛道上
a16z在报告中明确指出
Claude Opus 4.6仍然是最智能的
但是它与排名第二的开源模型之间的差距
已经大幅缩小
而这份评价所指向的最好的开源模型
正是GLM-5
紧接着
GLM-5的完整技术报告正式发布
这份长达40页的文档
系统阐述了模型从架构设计、数据训练到工程落地的全流程创新
其中DSA稀疏注意力、完全异步的Agent RL训练框架、自研slime RL基础设施等核心技术点
更是成为AI行业热议的焦点
今天
我们就来解读一下这份技术报告
看看GLM-5为何能获得a16z的高度认可
它在技术上实现了哪些突破性进展
又将给开源大模型生态带来怎样的变革
首先，我们来看GLM-5的核心基本面
这是它能够立足顶尖梯队的基础
GLM-5沿用了混合专家架构
这种架构的核心优势在于推理时仅激活部分参数
在保证性能的同时大幅降低计算成本
具体来看
GLM-5的总参数达到七千四百四十亿
每次推理时激活的参数为四百亿
包含二百五十六个专家网络和八十层网络结构
对比上一代模型GLM-4.5
总参数翻了一倍
预训练数据量也从23万亿token
增加到了28.5万亿token
其中预训练阶段贡献27万亿token
中期训练补充了1.5万亿token
数据规模和模型体量的双重提升
为性能突破奠定了坚实基础
在权威评测中
GLM-5的表现堪称开源模型中的标杆
在人工智能分析指数中
GLM-5以50分的成绩位居开源模型第一
该指数包含了10项核心评估
全面覆盖推理、编码、工具使用等多个维度
GLM-5的夺冠意味着其综合能力已经达到开源领域的顶尖水平
而在LMArena评测中
GLM-5在文本竞技场和代码竞技场均斩获开源第一
整体得分一千四百五十六分
与Claude Opus 4.5、Gemini 3 Pro处于同一梯队
成为少数能与顶尖闭源模型正面抗衡的开源模型
与上一代GLM-4系列相比
GLM-5的架构升级主要集中在三个方面
第一个核心架构创新是MLA加Muon Split的注意力机制组合
GLM-5采用的MLA机制
与DeepSeek的V3模型同源
核心设计是通过压缩键值对缓存的维度来节省显存空间
这使得模型在处理长文本时的速度
相比传统注意力机制有明显提升
但是研发团队在训练过程中发现了一个关键问题
当MLA机制与Mu·on优化器配合使用时
模型效果始终无法追上结构更简单的GQA-8方案
为了解决这一难题
团队提出了Muon Split的优化策略
将原本对整块投影矩阵进行的正交化操作
改为按每个注意力头单独执行
这个调整让不同的注意力头
能够按照自己的节奏更新参数
不仅最终效果追平了GQA-8方案
还带来了一个意外的附加收益
注意力分数在训练过程中能够自动保持稳定
无需额外的裁剪操作来防止数值溢出
在此基础上
团队还开发了MLA-二五六变体
将每个注意力头的维度从一百九十二提升至二百五十六
同时将注意力头的数量减少三分之一
在保持参数总量不变、性能持平的前提下
进一步降低了推理时的计算量
从实测数据来看
MLA加Muon Split组合
在MMLU等多个数据集上的表现
均优于基础的MLA机制
其中MMLU数据集得分从61.5
提升至62.5
验证了这个优化的有效性
第二个架构创新是参数共享的多token预测
简称MTP
在大模型推理中
推测解码是一种常用的加速技术
其核心逻辑是用一个轻量级的小模型
快速预测接下来的多个token
再由主模型进行验证
如果预测准确则可以节省主模型的计算步骤
DeepSeek-V3模型采用的是1个MTP层训练、推理时预测2个token的方案
但是这种训练与推理方式不一致的设计
导致第二个token的预测准确率偏低
GLM-5对此进行了针对性优化
采用3个MTP层进行训练
但是这3层共享同一套参数
这样既保证了推理时的内存开销与DeepSeek-V3相当
又通过多层训练提升了预测准确率
实测数据显示，在4步推测解码场景下
GLM-5的平均接受长度达到2.76
而DeepSeek-V3.2仅为2.55
预测效率的提升
直接转化为推理速度的优化
让大模型在处理长文本生成任务时更加高效
第三个
也是GLM-5在效率优化上最核心的创新
DSA稀疏注意力
传统的全量注意力机制存在一个致命缺陷
计算量会随着上下文长度的增加呈平方倍增长
这使得大模型在处理超长文本时的成本极高
难以落地应用
DSA稀疏注意力的核心思路
是引入一个轻量级的索引器
在进行注意力计算前
先快速扫描所有token
筛选出与当前token最相关的top k个token
仅对这部分相关token进行注意力计算
其余不相关的token则直接跳过
与传统的滑动窗口机制不同
DSA是基于内容相关性来选择关键token
而非依赖位置信息
这使得模型在节省计算量的同时
不会丢失重要的长程依赖关系
更令人惊叹的是GLM-5在DSA训练上的效率突破
研发团队从中期训练结束后的基础模型开始
先进行一千步预热训练
随后进行两百亿token的稀疏适配训练
总训练token预算仅为两百亿
而对比来看
DeepSeek-V3.2的DSA训练耗费了九千四百三十七亿token
是GLM-5的将近50倍
但是GLM-5通过优化的训练策略
最终在长上下文基准测试中
达到了与DeepSeek-V3.2相当的效果
最终数据显示
DSA模型在长上下文任务上的表现
与原始MLA模型基本持平
且经过SFT后的训练损失曲线几乎完全重合
这意味着在几乎不损失性能的前提下
GLM-5将长序列注意力计算量降低了1.5到两倍
对于后续Agent推理中动辄两百K上下文的场景来说
这个优化直接将GPU成本砍半
为大模型的大规模落地扫清了重要障碍
为了验证DSA的优越性
研发团队还进行了一组消融实验
对比了朴素滑动窗口交错、基于搜索的SWA模式、GDN、SimpleGDN等多种高效注意力方案
结果显示
朴素滑动窗口交错方案在128K上下文下
Ruler数据集得分暴跌30.35分
基本失去实用价值
基于搜索的SWA模式效果相对较好
但是在细粒度检索任务上仍会丢失5到7分
而DSA通过token级的动态选择
能够完整保留长程依赖
在各项长上下文基准测试中均表现最优
充分证明了技术的先进性
架构之外
数据质量是大模型性能的另一个核心支撑
GLM-5在预训练数据和中期训练上都进行了全面升级
预训练数据主要来自三个核心来源
在网页数据方面
GLM-5在4.5的原有数据管线上
新增了基于句子嵌入的DCLM分类器
专门挖掘标准分类器遗漏的高质量内容
同时训练了一个世界知识分类器
利用维基百科条目和大模型标注数据作为训练样本
从中低质量网页中筛选出有价值的长尾知识
进一步丰富模型的知识储备
在代码数据方面
团队刷新了主要代码托管平台的快照
经过模糊去重处理后
独特token数量增加了28%。
同时修复了Software Heritage的元数据对齐问题
并为Scala、Swift、Lua等低资源编程语言
训练了专用分类器
提升了模型对小众编程语言的支持能力
在数学与科学数据方面
数据主要从网页、书籍、学术论文中收集
通过大模型打分
筛选出最具教育价值的内容
对于长文档采用分块聚合评分的方式确保质量
同时严格排除合成数据和AI生成数据
避免低质量数据对模型性能的干扰
中期训练则聚焦于上下文窗口的逐步扩展
分为三个关键阶段
32K上下文窗口阶段训练1万亿token
128K阶段训练五千亿token
新增的两百K阶段训练五百亿token
GLM-4.5的最大上下文窗口为128K
而GLM-5新增的两百K阶段专门用来处理超长文档和多文件代码库
满足更复杂的实际应用场景
在数据类型上
中期训练重点扩充了软件工程数据
通过放宽仓库级筛选条件
获得了大约一千万个Issue和PR对
同时加强了单个issue的质量过滤
最终形成约一千六百亿token的高质量软件工程数据
长上下文数据则包含自然数据和合成数据
合成数据采用NextLong和EntropyLong的思路构建长程依赖
两百K阶段还额外加入了MRCR类数据的多种变体
专门增强模型
在超长多轮对话中的召回能力
训练工程的优化
是GLM 5能够顺利落地的重要保障
技术报告用大量篇幅阐述了训练基础设施的创新
这些优化虽然并非单个突破性技术
但是组合在一起实现了一加一大于二的效果
让七千四百四十亿参数的大模型
能够在合理的硬件规模上完成训练
其中关键的优化点包括MTP布局优化
将MTP模块的输出层与主输出层
放在流水线最后一个阶段共享参数
其余部分向前迁移
有效平衡了各rank的显存占用
Zero 2梯度分片技术
让每个阶段仅存储dp分之1的梯度
配合双缓冲机制
在不增加同步开销的前提下
大幅降低了梯度显存占用
还有Muon优化器的零冗余通信设计
将all-gather操作限制在本rank负责的参数分片内
减少了通信开销
以及流水线激活卸载技术
在前向计算完成后将激活值按层卸载到CPU
反向计算时再重新加载
与计算过程重叠执行
提升了硬件利用率
最后是序列分块输出投影和Int4量化感知训练
前者针对长序列下输出层和损失函数的显存峰值过高问题
按序列维度分块处理
降低了显存压力；
后者在SFT阶段就引入量化训练
开发了训练与推理比特位对齐的量化内核
为后续的高效部署奠定基础
后训练流程是GLM-5实现能力飞跃的关键环节
团队设计了一条完整的流水线
从有监督微调SFT
到推理强化学习Reasoning RL
再到智能体强化学习Agentic RL
再到通用强化学习General RL
最后到跨阶段在线蒸馏
每个环节都有明确的优化目标和创新设计
SFT阶段的训练数据分为三大类
包括通用对话数据、推理数据、编程与Agent数据
最大上下文长度扩展到了二十万二千七百五十二个 token
充分满足了超长文本处理需求
为了提升不同场景下的表现
SFT阶段还设计了三种思考模式
分别是交错思考
在每次响应和工具调用前都进行一轮思考
提升指令遵循度和生成质量
保留思考，在编程智能体场景中
多轮对话之间保留所有思考内容
无需重新推导
适合长程复杂任务，减少信息丢失
以及轮级思考
按轮次控制思考模式的开关
简单请求关闭思考以降低延迟
复杂任务开启思考以提升精度
值得注意的是
编程和Agent相关的SFT数据
采用了专家强化学习和拒绝采样技术来提升质量
团队还特意保留了轨迹中的错误片段
但在计算损失时用掩码进行屏蔽
让模型能够学习到错误发生的原因和纠错方法
同时避免被训练去重复错误动作
推理强化学习阶段基于GRPO和IcePop算法进行改进
核心创新是明确区分了用于梯度更新的训练模型
和用于生成轨迹的推理模型
并且去掉了KL正则项
大幅加速了训练进程
这个阶段采用了纯在线策略训练
分组大小和批次大小均设为32
在训练过程中
研发团队还发现了一个影响重大的工程细节
DSA的索引器在每个token位置需要进行top k检索
其中k等于两千零四十八
而SGLang推理引擎中基于Cuda的top k实现虽然速度快
但是结果存在随机性
同样的输入可能得到不同的排序结果
而将它替换为PyTorch原生的torch点topk函数后
虽然速度稍慢
但是输出结果完全一致
实验证明
使用非确定性的Cuda top k
会导致RL训练几步后就出现熵值骤降、性能急剧退化的问题
因此最终方案确定为全程使用torch点topk
并在RL阶段冻结索引器参数
从而确保训练的稳定性
推理强化学习在数学、科学、代码、工具集成推理四个领域进行混合训练
难度过滤逻辑非常严格
只保留了GLM-4.7无法完成
但OpenAI的GPT-5.2 high版本
或者Gemini 3 Pro Preview能够完成的题目
通过高难度数据倒逼模型推理能力提升
智能体强化学习是技术报告中篇幅最大的部分
也是GLM-5实现智能体能力突破的核心
智能体任务的最大挑战在于rollout时间极长且差异巨大
一条软件工程任务可能需要几分钟
另一条则可能需要半小时
传统同步RL的做法是等待所有rollout完毕后再统一训练
导致最慢的轨迹成为瓶颈
大量GPU资源处于闲置状态
为了解决这个问题
GLM-5采用了完全异步的训练架构
训练GPU和推理GPU物理分离
推理端持续不断的rollout
攒够一批轨迹后就发送给训练端
推理端的模型权重每隔K步与训练端同步一次
彻底打破了同步训练的效率瓶颈
为了支撑大规模异步训练
团队还设计了多任务轨迹生成编排器
不同类型的智能体任务
各自作为独立的微服务注册到中央编排器
由编排器统一控制任务比例和生成速度
支持超过一千的并发轨迹生成
同时
为了保证异步训练的稳定性和效果
团队还提出了多项关键设计
比如Token-in-Token-out机制
训练流程直接消费推理引擎生成的token ID序列和元数据
避免了传统文本往返过程中
重新分词带来的细微差异
确保token级别的精确对应
还有直接双侧重要性采样
在异步场景下
推理引擎的模型可能在一条rollout过程中多次更新
追踪完整的历史策略概率不现实
因此直接使用rollout时记录的对数概率作为行为代理
计算重要性比率
将落在信任域外的token直接屏蔽梯度
防止偏差过大的样本影响训练
以及样本过滤机制
记录每条轨迹的模型版本号
丢弃版本差距超过阈值的样本
同时排除因环境崩溃导致失败的样本
最后是DP-aware路由
在多轮智能体任务中
通过一致性哈希将同一个轨迹生成的后续请求
路由到同一个数据并行rank
复用KV缓存
让预填充成本仅与增量token成正比
提升训练效率
通用强化学习阶段的优化目标涵盖三个维度
分别是正确性、情商和特定任务能力
这个阶段的奖励系统采用三种信号混合设计
包括规则奖励、判别式奖励模型ORM
以及生成式奖励模型GRM
通过三者的互补实现更全面的奖励评估
一个非常有意思的设计是
在RL中引入了人类撰写的高质量回复
作为风格和质量的锚点
因为纯模型的RL训练
容易让模型收敛到冗长、公式化的机器感表达模式
这类表达虽然在奖励函数上得分较高
但是读起来不够自然
而人类的高质量回复
能够有效将模型风格
拉回到更符合人类交流习惯的轨道上
跨阶段在线蒸馏则是为了解决多阶段RL训练中的灾难性遗忘问题
GLM-5在训练流水线的最后加入了蒸馏阶段
将前面几个阶段的checkpoint作为教师模型
学生模型通过计算与教师模型输出logits的差距
直接得到优势函数
无需大的分组规模
同时将批次大小提升到了一千零二十四
大幅提升了训练吞吐量
有效保留了各阶段学到的核心能力
最后
我们再来介绍一下GLM-5后训练自研的slime RL框架
这个框架围绕三个核心设计重点构建
确保了大规模RL训练的稳定性和高效性
横向扩展方面
slime框架提供高度可定制的rollout接口
并且通过HTTP API暴露推理服务
不同的智能体框架可以像调用普通推理引擎一样与slime交互
实现了训练逻辑和推理逻辑的完全解耦
便于灵活扩展不同类型的训练任务
纵向扩展方面
slime框架将RL推理的优化目标聚焦于端到端延迟
针对最慢轨迹成为瓶颈的问题
采用多节点推理部署
通过FP8精度进行rollout
降低单token延迟
MTP技术在小批次解码下的收益尤其显著
同时采用预填充和解码分离调度的方式
确保多轮交互中解码速度稳定
大幅提升了整体训练效率
容灾能力方面
推理服务会定期发送心跳信号
不健康的节点会被自动终止并从路由中注销
请求会自动重试到健康节点
确保了大规模分布式训练的稳定性
避免因单个节点故障导致整个训练流程中断
技术报告的最后还藏着一个有趣的彩蛋
GLM-5在正式发布前
曾经以匿名身份Pony Alpha
在OpenRouter平台上线
没有公开任何品牌信息
纯靠模型自身的性能表现吸引用户
上线几天后
OpenRouter社区就对这个神秘模型产生了浓厚兴趣
开发者们发现它在复杂代码生成、智能体任务链路和角色扮演等场景下的表现尤为突出
纷纷猜测其真实身份
根据社区统计
25%的用户推测它是Anthropic的Claude Sonnet 5
20%的用户认为是Grok的新版本
10%的用户猜测是DeepSeek V4
直到正式发布后
大家才知道这个匿名模型正是GLM-5
这也从侧面印证了其性能已经达到了与顶尖闭源模型难分伯仲的水平
总的来说
GLM-5通过架构创新、数据优化、工程突破和生态构建
不仅获得了a16z眼中最好的开源模型的高度认可
更在多个核心能力上实现了对开源模型的超越
甚至逼近顶尖闭源模型
它的发布不仅为开发者提供了一个高性能、可定制的开源大模型选择
更推动了开源大模型生态的技术进步
其中的架构设计、训练策略和工程优化思路
都值得其他模型深入学习和借鉴
感谢收看本期视频，我们下期再
