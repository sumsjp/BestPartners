大家好这里是最佳拍档我是大飞
前两天
Meta发布了免费可商用的Llama 2
在整个AI圈也是刷屏了
我们也做了一期节目介绍
之前的Llama 1版本
因为开源协议的问题
一直不可以免费商用
现在，随着Llama 2的发布
这个限制正式被打破
在模型发布之后
Llama-2-70B-Chat迅速登顶Hugging Face的开源大模型榜单
但是就在昨天
已经有新的模型把宝座又抢走了
我们后续也会再介绍一下
用我同事的话说
中午睡了半个小时的午觉
大模型又变天了，真的是太夸张
不过，今天我们主要是想给大家
介绍一下Llama 2的更多技术细节
巧的是
除了官方公开的技术资料以外
来自伯克利大学
同时也是Huggingface的人工智能科学家
内森·兰伯特Nathan Lambert
也在自己的博客上发表了一篇文章
分析来看
他认为Llama 2是Llama架构的延续
在数据质量、培训技术、能力评估、安全培训和责任发布方面
都进行了大量的技术更新
我们也在这里
给大家分享一下其中的精华内容
首先兰伯特回顾了一下Meta在论文中提到的几个要点
Llama 2相较于上一代模型
训练数据提升了40%，
包含了70亿、130亿和700亿参数 3个版本
Llama 2接受了2万亿个token的训练
上下文长度是Llama 1的两倍
达到了4K
而且采用了分组查询注意力机制
对应的微调模型也接受了超过100万个人类注释的训练
虽然大家都叫Llama 2是开源模型
但是实际从技术角度讲
并没有完全开源
不过对开源社区还是非常有用的
那通过一系列的基准测试
兰伯特第一次确信
开源模型的能力达到了ChatGPT的水平
当然除了编程能力以外
Llama 2的成本至少超过2500万美元
此外
Meta还提出了一种提高多轮一致性的新方法GAtt
它的灵感来源于上下文蒸馏法
此外
还有一些对奖励模型、RLHF流程、安全评估和许可声明的看法
我们这里先略过
从Meta发表的论文来看
Llama 2主要是在原有的基础上进行了一次扩充
它的下一代模型应该也正在训练中
根据论文的显示
Meta在很大程度上倾向于通过开源实现人工智能的民主化
这很可能意味着Meta正在争分夺秒
争取在Reddit和Twitter等网站被完全封锁之前
获得所有可用的互联网数据
在基础模型方面
除了增加上下文长度和分组查询注意力（GQA）以外
Llama 2在架构和所有方面都与一代非常相似
主要变化就是在数据和训练过程中
其中上下文长度提高了聊天用例的可用性
而GQA则提高了模型的推理速度
论文的大部分内容都是关于评估和微调的
他们致力于在偏好数据上训练奖励模型
然后采用强化学习来进行优化
从而提高生成的质量
兰伯特还认为
这恰恰证明了他从Anthropic和OpenAI那里听到的一个传言
那就是奖励模型是人类反馈强化学习的关键
也是模型的关键
为了得到一个好的奖励模型
Meta不得不花大力气来收集偏好数据
这些数据远远超过了开源社区目前使用的数据量
此外
Meta采用了二分类得模型评价指标
并没有使用更加复杂的反馈类型
收据收集的重点也放在了有用性和安全性上
而且对每个数据源使用不同的指导原则
而且，Meta在收集的信息中
还添加了额外的安全元数据
并且采取了迭代式的数据收集方式
每周分批收集人工注释
随着收集到更多的偏好数据
奖励模型也得到了改进
如果按照市场价格来算的话
仅数据一项就可能花费了2000多万美元
关于奖励模型的部分
主要可以用两个重要细节来概括
首先是Meta训练了两个独立的奖励模型
一个针对有用性进行了优化
另一个针对安全进行了优化
这两个模型都建立在基础语言模型上
用线性回归层取代了普通语言模型
其次是迭代部署以及需要使用的偏好数据量
在这个过程中还有一些值得注意的技术细节
1
在没有详细解释为什么需要的情况下
Meta仍然保留了一些Anthropic的无害数据
2
为了避免奖励模型容易出现过拟合
他们只训练了一个epoch
3
奖励模型的平均准确率仍然只有65-70%，
但是当标注者的偏好一致性较强的时候
准确率可以达到80-90%。
在准备微调部分的时候
Meta隐藏了一个爆炸性的真相
那就是他们注意到
奖励模型的准确性
是Llama 2-Chat最终性能的最重要代表之一
但是这部分并没有详细展开来讲
接下来
在论文的人类反馈强化学习和微调部分
Meta采用了最佳奖励模型
并且在这个基础上对各种模型进行了评估
这么做，也是为了说明
奖励模型会改善模型的最终输出
Meta一共迭代训练了5个RLHF版本
分别从V1-V5
而且，从一开始
Meta就指出了数据质量对这些模型的重要性
原话是Quality Is All You Need
Meta在发现第三方数据集质量不高的情况后
果断放弃
采用了自己标注的、质量更高的数据
结果模型性能有明显改善
而且仅仅对27540条高质量数据的监督微调
就可以达到接近Anthropic和OpenAI的效果
Meta还观察到
不同的注释平台和供应商提供的数据
可能会导致下游模型性能的不同
这表明即使是供应商注释的数据
后续检查也是很重要的
数据质量建立起来后
Meta开始专注于强化学习组件
并且在论文中明确指出
人类反馈强化学习能够从根本上提高模型的性能上限
而其他的研究虽然也认为RLHF很重要
但是通常只是把它当作一种安全工具
兰伯特认为
要想切实有效地开展RLHF
至少需要一个规模适中的团队
至少需要6-10个人
随着时间的推移，人数会逐渐减少
但是由于需要与外部公司保持紧密的合作和沟通
人也不会太少
论文中使用了两阶段的RLHF方法
首先使用拒绝采样（Rejection Sampling）
就是在模型输出时采样K个结果
选择奖励值最高的一个
在强化学习阶段进行梯度更新
然后再结合近端策略优化PPO
这也是标准的强化学习算法
进行拒绝采样加近端策略优化的处理
拒绝采样的搜索范围更广
而PPO对每个奖励模型的更新会更多
不过
不同方法之间的最终差异并不明显
在模型的评估阶段
论文从多个方面对模型进行了评估
首先，在自动基准评估上
Llama 2在所有规模上
都比任何其他的开源模型要好得多
但是闭源模型是一个都打不过
不过，Meta在论文中并没有详细说明
大量的数据工作可能才是这些基本评估最重要的基础
其次
基本模型评估在某种程度上是在进行一场不公平的游戏
这些模型可以在没有开源验证的情况下
很容易被提示和操纵来获得高分
在论文中
Llama 2-Chat模型在单回合和多回合提示上
都显著优于其他开源模型
特别是
Llama 2-Chat 7B模型在60%的提示上
胜过了MPT-7B-chat模型
而Llama 2-Chat 34B模型在与容量相当的 Vicuna-33B
和Falcon 40B模型对战中
总体胜率超过75%。
在安全性方面
论文中包括了偏差、红队、预训练步骤等内容
这部分兰伯特以后会做更详细的分析
最后
对于Meta分别在RSC超级集群
和内部生产集群上对模型进行预训练
兰伯特觉得这可能更多是出于计算能力的限制
而不是论文中说的
是为了比较大模型训练的适用性
以及提到了Llama 2商业许可中
对7亿月活用户以上
必须向Meta申请许可的事
通篇下来
Llama 2我觉得最重要的一个启示就是
奖励模型极其重要
不仅是人类反馈强化学习的关键
也是整个大模型效果的关键
其中数据的质量又是关键中的关键
在论文中被多次提到
却又一直语焉不详
希望能有人进行更多的研究
好了，本期视频内容就到这里
后续有关Llama 2的一些更详细的技术细节
我们有机会再跟大家分享
感谢大家的观看，我们下期再见
