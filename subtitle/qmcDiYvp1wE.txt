大家好，这里是最佳拍档，我是大飞
没想到
AI竟然已经可以通过标准的三方图灵测试了？
最近
来自加州大学圣迭戈分校的研究人员
对4个AI模型进行了系统评估
最终证明大语言模型成功通过了图灵测试
这意味着
以后在网络上和你聊天的那个“熟悉的陌生人”，
也许就不再是人类了
那么
研究人员究竟找到了什么样的证据
今天我们就来解读一下这篇论文
我们先简单介绍一下图灵测试是什么
时间回到75年前
计算机科学领域的先驱艾伦·图灵提出了一个名为“模仿游戏”的概念
它后来成为了判定机器是否具备智能的一种方法
也就是我们现在所说的图灵测试
当时，艾伦·图灵设想了这样一个场景
那就是测试人员通过纯文字的聊天界面
同时开启与两个人的“聊天”，
其中一个是真人，另一个则是AI
这两方都要拼命向测试员证明
自己才是如假包换的人类
而对方是AI机器人
如果测试员无法准确分辨出谁是真人
那么就说明机器通过了测试
长期以来
图灵测试一直被视为一个衡量AI达到人类智能程度的重要标尺
在过去的75年里
不少人都尝试开发能通过图灵测试的系统
但是都没有成功
不过，随着人工智能技术的不断发展
尤其是大语言模型的兴起
让大家重新把目光聚焦到了图灵测试上
之前有研究发现
当GPT-4按照要求扮演特定的角色时
有54%的概率会被误判为真人
跟瞎蒙的概率差不多
但是在进行三方图灵测试的时候
有超过90%的模型都可以被识别出不是真人
为了看看如今最新的大语言模型到底能不能被人识破
加州大学圣迭戈分校的研究人员
这次开展了随机对照的三方图灵测试
测试分别评估了四个模型的表现
包括ELIZA、GPT-4o、LLaMA-3 1.4-405B和GPT-4.5
其中ELIZA是约瑟夫·维森鲍姆（Joseph Weizenbaum）1966年在麻省理工学院研发的一个聊天机器人
它会模拟一位罗杰斯式的心理治疗师
来回答使用者提出的文字陈述或者问题
而其他几个模型
都是当今较为先进的前沿大模型
研究团队一共招募了 126 名加州大学圣迭哥分校的心理学本科生
和158 名 Prolific 平台的用户作为实验参与者
每位参与者都要完成 8 轮测试
每一轮测试都包含两段对话
审问者会同时与两位证人进行文字聊天
一位是真人，另一位是 AI
参与者会被随机分配扮演审问者或者证人的角色
并且会随机匹配到另一位参与者
一起进行测试
而AI 证人又被分成了 4 个组
在每位参与者担任审问者的四轮测试中
他们会随机与每个组里的一个 AI 证人进行对话
这四个组分别是，对照组 (Control)，
包含 ELIZA 和 GPT-4o-NO-PERSONA 这两个模型
无人格组 (No-Persona)，
包含 GPT-4.5-NO-PERSONA 和 LLAMA-NO-PERSONA 这两个模型
LLaMa 人格组 (LLAMA-PERSONA)，
包含LLAMA-PERSONA 模型
GPT-4.5 人格组 (GPT-4.5-PERSONA)，
包含 GPT-4.5-PERSONA 模型
这里的人格
指的是实验中设置的一个关键变量
那就是部分 AI 被赋予了一个特定人设的提示词（Persona Prompt）
要求模型模仿一个 19 岁、有点内向、熟悉网络文化和视频游戏、说话带点俚语、不喜欢用句号的年轻人
提示词中还增加了一些 “以假乱真” 的效果
比如会引导 AI 故意犯一些打字上的小错误
从而模拟人类在日常交流中不可避免出现的错误
降低 AI 输出的 “完美感” 和 “机器感”。
在实验的游戏界面中
审问者通过分屏界面同时与两位证人交流
审问者先给两位证人发送第一条消息
然后大家轮流发消息
一次只能发一条
证人之间看不到彼此的对话内容
每轮测试有 5 分钟的时间限制
时间到了之后
审问者需要判断哪个证人是人类
并且给出对这个判断的信心程度
以及判断的理由
测试中主要通过评估对话中人类语言风格模拟、上下文理解能力、情感表达与逻辑推理
来推测是否是 AI 在对话
为了衡量 AI 模型的表现
研究团队使用了“胜率”和“审问准确率”两个指标
其中胜率指的是
当审问者判断某个证人是人类实际上是 AI 模型的次数
占所有这类判断的比例
简单来说
就是AI被误认为人类的比例
而审问准确率
指的是审问者正确选择出人类证人的次数比例
在统计学分析中
研究团队还将AI模型没能通过图灵测试
设定为一个需要检验的零假设
这意味着
研究人员预设了如果 AI 模型没有成功的骗过人类
那么它的胜率就不会显著低于 50%。
因为 50% 的胜率
相当于审问者几乎是随机猜测的结果
如果实验结果显示 AI 模型的胜率显著高于 50%，
那么就意味着我们可以推翻这个零假设
从而推导出AI 模型在一定程度上通过了图灵测试的结论
最后，实验结果显示，在两项研究中
GPT-4.5-PERSONA 的胜率为 73%，
其中在本科生中为 69%，
在 Prolific 参与者中为 76%。
LLAMA-PERSONA 取得了 56% 的胜率
其中本科生为45%，
Prolific为65%。
GPT-4.5-NO-PERSONA 和 LLAMA-NO-PERSONA 的总体胜率分别为 36% 和 38%。
而基线模型 GPT-4o-NO-PERSONA 和 ELIZA 的胜率最低
分别为 21% 和 23%。
这说明，高达 73% 的情况下
人类评委会把 GPT-4.5 错判为真人
而坐在屏幕另一端的、货真价实的人类参与者
被认出来的概率只有 27%。
另外，实验结果还表明
如果缺少了明确的人格设定提示词
即使是像GPT-4.5 和 LLaMA 这样的前沿大语言模型
在图灵测试中的表现也会显著下降
胜率骤降到36%-38% 左右
这说明了提示词的重要性
如果只依赖于模型的的语言生成能力
并不足以让AI系统成功伪装成人类
通过图灵测试
那么在图灵测试中
究竟什么才是识别 AI 的有效方法呢？
研究团队也分享了几个不太奏效的策略
比如依赖于日常闲聊和询问情感感受
这些AI都已经非常成熟，难以区分
而真正能够发挥作用的识别策略
包括出其不意地提出一些怪异的问题或者指令
以及设置一些知识盲区陷阱
提问一些非常冷门、专业
甚至略显生僻的问题
面对不熟悉的知识领域
真正的人类通常会坦然承认自己不知道、不了解
但是，AI 为了避免显得无知
可能会倾向于根据已有信息进行推测或者生成答案
反而更加容易暴露
这也说明了模型有时需要假装自己是无知的
才会更像是真正的人类
除此以外
在跟 GPT-4.5 和Llama模型进行讨论的时候
很多用户还感觉到
顶尖大模型已经发展出层次化的人设构建策略
能够根据提示的要求
在社会身份、认知水平、语言风格乃至价值观体系等多个维度进行模拟
比方说
当被问到纸杯蛋糕食谱的时候
AI回了句“不了
我一般就买那种预拌粉，哈哈”。
这种漫不经心、带点自嘲又符合年轻人设的回答
瞬间就拉近了距离
充满了人味儿
另外LLaMa 模型在被问到喜欢的奇怪动物时
回答“树懒！
我可以看它们慢吞吞动好几个小时”，
这种口气也同样符合人设的个性化表达
这不禁让我们开始思考
这种基于指令的、高超的角色扮演能力
本身是否就是一种高级智能的体现？
或者说，是一种新的智能形式？
“机器能思考吗？
”这是图灵在 1950 年提出模仿游戏时
试图回答的终极问题
但是在逻辑、计算、知识存储等方面
AI早已超越人类的今天
图灵测试的意义似乎正在发生深刻的转变
论文的研究者们认为
图灵测试如今与其说是来衡量通用智能
还不如说衡量的是“类人性”（humanlikeness）和“社交可替代性”（substitutability）
它测试的是
AI 能否在社交互动场景中不被察觉地扮演人类的角色
当AI在硬技能上已经遥遥领先的时候
像是模仿人类的情感、个性和社交细节等软技能
就开始成为了新的竞技场
不可否认的是
当AI开始通过图灵测试的时候
表明AI已经可以无形地补充、或者替代那些需要与他人进行简短对话的经济角色
并且逐渐演化成一种所谓“伪造的人类”，
伪人
而与这种伪人的广泛互动
可能潜移默化地改变人类的自我认知、社会连接方式
以及对真实连接的感知价值
我们是否正在走向一个法国社会学家让鲍德里亚所描述的、真实与模拟界限模糊的“超真实”社会呢？
人类的独特性究竟在哪里呢？
我们又该如何自处呢？
美国作家布萊恩·克里斯汀 Brian Christian曾经在他的著作 《人性较量（The Most Human Human）》中提过一个观点
那就是AI的进步
最终可能会反向激励人类
更加关注和发展自身独特的核心价值
变得“比以往更加人性化”。
他认为，图灵测试的真正挑战者
不应该是机器，而是我们人类自己
另外，我们还要警惕的一点是
未来那些控制着大量AI伪人的巨头
将会握有影响人类用户意见和行为的权力
好了
以上就是这篇论文的主要内容了
建议感兴趣的朋友可以去阅读原文
面对越来越像人类一样的 AI
你是否做好了准备呢？
人类最应该坚守的特质又是什么呢？
欢迎在评论区留言
感谢大家的观看，我们下期再见
