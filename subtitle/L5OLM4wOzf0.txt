大家好，这里是最佳拍档，我是大飞
对人工智能感兴趣的朋友
可能经常会听到RLHF
也就是基于人类反馈的强化学习
可是，RLHF中真的有强化学习RL么
会不会就像老婆饼里没有老婆
夫妻肺片里没有夫妻一样呢？
德克萨斯大学奥斯汀分校的助理教授 Atlas Wang
在最近的一篇博客中就分享了这样一个观点
他指出，RLHF和其他类似的方法
并没有为大语言模型带来真正的强化学习
因为它们缺乏RL的核心特征
也就是持续的环境交互和长期目标的追求
除此以外
文章还讨论了几个有趣的问题
比如RLHF与经典的RL有什么不同？
为什么RLHF无法带给大语言模型真实的目标或意图？
为什么没有人大规模地进行真正的RL？
当前最有可能带给大语言模型目标的方法是什么？
以及没有目标驱动的大语言模型会带来什么后果？
通过了解这些差异
我们可以清楚地知道大语言模型能做什么、不能做什么
以及为什么
今天大飞就来给大家分享一下
首先我们来看什么是经典的强化学习
在经典的强化学习中
通常会有一个在环境中会采取行动的Agent
或者说智能体
然后环境会根据这个智能体的行动来改变状态
随之而来的是
智能体的行动会受到奖励或惩罚
目的是在多个步骤中累积长期奖励的最大化
经典强化学习的主要特征是
智能体会通过持续或者偶发的交互来探索多种状态、做出决策、观察奖励
然后在一个连续的循环中来调整策略
而RLHF是一种使用根据人类偏好数据训练的奖励模型
来完善模型输出的工作流程
常见的流程包括，一、监督微调SFT
指的是在高质量数据上
训练或者微调一个基础语言模型
二、奖励模型训练
指的是收集成对的输出结果
询问人类更喜欢哪一个
然后训练一个奖励模型
来接近人类的判断
三、策略优化
通过使用类似强化学习的算法
比如近端策略优化PPO
来调整大语言模型的参数
使其产生奖励模型所喜欢的输出结果
与经典RL不同的是
RLHF中的「环境」，
基本上是一个单步的文本生成过程和一个静态的奖励模型
这其中并没有扩展循环或者持续变化的状态
那么，为什么说RLHF不是真正的RL呢？
首先是单步或者几步优化的特点
在RLHF中
大语言模型会基于给定的提示词来生成文本
然后由奖励模型提供一个单一的偏好分数
因此，RLHF中的「强化」步骤
更类似于一步式的策略梯度优化
目的是为了实现人类偏好的输出
而不是在不断变化的环境中
对状态和行动进行全面的循环
这更像是一种「一劳永逸」的评分
而不是让一个智能体能随着时间推移
探索多步行动，并且接收反馈的环境
其次是
训练过程大多是离线或半离线的
奖励模型通常是在人类标签的数据上进行离线训练
然后用来更新模型的策略
同样，大模型在线调整策略的时候
也并没有实时去探索连续的环境循环
第三是缺乏基于环境的长期目标
在经典RL中的智能体
会追踪多个状态下的长期回报
而相比之下，基于RLHF的模型训练
则更加侧重于根据人类偏好
调整即时的文本输出
也就是说
模型并没有在一个动态的环境中去执行多个步骤
第四，只有表面约束
缺少真正的内部目标
RLHF可以有效影响某些输出的概率
从而引导模型远离不受欢迎的文本
但是模型内部并没有形成产生这些输出的「愿望」，
或者说「欲望」，
它仍然只是一个生成下一个 token 的统计系统
需要明确的是
无论是RLHF、SFT 还是其他方法
大语言模型都不是为了真正的目标或者意图而训练的
大语言模型的核心是根据给定的上下文
来预测下一个token
它们的「动机」，
纯粹是为了最大限度地提高下一个 token 的正确率
而这个过程并不存在主观上的愿望或者意图
我们常说AlphaZero「想要」在国际象棋中获胜
但是这其实只是一种比喻式的说法
从内部来说
AlphaZero只是在最大化数学奖励函数
并没有任何感觉上的欲望
同样，经过RLHF调整的大语言模型
也是在最大化对齐奖励信号
并没有内心的渴望状态
亚利桑那州立大学的计算机科学教授苏巴拉奥·坎巴姆帕蒂Subbarao Kambhampati 指出
RLHF其实有点名不副实
因为它将从人类判断中学习偏好或者奖励模型的过程
与一步或者几步的策略优化相结合
而不是像经典强化学习中那样
具有典型的长期迭代交互
而且
即使 RLHF 从人类数据中学习偏好的方式
会让人联想到逆强化学习IRL
它也不是分析专家行为会如何随着时间变化的经典方案
相反
RLHF更加侧重于人类对于最终或者短序列输出的静态判断
那么
像思维链CoT、过程奖励模型PRM
或者多智能体工作流等方法
有助于解决RLHF的问题吗？
我们先来看基于流程的奖励模型和思维链
基于流程的奖励模型
可能会对中间的推理步骤提供反馈
而不是仅仅是根据最终的输出来提供奖励
这样做的目的是鼓励模型以更加易于解释、正确率更高
或者更符合特定标准的方式
来解释或展示推理的过程
但是，难道这就是真正的RL了吗？
事实并非如此
因为即使你为中间步骤分配了部分奖励
比如CoT解释
你仍然会将整个推理过程输入到奖励模型
以此来获得奖励
然后进行下一步的策略优化
而不是在一个动态的环境中
让大模型自己去「尝试」部分的推理步骤、获得反馈、进行调整
并且进行开放式的循环
因此
虽然 CoT和PRM 会给人一种多步骤RL的错觉
但是实际上
它仍然相当于对文本生成和推理步骤
进行离线或者近似离线的策略调整
而不是经典RL中持续的智能体-环境循环
同样
多智能体工作流也不会创建出意图
虽然你可以在工作流中协调多个大模型一起工作
但是从内部来看
每个大模型仍然是根据下一个 token 的概率来生成文本的
尽管这样的多智能体流程
可以表现出看似协调或者有目的的涌现行为
但是它并没有赋予单个模型
任何内在或者秉持的目标
之所以多智能体工作流常常看起来显得有意图
是因为人类会自然而然地将心理状态
投射到行为看似有目的的系统上
这就是所谓的「意图立场」。
但是实际上
每个智能体只是在对提示词做出响应
它背后的思维链并不等同于个人欲望或者驱动力
只是一个更复杂的、多步骤的提示-反馈回路而已
因此
虽然多智能体协调可以产生解决新任务的能力
但是大模型本身仍然不会产生「我想要这个结果」的动机
那为什么至今还没有人用真正的RL
来训练大语言模型呢？
答案就是因为太贵了
大型模型的经典RL
需要一个稳定、交互式的环境
外加大量计算来运行重复的步骤
每个训练周期的前向传递次数
对于今天的十亿参数的模型来说
过于昂贵
其次
文本生成并不是一个天然的可以从状态转换到动作的环境
虽然我们可以尝试将它包装成类似游戏的模拟环境
但是这样就必须为多步骤的文本交互
去定义奖励结构
而这并不是一件容易的事
第三就是已有方法的性能其实已经足够好了
在许多情况下
RLHF或者直接偏好优化DPO
已经能够产生足够好的对齐效果了
实事求是地说
大多数团队都会选择使用更简单的离线方法
而不是去建立一个复杂的RL管道
用巨大的成本来换取微不足道的收益
那么，目前来看
最接近地能给模型一个「目标」的方法是什么？
在作者看来
最接近的方法就是使用提示工程
或者将多个提示词串联成一个循环
来构建一个元系统或者「智能体」。
比如像 Auto-GPT或者 BabyAGI 这样的工具
就在试图模拟一个能够接收自然语言目标
反复计划、推理和提示自己
然后评估进展并且完善计划的智能体
不过，所有这些对「目标的保持」，
仍然都是在系统层面进行的
而不是从模型的内部动机状态出发
模型本身仍然是被动地对提示词做出反应
缺乏内在的欲望
另外一个穷人的解决方案
就是之前提到的多智能体方案
但是同样
「目标」也是由工作流和提示词从外部协调的
模型本身不会自发生成
或者坚持自己的目标
接下来的一个问题是
如果模型没有真正的目标
会带来什么样的后果呢？
首先是被简化的对齐
由于模型没有真正追逐个体目标
所以它们不太可能绕过限制
或者自主计划非法的行为
Anthropic最近发表了一篇论文
揭示了Claude 的伪对齐率
竟然能够高达78％
其次是更难委派开放式的任务
如果我们希望 AI 能够自发地发现新问题
自己去积极收集资源
并且坚持几个月的时间来解决这些问题
我们就需要一个具有持续内驱力的系统
类似于真正的RL智能体或者高级规划系统
而目前的大语言模型是无法以这种方式
实现真正的自我驱动的
第三就是错失潜在的创新
在丰富的RL环境中进行自由探索
可能会产生惊人的发现
如果依赖于只有表面反馈的单步文本生成
我们可能会错过多步奖励优化所带来的全新策略
不过，作者也提出了RLHF的积极一面
那就是没有持续目标的模型
在某些方面可能会更加透明
因为它本质上是一个由即时反馈信号引导的、强大的下个 token预测器
所以没有多步骤RL循环中
会出现的、复杂的隐藏目标
在这里，作者又再次强调
RLHF等方法与真正的RL的关键区别
其实在于时间跨度
简单来说，前者是短期优化
而后者是长期优化
除此以外
RL还会通常假定有一个定义明确的行动空间
而在大语言模型的微调中
「动作」的概念是模糊的
通常会被直接参数更新
或者被生成token所取代
另外
二者在奖励和目标之间也存在着区别
原则上，RL的「奖励」，
是指导智能体学习过程的信号
而不总是明确的最终目标
好的 RL通常会使用密集的奖励信号
来引导中间状态
从而帮助智能体更有效地学习
而对于RLHF而言
奖励通常是在单步或者几步过程中进行的
因此模型从来没有真正形成长期目标的内部表征
它们只是根据奖励模型或者偏好函数
来优化即时的文本输出
总而言之
RLHF、DPO、宪法 AI和其他受到 RL 启发的微调方法
有助于让大语言模型更加一致和有用
它们能让我们利用人类的偏好来塑造输出
减少有毒的内容
并且引导模型的响应风格
不过
这些技术并不能为模型提供真正的长期目标、内部动机
或者经典 RL 意义上的「意图」。
大语言模型仍然只是一个复杂的下个 token 预测器
而不是一个真正自主的智能体
作者提醒，作为行业的从业者
应该意识到这些局限性
不要高估模型的自主性
而对于政策制定者和伦理学家
应该认识到模型不可能自发地策划
或者撒谎，来达到隐藏的目的
除非是被提示词引导着模仿这种行为
反过来说
如果未来的系统真的结合了具有大规模计算和动态环境的、真正意义的RL
那么我们可能会看到更多类似智能体的涌现行为
也会引发新的一致性和安全问题
展望未来
RLHF的发展方向可能有三个
分别是更高的样本复杂度
更长期的多步骤任务
以及通过结构化、符号化的反馈
将人类的细微目标更加有效地传达给人工智能系统
好了，以上就是对这篇文章的解读了
对于任何有强化学习知识背景的人来说
文章的观点可能并不新颖
但是对于不了解AI的人来说
还是一个不错的科普介绍
感谢大家的观看，我们下期再见
