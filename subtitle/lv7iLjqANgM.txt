大家好，这里是最佳拍档
如果要问当下AI领域最热门也最具有争议的话题
那么AGI实现的时间线
绝对能够排进前三
有人说还有几十年，有人说近在眼前
作为AI行业的标杆之一
DeepMind的CEO德米斯·哈萨比斯在CNBC的开年采访中
认为还有五到十年实现AGI
因为AGI还差一两块关键的拼图
其中世界模型正是弥补这些能力缺口的关键
不过
哈萨比斯毕竟是从CEO的战略视角出发
更多是方向上的指引
关于世界模型的具体技术细节、用来训练Agent的核心机制
以及当前真正的瓶颈在哪里
这些关键问题并没有得到充分的解答
巧的是
DeepMind的资深研究员达尼贾尔·哈夫纳
最近参加了BuzzRobot频道的一期播客采访
他不仅是DeepMind的资深科学家
还是Dreamer系列模型的核心作者
而Dreamer正是DeepMind世界模型的重要研究路线之一
更重要的是
哈夫纳不仅深耕前沿理论的研究
还亲手实现了当前前沿的视频模型
因此他的视角既有理论深度
又充满工程实践的务实性
在这期播客中
哈夫曼不仅毫无保留的分享了DeepMind在世界模型方面的很多进展
甚至包括了一些尚未发表的实验结果
对于我们理解世界模型的真实发展水平
无疑是非常珍贵的信息
今天我们就来给大家分享一下
首先，到底什么是世界模型呢？
哈夫曼指出
它的核心思想其实非常的简洁
但是背后的逻辑却颠覆了传统AI的训练模式
简单来说
世界模型就是让AI先学习一个能够精准预测物理世界变化的模型
然后在这个想象出来的虚拟世界里进行大量的训练
而不是让机器人在真实的世界里反复试错
比如，你想让机器人学会走路
传统的方法是让它在真实环境中摔倒一万次
这个过程不仅成本高昂
还存在着安全的风险
而且迭代的速度非常缓慢
但是如果有了世界模型
机器人就可以在想象中摔倒一万次
成本几乎为零
训练效率也能够提升几个数量级
这和传统强化学习的本质区别在于
传统方法是让Agent直接和真实环境交互试错
每一次试错都要付出实实在在的成本
而世界模型的思路是
先让AI学会预测
然后让Agent在这个预测出来的虚拟世界里反复的练习
优化自己的策略
最后再到真实环境中进行少量的验证和微调
这种先虚拟后真实的训练模式
彻底解决了传统强化学习在复杂任务中
成本高、效率低、风险大的痛点
而DeepMind在世界模型方向上
其实布局了三条并行的研究路线
分别是Genie、Veo和Dreamer
它们的定位和侧重点是完全不同的
首先是Genie，它的核心是环境的生成
模型根据文本或者图像的提示
生成多样化的，可以交互的3D环境
然后用户可以在这个虚拟环境中进行导航、探索和简单的交互
最新的Genie 3版本
已经能够实时生成高清的交互世界
主要用来训练具身智能体
第二条路线是Veo
它的核心是高质量的视频生成
但是它不仅仅是生成漂亮的画面
而是要通过视频的生成
展现对物理世界的深度理解
哈夫纳在采访中提到
Genie 3的物理理解能力
正是建立在Veo 3的技术基础之上的
Veo通过学习海量的视频数据中的物理规律
能够精准的预测物体的运动轨迹、相互作用的关系
这种对物理世界的理解
被迁移到了Genie的环境生成中
第三条路线就是哈夫纳主导的Dreamer系列
它的定位和Genie、Veo有着本质区别
Dreamer更侧重于训练Agent
也就是在准确的世界模型中
通过强化学习训练Agent
完成具体的控制任务
哈夫纳在Dreamer 4的论文中明确指出过
Genie 3目前只支持摄像机动作和一个通用的交互按钮
而像《我的世界》这样的复杂游戏
需要完整的鼠标键盘动作空间
Genie显然无法满足
此外，Genie虽然能生成多样化的场景
但是在学习物体交互和游戏机制的精确物理方面
仍然困难
比如它无法准确模拟的方块堆叠、工具使用等需要精细物理计算的场景
而Dreamer的核心优势正是准确的物理预测
它真的学会了《我的世界》中的核心游戏机制
而且能够实现单GPU的实时推理
这对于Agent的高效训练至关的重要
目前
Dreamer系列已经迭代到了第四代
每一代都解决了一个关键问题
前三代Dreamer专注于在线学习
也就是让Agent从头开始
通过和环境的实时交互进行学习
核心追求是数据效率和最终性能
在Dreamer 2之前
行业内存在一个普遍的矛盾
模型基础的强化学习算法
学习速度快、数据效率高
但是性能天花板比较低
学到一定程度就会停滞不前
而模型自由的算法
虽然需要更多的数据
但是性能天花板更加高
而Dreamer 3的突破
就在于解决了这个矛盾
它既实现了模型基础算法的快速和高效
又达到了模型自由算法的高性能
而且不需要手动调整超参数
为了验证这个突破
DeepMind用《我的世界》的钻石挑战进行了测试
让Agent从完全零基础开始
只依靠稀疏的奖励
自主的学会了获取钻石的完整流程
包括挖矿、收集材料、制作工具、探索地形等一系列复杂步骤
被广泛认为是AI领域的一个重要里程碑
因为它证明了
世界模型能够支持Agent完成长期、复杂的目标规划
到了Dreamer 4，研究方向则完全反转
开始专注于离线学习
因为在真实场景的很多情况下
Agent和环境的交互是危险的、昂贵的
或者根本是无法实现的
这时候，离线学习就变得非常重要
为了验证数据的提取能力
Dreamer 4依然选用钻石任务进行测试
但是这次只用人类玩家的游戏数据
而且数据量只有OpenAI的VPT离线Agent的百分之一
结果显示
Dreamer 4依然能够学会获取钻石的完整流程
这证明了世界模型在离线学习场景下的强大潜力
即使没有实时的环境交互
只要有高质量的离线数据
依然能训练出高性能的Agent
不过哈夫纳也坦诚的说
无论是Dreamer 3还是Dreamer 4
都不是完美的解决方案
它们只是在隔离的实验设置中解决了特定的问题
未来的发展方向
必然是将这些技术融合在一起
形成一个既能在线快速适应新环境
又能利用离线数据高效学习的通用系统
在播客中
哈夫纳还提出了一个非常反直觉的判断
那就是几乎任何架构都能够带我们到达AGI
也就是说
无论是现在主流的Transformer
还是近年来热门的Mamba、SSM
甚至是更早的RNN，最终都能实现AGI
它们之间的差别
只是计算效率和当前硬件的适配程度
比如RNN的训练速度会慢一点
但是推理速度更快
可能需要更大的模型规模来弥补架构本身的瓶颈
但是从理论上来说
最终都能够达到相同的目标
所以在哈夫纳看来
当前行业内关于架构方面的争论
更多的是效率层面上的争论
而不是根本性的、方向性的问题
既然架构不是瓶颈
那什么才是决定AGI能否实现的关键呢？
哈夫纳明确的列出了四件事
分别是计算资源、目标函数、数据
以及强化学习的算法细节
比如长期信用的分配
也就是Agent如何将最终的奖励
合理分配到之前的每一个动作上
在此基础上，哈夫纳认为
大语言模型能不能到达AGI这个问题
本身已经过时了
为什么呢？
因为当下部署的前沿AI模型
已经不再是单纯的语言模型了
它们融合了图像理解、图像生成、视频理解的能力
而视频生成的能力
也正在快速的整合进来
所以，讨论纯语言模型的局限
就像讨论汽车能不能够上天一样没有意义
正如汽车本身确实不能飞行
但是如果给汽车加上翅膀
也就是融合其他模态的能力
它就有可能实现飞行
现在的AI模型
正在通过融合多模态能力
不断的突破纯语言模型的局限
而世界模型
正是这种多模态融合的核心载体
那么，当前的AI系统
距离AGI还缺少哪些关键的能力呢？
哈夫纳重点提到了两个方面
第一个是长上下文的理解能力
现在很多大模型都号称支持百万token的上下文
但是对于视频数据来说
这还远远不够
哪怕一段十分钟的的高清视频
它包含的token数量都是一个很大的数字
当前的上下文窗口几乎无法承载
更重要的是
即使有了足够大的上下文窗口
模型真正基于全部上下文进行检索和推理的能力
也还没有到位
它可能只能关注到上下文的局部信息
而无法把握长期、全局的逻辑关联
针对这个问题
可能的解决方向包括几种
一、混合检索模型
将上下文信息和外部检索结合
在学习状态表示的同时进行注意力计算
二、类似Transformer、但是不需要回溯所有历史信息的关联记忆机制
哈夫纳还提到
其实在Transformer出现之前
行业内就有很多关于长期记忆、智能寻址机制的想法
只是当时的计算资源和数据量都不够
这些想法无法落地
而现在
计算和数据的瓶颈已经被大幅缓解
这些被搁置的想法可能会重新焕发生机
第二个关键缺口是
超越人类的推理能力
当前的AI模型
本质上都是在学习人类数据中的模式和规律
它们的推理能力有上限
很难超越训练数据中蕴含的人类推理水平
但是AGI需要的
是能够提出人类从未想过的科学假设
发现人类还没有发现的自然规律
这就要求AI能够自己发现全新的推理方式
这也意味着
AI不能只停留在学习人类的既有知识
而需要从原始的高维数据
比如视频、音频、人类的生活数据、机器人的传感器数据中
自主提取抽象概念
然后在这些概念之上去构建一个全新的推理体系
哈夫纳坦言
他认为人类还没有很好地掌握如何做到这一点
所以这可能是通往AGI之路中
比技术实现更难的一个理论挑战
除了这两个核心的能力缺口
哈夫纳还深入探讨了上下文学习的根本性局限
我们知道
上下文学习是当前大模型的核心能力之一
它能在不更新模型权重的情况下
通过少量示例
在上下文窗口中快速学习新的任务
但是哈夫纳指出
这种学习机制存在一个本质上的缺陷
就是模型只是学会了
以看起来好像学习的方式进行泛化
但是系统里没有任何东西
会让它真正努力的优化任何目标
简单来说
上下文学习更像是一种模仿学习
模型在模仿它训练数据中见过的学习过程
但是它本身并没有一个明确的优化目标
也不会主动的去深化对新知识的理解
那么，怎么样去突破这个局限呢？
哈夫纳提出了几个潜在的方向
第一个是嵌套学习
让模型的一部分
在推理时快速学习上下文的信息
并且将学到的知识保留下来
而不是像现在的GPT系列那样
上下文窗口关闭之后
所有临时学到的信息都会被丢弃
第二个是多学习时间尺度
快的时间尺度用来快速适应新的任务、新的信息
训练效率更高
慢的时间尺度用来深度学习、巩固知识
形成长期记忆
哈夫纳甚至想象了一种通用算法
你可以直接指定要5个学习时间尺度
模型会自动分配不同时间尺度的学习任务
不过他也承认
目前还没有真正有效的算法
但是这是一个非常有潜力的研究方向
还有一种更激进的思路
就是利用大规模的用户交互数据
进行实时更新
比如GPT 4发布后
它和用户交互产生的海量数据
往往需要一到两年的时间
才能整合到下一代模型
如果能够把这个周期缩短到几天
甚至几秒
比如每收集1万个用户的交互数据
就对模型进行一次小批量的更新
让模型真正的实现持续学习
那么上下文学习的局限性
可能会被大幅缓解
但是哈夫纳也指出
这个思路面临了巨大的挑战
首先，大模型的训练成本非常高
频繁更新的经济成本难以承受
其次，在线更新的时候
怎么保持模型的安全性和一致性
避免模型被恶意数据污染
或者出现性能波动
这是一个还没有解决的工程难题
最后
静态模型更容易被研究人员分析、调试
修复其中的漏洞和偏见
而动态更新的模型会变得非常复杂
难以控制
这些关于学习机制的思考
很多都受到了神经科学的启发
哈夫纳在播客中提到了一个很有意思的细节
哈萨比斯曾经在二零一五年的时候认为
构建通用智能是百分之八十的神经科学加百分之二十的工程
而最近这个比例
被更新成了百分之九十的工程加百分之十的神经科学
不过
哈夫纳对这个观点有不同的看法
他觉得
既然我们已经把工程推得这么远了
模型规模、计算资源、数据量都达到了前所未有的水平
那么回过头去
从神经科学中获取直觉的价值
反而会变得更大了
因为人类的大脑经过了亿万年的进化
已经完美的解决了持续学习、多时间尺度记忆、长上下文推理等等问题
所以研究大脑的工作机制
可能会为AI的突破提供关键灵感
在播客中
哈夫纳还分享了一些还没有发表的、关于Scaling Laws的实验结果
其中一个发现非常震撼
那就是视频模型的规模天花板
比文本模型高至少一个数量级
为什么会有这样的差距呢？
核心原因在于视频蕴含的信息量远超文本
文本是人类对世界的抽象描述
它已经过滤掉了大量的物理细节、时空信息
而视频是对物理世界的直接记录
包含了物体的形状、颜色、运动轨迹、相互作用、因果关系等海量信息
哈夫纳直言
即使是当前最顶级的视频模型
基本上也是欠拟合的
也就是说
模型的能力还没有完全的发挥出来
因为它还没有足够的规模和数据
来学习视频中所有的信息
现在的很多视频生成模型
为了生成看起来漂亮的电影片段
往往会出现模式坍塌
也就是生成的内容缺乏多样性
或者不符合物理规律
因为它们的优化目标是视觉效果
而不是理解物理世界
但是如果我们将视频模型的目标
转向真正理解物理世界之后
那么扩展的空间就会变得无比巨大
哈夫纳举例说
在《我的世界》的库存预测任务中
如果模型规模太小
预测结果就会非常不准确
但是如果把模型规模扩大八倍
它就不再需要专门的数据集优化
自然就能精准预测库存的动态
他们还做过完整的YouTube预训练实验
抓取大规模的YouTube视频数据集
过滤掉低质量内容后进行训练
结果发现
模型的泛化能力得到了质的提升
能够处理很多从未见过的场景
这个发现和哈萨比斯的判断形成了完美呼应
哈萨比斯说
世界模型是AGI缺失的拼图
而哈夫纳从工程实践的角度证明
这块拼图的潜力才刚刚被挖掘了一小部分
不过
哈夫纳也没有回避世界模型当前的局限
其中最关键的就是反事实问题
他以Dreamer 4的离线训练为例
当模型只用人类玩家的《我的世界》数据进行训练时
发现人类玩家从不会尝试用错误的材料做镐子
比如没有人会用钻石去做木镐
因为这在游戏中完全没有意义
这就导致世界模型不知道这些错误的配方是不存在的
当强化学习Agent在虚拟世界中训练时
就会利用这个漏洞
它会假装用钻石做木镐
而世界模型因为没有见过这种情况
就会错误的奖励它
认为它成功做出了镐子
尽管这个配方在真实游戏中根本不存在
怎么样解决这个问题呢？
哈夫纳团队发现
只需要两到三轮真实环境交互的校正数据
这个问题就会消失
具体来说
就是让Agent在真实的《我的世界》环境中
尝试这些错误的配方
发现无法成功后
将这个反馈传递给世界模型
世界模型就会修正自己的预测
这里面存在一个非常重要的动态
强化学习Agent会主动寻找世界模型中所有的潜在漏洞
然后通过真实环境的反馈来修正这些漏洞
从而形成一种对抗博弈
在这个过程中
世界模型会变得越来越稳健
Agent的策略也会越来越强
这也印证了一个核心观点
纯离线数据在真实世界中不可能做到完美
因为它无法覆盖所有的反事实场景
只有让模型和真实环境进行交互
才能学到真正的因果关系
而不是表面的统计关联
除了架构和数据
哈夫纳还强调了一个被严重低估的改进方向
那就是目标函数
他将目标函数分为两类
每一类都有巨大的优化空间
第一类是偏好型目标函数
这类目标函数由人类的偏好决定
没有明确的数学公式可以描述
必须从人类的反馈中学习
比如我们希望AI生成的内容符合人类的价值观
或者希望机器人的动作足够的自然
这些都属于偏好型目标
需要通过大量的人类反馈来定义和优化
第二类是信息型目标函数
这类目标函数的核心是让模型理解数据本身的规律
比如预测视频的下一帧、重构输入的图像、探索未知的环境等等
目的是让模型掌握数据中的因果关系、物理规律等核心信息
哈夫纳认为
这两类目标函数都有很大的改进空间
对于文本模型来说
当前主流的目标函数是预测下一个token
这个目标函数确实能够让模型学到语言的语法和语义
但是还有很多可以优化的地方
比如让模型同时预测多个token
而不是只预测下一个
这样可以让模型更有远见
更好的理解长文本的逻辑结构
对于多模态模型来说
当前的目标函数更像是各种损失函数的缝合怪
不仅用的各不相同
还要花费大量的精力去平衡这些损失函数的权重
哈夫纳推测
可能会存在一种统一的目标函数
能够将所有模态的学习任务整合起来
这不仅能够让我们的研究更加简单
最终也能获得更好的性能
虽然不同的损失函数对不同的模态
有各自的优势
但是这并不是根本性的权衡
只要能够找到一个更高层次的抽象
这些优势就可以跨模态共享
而对于Agent训练来说
目标函数的缺失更为明显
当前的短期强化学习任务已经比较成熟
目标函数的设计相对简单
但是对于端到端的长程任务
比如《我的世界》的钻石挑战
需要上万步的连续动作
现有的目标函数还无法胜任
误差会在每个时间步累积
导致最终策略的失效
此外，怎样让Agent主动探索未知环境
怎样让Agent精准的达成长期目标
怎样设计一个不依赖特定场景的奖励函数
这些都是当前急需解决的问题
哈夫纳直言
现在唯一缺的基本上就是目标函数
你可以说我们没有数据
但是说实话数据就在那儿
人工收集也不难
真正缺的是如何构建这样的系统的想法
大家现在又回到了搞算法的阶段
在讨论了目标函数之后
话题自然涉及到了预训练和强化学习的分工问题
哈夫纳对此有着非常清晰的界定
他认为
预训练的核心是从样本中学习知识
效率极高，适合大规模的吸收信息
而强化学习的核心是从奖励中学习策略
适合优化具体任务的表现
为什么强化学习不适合学习知识呢？
因为用奖励学习知识的过程非常低效
你必须先让模型猜测一个知识点
然后通过奖励告诉它猜得对不对
这个过程就像瞎猫碰死耗子
远不如直接从样本中吸收信息来得直接
但是强化学习在优化策略上有着不可替代的优势
因为获取最优的控制数据
几乎是不可能的
人类的行为数据往往不是最优的
即使你聘请专业人员收集数据
也可能要扔掉百分之九十九的无效数据
而且要想获得最优的数据
还依赖于任务的时间跨度
理想情况下
我们需要的是长期时间跨度下的最优策略
这在现实中根本无法通过数据收集来实现
而强化学习的价值正在于
它不需要最优的数据
只需要让模型在虚拟世界中反复的试错
就能自主找到更好的策略
这和人类的学习逻辑非常的相似
我们通过观察来学习知识
通过试错来学习技能
当然，观察也能学到一些粗略的技能
因为我们在预测别人的行为时
会用到和自己相似的心理表征
所以能泛化到想象自己做这些事
但是要想掌握精准的技能
必须亲自试错
AI的学习过程
正在以一种更高效的方式复刻这个逻辑
预训练对应观察学习
强化学习对应试错学习
而世界模型则为这两种学习提供了一个统一的、高效的平台
那么
世界模型对机器人领域会带来怎样的影响呢？
哈夫纳认为，这种影响会分为两波
每一波都会彻底的改变机器人的发展模式
第一波冲击是表征优化
从视频预测模型中学习到的表征
对物理世界的理解深度
会远超当前的视觉模型
对于机器人控制来说
最关键的信息是什么呢？
是物体的精确位置、物理属性
比如这个盘子有多滑呢？
这个杯子要握多紧
才不会洒出里面的液体呢？
如果我从把手处拿起这个杯子
需要施加多大的力才能避免它滑落呢？
这些信息都是视觉模型无法提供的
因为视觉模型的核心是理解语言和图像的关联
而不是物理世界的规律
但是这些信息恰恰是视频预测模型的副产品
视频预测模型为了精准预测下一帧
必须学会把握这些物理细节
而这些细节正是机器人控制所必需的
传统的机器人策略训练
需要大量真实世界的数据
而且得到的策略往往非常脆弱
只能在特定的场景下工作
比如在实验室里训练的扫地机器人
到了真实的家庭环境中
可能因为家具摆放的微小变化
就无法正常工作
而用视觉模型的表征进行模仿学习
虽然效果会好一些
但是它的表征毕竟不是为物理层面的世界理解设计的
哈夫纳团队的实验显示
用视频预测模型的表征来做模仿学习
机器人的控制精度和泛化能力
都得到了非常大的提升
第二波冲击是虚拟训练
当世界模型经过了足够多样的预训练
再加上少量机器人的真实数据微调后
就能够模拟机器人在任意场景中的表现
哈夫纳在播客中形象的描述
你可以在数据中心里
让机器人在一百万个厨房里做一百万种餐食
全部并行的训练
而不用真的租一百万间房屋、造一百万台机器人、把它们运到城市各处
这意味着机器人的训练成本会大幅降低
训练效率会呈指数级的提升
目前
大规模实现这件事还面临一些挑战
比如如何让世界模型精准模拟不同场景的物理差异
如何将虚拟训练的策略
无缝迁移到真实环境
但是哈夫纳认为
这是机器人领域的第二个跨越式变革
而Dreamer 4的论文已经展示了完整的技术路线
通过添加Agent token训练行为克隆策略
然后训练奖励模型
最后进行强化学习微调
关于机器人的发展时间表
哈夫纳给出了一个非常具体的预估
认为机器人领域可能在三到五年内
实现实用型通用机器人产品的第一个版本
而更复杂的长期推理能力
可能需要五到十年才能完全攻克
但是实用型通用机器人并不需要等到长期推理问题解决
比如一款能完成家庭清洁、做饭等基础任务的机器人
不需要具备复杂的科学推理能力
只要掌握精准的物理控制和简单的场景适应能力即可
这个判断和哈萨比斯的观点不谋而合
哈萨比斯在CNBC的采访中也提到
二零二六年机器人领域会有非常有趣的进展
DeepMind正在用Gemini Robotics做一些雄心勃勃的项目
CNBC的主持人对此表示怀疑
因为很多所谓的智能机器人
其实还只是提线木偶
就是由控制室的人类远程操控
比如Tesla的Optimus机器人
就曾经被外界质疑存在着远程操控的可能
但是也正因为如此
世界模型才显得非常的重要
机器人要实现真正的自主运作
必须具备理解物理世界的能力
而世界模型正是实现这个目标的核心技术
最后，哈夫纳还对大语言的幻觉问题
给出了一个非常有趣的解释
这个解释和世界模型的核心逻辑是一脉相承的
他认为，Agent在训练的过程中
会收敛到一个特定的分布
在这个分布里面
它能合理的达成目标
也能合理的预测环境的变化
因为模型在这个分布上训练的数据最多
分配的模型容量也最多
所以在这个范围内
它的表现会非常稳定，很少会出错
但是与此同时
模型会逐渐遗忘分布之外的信息
另一种扩大模型能力的方式
是增大模型的规模、用更多的数据进行训练
从而扩大这个有效分布的范围
但是无论如何
分布的边缘地带会永远存在
在这些区域
模型没有足够的训练数据
也没有足够的模型容量
所以会出现泛化失败、产生幻觉的情况
哈夫纳认为
这就是我们现在在大语言模型上看到的现象
它们在大部分分布内的东西上都相当通用、相当好
但是在边缘地带就会产生错误泛化和幻觉
而解决这个问题的关键
在于引入在线强化学习的反馈机制
如果模型产生了幻觉
用户表示不满意
就会给模型一个负奖励
模型要么学会正确的答案
要么学会在不确定的时候说
我不知道
最终让模型的有效分布变得越来越稳固
幻觉现象自然会减少
总的来说，通过哈夫曼的这次访谈
我们可以看到，世界模型的出现
不仅为AGI指明了新的方向
也正在重塑机器人、视频生成、强化学习等多个细分领域的发展逻辑
相信随着世界模型的不断成熟
以及各个技术方向的深度融合
我们会离AGI的梦想更进一步
感谢收看本期视频，我们下期再见
