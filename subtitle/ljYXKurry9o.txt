大家好，这里是最佳拍档，我是大飞
近几年
大语言模型带来了我们很多震撼
但是可能很少有人会告诉你
这些看似强大的模型背后
隐藏着一个致命的先天缺陷
那就是它们本质上是静态的
就像得了一种特殊的记忆疾病
训练结束后就很难再学习新的知识
只能依赖有限的上下文窗口进行即时适应
一旦超出了这个范围
就会忘记之前的信息
更关键的是
深度学习几十年来依赖的层叠结构
其实在解决复杂算法的实现和持续学习等问题上
并没有我们想象中那么有效
今天
我们要聊的这篇来自谷歌研究院的重磅论文
《嵌套学习：
深度学习架构的幻象》（Nested Learning:
The Illusion of Deep Learning Architectures）
或许为这些问题提供了颠覆性的解决方案
论文的四位作者
提出了一种全新的学习范式
嵌套学习（NL）
不仅从数学上重构了深度学习的底层逻辑
还以此设计出了一个名为HOPE的模型
在语言建模和常识推理任务中
超越了Transformer、RetNet等主流架构
今天我们就来解读一下这篇论文
在深入嵌套学习之前
我们必须先搞清楚
当前深度学习的核心痛点到底是什么？
为什么层叠结构不是万能的？
几十年来
深度学习的发展路径似乎很明确
堆叠更多的层、扩大模型的参数规模
以及设计更复杂的目标函数
从CNN到Transformer
从百万参数到万亿参数
这条路径确实带来了巨大成功
但是作者们在论文中指出
这种层叠思维存在四个无法回避的局限
首先
模型的计算深度并不会随着层数增加而无限提升
很多深层模型的算法表达能力
其实和浅层模型相差无几
其次，部分参数的容量提升
会随着深度或宽度的增加而边际递减
继续扩大规模只是无效堆叠
第三，训练过程容易陷入次优解
这很大程度上是因为优化器及其超参数的选择不够合理
最后，也是最关键的一点
模型的快速适应能力、持续学习能力和分布外泛化能力
几乎不会因为堆叠更多的层而得到提升
这也是为什么模型训练结束后
就无法再高效学习新知识的核心原因
更形象地说，当前模型的记忆系统
类似于一种顺行性遗忘症（anterograde amnesia）
这是一种神经系统疾病
患者在发病后无法形成新的长期记忆
但是发病前的记忆依然完好
大语言模型的发病时间点
就是预训练结束的时刻
预训练阶段学到的知识被存储在MLP层中
相当于发病前的长期记忆；
而推理时的上下文信息
只能暂时存在注意力机制中
相当于即时的短期记忆
一旦上下文窗口超出限制
或者遇到预训练中没有见过的新知识
模型就无法将其转化为长期记忆
只能视而不见
那么
人类为什么能避免这种记忆缺陷
实现高效的持续学习呢？
这正是论文灵感的来源
神经科学中的人脑记忆机制
论文指出
人脑的持续学习能力源于神经可塑性（neuroplasticity）
也就是大脑能够根据新的经验、学习和损伤进行自我调整的能力
而记忆的巩固过程
主要分为两个互补的阶段
第一阶段是在线巩固（synaptic consolidation）
发生在学习后立即或者不久的时间内
即使在清醒状态下也会进行
这个阶段会将新的、脆弱的记忆痕迹稳定下来
并且开始从短期记忆向长期记忆转移
第二阶段是离线巩固（systems consolidation）
主要发生在睡眠中
通过海马体的尖波涟漪（SWRs）与大脑皮层的睡眠纺锤波和慢波协调
重复回放最近编码的记忆模式
强化并且重组记忆
最终将它转移到大脑皮层中进行长期存储
对于大语言模型来说
它们既没有在线巩固机制
也没有离线巩固机制
因此，论文的核心思路是
借鉴人脑的多时间尺度记忆巩固机制
来构建一种嵌套式的学习框架
让模型的每个组件都能在不同的时间尺度上更新
形成多层次的记忆系统
从而突破传统深度学习的局限
接下来，我们进入论文的核心部分
嵌套学习（NL）到底是什么？
论文给出的定义是
嵌套学习是一种将机器学习模型及其训练过程
表示为一组嵌套的、多层次的、并行的优化问题的范式
每个优化问题都有自己的上下文流（context flow）
简单来说，传统深度学习是扁平的
所有参数基本在同一个时间尺度上更新
而嵌套学习是立体的
模型的不同组件有不同的更新频率
形成层级结构
就像人脑中有不同频率的脑波
对应不同的信息处理速度
要理解嵌套学习
首先要明确两个核心概念
关联记忆（Associative Memory）和学习的区别
以及更新频率（Update Frequency）
首先是关联记忆与学习的定义
在神经心理学中
记忆和学习是两个不同的概念
记忆是输入引起的神经更新
而学习是获取有效且有用记忆的过程
论文将这个定义迁移到机器学习中
认为所有机器学习模型的组件
无论是神经网络本身，还是优化器
本质上都是关联记忆系统
它们的核心作用是压缩自身的上下文流
这里的关联记忆
被定义为一个将一组键（key）映射到一组值（value）的算子
比如，对于一个简单的1层MLP
训练过程就是让它学习将输入数据映射到局部意外信号（Local Surprise Signal
LSS）
也就是当前输出与目标函数要求的结构之间的不匹配的过程
为了让大家更直观的理解
论文举了一个非常经典的例子
用梯度下降训练1层的MLP
这个训练过程可以被重新表述为一个优化问题
MLP的权重W
本质上是一个关联记忆算子
目标是最小化输入数据x与局部意外信号u的点积
再加上权重的正则项
这里的局部意外信号u
就是输出y对目标函数L的梯度
也就是说，传统的梯度下降训练
本质上是让模型学习输入数据到局部意外信号的映射
而这个映射过程
就是记忆的压缩过程
如果把梯度下降换成带动量的梯度下降（SGD with Momentum）
情况就会变得更加复杂
也更能体现嵌套的含义
带动量的更新规则中，除了权重W
还有一个动量项m
论文指出
这个动量项m本身也是一个关联记忆模块
它要么是一个无键关联记忆
负责压缩梯度信息
要么是一个有键关联记忆
学习将数据点映射到对应的局部意外信号
此时
整个训练过程就变成了一个两层的嵌套优化问题
内层优化动量项m
压缩梯度信息，外层利用m更新权重W
这种结构
与快速权重程序（Fast Weight Programs
FWPs）的思路不谋而合
动量项是快速更新的权重
而MLP的权重W是慢速更新的权重
另一个例子是线性注意力（linear attention）的训练
线性注意力的核心是通过M_t = M_{t-1} + v_t k_t^⊤ 来更新记忆矩阵M
再通过y_t = M_t q_t 生成输出
论文发现，这个记忆矩阵M的更新过程
同样可以表述为一个关联记忆的优化问题
M的目标是最小化输入键k和值v的点积
加上与前一时刻M_{t-1}的正则项
这与梯度下降的更新规则完全等价
而线性注意力中的投影层（W_k、W_v、W_q）
它的训练过程本身也是一个关联记忆的优化问题
因此
线性注意力的训练是一个两层嵌套优化
外层优化投影层，进行慢速更新
而内层优化记忆矩阵M，进行快速更新
从这两个例子可以看出
传统的深度学习模型
其实都可以被拆解为多层次的嵌套优化问题
只是我们之前没有从这个角度去理解
而嵌套学习的核心贡献之一
就是将这种隐性的嵌套结构显性化
并且提出了更新频率这个关键指标
来定义不同组件的层级关系
论文对更新频率的定义是
对于模型的任何组件
它的频率f_A是单位时间内的更新次数
基于更新频率
我们可以对模型组件进行排序
如果组件A的更新频率高于组件B（f_A > f_B）
或者A和B频率相同、但是B的计算依赖A的状态
那么A比B更快（记为A ≻ B）
如果频率相同而且计算独立
则记为A ≡ B
基于这个排序规则
模型的所有组件会被划分为不同的层级
同一层级的组件更新频率相同
层级越高，更新频率越低
每个层级的组件都有自己的优化问题和上下文流
从而形成独立的梯度流
这意味着
嵌套学习允许模型在不同的抽象层次和时间尺度上处理数据
快速更新的组件处理即时的、细节的信息
慢速更新的组件处理长期的、抽象的信息
就像人脑的不同脑区处理不同时间尺度的信息一样
理解了嵌套学习的核心框架后
我们来看论文的三大核心贡献
它们都是基于嵌套学习的视角
对传统深度学习的组件进行重构和升级
第一个贡献是深度优化器（Deep Optimizers）
论文指出
我们常用的基于梯度的优化器
本质上都是关联记忆模块
核心作用是压缩梯度信息
基于这个洞察
作者们提出了四种更具表达力的优化器扩展方向
第一种是更具表达力的关联（More Expressive Association）
传统的动量法是无值关联记忆
只将梯度映射到单一的值
表达力有限
论文提出
给动量法增加一个值参数P_i
让动量项学习将梯度与P_i的映射关系
相当于对动量GD进行预条件化（preconditioning）
这种优化器的更新规则是这样的
其中P_i可以是梯度的函数
比如海森矩阵的信息
让映射关系更有意义
第二种是更具表达力的目标函数（More Expressive Objectives）
传统优化器的内部目标函数
多是点积相似度
容易导致记忆效率低下
论文提出用L2回归损失来替代点积相似度
最小化损失函数
这种更新规则基于delta规则
能让动量项更好地管理有限的容量
更高效地记忆过去的梯度序列
第三种是更具表达力的记忆（More Expressive Memory）
传统动量项是线性的矩阵值记忆
只能学习梯度的线性函数
论文提出用MLP来替代线性矩阵
作为动量项的记忆模块
形成深度动量梯度下降（Deep Momentum Gradient Descent
DMGD）
让动量项能够捕捉梯度的非线性动态特征
提升学习能力
第四种是非线性输出（Non-Linear Outputs）
在动量项的输出上增加非线性激活函数
比如牛顿-舒尔茨迭代（Newton-Schulz）
这种优化器与Muon优化器等价
能够进一步提升优化器的表达力
除此之外
论文还对传统的反向传播过程进行了扩展
传统反向传播中
权重更新规则本质上是忽略了数据样本之间的依赖关系
论文提出用L2回归目标来替代点积目标
得到新的权重更新规则
这种优化器能考虑数据样本之间的依赖
尤其适合在token空间中使用
因为token之间并不是独立的
第二个贡献是自修改Titans（Self-Modifying Titans）
Titans是作者们之前提出的一种序列模型
基于嵌套学习的洞察
他们对它进行了升级
让模型学会修改自身
也就是学习自己的更新算法
具体来说
自修改Titans将优化器的更新规则本身作为学习对象
通过嵌套的优化问题
让模型在训练过程中不断调整更新策略
从而适应不同的任务和数据分布
这种自修改能力
让模型摆脱了固定优化器的束缚
能够根据实际情况动态调整学习方式
这也是持续学习所必需的核心能力
第三个贡献是连续记忆系统（Continuum Memory System
CMS）
传统的记忆系统被划分为短期记忆和长期记忆
这种二分法过于简单
无法适应多时间尺度的记忆需求
论文提出的连续记忆系统
是一组MLP块的链
每个MLP块都对应一个特定的更新频率
每个MLP块的参数θ
会每隔C^(ℓ)步更新一次
而更新频率越高的MLP块儿
更新的间隔越短
连续记忆系统的输出的计算方式和参数更新规则是这样的
这里的f点是优化器的误差项
值得注意的是
传统的Transformer块
其实是连续记忆系统的一个特殊情况
当k=1时，只有一个MLP块
所有参数都在同一个时间尺度上更新
而连续记忆系统的核心优势在于
每个MLP块都负责压缩自己对应的上下文流
形成不同抽象层次的知识表示
高频更新的MLP块存储短期的、具体的信息
低频更新的MLP块存储长期的、抽象的信息
从而实现从短期记忆到长期记忆的连续过渡
完美复刻了人脑的多时间尺度记忆巩固机制
有了深度优化器、自修改Titans和连续记忆系统这三大组件
论文最终构建出了HOPE模型
一个具有自指学习能力和连续记忆的嵌套学习模块
HOPE的架构核心
是将自修改Titans与连续记忆系统融合
连续记忆系统提供多层次的记忆存储
自修改Titans提供动态的更新算法学习
再结合论文提出的改进型梯度下降优化器
形成一个端到端的学习框架
从结构上看
HOPE与Transformer有着明显的区别
Transformer只有单一频率的FFN层
而HOPE有高、中、低不同频率的FFN层
对应不同的记忆更新节奏
同时，HOPE的自修改Titans模块
能够动态调整键、值、查询的投影方式
而Transformer的投影层是固定的
为了验证HOPE的性能
论文在语言建模和常识推理两大类任务上进行了实验
对比的基线模型包括Transformer++、RetNet、DeltaNet、TTT、Samba、Titans（LMM）等主流架构
实验中
HOPE的参数规模分为340M、760M和1.3B三个等级
训练数据量分别为30B和100B tokens
我们先看语言建模任务的结果
在Wiki数据集上
1.3B参数的HOPE困惑度（ppl）达到了15.11
而同样参数规模的Titans（LMM）是15.60
Transformer++是18.53
RetNet是19.08；
在LMB数据集上
HOPE的困惑度为11.63
准确率为50.01%，
而Titans（LMM）的困惑度是11.41
准确率是49.14%，
虽然困惑度略高于Titans
但是准确率更高，综合表现更优
再看常识推理任务
这包括物理常识推理PIQA
日常场景推理HellaSwag
代词指代推理Winograd
科学常识推理ARC-e/ARC-c、社交常识推理SIQA
以及自然语言是非问答BoolQ等8个任务
在1.3B参数规模下
HOPE在PIQA上的准确率达到73.29%，
超过Titans（LMM）的73.09%；
在HellaSwag上准确率为56.84%，
高于Titans的56.31%；
在Winograd上准确率60.19%，
超过Titans的59.81%；
在ARC-c上准确率41.24%，
高于Titans的40.82%；
在BoolQ上准确率61.46%，
高于Titans的60.97%。
最终，HOPE的平均准确率达到57.23%，
超过了所有的基线模型
包括混合架构Samba（54.00%）和Titans（LMM）（56.82%）
从实验结果可以看出
无论是语言建模还是常识推理
HOPE在不同参数规模下都表现出了优异的性能
尤其是在1.3B参数规模下
全面超越了传统Transformer和现代循环神经网络
这充分证明了嵌套学习范式的有效性
通过多时间尺度的记忆更新和自修改的学习算法
模型能更好地捕捉数据中的依赖关系
提升表达力和泛化能力
那么，嵌套学习的提出
到底给深度学习领域带来了哪些深远的影响呢？
首先
它打破了堆叠层数=模型能力的固有认知
为深度学习开辟了一个新的维度
也就是层级深度（level depth）
传统深度学习的深度是指网络的层数
而嵌套学习的深度是指优化问题的嵌套层级
这意味着，未来提升模型的能力
不一定需要堆叠更多的层
也可以通过增加嵌套层级、设计更丰富的多时间尺度更新机制来实现
其次
它为解决持续学习、长上下文推理等传统难题
提供了全新的思路
通过连续记忆系统
模型能够实现从短期记忆到长期记忆的平滑过渡
避免了灾难性遗忘
而多时间尺度的更新机制
让模型能同时处理即时上下文和长期知识
提升长上下文推理能力
这对于大语言模型的实用化至关重要
未来的模型可能不需要依赖越来越大的上下文窗口
就能高效处理长文本
并且持续学习新的知识
第三
它将神经科学与深度学习的结合推向了新的高度
嵌套学习的核心灵感
来自于人脑的记忆巩固机制和神经可塑性
而论文通过严格的数学推导
将这种神经科学洞察转化为可实现的机器学习框架
实现了从脑到AI的有效迁移
这种跨学科的研究思路
可能会引发更多神经科学与AI的交叉研究
推动AI向更接近人脑的方向发展
当然，嵌套学习也存在一些局限性
论文里提到
当前的嵌套学习框架主要关注的是在线巩固阶段
对离线巩固的研究还不够深入
同时
嵌套学习的计算复杂度相对较高
如何在保持性能的同时提升计算效率
是未来需要解决的问题
此外
嵌套学习的理论分析还需要进一步的完善
比如不同嵌套层级对模型性能的影响、更新频率的自适应调整等问题
这些都需要更加深入的研究
那么大家对于嵌套学习的理论有什么看法
欢迎在评论区留言
感谢收看本期视频，我们下期再见
