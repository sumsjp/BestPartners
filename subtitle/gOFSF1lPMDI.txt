大家好，这里是最佳拍档，我是大飞
最近AlexNet迎来了一个具有里程碑意义的时刻
那就是它的源代码正式开源了
而且这次公开的
是2012年杰弗里·辛顿Geoffrey Hinton团队亲手编写的原版代码
而且还保留了当年详细的注释
今天我们来简单聊聊这件事情的来龙去脉
我们都知道
深度学习如今已经广泛应用在了各个领域
从图像识别到自然语言处理
从自动驾驶到医疗诊断
它的影响力无处不在
但是深度学习的发展也并非是一帆风顺
1950年代末
研究人员就怀揣着探索人工智能的梦想
开始尝试构建简单的神经网络
他们试图模仿人类大脑的神经元结构
让计算机能够像人类一样进行思考和学习
然而
当时的计算机性能远远无法满足神经网络复杂的计算需求
数据量也十分有限
这使得早期的神经网络模型在实际应用中效果不佳
无法取得突破性的进展
到了1970年代
神经网络甚至一度被AI研究者们“打入冷宫”。
由于在当时表现出的局限性
许多研究者对神经网络的发展前景感到迷茫
将研究重心转向了其他机器学习算法
神经网络仿佛被遗忘在了历史的角落
无人问津
直到1980年代，转机出现了
杰弗里·辛顿和他的同事大卫·鲁梅尔哈特David Rumelhart、罗纳德·威廉姆斯Ronald Williams
重新发现了反向传播算法
这个算法就像是一把钥匙
为神经网络的发展打开了新的大门
反向传播算法能够通过调整神经网络中每一层的权重
使得模型能够逐步优化自己的表现
从而提高了预测的准确性
逐渐地
反向传播成为了深度学习的核心算法之一
让神经网络的研究重新焕发了生机
尽管如此
神经网络在很长一段时间内
仍然无法超越其他的机器学习算法
它还需要等待关键因素的出现
才能实现质的飞跃
好在很快
其中一个关键因素就出现了
那就是大数据
随着互联网的快速发展
数据量呈爆炸式的增长
大量的图像、文本、音频等数据被产生和存储
为机器学习提供了丰富的“燃料”。
而另一个关键因素则是GPU计算能力的飞速提升
其中英伟达发挥了重要作用
从2000年代开始
英伟达就开始致力于将GPU泛用化
不仅仅是用于图形处理的任务中
2007年，英伟达发布了CUDA
这个系统让GPU能够被广泛地应用于科学计算和机器学习领域
伴随着GPU强大的并行计算能力
也大大加速了神经网络的训练过程
使得大规模神经网络的训练成为可能
就在大数据和GPU计算能力逐渐成熟的时候
ImageNet数据集的出现又为神经网络的发展提供了绝佳的契机
ImageNet是由斯坦福大学教授李飞飞发起的一个项目
它的目标是构建一个涵盖英语所有名词的大规模图像数据集
并且利用WordNet词汇数据库进行分类
这项任务的工作量巨大
为了完成标注工作
李飞飞团队最终通过亚马逊的Mechanical Turk平台
将标注任务众包给了零工工人
经过不懈的努力，到了2009年
ImageNet数据集正式建成
它的规模远超以往的任何图像数据集
包含了数以百万计的图像
为计算机视觉研究提供了丰富的数据资源
随后
为了推动计算机视觉研究的发展
李飞飞发起了ImageNet竞赛
吸引了众多研究团队参与其中
2011年
伊利亚·苏茨克维尔 Ilya Sutskever 看到了卷积神经网络在图像识别领域的潜力
他说服了擅长GPU编程的亚历克斯·克里热夫斯基Alex Krizhevsky
开始一起尝试用卷积神经网络来完成ImageNet的挑战
值得一提的是
之前亚历克斯·克里热夫斯基已经为CIFAR-10数据集
编写了一个名为cuda-convnet的CUDA代码
并将它扩展成了支持多GPU的版本
积累了丰富的经验
于是在这个基础上
他开始了AlexNet的开发
令人惊讶的是
亚历克斯·克里热夫斯基竟然只是在自己的卧室里
利用两台英伟达显卡就完成了模型训练
在随后一年的时间里
他不断调整和优化模型
经历了无数次的试验和失败
终于在2012年的ImageNet大规模视觉识别挑战赛ILSVRC上
AlexNet一鸣惊人
当时，ImageNet竞赛已经举办了两年
许多业界顶级的研究团队都在努力提升计算机识别图片的能力
但是之前最好的方法
在Top-5错误率上也只能勉强降到26.2%。
而AlexNet一出现
就彻底打破了这一局面
它是一个由5层卷积层和3层全连接层组成的深度神经网络
总共有6000万个参数、65万个神经元
并且采用了GPU来加速训练
凭借这些先进的设计
AlexNet直接把Top-5错误率降低到了15.3%，
远远甩开第二名的26.2%。
这个突破可以说是具有划时代的意义
它不仅证明了深度学习在图像识别任务中的巨大潜力
也彻底改变了计算机视觉领域的研究方向
引发了AI领域的深度学习浪潮
随后
亚历克斯·克里热夫斯基、伊利亚·苏茨克维尔和杰弗里·辛顿发表了论文《使用深度卷积神经网络进行ImageNet分类》，
详细介绍了他们所使用的方法
这篇论文后来也成为了深度学习领域的经典之作
根据Google学术的数据显示
AlexNet相关论文的被引次数
已经超过了17万
它不仅为后来的研究者提供了宝贵的经验和思路
也推动了深度学习领域的快速发展
AlexNet的成功
不仅在学术界引起了轰动
也吸引了科技巨头的关注
2013年
杰弗里·辛顿带着亚历克斯·克里热夫斯基和伊利亚·苏茨克维尔创立了一家名为DNNresearch的公司
专门研究和开发深度学习技术
凭借着在AlexNet上的卓越成果
这家公司很快就被谷歌收购
三人也随即加入Google Brain
继续在深度学习领域深耕
推动技术的进一步发展
而AlexNet的代码
也从此成为了谷歌的私有资产
随着时间的推移
许多研究者和AI历史爱好者
都希望能够看到AlexNet的原始代码
了解它的设计思路和实现细节
2017年
计算机历史博物馆的软件历史中心策展人许汉森（音译）Hansen Hsu
开始试图联系亚历克斯·克里热夫斯基
希望能将AlexNet的代码开源
然而，由于DNNresearch早已归属谷歌
所以AlexNet代码的开源许可问题变得十分复杂
许汉森经过多方联系
最终找到了当时还在谷歌工作的杰弗里·辛顿
杰弗里·辛顿对开源代码的想法表示支持
并且直接把许汉森介绍给了谷歌的相关负责人大卫·比伯
这场关于AlexNet代码开源的谈判和准备工作
一共持续了五年
期间
计算机历史博物馆与谷歌共同筛选了AlexNet的多个版本
经过仔细的审查和评估
最终确认并且发布了2012版的原始代码
这些代码被上传到了计算机历史博物馆的官方GitHub仓库
在此之前
虽然GitHub上已有多个AlexNet的相关代码库
但是大多都是基于论文重现的版本
并非是原始的代码
而这次开源的代码
采用了早期的CUDA和C++代码
并且带有大量的开发者注释
完整保留了当年团队的开发思路
就连HuggingFace 的联合创始人托马斯·沃尔夫 Thomas Wolf 也说道
也许真正的历史
正是 AlexNet 代码中每个实验配置文件末尾的注释记录
我们可以从中看到
一个开创性的神经网络正在诞生
应该说
AlexNet代码的开源具有十分独特的意义
对于研究者来说
它提供了一个深入了解深度学习早期发展的窗口
能够帮助他们更好地理解现代深度学习的起点
为未来的研究提供宝贵的参考
对于开发者来说
可以从中学到早期深度学习模型的设计和优化技巧
借鉴当年的开发经验
应用到自己的项目中
而对于AI历史爱好者来说
他们可以通过这些代码
亲自体验当年AlexNet是如何训练和推理的
感受深度学习发展历程中的重要时刻
建议对AI研究和开发感兴趣的朋友
都可以亲自去阅读一下源代码
有机会大飞也会做一期视频来梳理和讲解一下
一起来学习一下这份珍贵的资料
好了，本期视频内容就到这里
感谢大家的观看，我们下期再见
