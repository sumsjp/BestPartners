大家好，这里是最佳拍档，我是大飞
这两天呢
北京进入了史无前例的梅雨季节
从昨天开始就下雨然后
预计这几天还都有雨
所以哪也去不了啊
正好在家里给大家录视频吧
今天我们来聊聊具身智能方面的最新进展
我们也都清楚
在掌握了网络中的语言和图像之后
大模型终究要走进现实世界
而「具身智能」应该是下一步发展的方向
上周五，谷歌DeepMind就宣布推出了
全球第一个控制机器人的视觉-语言-动作VLA模型
RT-2
现在不再需要使用复杂的指令
机器人也能直接像ChatGPT一样操纵了
RT-2究竟到达了怎样的智能化程度？
我们先来看一段演示视频
为了方便大家观看
我在原视频2倍速的基础上
又增加了2倍速
在视频中
机器臂可以把香蕉放到数字3上
把香蕉放到德国国旗上
甚至可以把可乐罐递给泰勒・斯威夫特的照片
而在这之前
机器人并没有学习过相关的知识和概念
谷歌DeepMind机器人技术主管文森特·范霍克表示
RT-2是机器人制造和编程方式的重大飞跃
由于这一变化
他们不得不重新考虑整个研究规划
之前所做的很多事情都完全变成无用功了
那么究竟什么是RT-2
DeepMind又是如何实现的呢？
DeepMind这个RT-2
拆开了读就是Robotic Transformer
也就是机器人的transformer模型
我们都知道
要想让机器人能像科幻电影里一样听懂人话
展现生存能力，并不是件容易的事
相对于虚拟环境
真实的物理世界复杂而且无序
机器人通常需要复杂的指令引导
才能为人类做一些简单的事情
相反，人类本能地知道该怎么做
在以前
训练机器人都需要很长的时间
研究人员必须为不同任务单独建立解决方案
但是借助RT-2
机器人可以自己分析更多信息
自行推断下一步该做什么
RT-2不仅建立在视觉-语言模型VLM的基础上
还创造了一种更新的概念
视觉-语言-动作（VLA）模型
它可以将从网络和机器人中学到的知识
并将这些知识转化为机器人可以控制的通用指令
这个模型甚至能够使用思维链提示
比如识别出哪种饮料最适合给疲惫的人喝
比如说我们常常见的一些功能饮料
其实早在去年
谷歌就曾推出过RT-1版本的机器人
只需要一个单一的预训练模型
RT-1就能从不同的感官输入
比如视觉或者文本中生成指令
从而执行多种任务
在RT-1的架构中
模型采用文本指令和图像集作为输入
通过预先训练的FiLM EfficientNet模型
将它们编码为token
并且通过TokenLearner压缩它们
然后将这些输入到Transformer中
由Transformer输出操作token
因此，与一般机器相比
RT-1具有更好的性能和泛化能力
而RT-2建立在了RT-1的基础上
并且使用了RT-1的演示数据
这些数据是由13个机器人在办公室、厨房环境中收集的
历时17个月
但是RT-2比RT-1多了一个机器动作（action）的模态
前面我们已经提到RT-2建立在VLM基础之上
其中VLM模型已经在Web规模的数据集上训练完成
可以用来执行像视觉问答、图像字幕生成
或者物体识别等任务
此外
研究人员还对先前提出的两个VLM模型
PaLI-X和PaLM-E进行了适应性调整
啊我们之前也做过一期节目
介绍这个PaLM-E
把它们当做RT-2的主干
并将这些模型的视觉-语言-动作版本
称为RT-2-PaLI-X以及RT-2-PaLM-E
为了使视觉-语言模型能够控制机器人
还差对动作控制这一步
DeepMind采用了非常简单的方法
他们将机器人动作表示为另一种语言
即文本token
并且与Web规模的视觉-语言数据集
一起进行训练
对机器人的动作编码基于布罗汉Brohan等人
为RT-1模型提出的离散化方法
如图所示
研究人员将机器人动作表示为文本字符串
这种字符串可以是机器人动作token编号的序列
例如「1 128 91 241 5 101 127 217」。
这个字符串以一个标志开始
这个标志指示机器人是继续还是终止当前环节
然后机器人根据指示
改变末端执行器的位置和旋转
以及机器人抓手等命令
由于动作被表示为文本字符串
因此机器人执行动作命令
就像执行字符串命令一样简单
有了这种表示
我们可以直接对现有的VLM进行微调
并将其转换为视觉-语言-动作
VLA模型
在推理过程中
文本token被分解为机器人动作
从而实现闭环控制
此外，研究人员还对RT-2模型
进行了一系列定性和定量实验
在语义理解和基本推理方面，例如
对于「把草莓放进正确的碗里」这一项任务
RT-2不仅需要对草莓和碗进行表征理解
还需要在场景上下文中进行推理
才能知道草莓应该与相似的水果放在一起
而对于「拾起即将从桌子上掉下来的袋子」这一任务
RT-2需要理解袋子的物理属性
才能消除两个袋子之间的歧义
并识别处于不稳定位置的物体
需要说明的是
所有这些场景中测试的交互过程在机器人数据中从未见过
以上的每项任务都需要理解视觉语义概念
以及执行机器人控制的能力
研究人员在RT-2模型上，一共进行了6
000多次机器人试验
具体来讲
谷歌团队探索了RT-2的三项技能
分别是符号理解、推理、人类识别
在所有类别中，研究人员观察到
在四个基准测试上
RT-2模型优于之前的RT-1和视觉预训练基线
泛化性能提高了3倍以上
此外
研究人员还进行了一系列定量评估
首先是机器人数据中有实例的原始RT-1任务
然后对机器人先前未见过的物体、背景和环境
这些任务可以让机器人从VLM预训练中学习泛化
结果发现
RT-2不仅保留了机器人在原始任务上的性能
还提高了机器人在以前未见过场景中的性能
从RT-1的32%提高到62%。
团队还在开源语言表机器人任务套件上评估了模型
模拟中的成功率高达90%，
比BC-Z、RT-1和LAVA等以前的基线模型有了大幅提高
一系列结果表明
视觉-语言模型是可以转化为强大的视觉-语言-动作模型的
通过将VLM预训练与机器人数据相结合
可以直接控制机器人
更重要的是
RT-2还带来了显着更好的泛化能力、以及应对突发问题的能力
它不仅是对现有VLM模型的
简单而有效的修改
而且还展示了构建通用实体机器人的前景
让机器人可以推理、解决问题和解释信息
从而在现实世界中执行各种任务
和ChatGPT类似
这种能力如果大规模应用起来
估计又会给世界带来不小的变化
不过谷歌没有立即应用RT-2机器人的计划
只是表示研究人员相信
这些能理解人话的机器人
绝不只会停留在展示能力的层面上
我们不妨简单想象一下
具有内置语言模型的机器人
完全可以成为你的家庭助理
比方说折叠衣物、从洗碗机中取出物品、在房子周围收拾东西
更不要提在工业领域的更多使用场景了
之前OpenAI在预测ChatGPT影响工作岗位的报告中
提到体力劳动者不会被首先替代
现在RT-2出现后
这个结论我觉得也不好说了
因为在工厂里的很多工作
可能会更容易的被这些机械臂所取代
好了，本期视频内容就到这里
感谢大家的观看，我们下期再见
