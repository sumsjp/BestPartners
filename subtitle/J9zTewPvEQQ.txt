大家好，这里是最佳拍档，我是大飞
自从OpenAI o1发布之后
AI领域就掀起了一场地震
为什么呢？
因为o1不仅能像人类一样思考复杂问题
还拥有优秀的通用推理能力
在未经专门训练的情况下
o1能够直接拿下数学奥赛金牌
甚至能在博士级别的科学问答环节上
超越人类专家
除了性能跃升以外，更重要的是
它还揭示了大模型进化范式的转变
那就是通过更多的强化学习和更多的推理
模型可以获得更为强大的性能
根据Rich Sutton的《苦涩的教训》，
能够充分利用计算能力的方法
最终才是最有效的方法
而搜索和学习
正是两种会随着算力的增加而继续扩展的方法
就连Sam Altman也曾经说过
在未来的一段时间里
新范式进化的曲线会非常陡峭
而从训练Scaling到推理Scaling的范式转变
也引发了关于计算资源分配和硬件选择的重新思考
AI领域内的研究者和从业者开始认识到
一方面
应该在推理阶段投入更多的计算资源
另一方面，通过优化硬件配置
来提升大模型推理的效率
将会是下一阶段的攻关重点
而大模型要进行推理Scaling
实际上比训练Scaling
对芯片并行处理能力的要求更高
而最初设计为用来进行图形渲染的GPU
由于优秀的并行处理能力
在过去几年里已经成为了训练大模型的热门选择
不过
虽然GPU非常适合进行神经网络的训练工作
但是在全新的范式下
由于它在延迟、功耗等方面表现不佳
所以不是进行大规模推理的最好选择
那么，除了GPU以外
还有什么是大模型推理的更好选择呢？
现在的 AI 芯片有各种流派
包括ASIC、FPGA、DSP、Neuromorphic Chip
以及大量DSA 芯片
其中，以SambaNova RDU为代表的
基于动态可重构数据流架构的芯片
能够通过并行处理和高效的数据移动
来优化芯片的性能和效率
近几年来也获得了越来越多的关注
在近日举办的芯片盛会Hot Chips上
SambaNova发布了最新一代的RDU产品
SN40L
今天大飞就来给大家介绍一下
Sambanova是如何实现大模型的快速推理
以及如何成为除GPU以外的更佳选择
如今，我们都知道
大模型在推理的时候
会逐步生成输出序列的Token
而每生成一个token
都会需要把模型的参数
从HBM高带宽内存中
搬运到片上进行计算
对于利用HBM来推理的芯片来说
HBM 的利用率就成了推理速度的关键
越快能从内存中访问到数据
就越能缩短处理的时间
而SambaNova的RDU
可重构数据流单元
是目前唯一一款
采用紧耦合三层内存系统的AI加速器
由SRAM、HBM和DDR DRAM组成
这种独特的解决方案具有以下几个优势
首先DDR可以在单个插槽上
托管数百个异构模型和检查点
还可以支持万亿参数专家模型组合和其他Agent工作负载
同时可以在模型间快速切换
不受主机PCIe带宽的限制
其次HBM可以保存当前运行的模型
并且缓存其他模型
第三，大型分布式片上SRAM
可以通过空间内核融合和库级并行
实现高强度的运算
由于它的架构可以自动做到极致的算子融合
实现90%以上的HBM利用率
所以使得RDU相比GPU
有着2-4倍的性能优势
也正因为如此
在当前的所有AI推理平台中
SambaNova 是唯一一个
能在Llama 3.1 405B上
提供每秒超100个Token推理速度的平台
甚至超过第二名Fireworks将近一倍
从SambaNova公开的资料中我们可以看到
其中的每个框都是一个算子
一般来说，多个算子会同时运行
并且将数据保存在芯片上
以便重复使用
但是在RDU中
整个解码器就是一个Kernel 调用
这就意味着调用开销会显著的减少
相对应的
芯片对数据进行有效处理的工作时间就会增加
另一方面，极致的算子融合
使得RDU能够达到类似于GPU的批处理能力
我们都知道，GPU有很好的批处理能力
比如批处理大小从1到16
可以将吞吐量提高12到15倍
而RDU参考了GPU的设计
当编码器decoder0在进行批处理运算的时候
可以同时从HBM读取decoder1的参数
除了相对于主流GPU的速度优势以外
SambaNova RDU的数据流架构
也开始被越来越多的从业者所重视
这是一种和以前用GPU进行处理
完全不同的思路和方法
与GPU本质上不同的是
数据流架构是通过数据流动
来驱动计算过程的
而不是通过常规的指令流动
在数据流架构中
程序会被表示为一个一个的数据流向图
其中的节点，代表计算操作
而边，代表数据之间的依赖关系
每个节点在它的所有输入数据准备好以后
会立即执行
并且将结果传递给下游的节点
这种架构天然就支持并行处理
使得多个独立的计算操作可以同时执行
从而显著提高了计算性能
除此以外
SambaNova RDU的片上空间数据流
还可以进行自动的算子融合
与GPU上传统的 kernel-by-kernel 运行相比
能够明显消除大量的内存流量和开销
所以最近几年以来
GPU厂商们也逐渐意识到了非数据流架构的短板
并且开始为GPU引入一些数据流的功能
例如，从H100开始
英伟达的GPU不仅开始加入了分布式共享内存
还加入了新的张量内存加速器单元
从某种程度上模仿了片上空间流水线运行的模式
但是，这种程度的改动还远远不够
GPU 速度的提升
恐怕已经跟不上AI推理需求的暴涨了
由于很多GPU最初不是专门为 AI 而设计的
所以很难在不影响主营业务的情况下
对GPU的基本架构做完全的重新设计
即使增加了一些修修补补的工作
也无法完全改用高效的数据流架构
这也从根本上限制了GPU推理速度的提升
目前，几家主流的AI芯片初创公司
已经全部都选择了数据流架构
其中SambaNova的RDU
更是展现出了独特的优势
也被视为GPU的最有力竞争者
与英伟达相比
Sambanova在最新的Llama 3.1模型上
生成token的性能已经快了不止10倍
并且通过 cloud
sambanova
ai，公开提供给开发人员们使用
从最基础的成本角度来看
由于Sambanova RDU不仅拥有大容量的片上SRAM
同时也拥有HBM
相比于其他几家单纯依靠片上SRAM的数据流企业来说
用户需要用来支持大语言模型的基础设施更少
比方说
如果想要在Llama70B上进行推理
Groq需要9个机架
每个机架8个可用节点
每个节点8个LPU，总计576个芯片
Cerebras需要4个机架
总计336个芯片
而 SambaNova 只需要1个机架16个SN40L芯片即可
更进一步来说
RDU带来的推理速度提升
不只是体现在效率和质量上
也体现对AGI探索的加速上
受到OpenAI o1推理缩放法则的启发
人们意识到，在推理端
更多的算力同样会带来更强的智能
因为在同一时间单位内
推理速度越快
就能实现越复杂的推理
解锁越复杂的任务
大模型应用的天花板也就越高
这就意味着
如果我们想要更快的实现 AGI
本质上就需要建设足够的基础设施
并且持续的降低计算成本，同时
计算资源还要更多地向推理侧增加
但是在目前的条件下
算力往往是大模型厂商们
突破技术上限要面对的头一道难关
即使对于实力雄厚的玩家们也是一样
OpenAI 在发布o1的时候
似乎就遇到了这个问题
机器学习研究员Nathan Lambert在博客文章
《逆向工程 OpenAI的o1》中就写到
在已经发布的基准测试分数和曲线图中
o1 preview并不是能力最强的
但是OpenAI并没有立即发布最强版本的o1
原因是由于最高的配置过于昂贵
没有对应的基础设施能够支持大规模的部署
但是推理的算力需求
也并非是天堑不可跨越
在o1发布后不久
SambaNova便在Hugging Face上
发布了Llama 3.1 Instruct-O1的演示
这个项目由SambaNova的SN40L RDU提供算力支持
用户可以与Llama 3.1 405B-instruct 模型
进行实时的对话
体验风驰电掣般的o1 推理过程
这也意味着，在强大算力的支持下
开源大模型的推理能力会不断提升
甚至触达更高级的智能也指日可待
我们再来简单介绍一下SambaNova这家公司
在AI芯片赛道的诸多初创公司中
SambaNova是目前估值最高的一家独角兽
SambaNova 成立于2017年
公司CEO Rodrigo Liang 毕业于斯坦福大学
在创立SambaNova之前
他曾经领导了甲骨文和Sun Microsystems 的工程团队
负责SPARC处理器和ASIC 芯片的开发
三位创始人中的其他两位
也都来自斯坦福大学
此外
被誉为“芯片风险投资教父”的陈立武
自SambaNova创立之初
就作为创始投资人和董事会主席加入公司
并且于2024年5月出任执行主席
从而加速和扩大公司的发展
自1987年创立华登国际以来
陈立武投资了许多芯片公司
在推动半导体创新和发展方面发挥了重要作用
好了
以上就是对SambaNova RDU的介绍了
自从大模型的Scaling Law
开始从预训练向后训练和推理侧转移之后
一个新的时代正在开启
芯片厂商们在算力层面的分配与设计
也会更为深刻的影响大模型领域的竞争格局
而对于 SambaNova
或者其他以提供算力和计算基础设施为主的公司来说
相信接下来会迎来前所未有的机遇
今天的视频就到这里
感谢大家的观看
我们下期再见
