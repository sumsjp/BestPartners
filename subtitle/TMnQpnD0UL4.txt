大家好，这里是最佳拍档，我是大飞
如何让机器实现更深层次的思考
这是AI领域中一直在尝试回答的一个根本性问题
也由此诞生了大量的前沿研究
前两天，前OpenAI的安全研究副总裁
现Thinking Machines Lab的联合创始人
Lilian Weng
又更新了一篇超长的万字博客
《Why We Think》。
在这篇博文中
Lilian系统梳理了当前让AI模型产生思考的各种前沿方法以及背后的逻辑
无论是通过生成更长的思维链、利用外部的工具、在连续空间中进行循环计算
还是将思考过程视为潜变量来进行优化
核心的目标都是为了提升模型解决复杂问题的能力
此外，文章中还深入剖析了
如何通过思维链、智能解码策略、潜在思考等多样化的机制
来有效地利用“思考时间”，
从而突破当前AI性能瓶颈的各种理论基础、技术实现和前沿进展
可以是一篇对了解模型思考过程不可多得的综述
网友们看完也是纷纷打出了精彩二字
更有人感觉打开了对AI理解的一个全新维度
所以今天大飞就来给大家总结一下其中的重点内容
建议大家有时间还是仔细去阅读一下原文
Lilian首先从三个层面
阐述了延长模型思考时间的动机
首先是心理学上的类比
人类面对复杂问题
比如12345乘以56789等于多少的时候
不会瞬间给出答案
而是需要时间思考
正如丹尼尔卡尼曼在《思考
快与慢》一书中提出的双过程理论
其中系统1也就是快思考，靠的是直觉
它的特点是快速、自动，不费力
而系统2也就是慢思考
则需要深思熟虑、逻辑推理
需要主动消耗认知资源
而AI模型也是类似的
虽然可以依赖“直觉”快速给出答案
但是如果能给模型更多的计算时间
就能够激活更深层次的“慢思考”，
从而提升模型回答的准确性和逻辑性
其次，计算也是一种资源
深度学习模型从本质上来说
还是一种计算和存储的组织形式
如果我们设计出了能在测试时进行更多计算的一种架构或者系统
并且对它进行有效的训练
让它能够利用这些资源
那么模型的性能自然就会更好
比如在Transformer模型中
每个生成token的计算量
大约等于2倍参数量
在混合专家MoE模型中
则是2*参数量/稀疏度
其中稀疏度是活跃专家的比例
相比之下
思维链CoT则允许模型为答案中的每个token
执行远超这个数据量的计算
而且还能够根据问题的难度
动态调整计算量
第三是潜变量建模
我们可以将模型的思考过程视为一个潜变量z
给定问题x，模型生成答案y
那么优化的目标就是最大化 P(y|x) 。
这种视角有助于理解并行CoT或者搜索CoT
因为它们可以看作是从后验 P(z|x
y) 中采样
这也解释了为什么对数损失目标函数在预训练中如此有效
那么，在了解了思考的动机之后
大模型都有哪些思考方式呢？
Lilian分成了三大类
分别是Token级思考
连续空间思考和潜变量思考
我们先来说第一类Token级思考
这也是目前研究最深入、应用最广的领域
指的是模型会通过生成一系列的中间token
也就是思考步骤
来辅助做出最终的决策
这个方向的发展历程是这样的
2017年
Ling等人在AQUA-RAT数据集中
首次探索了为数学问题生成中间的步骤
然后在2021年
由Cobbe等人扩展为了GSM数据集
以及通过监督学习来训练生成器和验证器
2021年
Nye等人实验性的使用了“草稿纸”的概念
生成中间token，终于在2022年
Jason Wei等人正式命名了思维链CoT
早期的CoT依赖于人工编写的推理路径
或者由模型筛选出的正确路径
来进行监督学习
一些简单的提示
比如“think step by step”或者更复杂的一些提示
能够显著提升模型的性能
但是后来发现
通过在可以自动检查答案的数据集上进行强化学习
也可以大幅提升CoT的推理能力
OpenAI的o1-preview、o3以及DeepSeek R1的技术报告
都展示了简单的策略梯度算法的强大潜力
接下来，Lilian指出
测试时计算（test-time compute）的根本目的在于
在推理过程中动态地调整模型的输出分布
目前主要有两种方法可以提升生成质量
分别是并行采样（parallel sampling）与序列修正（sequential revision）
其中，并行采样会同时生成多个输出
然后通过过程奖励信号或者最终的验证器
来选择出最佳的答案
目前广泛采用的解码方法包括best-of-N 和束搜索
Best-of-N是最简单的方法之一
只需要收集N个独立样本
然后根据评分函数选择排名最高的样本
而束搜索是一种更复杂的搜索算法
它能够自适应地分配更多计算资源
到解空间中更有潜力的区域
从而优化搜索过程
但是在无法获得标准答案的情境下
常常会使用自洽性策略（self-consistency）
对多个CoT结果进行多数投票
来选择最终答案
而序列修正是指基于模型前一轮输出的结果
进行反思与迭代性修正
从而引导模型在后续输出中
主动识别并且纠正可能的错误
这种修正流程通常需要在经过微调的模型上实现
如果仅仅依赖模型自身进行内生性的自我修正
缺乏外部的监督反馈
往往难以取得明显的提升
简单来说
并行采样的方法简单、直观、易于实现
但是由于受到模型能力的限制
无法保证一次就得到正确的解
而序列修正会虽然会让模型对错误进行反思
但是它的速度较慢
在执行过程中需要格外的小心
因为它存在把正确的预测修改为不正确的预测
或者引入其他幻觉的风险
实际中，这两种方法可以一起使用
Snell等人的研究也表明
对于简单问题
单一使用序列化策略的测试时计算效果最佳
而对于高难度问题
则通常需要在并行计算与序列修正之间
找到最优的组合比例
才能获得最好的任务表现
2023 年
韦莱克等人提出的自我修正学习算法
目的是在固定的生成模型基础上
训练一个修正模型
其中生成模型保持通用性
而修正模型则可以针对具体的任务进行定制
并且只在给定初始模型响应和某些附加反馈的条件下进行生成
生成过程大概是这样的
首先
自我修正学习针对数据池中的每个提示
生成多个候选输出
然后为每个提示构造一个价值提升对
也就是挑选出两个输出
如果其中一个相较于另一个在评价指标上更优
那么就组成（提示 x，初始答案 y
修正答案 y′）的一个三元组
接着
根据修正带来的价值提升幅度 v (y′) − v (y)，
以及两个输出之间的相似度 Similarity (y
y′)，对这些样本对进行加权采样
来训练修正模型
同时，为了鼓励探索
修正器生成的新的输出
也会加入到数据池中，这样在推理时
就可以反复的调用修正器
从而形成一个连续改进的修正轨迹
而Kumar等人在2024开发的SCoRe方法
则采用了多轮次强化学习的策略
通过奖励模型在第二次尝试时
生成优于首次尝试的答案
来实现自我修正
这个框架包含了两个训练阶段
其中第一阶段仅优化第二次尝试的准确率
同时对第一次尝试施加KL散度惩罚项
来防止初始响应过度偏离基础模型的行为特征
而在第二阶段中
会联合优化第一次和第二次尝试的响应准确率
理论上
这两个阶段的响应质量都应该能得到提升
因为第一阶段的设计
有效规避了模型对初始响应仅作微小修改
或者完全不修正的行为坍缩现象
而第二阶段的实施则进一步提升了整体的修正效果
接下来
Lilian解释了为什么强化学习会带来更好的推理
并且以DeepSeek R1作为例子
这部分和接下来的外部工具的使用
由于之前我们做了不少节目介绍
所以这里就暂时跳过了
大家有兴趣的可以去阅读原文
现在，虽然CoT提供了宝贵的可解释性
但是前提得是它真实反映了模型的“思考过程”，
而恰恰在某些模式下
CoT的忠诚度会失效
这其中就包括提前回答、无效令牌和人类不可读编码等情况
其中提前回答指的是CoT没有完成就已形成结论
无效令牌指的是用无意义的填充物来替换CoT
导致部分任务的性能不降反升
而人类不可读编码 (Human-unreadable encoding)指的是将CoT改写成人类难以读懂
但是信息等价的形式
那么我们如何来测试忠诚度呢
可以通过扰动提示的方法
比如在few-shot示例中
故意标错答案
或者在提示中加入误导性的线索
结果显示
推理模型比非推理模型更能够忠实地承认提示的影响
那么，CoT面临的优化压力
到底是好是坏呢？
研究发现
直接将CoT的监控器信号加入强化学习奖励
会导致模型学会“隐藏”真实的意图
出现“混淆作弊”的现象
而只对CoT的长度进行奖励
可能会导致模型在难题上重复无意义的文本
而非尝试去解决问题
所以，针对这些情况，Lilian建议
对CoT进行直接的优化压力要非常谨慎
甚至应该尽量避免
模型的第二类思考方式
就是连续空间思考
不同于离散的token序列
这种模式允许模型在“连续”的时间维度上思考
Alex Graves 于 2016 年提出的自适应计算时间
开创了模型动态决定计算步数的先河
通过让模型能够在推理时动态决定计算步骤的数量
可以视为让模型在测试时能够在连续空间中思考得更多
之后
为了使 Transformer 架构成为递归架构
实现自适应的测试时间计算
人们提出了许多的架构变体
其中
Universal Transformer 将 Transformer 中的自注意力与 RNN 中的递归机制相结合
利用自适应计算时间来动态的调整步数
另外
Geiping 等人提出的最新递归架构设计
在标准Transformer上叠加了一个循环块R
通过随机化循环次数进行训练
并且只反向传播最后的k步
不过
训练这种模型对初始化、归一化等超参数都非常敏感
而在模型的训练和推理过程中
有一类不具有直接语言意义的隐含 token
称为思考token
它们的作用是为模型提供额外的计算时间
在这方面也有很多的研究和创新
其中
Herel和Mikolov在2023年的研发发现
在句子中每个词的后面插入<T>这个思考token
可以降低模型的困惑度
Goyal等人在2024提出了Pause Tokens
也就是在训练和推理时
在输入序列的末尾追加
或#等哑token
并且在训练时忽略掉它的损失
Zelikman等人在2025年提出的Quiet-STaR方法
引入了token级的推理
让模型在预测每个未来token前
先生成一个“理由”（rationales）
并且通过REINFORCE来优化理由的质量
从而在Mistral 7B上取得了显著的零样本提升
模型的第三类思考方式
是将测试时的思考步骤视为潜变量
通过优化这些潜变量来改善模型的性能
其中期望最大化EM是一种常用的迭代算法
可以用来优化具有隐含潜变量的模型参数
通常我们在期望的E步和最大的M步之间迭代
E步中我们猜测关于潜变量的缺失信息
也就是如何采样更好的思维链
而M步中我们基于潜变量来优化模型的参数
也就是如何采样更好的答案
直到收敛
由于无法直接从 p(z|x,y) 中采样
所以研究者还需要依赖人工标注
或者带有特殊重要性权重的蒙特卡洛采样
而由于预训练模型已经具备了生成思维链的能力
因此我们可以直观地设计一个迭代改进的过程
在这个过程中，我们会生成多个 CoT
并且只根据能够得出正确答案的理由
对模型进行微调
不过这种设计的问题在于
模型在无法解决的问题上得不到学习信号
因此
2022年Zelikman等人提出了STaR方法
通过为失败的尝试添加一个合理化过程
来解决这个局限性，在这个过程中
模型会根据问题和基本的真实答案
生成良好的后向 CoT
从而让模型可以生成更合理的 CoT
然后
再根据正确的解决方案对模型进行微调
这些解决方案要么能够产生正确的输出
要么是通过合理化生成的
研究观察到
STaR 的性能会随着训练迭代的增加而提高
从而生成更好的 CoT 的“合理化 ”过程
加速学习
另外
高温采样会增加通过错误推理获得正确答案的机会
但是用这类数据来微调模型
反而会损害模型的泛化性
到目前为止呢
已经有大量的证据表示
允许模型在推理时投入额外的计算资源
让它在产生最终答案之前进行思考
可以显著提升模型的性能
这本质上是引入了一个提高模型智能的新维度
与 scaling law 中定义的参数规模、算力和数据量等已有的因素形成了补充
最近的研究表明
优化大语言模型的测试时计算
可能比扩展模型的参数更加有效
结合先进推理算法的小型模型
也可以在成本和性能之间提供帕累托最优的权衡
Snell等人评估并且比较了测试时计算和预训练计算
发现它们并非是 1:
1 可互换的
当模型能力差距很小的时候
测试时计算可以轻松弥补简单和中等难度问题上的差距
但是对于困难问题的效果则较差
另外
预训练和推理的 token 预算比例非常重要
只有当推理 token 显著少于预训练 token 的时候
测试时计算才更有优势
这表明
利用充足的预训练数据和算力
开发有能力的基础模型
仍然至关重要
因为测试时计算无法解决所有的问题
也无法填补大的模型能力差距
最后
Lilian还提出了一系列亟待解决的开放性问题
包括如何在强化训练中激励模型
产生人类可读的、忠实的推理路径
同时避免奖励作弊呢？
如何定义和捕捉奖励作弊
又该如何防止“打地鼠”式的修复呢？
以及如何在无真实标签的时候
训练模型进行无幻觉、无退化的自修正等等
这些问题的答案还有待更多的人去探索
好了
以上就是这篇博文的主要内容了
希望对大家了解思维链CoT的发展现状有所帮助
感谢大家的观看，我们下期再见
