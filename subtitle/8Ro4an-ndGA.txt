大家好，这里是最佳拍档，我是大飞
今天咱们要聊的话题
可能和每一位使用Claude的朋友都息息相关
如果你最近打开过Claude
无论是免费版、专业版还是Max版
应该都能看到一个弹窗
这个弹窗背后
是Anthropic这家公司对用户隐私承诺的彻底反转
曾经
Anthropic在Claude刚上线的时候
明确说过“绝不使用用户数据来训练模型”，
这也是很多用户选择Claude的重要原因
但是现在，他们不仅要让用户选择
是否同意将数据用于训练
还设置了一个月的“最后通牒”，
甚至把同意数据训练后的保留期
延长到了5年
这件事在国外科技圈已经炸了锅
网友们不仅怒斥Anthropic“背刺”用户
还翻出了这家公司过往一系列让用户寒心的操作
今天咱们就借着这件事
聊聊这次条款更新的细节
用户的真实反馈
以及整个AI行业的隐私困境
首先，咱们得先弄清楚
这次Anthropic到底更新了什么政策
又给用户挖了哪些“坑”。
根据Anthropic在2025年8月30日
通过官方博客发布的消费者条款更新通知
这次调整的核心是
用户数据是否用于模型训练的选择权
但是这个所谓的选择权
其实充满了各种限制
对于已经在使用Claude的现有用户来说
Anthropic给出的期限是到2025年9月28日
在这之前
如果你选择“接受”数据用于训练
那么从你点击接受的那一刻起
所有新的或者重新开始的聊天会话、编程会话
包括使用Claude Code的操作
都会被纳入训练数据
如果到了9月28日之后你还没做选择
那不好意思
你必须先去模型训练设置里选好
否则就用不了Claude了
而新注册的用户则更为直接
注册流程里就会强制让你先做这个选择
不选就没法用
除了选择机制以外
数据保留期的变化也很关键
如果用户同意数据用于训练
那么这些数据会被Anthropic保留5年
只有当你主动删除某段对话的时候
这段对话才不会被用于未来的训练
但是如果用户不同意
那么Anthropic就会继续执行之前的政策
也就是数据保留期30天
这里有个细节需要注意
Anthropic明确说了
这次调整只针对“消费级用户”，
也就是使用Claude免费版、专业版、Max版
包括Claude Code的个人用户
而那些使用政府版（Claude Gov）、企业版（Claude for Work）、教育版（Claude for Education）
或者通过API接入的商业客户
是不受这次政策影响的
换句话说
Anthropic把个人用户和商业客户的隐私待遇
做了一个明确的区分
商业客户的隐私似乎更加受“重视”，
而个人用户则成了数据训练的潜在“贡献者”。
可能有朋友会问
Anthropic为什么要这么做呢？
他们在博客里给出的理由
听起来似乎很“冠冕堂皇”，
说如果用户同意数据用于训练
能够帮助提升模型的安全性
让检测有害内容的系统更加准确
减少把无害对话误判为违规的情况
还说这能提升未来Claude的编码、分析和推理能力
最终让所有用户受益
但是稍微懂点AI行业逻辑的朋友都知道
这背后的真实原因
其实是“数据焦虑”。
大模型的竞争，本质上是数据的竞争
只有拿到更多真实用户场景下的交互数据
模型才能在推理、编码这些核心能力上超越对手
Anthropic作为OpenAI、谷歌的直接竞争对手
自然需要大量的用户数据来缩小差距
而消费级用户的对话、编程记录
正是最贴近真实使用场景的“金矿”。
所以这次政策调整
与其说是给用户选择权
还不如说是Anthropic为了获取训练数据
给用户设下的一道必答题
而网友们的愤怒，恰恰就来自这种
表面给选择权
实则逼用户同意的操作
尤其是界面设计上的“小心思”。
很多用户反馈
Claude弹出的政策更新窗口
设计得特别有引导性
标题是大号加粗的“消费者条款和政策更新”，
左边是一个非常显眼的黑色“接受”按钮
几乎占据了半个屏幕
而“是否同意数据用于训练”这个关键选项
却藏在“接受”按钮下方的小字里
而且那个切换开关
默认就是“开启”的状态
这种设计是什么概念？
就像咱们以前装软件的时候
那些默认勾选“同意安装全家桶”的选项一样
大多数用户可能只是习惯性地点一下“接受”，
根本没注意到自己已经同意把数据给Anthropic训练5年了
有网友就吐槽说，这不是选择权
而是‘套路权’，
Anthropic把用户当傻子耍
更让用户无法接受的是
这次政策反转
其实已经不是Anthropic第一次“背刺”用户了
有一位名叫艾哈迈德（Ahmad）的网友
在自己的推文中就详细盘点了Anthropic从Claude上线
到现在的一系列争议操作
每一条都戳中了用户的痛点
比如
除了这次数据保留5年并用于训练以外
艾哈迈德说Anthropic会在白天悄悄把用户使用的模型
换成1.58 bit的缩水版量化模型
通过降低参数的精度来节省算力
代价是模型的推理精度和响应质量也会下降
而用户对此完全不知情
相当于“付了完整版的钱
却用了阉割版的服务”。
再比如，Plus会员本来是付费用户
按理说应该享受更好的服务
但是在Claude Code功能里
却用不了Anthropic最强的Opus 4模型
这就相当于“花钱买了个半成品”。
还有更离谱的是，6周前
Anthropic没给任何通知
就把Max套餐的用量限制
直接砍了一半
要知道，Max套餐的价格比Plus贵不少
很多用户都是冲着“大用量”才买的
结果突然减量，连个解释都没有
而且不管是Plus还是Max套餐
Anthropic从来没有公开过每周用量限制的具体数字
用户只能凭感觉来使用
用到上限了才知道
哦，这周不能用了”。
更夸张的是
Anthropic之前宣传的“5倍用量套餐”和“20倍用量套餐”，
实际给的量根本没有达到宣传的一半
有用户测算过，所谓的“5倍套餐”，
实际用量只比基础Plus套餐多3倍
“20倍套餐”只多8倍
这完全是虚假宣传
艾哈迈德还提到
自己的一个项目因为Anthropic乱发DMCA
也就是数字千年版权法案的通知
导致相关的代码仓库被下架，关键是
他的项目并没有侵权
Anthropic这种“一刀切”的DMCA操作
直接影响了开发者的正常工作
另外的操作还包括
禁止Windsurf的用户使用Claude 4模型
以及切断了对OpenAI API的访问权限
这些种种操作
每一条都体现出Anthropic对用户的不尊重
也难怪这次政策反转会引发这么大的愤怒
其实，Anthropic这次的操作
并不是AI行业的孤例
整个大模型行业
都在围绕着“用户数据”展开竞争
很多大厂都在悄悄的调整隐私政策
目的就是获取更多的训练数据
比如谷歌
最近也把旗下Gemini的“Gemini Apps Activity”，
改名为了“Keep Activity”，
并且明确说明，从2025年9月2日起
只要用户开启这个设置
部分上传的样本就会被用来
帮助为所有人改进谷歌服务
本质上和Anthropic要用户数据来训练模型是一个逻辑
只是换了个说法
还有OpenAI
最近因为《纽约时报》的版权诉讼
首次公开承认了一个让用户震惊的事实
从2025年5月中旬开始
他们一直在悄悄保存用户已经删除的聊天记录
还有那些临时的会话内容
很多网友看到这个消息后都懵了
原来我删掉的ChatGPT对话
根本就没有真的删掉
而是被OpenAI存档了？
这不是侵犯隐私吗？
但是OpenAI给出的理由是为了应对诉讼需要
可是这种“事后告知”的做法
还是让用户对隐私保护失去了信任
为什么这些大厂都这么执着于用户数据呢？
因为对于大模型来说
真实的用户交互数据可以说是“刚需”。
实验室里的训练数据再丰富
也不如用户在实际场景中输入的提示词、对话内容、编程需求来得真实
这些数据能够让模型更懂用户的需求
比如编码时更符合开发者的编程习惯
分析问题时更贴近用户的思考逻辑
甚至能减少一本正经胡说八道的情况
所以
为了在和竞争对手的比拼中占据优势
大厂们宁愿冒着失去用户信任的风险
也要调整政策获取数据
但是这里的核心问题是
用户的隐私该由谁来保护呢？
这些大厂在调整政策的时候
往往会用“提升服务质量”、“优化模型体验”这些话来包装
但是却刻意淡化数据如何使用、如何保护的细节
比如Anthropic
虽然说删除对话后就不会用于训练
但是没说已经用来训练的数据
会不会被删除
虽然说“保留5年”，
但是没说5年后这些数据会如何处理
是彻底删除还是会以其他的形式留存呢
这些模糊的表述
让用户根本没法判断自己的隐私到底有没有保障
更关键的是
大多数用户其实没有精力去仔细阅读冗长的隐私条款
一份隐私条款少则几千字
多则上万字，里面全是专业术语
普通人根本看不懂
而大厂们恰恰利用了这一点
把关键的隐私条款藏在密密麻麻的文字里
或者用界面设计引导用户“默认同意”。
这种情况下
用户的“选择权”其实是形同虚设的
因为你根本不知道自己选了什么
也不知道这个选择会带来什么后果
从行业监管的角度来看
目前对AI大厂数据使用的监管还处于相当滞后的状态
虽然欧美有一些数据保护法规
比如欧盟的GDPR
美国的CCPA
但是这些法规在面对大模型的数据需求时
往往显得“力不从心”。
比如GDPR要求数据处理必须获得用户明确同意
但Anthropic这种默认开启+界面引导的同意方式
到底算不算明确同意呢？
目前还没有统一的判断标准
这就给了大厂们“钻空子”的空间
回到Anthropic这次的事件上
其实它反映出的是整个AI行业的一个矛盾
一方面，大模型需要用户数据来进步；
另一方面，用户又担心隐私被侵犯
如何平衡这两者
应该是所有AI公司都必须面对的一个问题
而不是像现在这样
用套路来让用户妥协
比如
Anthropic完全可以把“是否同意数据用于训练”，
做成一个更为显眼的选项
用简单易懂的语言
告诉用户同意后数据会怎么用、保留多久
不同意会有什么影响
而不是藏在底下的小字里
再比如
可以公开数据使用的具体流程
让用户能够查到自己的数据有没有被用来训练
什么时候被用的
只有透明化，才能重建用户的信任
作为一个科技频道
从22年底做到现在也快三年了
大飞我始终觉得
AI技术的进步应该以保护用户的权益为前提
而不是以牺牲用户的隐私为代价
如果一家AI公司连最基本的隐私承诺
都能够随意推翻
那就算它的模型再强
用户也不会真正信任它
最后
给所有使用Claude的朋友提个醒
如果你还没处理这次政策更新
一定要在9月28日之前
打开Claude的“模型训练设置”，
仔细看清楚“是否同意数据用于训练”的选项
根据自己的隐私需求做出选择
不要习惯性地点“接受”。
如果你没有使用Claude
未来在选择其他AI工具的时候
也一定要花几分钟看看隐私政策
重点关注“数据是否用于训练”、“数据保留多久”、“是否可以随时撤回同意”这几个问题
毕竟
你的每一条对话、每一段代码
都是你的个人信息
不应该被轻易的“贡献”出去
而对于Anthropic这类AI公司
我也希望它们能明白
用户的信任是最宝贵的资产
失去了信任，再强的技术也走不远
如果真的需要用户数据来提升模型
不如坦诚地和用户沟通
给出合理的补偿
而不是用“最后通牒”和“界面套路”来逼迫用户
这才是一家负责任的科技公司该有的样子
好了
今天关于Claude隐私政策反转的话题就聊到这里
如果你对这次事件有什么看法
或者有其他AI隐私相关的问题
欢迎在评论区留言
感谢大家的观看，我们下期再见
