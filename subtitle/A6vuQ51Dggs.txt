大家好，这里是最佳拍档，我是大飞
今天我们继续OpenAI 12天连播的第二天
仍然是短短二十分钟
但是重要性却不低
大家之前也许都听说过监督微调SFT
而这次登场的，则是强化微调
Reinforcement Fine-Tuning
为什么说重要呢，因为首先
这是OpenAI第一次将之前仅限于给自家模型使用的强化学习技术
开放给外部开发者
其次
开发者只需提供最低几十个的高质量任务
就能够通过强化微调
实现领域专家模型的定制
而且还能根据提供的参考答案
对模型的回应进行评分
最后
强化微调不仅加强了模型在处理领域特定问题时的推理能力
还提升了在特定任务上的准确性
对于那些要求高精确性和专业知识的领域
强化微调将会发挥至关重要的作用
从OpenAI的官方演示中可以看到
经过强化微调的o1 mini
竟然全面超越了当今最强的基础模型o1
其中，强化微调版的o1 mini
在Top-1准确率上直接跃升180%，
达到了31%，远超o1的25%。
对此，奥特曼激动地表示
这项工作效果出奇的好
是我2024年最大的惊喜之一
非常期待大家会用它去构建什么
只不过，强化微调还是个期货产品
将于2025年的第一季度公开发布
目前已经对企业、大学和研究院
开放了申请测试的通道
好了，我们先来回顾一下这场发布会
再来聊聊究竟什么是强化微调
这次上阵直播的四人
是OpenAI的研究员Mark Chen、约翰·阿拉德John Allard、Julie Wang
以及伯克利实验室的计算生物学家贾斯汀·里斯Justin Reese
根据他们的介绍
现在用户已经可以在自己的数据集上微调o1模型了
不过要强调的是
这次并不是传统的微调
而是强化微调
简单来说
强化微调能够让开发者、研究人员和机器学习工程师
首次有机会使用强化学习来创建专家级的模型
帮助用户把自己的优质数据集
转化为独一无二的产品
在特定领域的任务中带来前所未有的卓越表现
对于法律、金融、工程、保险等领域来说
这项技术简直是量身打造的
举例来说
OpenAI最近和汤森路透合作
利用强化微调对o1 Mini进行了微调
把它变成了一名法律助手
帮助法律专业人士完成了一些复杂、需要深入分析的工作流程
在去年年初
OpenAI推出了监督微调API
这项技术在当时非常强大
核心目标就是让模型复制在输入的文本或图像中发现的特征
而这次在强化微调中
它不仅是教模型去模仿输入
更是去学习如何在自定义领域上
以全新的方式进行推理
当模型看到一个问题的时候
研究者会给它空间来思考问题
然后给它的最终答案进行评分
接下来，利用强化学习的强大能力
他们会强化那些导致正确答案的思维路径
同时抑制那些导致错误答案的思维路径
这样一来
只需要数十到数千个高质量的示例
模型就能学会以新的、有效的方式
在特定领域的问题中进行推理了
这是传统微调技术所难以实现的
这也是史上首次
OpenAI在模型定制平台上支持了强化学习
而这也是OpenAI内部用来训练GPT-4o和o1等顶尖模型所使用的技术
随后，伯克利实验室的贾斯汀
介绍了强化微调给他的研究带来的巨大帮助
他研究的是
使用计算方法来理解罕见疾病背后的遗传原因
不过
现在对罕见疾病的评估并不容易
首先你要对医学有专业领域知识
还要能够对生物医学数据进行系统化的推理
而这些
o1模型都可以凭借高级的推理能力
提供帮助
在这个项目中
Justin和同事们从数百篇关于罕见疾病的科学病例报告中
提取了疾病的相关信息
包括患者的体征和症状
他们希望能够根据患者的症状
找出可能发生突变、导致这些症状的基因
为此
他们和OpenAI团队一起训练了o1模型
让它能够更加高效地推理疾病的成因
而在「根据一系列症状
来预测可能引发遗传疾病的基因」这个任务上
经过强化微调后的o1-mini模型
表现竟然超越了o1模型
这一点非常重要
因为o1-mini比o1更小、更快、成本更低
而实际上，在OpenAI的开发平台上
他们已经对模型进行监督微调一年多了
为了训练模型
他们上传了一个训练数据集
包含1100个示例
我们拿一个单独的数据点为例
其中包括病历报告、指令、正确答案三个部分
在这份病例报告中显示
患者为51岁女性
疾病发病时间未具体说明
已有的症状包括
眼距过宽、睑裂狭小、小颌畸形、软腭咽闭合不全、甲状旁腺功能减退、全身发育迟缓和感觉神经性听力障碍
未表现出的症状有
腭裂、法洛四联症、肺动脉瓣闭锁、心房隔缺损、主动脉肺动脉侧支血管
指令部分为
请列出所有可能导致这些症状的基因
从可能性最大到可能性最小
并且解释为什么你认为这些特定的基因可能是原因
最后就是正确答案
注意，在训练过程中
研究人员并不会向模型展示这个答案
否则就是作弊了
但是，研究人员会在训练过程中
用这个答案来评估模型
可以看出，这个任务的难度
已经远远超越了「Strawberry中有几个r」的级别
接下来，他们上传了一些验证数据
格式与训练数据完全相同
但是验证数据集和训练数据集之间
没有重叠的正确基因
这也就意味着，模型不能作弊
不能只是通过简单地记住症状列表
然后与基因匹配来进行回答
它必须真正地从训练数据集泛化到验证数据集上
为了采用强化学习
研究人员引入了评分器的概念
将模型的输出与正确答案进行比较
返回0到1之间的一个分数
0表示模型完全错误
1表示模型完全正确
在这个例子中，模型得到了0.7的分数
因为FOXE 3是正确答案
在基因列表中排第二位
如果它在列表中越往后
分数就会越接近0
最终
研究人员提供了一套评分器的合集
能够有效覆盖在强化微调时
可能会有的各种意图空间
接下来
研究人员就可以快速地复制一下评分器
然后启动一个训练任务
这里厉害的地方在于
你只需要提供能够体现领域专业知识的数据集和评分器
就可以利用OpenAI强化学习算法的全部能力
以及完整的分布式模型训练技术栈
来为自己的使用场景
定制一个前沿的大模型了
简单来说
就是只需要提供你的数据集和评分器
OpenAI就会给你一个微调模型
从模型的训练结果中
我们也可以看到
验证集的奖励分数呈上升趋势
由于训练数据集和验证数据集之间的基因没有重叠
这就意味着
模型确实学会了在这项任务中进行泛化
为了更深入地了解
模型微调过程中发生了什么变化
我们还可以查看评估仪表板
为此
研究人员设置了三个不同的运行
分别是运行在o1、o1 mini和强化微调后的o1 mini上的任务
可以看到
奖励分数呈现右上角上升的趋势
但是这对于任务来说意味着什么呢？
于是
研究人员又设置了三个不同的评估指标
分别是Top-1
模型一次答对的概率、Top-5模型前五次预测中有正确答案的概率
和Top-max
模型预测中有正确答案的概率
在Top-1指标中
o1 mini在大约200条数据上的得分是17%。
o1得到了25%，而微调后的o1 mini
得到了31%。
这显示出，模型确实学会了
如何在这类数据上进行推理的通用能力！
在Justin看来
强化学习将极大地振奋生物学的研究社区
近期内的最佳解决方案
可能就是结合现有的生物信息学工具
以及类似于o1的微调模型
而这个还仅仅是强化微调在科学研究中的一个应用而已
除了已经验证的生物化学、AI安全、法律以及医疗保健数据集以外
强化微调模型还将会在数百种其他的应用场景上发挥作用
OpenAI的这一步计划
显然是想让更多人在最重要的任务上
推动o1模型的能力边界
在直播最后
依然是OpenAI风格的一则圣诞冷笑话
那就是最近
圣诞老人在尝试制造一辆无人驾驶雪橇
但是不知道为何
他的模型总是无法识别树木
导致雪橇不停地撞上道路两旁的树
你们猜这是为什么？
答案是
因为他忘了给模型进行「pine-tune」，
松树微调，确实相当的冷
不过实际上
强化微调这项技术并不是OpenAI独创
而是来自于ACL 2024的一篇Oral论文
在这篇论文中
研究人员提出了一种简单而有效的方法
来增强大语言模型推理时的泛化能力
也就是强化微调（Reinforced Fine-Tuning
简称ReFT）
简单来说，ReFT 由两个阶段组成
分别是预热（Warm-up）阶段和强化学习RL阶段
在预热阶段
ReFT会先使用监督微调SFT对模型进行预热
通过对包含“Question”和“思维链CoT”元组的数据集上
进行1到2个epoch的微调
就可以将模型的预测能力
调整到能够生成适当响应的水平上
在预热阶段之后
模型会进入到强化学习阶段
这个阶段会使用PPO（Proximal Policy Optimization）算法来进一步提升模型的性能
通过在包含“Question”和“Answer”元组的数据集上
反复生成多种可能的CoT推理路径
以及用评估器来专门评估响应的答案正确性
从而生成奖励信号反馈
正确的答案会给予正奖励
错误的答案则不给予奖励
这个过程
就类似于AlphaZero在围棋领域的自对弈
也就是self-play的学习过程
从结果上看
在GSM8K、MathQA和SVAMP等数据集上的大量实验都表明
ReFT的效果显著优于SFT
特别是在CodeLLAMA模型上
ReFT在GSM8K数据集上的准确率
比SFT提高了将近10个百分点
此外
通过结合多数投票和重新排序等策略
还可以进一步地提升模型性能
不仅如此
ReFT还有着卓越的泛化能力
在训练中只需要使用与SFT相同的问题集
而无需依赖额外或增强的训练数据
以上种种结果都在说明
ReFT将会是取代SFT的不二选择
好了
以上就是OpenAI圣诞活动第二天的内容了
希望对大家有所帮助
对强化微调感兴趣的朋友
可以去提交一下申请测试
希望我们后续能看到ReFT的更多精彩表现
感谢大家观看本期视频
我们下期再见
