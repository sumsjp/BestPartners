大家好，这里是最佳拍档，我是大飞
在2024年年底
有很多AI领域的领军人物
不约而同地都做出了一个大胆的预测
那就是2025年将是AI Agent的元年
但是随着Agent 进入更多的应用场景
无论是单个 Agent 工作时产生的记忆
还是多个 Agent 协作所产生的上下文
都对模型的上下文长度提出了更多的要求
而就在1月15日
MiniMax开源了最新的基础语言模型MiniMax-Text-01
和视觉多模态模型MiniMax-VL-01
在业内首次大规模实现了线性注意力机制
大大增加了上下文窗口长度
甚至一次可以处理 400 万的token
是其他模型的20到32倍
MiniMax相信
这些模型能够给接下来一年 Agent 的应用爆发做出贡献
并且断言
传统Transformer架构不再是唯一选择
在这之前
MiniMax一直是以闭源模型的身份示人
外界对底层模型的细节知之甚少
这次不仅是MiniMax首次将模型开源
也是MiniMax首次公开技术细节
所以今天
我们就来介绍一下这两个模型
以及解读一下背后的技术报告
我们都知道的是
目前大多数领先的大语言模型
都是基于Transformer架构的
Transformer的核心机制是自注意力机制Self-Attention
它允许模型在处理文本的时候
能够关注到文本中不同位置的信息
从而捕捉到更为复杂的语义关系
然而
自注意力机制也存在一个显著的缺点
那就是计算成本高昂
具体来说
自注意力机制的计算复杂度跟输入序列长度是平方关系
也就是我们常说的（O(n²)）
这意味着当输入的上下文窗口
也就是模型一次能够处理的文本长度增加时
模型的计算量会呈指数级的增长
这样会导致两个问题
一是在处理长文本的时候
模型的计算速度会变得非常缓慢
二是模型在训练时需要消耗大量的计算资源和电力
为了解决这些问题
研究人员提出了许多种优化方案
包括稀疏注意力、低秩分解和线性注意力等等
而这一次
MiniMax在他们的模型中引入了一种名为Lightning Attention的线性注意力机制
线性注意力机制是一种改进的注意力机制
它通过将计算降低到线性复杂度
使得模型能够更高效地处理长序列数据
线性注意力机制其实并不是MiniMax提出的
但是之前线性注意力机制主要是用在学术研究以及小规模试验
而MiniMax这次算是首次实现了线性注意力模型的大规模训练
而Lightning Attention是一种基于TransNormer架构的改进版本
TransNormer最早出现在2022年的一篇论文中
它的核心思想是将传统的自注意力机制
转换为一种线性变体
通过使用“右侧矩阵乘法”，
将计算复杂度从O(n²)降低到O(n)。
而Lightning Attention是MiniMax在TransNormer的基础上
进行了I/O 感知优化的实现
在实验中
Lightning Attention在处理长序列的时候
可以表现出稳定的训练速度
并且在大多数下游任务中
与 softmax 注意力机制性能相当
甚至在检索任务上表现更好
为了在保持效率的同时
提升模型的性能
MiniMax还提出了Hybrid-lightning策略
具体来说
在每七个使用Lightning Attention的transnormer块后
会跟随一个使用softmax注意力的transformer块
这种混合策略的优势在于
一是通过使用Lightning Attention
模型的计算速度得到了显著提升
二是利用传统的softmax注意力机制
能够捕捉到更为复杂的语义关系
从而保证了模型的整体性能
除了Lightning Attention
MiniMax还采用了混合专家MoE架构
而这个架构
是MiniMax从2023年夏天就开始研发
投入了80%的算力与研发资源
经历了两次失败才成功的作品
MoE架构的核心思想是
将模型划分为多个“专家”，
每个专家擅长处理特定类型的任务
同时在推理过程中
根据输入数据的不同
动态选择最合适的专家来处理
与传统的密集模型相比
MoE架构不仅效率更高
性能也更加优秀
MiniMax还进行了一系列比较实验
结果表明，在相同的计算负载下
MoE模型的表现要大幅优于密集模型
为了训练这次的模型
MiniMax 先通过小规模实验
验证上面这些技术改进的有效性以及 Scaling Laws
然后再开始着手大规模的训练
为此
MiniMax 采用了 1500 到 2500 台 H800 GPU
并且在训练过程中动态调整GPU 的数量
在训练中
MiniMax也面临了许多的挑战
其中最大的挑战就是长上下文的训练
首先，对于 MoE 架构
最主要的优化目标是降低通信负载
尤其是对于采用 all-to-all（a2a）通信的 MoE 模型
MiniMax 的解决方案是采用一种基于 token 分组的重叠方案
通过对专家权重和数据并行的精细划分
设计了专家张量并行 (ETP) 和专家数据并行 (EDP) 两个进程组
从而实现了存储和计算的最佳平衡
其次，对于长上下文训练
一个很大的挑战是
难以将真实的训练样本标准化到统一的长度
传统的方式是进行填充
但是这种方法非常浪费计算
MiniMax 的解决思路是进行数据格式化
其中不同的样本会沿着序列的维度首尾相连
他们将这种技术命名为 data-packing
这种格式可以尽量地降低计算过程中的计算浪费
为了提高训练效率
MiniMax还采用了token-drop的策略
给每个专家分配一个容量限制
指定它最多可以处理的token数量
一旦达到这个容量
任何额外路由到该专家的token都将被丢弃
最后
为了实现大规模的 Lightning Attention 训练
MiniMax 还采用了四项优化策略
包括分批内核融合、分离式的预填充与解码执行、多级填充、跨步分批矩阵乘法扩展
其中分批内核融合
指的是将多个内存密集型的内核融合
并且扩展支持所有的批量输入
减少中间结果的存储和内存访问操作
分离式的预填充和解码执行
指的是将长度为 1 的 token与长度大于 1 的 token 分开处理
并且使用不同的 CUDA 流进行调度
从而提高计算效率
多级填充指的是根据输入序列的长度
动态选择计算规模
最小化填充开销
跨步分批矩阵乘法扩展
指的是利用 NVIDIA cuBLAS 库中的优化函数
并且集成张量内存加速器 TMA 的异步操作
来提高计算效率
这些优化策略使得模型能够在GPU集群上高效运行
在英伟达H20上达到超过75%的MFU
同时保持模型的性能和推理效率
最终
MiniMax打造出了拥有4560亿参数的MiniMax-Text-01模型
总共有32个专家
每个token都会激活其中459亿个参数
通过三阶段的训练方法
模型的训练上下文窗口可以达到100万token
而在执行推理的时候
上下文长度最高可以外推到400万token
是其他模型的20到32倍
我们再来看看MiniMax的性能测试结果
在常见的学术测试集上
MiniMax-Text-01的表现可以与GPT-4o、Claude 3.5 Sonnet等顶尖闭源模型
以及Qwen2.5、DeepSeek v3、Llama 3.1等顶尖开源模型相媲美
甚至在某些方面更胜一筹
比如在GPQA Diamond数据集上
MiniMax-Text-01取得了54.4的成绩
超过了大多数开源指令微调的大模型
以及最新版本的GPT-4o
而在MMLU、IFEval和Arena-Hard这些测试中
MiniMax-Text-01也取得了前三名的成绩
展示了强大的知识应用能力和对人类偏好的理解能力
此外，长上下文理解任务上
MiniMax-Text-01的表现更为突出
在Ruler和LongBench v2这两个常见基准上
当上下文长度在64k或者更短的时候
MiniMax-Text-01与其他SOTA模型表现相当
而当上下文长度超过128k时
MiniMax-Text-01的优势就明显体现出来了
在长文本学习能力方面
MiniMax-Text-01也达到了SOTA水平
在MTOB基准上
MiniMax-Text-01的表现也同样出色
在实际应用案例中，比方说
在翻译一门新几内亚的小众语言Kalamang时
MiniMax-Text-01能够根据提供的指令、语法书、单词表和对照例句
给出与标准答案基本一致的翻译结果
而在长对话记忆任务中
MiniMax-Text-01能够准确地记住对话中的细节
并且做出相应的回应
这次基于MiniMax-Text-01
MiniMax还开发了一个多模态版本
MiniMax-VL-01
这个模型的思路很简单
就是在文本模型的基础上
整合一个图像编码器和一个图像适配器
简单来说
就是将图像转换为大语言模型能够理解的token形式
MiniMax-VL-01的整体架构符合ViT-MLP-LLM的范式
其中MiniMax-VL-01 作为基础模型
使用了一个303M参数的ViT模型
以及一个随机初始化的两层MLP projector
为了确保MiniMax-VL-01的视觉理解能力足够得好
MiniMax还设计了一个专有数据集
并且实现了一个多阶段的训练策略
最终
MiniMax-VL-01在各个基准上的表现
可以与其它SOTA模型媲美
并且在某些指标上达到最佳
不过
MiniMax也提出了01模型的局限性
一是长上下文评估
当前长上下文检索任务的评估数据集主要为人工或简化场景设计
实际应用中
比如对文档分析的长文本推理能力的评估
仍然有限
MiniMax计划在更现实的场景中增强长上下文检索
并且在更广泛的任务中扩展长上下文推理的评估
二是模型架构
模型目前仍然保留了1/8的组件
使用传统的softmax注意力
对此
MiniMax表示正在研究更高效的架构
可以完全消除softmax注意力
有可能实现无限的上下文窗口
而且无需额外的计算开销
三是复杂的编程任务
当前模型在高级编程任务上的性能任然需要改进
因为预训练阶段的编码数据集仍然有限
除了这些以外
MiniMax还在探索如何将长上下文能力应用到多模态任务中
毕竟，在现实生活中
多模态的任务远比纯文本的任务更为常见
随着多模态token的加入
AI Agent也将逐步进入物理世界
MiniMax的创始人闫俊杰曾经表示
下一代人工智能将是无限接近通过图灵测试的Agent
交互自然
触手可及，无处不在
总的来说
这次MiniMax-01系列的两个模型
展示了处理长上下文方面的卓越性能
以及处理更长上下文上的潜力
相信很多观众会很想了解一下MiniMax-01和DeepSeek v3的对比情况
我也替大家整理了一个对比表格
大家可以暂停观看一下
不过在推理模型方面
MiniMax还没有发布相关的模型
不知道接下来是否会有相关动作
好了，本期视频内容就到这里
希望能够帮助大家更好地理解MiniMax的开源模型
以及AI Agent的未来发展方向
感谢大家的观看，我们下期再见！
