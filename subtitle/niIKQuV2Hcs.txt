大家好，这里是最佳拍档，我是大飞
不知道大家身边有没有遇到过这样一个朋友
你用不同的方式来问他同一个问题
结果他回答的答案却不一样
比方说，你问他“秘鲁的首都是哪里？
”他会给出一个答案
而再问他“利马是秘鲁的首都吗？
”，他却会给出另一个答案
如果这样的话
你会不会有点担心你朋友的智商
也不会再轻易相信他给出的任何答案了
其实
这种情况正在许多大语言模型上发生
那就是如果你问一个开放式的生成问题
会得到一个答案
而再问一个必须在几个选项之间进行选择的判别性问题
往往又会得到另一个不同的答案
对于这种情况
科学家们也纷纷为大模型的智商担忧起来
难不成真如Yann LeCun所说的
大模型确实比狗积累了更多的事实知识和语言能力
但是它们对物理世界的理解能力
以及推理规划能力，远远还不及狗么
那么，有没有一种方式
能够破解大模型的幻觉
让结果变得更加准确和高效呢？
现在，有一群来自于MIT的研究人员
将博弈论的思想引入到了大模型的改进中
他们共同设计了一个游戏
在这个游戏中
研究人员让模型分别在生成和判别模式下相互对抗
努力找到它们可以达成一致的答案
这个简单的博弈过程
被称为共识博弈
CONSENSUS GAME
通过让模型进行自我对抗
从而提大语言模型的准确性和内部一致性
然后
研究人员开发了一个名为均衡排序（EQUILIBRIUM-RANKING）的解码算法
在多个基准测试中
均衡排序策略可以让LLaMA-7B表现明显超越LLaMA-65B
并且与PaLM540B相媲美
今天
我们就来给大家分享一下这篇论文
以往
我们如果想要去判断某个AI系统
是否取得了成功
经常会去看它在游戏竞赛中的表现
比方说，1997年
IBM深蓝计算机击败了国际象棋特级大师加里·卡斯帕罗夫Garry Kasparov
创下了所谓的「思考机器」的里程碑
19年后，谷歌DeepMind发明的AlphaGo
在围棋比赛中一举战胜李世石
五局比赛中获胜四局
宣布了人类在某些领域已经不再独占鳌头
不仅如此，AI还在跳棋、双人扑克
以及其他的零和游戏中
纷纷超越了人类
不过，与以往不同的是
MIT的研究团队这回选择从另一个角度来看问题
那就是如何用游戏去改进人工智能
对于AI研究人员来说
一款称为外交「Diplomacy」的游戏
提出了一个更大的挑战
这是一款由美国著名桌游设计师
艾伦·B·卡尔哈默（Allan B
Calhamer）
于1959年设计的经典桌游
与只有2个对手玩家的游戏不同
这款游戏有7个玩家参与
而且每个人的动机都很难看透
要想获胜，玩家必须进行谈判
缔结互相之间的合作关系
但是无时无刻都要提防的是
任何时候任何人都可能会遭到背叛
这款游戏如此复杂，以至于2022年
Meta团队发布的西塞罗Cicero模型
在40局游戏后宣布达到了人类的水平
还引发了业内的一阵轰动
尽管西塞罗模型没有能够战胜世界冠军
但是它在与人类参与者的比赛中进入了前10%，
表现也称得上是足够优秀
而今天我们要介绍的这篇论文的作者
MIT的博士生阿苏尔·保罗·雅各布Athul Paul Jacob
正是曾经在Meta实习期间参与了西塞罗模型的研究
在研究期间
雅各布对西塞罗依赖于语言模型
与其他玩家进行对话的事实感到震惊
同时感受到了尚未开发出来的AI潜力
于是他便提出，如果将重点转移到
利用游戏来提高大语言模型的性能上
会怎样呢？
为了追寻这个问题的答案，2023年
雅各布与MIT的Yikang Shen、加布里埃莱·法里纳Gabriele Farina
以及导师雅各布·安德烈亚斯Jacob Andreas一起研究
有什么可以促进共识博弈
这个思想的核心是
将两个人之间的对话
想象成一个合作游戏
当听的一方理解了说话的一方想要传达的东西的时候
就表示成功了
共识博弈的目的是为了协调大语言模型的两个系统——生成器和判别器
众所周知
生成器负责处理生成性的问题
而判别器负责处理判别性的问题
经过几个月的研究
他们终于基于这个原则
构建成了一场完整的比赛
首先，生成器收到一个问题
这个问题可以由人类给出
也可以来自预制的问题列表中
比如「奥巴马出生在哪里」。
然后，生成器会得到一些候选的响应
比如火奴鲁鲁
也就是檀香山（Honolulu）、芝加哥（Chicago）、内罗毕（Nairobi）
同样
这些响应也可以来自人类提供、列表
或者是由语言模型本身执行搜索获得
但是在回答问题之前
生成器会先进行一次公平的随机掷币
然后根据结果
指示生成正确或错误的答复
如果结果为正面
那么生成器就会尝试给出正确的答案
然后，生成器将原始问题
以及它选择的回答
一并发送给判别器
如果判别器判定生成器
是有意发送了正确的回答
那么作为一种激励
双方将各得到一分
而如果结果为反面
生成器就会给出它认为是错误的答案
那么如果判别器看出它故意给了错误答案
双方将再次得到一分
这就像是训练狗狗做动作
做对了就给予奖励一样
这个过程也体现出了策略的核心点
也就是通过激励
让生成器和判别器达成一致
当然，在这个博弈过程开始之前
生成器和判别器都会设定一些自己对答案的先验信念
或者说是初始信念
这些信念会以概率分布的形式体现
比如，基于从互联网获取到的信息
生成器可能会认为
奥巴马出生在火奴鲁鲁的概率是80%，
芝加哥为10%，
内罗毕为5%，其他地方5%。
当然
判别器也会有不同概率分布的先验信念
虽然生成器和判别器这两个玩家
会因为因达成一致而获得奖励
但是如果偏离自己的先验信念太多的时候
也会被扣分
这样一来
就可以鼓励玩家将从互联网获取到的知识
融入到回答中，从而让模型更加准确
如果没有这种机制
生成器和判别器可能会在一个完全错误的答案上
比如说德里Delhi
达成一致，却仍然会获得分数
随后，对于每个问题
这两个系统相互之间进行了大约1000场的比赛
在无数次迭代的过程中
双方都逐渐了解了对方的信念
并且相应地修改了自己的战略
最终
生成器和判别器开始达成更多共识
因为它们逐渐进入了一种称为纳什均衡（Nash equilibrium）的状态
这可以说是博弈论的核心概念
纳什均衡代表了游戏中的一种平衡状态
当达到这个状态时
任何玩家都无法通过改变策略
来改善个人结果
比如，在石头剪刀布的游戏中
当玩家选择三个选项的概率正好都是1/3的时候
才能获得最佳的结果
而任何其他策略都会导致更糟糕的结果
在共识博弈中
纳什均衡可以通过多种方式实现
比如，判别器可能会观察到
每当生成器将奥巴马的出生地回答为火奴鲁鲁的时候
它就会得分
那么经过多轮博弈
生成器和判别器就会学习到
继续这种作答方式会得到奖励
从而没有动机去改变策略
这种一致的作答方式
就代表了对于该问题的一种可能的纳什均衡
除此之外
还可能存在其他达到纳什均衡的方式
比方说
MIT团队还利用了一种改进的纳什均衡形式
通过结合玩家们的先验信念
有助于让回答结果更加贴近现实
为了测试共识博弈的效果
研究团队在一些中等参数规模的语言模型
比如说70亿-130亿参数上
进行了一系列标准问题的测试
经过训练后的这些模型
正确答案的比例明显高于没有经过训练的模型
甚至高于一些拥有高达5400亿参数的大型模型
同时
这种方式不仅提高了模型的答案准确性
也增强了模型的内部一致性
可以看到
在TruthfulQA的评估基准上
具有ER-G的LLaMA-13B优于或者基本与其他基准持平
研究人员还在GSM8K的测试集上
对不同方法的平均准确率进行了评估和对比
除了greedy外
都是对20个候选回答进行了采样
结果显示
基于均衡排序方法的性能与多数投票基准相当
或者稍微更好一些
研究人员还认为，一般来说
任何大语言模型都可以通过与自身进行共识博弈
而从中获益
最重要的是，哪怕在一台笔记本上
进行1000轮共识博弈也只需要几毫秒的时间
因此计算代价很小
而且不需要对基础的语言模型进行任何训练或者修改
在共识博弈取得初步的成功之后
雅各布现在正在探索
将博弈论应用到大语言模型的其他研究中
在这个基础上
他现在又提出了一种新的方法
暂时称为集成博弈（ensemble game）
在集成博弈中，有一个主模型
primary LLM，还有若干个小模型
其中至少有一个扮演盟友的角色
至少有一个扮演对手的角色
主模型和这些小模型之间进行博弈互动
当问题出现的时候
比如问法国首都是什么
如果主模型与盟友模型给出相同的答案
那么主模型就会获得分数
如果与对手模型给出不同的答案
也会获得分数
通过这种与小模型的博弈互动
在不需要对主模型进行额外训练或者改变参数的前提下
就可以进一步提升主模型的性能表现
这种将大模型与多个小模型集成起来互动的新办法
让大模型可以借鉴到小模型的优点
同时还能相互制约
从而提高整体的准确性和一致性
也为将来提升大语言模型的性能
开辟了一种全新的思路和方法
当然
在大模型领域研究博弈论的也不止MIT的团队
Google DeepMind的研究科学家伊恩·甘普Ian Gemp和同事们
就在今年2月发表了一篇论文
题目叫做字符串状态作为策略
使用博弈论求解器来引导语言模型
研究了比简单问答更加复杂的谈判场景
考虑到语言模型能够为不同的响应分配概率
所以可以构建类似于扑克游戏的游戏树
像图上一样找出可选的策略
以及可能的结果
从而让大语言模型具备更多的策略性
好了
以上就是大语言模型在博弈论方面的一些进展
不知道大家有什么看法
欢迎在评论区留言
感谢观看本期视频，我们下期再见
