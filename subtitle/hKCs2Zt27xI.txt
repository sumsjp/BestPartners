大家好，这里是最佳拍档，我是大飞
常看我们频道的观众应该都知道
自从2017 年 Attention Is All You Need 一文引入注意力机制
Transformer架构横空出世
成为了现代大语言模型发展的关键基石
不过
就像再强大的战士也有弱点一样
Transformer 在面对长序列处理时存在着严重的缺陷
难以扩展到更长的上下文
为了解决这个问题
谷歌的研究团队最近提出了一个新的架构
Titans
创新性地引入了神经长期记忆模块
有望解决Transformer的长序列处理难题
今天大飞就来为大家解读一下这篇论文
在介绍Titans 的架构之前
我们先来看看传统架构的工作方式是什么样子的
论文的研究人员指出
大多数现有架构都是将记忆
视为由输入引起的神经更新过程
学习则是在给定目标下有效获取有用记忆的过程
以循环神经网络RNN为例
它具有向量值记忆模块
也就是隐藏状态
在时间t接收到新输入 X_t的时候
会经过更新记忆和检索记忆两个步骤
而Transformer 架构其实也有类似之处
它可以看作是具有不断增长记忆的架构
它的步骤包括通过将键和值附加到记忆中来更新记忆
然后通过查找查询向量与键向量的相似性
来检索查询向量的对应记忆
并且利用这些记忆的加权值向量
来生成最终输出
但是
记忆其实是分为短期记忆、工作记忆和长期记忆的
它们在不同场景下发挥着不同作用
并且具有不同的神经结构
为此
Titans的研究人员提出了两个关键问题
一是如何巧妙地设计一个高效的架构
将这些不同但是又相互关联的记忆模块
整合在一起
二是是否需要一个深度记忆模块
以便更加有效地存储和记住长期历史信息
为了解决这些问题
谷歌团队设计了一个独特的神经长期记忆模块
这个模块的设计灵感
来源于人类的长期记忆系统
它能够存储和检索过去的信息
不过
它并不是一股脑儿地记住所有的信息
而是会通过一种称为“惊讶度”的机制
来选择性地记住那些重要或者令人惊讶的信息
那这里的“惊讶度”是怎么衡量的呢？
其实是相对于输入的梯度，梯度越大
说明输入数据与过去数据的差异就越大
也就越值得被记住
而且
这个记忆模块也不是一成不变的
它会根据新的信息来动态更新
就像人类在不断学习新知识的过程中
会更新自己的记忆一样
这使得模型能够很好地适应新的数据和任务需求
为了更好地管理有限的内存资源
研究人员在模块中还引入了衰减机制
这个衰减机制会根据记忆的大小和数据的惊讶程度
来调整记忆的权重
比如说
如果某个记忆信息的惊讶度较低
而且占用内存较大
那么通过衰减机制
它的权重就会降低
从而优化内存的使用
在设计好神经长期记忆模块后
如何将它高效地整合进深度学习架构就成了关键
于是，Titans 架构应运而生
它主要由三个重要的模块构成
首先是核心模块（Core）
这个模块包含短期记忆
负责主要的数据处理流程
它采用了具有有限窗口大小的注意力机制
就像是一个数据处理的“小助手”一样
能够快速地对输入数据进行初步的处理和分析
其次是长期记忆模块（Long - term Memory）
这就是我们前面提到的那个精心设计的神经长期记忆模块
它的主要任务是存储和记住长远的历史信息
为模型在处理长序列数据时
提供关键的历史背景支持
最后是持久记忆模块（Persistent Memory）
它是一组可学习、但是与数据无关的参数
主要作用是对任务知识进行编码
相当于给模型提供了一个先验知识的“宝库”，
让模型在处理任务的时候
能够借鉴以往的经验
在此基础上
研究者还进一步提出了 Titans 架构的三种变体
每种变体都有其独特之处
第一种是记忆作为上下文
Memory as Context
简称MAC的架构
在这个架构中
核心分支会把对应的长期记忆、持久记忆和当前输入信息
拼接在一起
然后利用注意力机制来处理这个丰富的上下文信息
并且决定哪些信息应该存储到长期记忆中
在测试的时候
与上下文记忆对应的参数负责持续学习
与核心分支对应的参数负责上下文学习
而持久记忆的参数由于已经编码了任务相关知识
所以是固定不变的
第二种是记忆作为门控
Memory as Gate
简称MAG的架构
在这个架构里
有一个分支专门利用输入数据来更新长期记忆
而另一个分支则使用滑动窗口注意力SWA
最后
这两个分支的结果会通过门控机制巧妙地组合在一起
这里的滑动窗口注意力
就像是一个精确的短期记忆“守护者”，
而神经记忆模块则作为模型的衰减记忆
这种架构设计有点像多头架构
只不过每个头的结构都不一样
与MAC架构不同的是
MAG架构只会将持久记忆融入到上下文
并且通过门控机制将记忆与核心分支紧密结合起来
这个门控机制就像一个“调节阀”，
决定了来自持久记忆的信息
在多大程度上会影响核心分支的处理结果
第三种是记忆作为层
Memory as Layer
简称MAL的架构
在这个架构中
神经记忆模块会被当作深度神经网络中的一层
并且结合滑动窗口注意力机制
记忆层的核心功能是对过去和当前的上下文信息进行压缩处理
经过压缩处理后的结果会传递给注意力模块
以便进一步的分析和处理
对于神经网络来说
记忆能力可以说是一把双刃剑
一方面
它有助于模型学习和处理信息
但是另一方面
它也可能会限制模型的泛化能力
甚至引发隐私问题
导致在测试时性能下降
这是因为测试数据有可能是分布外数据
那样训练数据的记忆可能就派不上用场了
所以，研究人员认为
训练长期记忆的关键在于将它视为一个在线学习问题
让模型学会在测试时
如何恰当地记住或者忘记数据
在这个学习过程中
模型学习的是一个能够记忆的函数
但是又不会过度拟合训练数据
从而在测试时实现更好的泛化效果
具体来说
模型的学习过程与目标函数是紧密相连的
前面提到的“惊讶度”是相对于输入的梯度
利用这个“惊讶度”，
我们可以用这个公式来更新记忆
从而将过去的信息 x_1到x_t
压缩到长期神经记忆模块的参数中
但是
这种单纯用梯度来衡量惊讶度的方法
还存在着一定的缺陷
有可能会导致错过一些重要信息
因为在经过若干个惊讶步骤之后
梯度可能会变得非常小
模型就会陷入局部最小值
从而错失序列中的某些关键信息
这就好比从人类记忆的角度来看
有些事件虽然值得记住
但是可能并不会一直让我们感到惊讶
为了改进这个问题
论文作者将惊讶度分为了两个部分
一部分是过去的惊讶
用来衡量最近过去的惊讶度；
另一部分是瞬时惊讶
用来衡量即将到来的数据的惊讶度
在这个公式中
项 η_t 是数据依赖的惊讶衰减
它控制着惊讶随时间的衰减情况；
而θ_t 则控制着应该将多少瞬时惊讶
纳入到最终的惊讶度中
这里的数据依赖性非常重要
因为虽然前一个token的惊讶度可能会影响下一个token的惊讶度
但是需要所有token处在同一上下文的时候才有效
所以，通过调整数据依赖的 η
可以控制记忆是否需要忽略上一次的惊讶
还是完全采纳上一次的惊讶
在关联记忆方面
论文作者采用的方法是将过去的数据存储为一个键值对
具体来说，给定 X_t
会使用两个线性层把它投影为键和值
随后通过定义损失函数
来让记忆模块学习键和值之间的关联
通过在元模型的内循环中优化这个损失函数
模型就能够学会如何在测试时记忆键与值之间的映射关系
另外，在处理非常大的序列时
明确哪些过去的信息应该被遗忘
是至关重要的
为此
论文作者设计了一种自适应的遗忘机制
这个机制允许内存遗忘那些不再需要的信息
从而更好地管理内存的有限容量
其中α_t是一个灵活控制记忆的门控机制
决定了应该遗忘多少信息
当α_t 趋向于 0 的时候
就会更新记忆且不影响过去的信息；
而当α_t趋向于1的时候
就会清除整个记忆
而在检索记忆的时候
作者采用了一种简单有效的方法
也就是使用不更新权重的前向传递
来检索与查询对应的记忆
形式上，对于给定的输入 x_t
模型会使用线性层 WQ 来投影输入
然后通过这个方式从记忆 y_t 中检索出对应的信息
说了这么多的理论和机制
那么 Titans 架构在实际任务中的表现究竟如何呢？
接下来我们就来看看实验结果
在语言建模及常识推理任务中
研究人员对 340M、400M、760M 等不同参数规模下的 Titans 变体
与多种基线模型进行了对比
在非混合模型里
Titans (LMM) 在困惑度和准确率上表现优异
在混合模型的对比中
Titans 的三个变体都比基线模型表现得更好
其中
MAC 和 MAG变体的整体性能高于 MAL
它们在整合注意力和记忆模块方面更为出色
在 S - NIAH 任务里
研究人员基于 RULER 基准测试
对 2K、4K、8K 和 16K 长度的序列进行了评估
结果显示
神经记忆模块相较于基线模型具有显著的优势
在 Titans 变体中，MAC 的性能最佳
比如在 2K 序列长度的时候
TTT 的评估指标为 98.4
而 Titans (MAC) 则达到了 99.2
并且随着序列长度的增加
优势依然明显
在BABILong 基准测试中
Titans (MAC) 展现出了卓越的性能
它能够有效扩展到超过 200 万的上下文窗口
超过了 GPT - 4、Llama3 + RAG 和 Llama3 - 70B 等大模型
而且 Titans (MAC) 的参数量远少于这些基线模型
充分展示出了在长序列推理方面的高效性和强大能力
在微调设置环节
Titans（MAC）的表现也更为出色
进一步证明了其在实际应用中的潜力
研究人员还发现
增加记忆的深度能够提升模型在较长序列上的性能
并且改善困惑度
但这也会导致训练的速度降低
说明性能与效率之间需要权衡
在 Simba 框架中进行的实验里
通过替换 Mamba 模块
并且在 ETT、ECL、Traffic 和 Weather 等基准数据集上进行测试
神经记忆模块超越了所有的基线模型
在 DNA 建模任务中
Titans 架构同样展示了强大的长序列处理能力
实验结果表明它能够有效地利用历史信息
从而提高模型的性能
为了进一步探究各个组件的重要性
研究人员还进行了消融研究
结果表明
神经记忆模块的所有组件对模型性能都有积极的贡献
特别是权重衰减和动量
在语言建模和常识推理任务中
MAC 和 MAG 表现相近
但是MAC 在长上下文任务中表现最佳
最后
我们再来给大家介绍一下这项研究的作者团队
阿里·贝鲁兹Ali Behrouz 目前是康奈尔大学计算机科学系的二年级博士生
同时也是 Google Research 的研究实习生
在哥伦比亚大学读硕士的时候师从玛戈·塞尔策Margo Seltzer 教授
他的研究方向是深度学习架构、图表示学习、医疗机器学习以及计算神经科学等领域
Peilin Zhong 现为谷歌算法与优化团队的研究科学家
他本科毕业于清华姚班
在哥伦比亚大学获得博士学位
在 2016 年
他以第一作者发表的论文被顶会 STOC 2016 接收
创下了中国本科生首次在 STOC 上发表一作论文的记录
在学术领域展现出了极高的天赋和潜力
瓦哈卜·米罗克尼Vahab Mirrokni 目前领导谷歌在纽约的算法与优化团队
包括市场算法、图挖掘和大规模优化小组
此外
他还担任纽约大学库朗研究所的兼职副教授
讲授互联网算法与经济学
在学术和工业界都有着丰富的经验和深厚的影响力
好了
以上就是对Titans这篇论文的解读了
期待这个架构能够为长序列处理带来新的思路和突破
感谢大家的观看，我们下期再见
