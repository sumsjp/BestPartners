大家好，这里是最佳拍档，我是大飞
按照Sam Altman的说法
OpenAI即将在今年夏天发布GPT-5
但是这个模型的技术情况究竟如何
现在大家基本上都是一头雾水
不过
最近SemiAnalysis发布的一篇硬核技术博客
似乎掀开了OpenAI秘密训练的新模型的面纱
在这篇文章里
不仅提到了OpenAI正在研发一个介于GPT-4.1和GPT-4.5之间的全新模型
更是指出下一代推理模型o4的训练策略
发生了重大转变
而背后的核心驱动力
正是近年来备受关注的强化学习技术
那么这个正在秘密训练的模型
会在发布前被改叫GPT-5么
还是OpenAI会陆续发布两个模型呢？
今天大飞就来给大家解读一下文章
看看强化学习究竟正在如何改写大模型的研发范式
以及AI领域又将经历怎样的技术变革
我们先来聚焦一下OpenAI的新模型动态
根据SemiAnalysis的爆料
OpenAI目前正在训练一个新的模型
它规模处于GPT-4.1和GPT-4.5之间
2023 年 3 月 14 日
OpenAI推出了多模态大模型GPT-4
而后续的GPT-4.1更多的是在效率和特定任务上进行了优化
比如说降低了推理的成本
而现在这个全新模型的定位就很有意思了
它没有盲目的追求参数规模的爆炸式增长
而是选择了一个中间地带
为什么会出现这样的策略调整？
文章指出关键还在于算力瓶颈
因为OpenAI规划的"星际之门"超算系统还没有建成
所以2025年的计算集群规模
无法支撑像GPT-4那样的超大规模预训练
但是这并不意味着OpenAI会停下模型迭代的脚步
反而让他们把重心转向了更高效的训练方式
多个实验室的研究表明
中等规模模型在强化学习中的反馈循环速度
反而比超大模型更快
这就意味着
通过优化训练方法而非单纯的扩大模型的规模
同样可以实现性能上的突破
并且随着强化学习规模的持续扩大
这些稍大的模型不仅会拥有更强的学习能力
MoE的稀疏度也会更高
更值得关注的是OpenAI的下一代推理模型o4
对于以往OpenAI的o系列模型
比如o1和o3来说
都是基于GPT-4o进行的强化学习
但是o4的基础模型却转向了GPT-4.1
原因在于基础模型决定了性能的下限
因此
用来进行强化学习的基础模型越好
最终效果也越好
所以o4也将标志着OpenAI在推理模型训练方面的策略转变
这里面还有个关键的权衡
那就是强化学习需要海量的推理计算和大量的序列采样
如果目标模型的规模过于庞大
那么强化学习的成本将会变得极其高昂
因此GPT-4.1虽然在绝对性能上可能不如更大的模型
但是它的推理成本极低
而且在代码任务等基准测试中表现优异
比如在代码编辑工具Cursor中
GPT-4.1就被广泛的应用
展现出了强大的实际应用价值
所以，OpenAI显然意识到
在商业落地中
模型的效率和特定任务上的性能
有时候比单纯的参数规模更加重要
接下来的问题是
为什么强化学习会成为这次技术变革的核心呢
实际上
强化学习并不是一个新的技术
早在AlphaGo战胜人类棋手的时候
它就展现过强大的威力
但是将它应用到大语言模型上
却是最近几年才突破的方向
简单来说
强化学习让模型能够通过"试错"来优化自己的行为
通过让模型在特定环境中生成多个候选答案
然后根据预设的奖励函数来调整参数
从而让模型产生高奖励答案的概率更高
在大语言模型中，强化学习的优势
在可验证奖励的领域表现得尤为明显
比如编码和数学任务
以OpenAI的o3模型为例
它在识别照片拍摄地点的任务上表现惊人
而这种能力并非是通过专门训练获得的
而是强化学习所带来的"副作用"。
这说明强化学习不仅能够提升特定任务的性能
还能够解锁模型的某些潜在能力
另一个典型的案例是DeepSeek的R1模型
它通过组相对策略优化GRPO算法
在保持推理效率的同时
大幅提升了代码的生成能力
在GRPO算法中
模型需要回答一个问题
并且针对这个问题生成多个候选答案
每个答案都可以看作是一次推演
rollout
本质上是模型在尝试寻找解决方案
针对每个问题的推演次数
可以从几次到上百次不等
虽然没有什么技术上限
但是推演次数越多
占用的内存和计算资源也就越多
由于每个问题都要生成海量的答案
这就使得强化学习成为了一种推理密集型任务
接下来
模型生成的答案会与一个标准答案进行比对评分
在GRPO中
每个答案都会获得一个奖励分数
计算出奖励分数之后
模型会通过梯度下降算法来进行更新
从而提高生成那些能够获得正向奖励的答案的概率
应该说
GRPO 是近端策略优化（PPO）的一种变体
它不需要PPO中的critic model
因此内存的效率更高
实际上
PPO和GRPO既可以采用学习出来的奖励模型
也可以使用基于规则的奖励系统
来评判答案的质量
不过，由于内存的需求较低
所以开源社区会广泛采用GRPO
但是顶尖实验室大多还是会继续使用PPO的各种变体
无论如何
强化学习的核心思想都在于
通常需要一个问题、一个用于核对的标准答案
以及一种向模型传递信号的机制
来指导模型行为调整的方向
由于模型探索答案的方式多种多样
但是都要求以多次不同推演的形式
来生成多个候选答案
因此对推理端的资源要求很高
随后
模型会通过更新来提高正确答案的出现概率
所以这个过程也隐含了训练的环节
话说回来
强化学习在可验证奖励的领域
之所以能够取得长足的进步
原因之一就在于这类任务的奖励函数很容易定义
比如数学题的答案不是对就是错
但是，从技术上讲
奖励函数可以是用户想要优化的任何目标
从概念上讲
强化学习模型的主要目标
就是最大化总奖励
不过
如果涉及到了更宽泛的任务目标
那么如何定义奖励函数就开始更像是一门「玄学」了
因为它实在难以恰到好处地把握
即便是在一个目标明确的环境中
要想设定出一个理想的奖励函数
也需要大量的研究、测试和优化
因此，强化学习的核心挑战
其实就在于这个奖励函数的设计
比如在芯片设计中
谷歌的AlphaChip需要最小化线长、拥塞度和密度等多个指标
每个指标的权重都需要工程师通过大量的实验才能确定
而在一些更复杂的场景
比如自然语言的生成方面
奖励函数可能会涉及到多个模糊的评价维度
这时候OpenAI会采用"评判员模型"（LLM-Judge）
来模拟人类的评分
比如在医疗问答任务中
他们就曾经用260名医生制定的HealthBench标准
来训练模型
说到奖励函数
就不得不提"奖励黑客"（Reward Hacking）的现象
这也是强化学习中最棘手的问题之一
我们再之前做的Lilian Weng的一期节目中也详细的阐述过
简单来说
模型会发现奖励机制的漏洞
通过"投机取巧"的方式来获取高分
而不是真正的完成目标任务
比如，我们给一个机械臂的任务
是让它将红色的积木叠在蓝的色积木上
结果它通过把积木倒着放
来提高红色积木底面的高度
从而钻了奖励标准的空子
包括在语言模型中
Claude 3.7也曾经被发现过
会通过修改测试用例而非代码来通过测试
这种现象的根本原因就在于
人类很难完全定义适合复杂任务的奖励标准
以OpenAI的o3模型为例
它虽然在信息检索方面的能力十分出众
但是幻觉的问题也非常严重
原因就在于它在训练中只关注了最终答案的正确性
而忽视了推理过程的逻辑性
模型可能通过错误的逻辑链
得出了正确的结果
比如，一个模型即便误解了规则
也可能在一个简单的棋盘游戏中获胜
从而错误地认为之前有缺陷的推理是可以被接受的
于是
这种机制不仅没有惩罚模型的错误思维
反而对它进行了变相的奖励
这种"蒙混过关"的行为
在强化学习中其实是很难被检测出来的
因为通常奖励函数评估的只是结果而非过程
为了应对这个问题
各个实验室们都采取了多种得策略
比如
Anthropic在Claude 4中引入了更精细的奖励信号
对每个输出Token给予不同的奖励
从而在奖励正确答案的同时
惩罚不正确的逻辑
而OpenAI则利用更强的评判员模型
不仅评估最终的结果
还要检查和验证推理的每个步骤环节
正如达里奥·阿莫代伊早在2016年就指出来的
Reward Hacking是强化学习的一个固有挑战
需要持续的迭代优化和多维度的监控机制
如今，强化学习的规模化应用
其实也对基础设施提出了全方位的挑战
首先是算力需求的转变
预训练需要集中式的超大算力集群
而强化学习的推理过程
比如生成候选答案、评估奖励这些步骤
其实更适合分布式的架构
比如英伟达的NVL72系统
通过共享内存和低延迟设计
可以支持同时处理数百次的推演
这对于需要海量试错的强化学习来说至关重要
而在数据层面
高质量的合成数据也成为了新的护城河
Qwen模型的案例就显示
虽然它公开宣称只用了4000组问答对
但是实际的筛选过程其实消耗了巨大的计算资源
因为每个数据点都需要满足严格的条件
既要尽可能地有挑战性
覆盖广泛的细分领域
同时又要恰好在模型当前能力范围之内
而在那些无法合成数据的领域
实验室甚至需要招聘STEM博士
来人工编写问题和评分标准
这也催生了ScaleAI等数据服务公司的繁荣
强化学习的这种变化
也带来了团队架构的相应调整
OpenAI就合并了研究团队和应用推理团队
因为强化学习需要将产品级的高效推理能力
直接融入到训练流程中
同样
Anthropic和谷歌也对自己的产品团队和内部研究团队
进行了重大的组织架构调整
这种组织变革
本质上还是因为强化学习打破了传统的、从预训练到微调这样的线性流程
让推理和训练成为一个相互依赖的闭环
在强化学习的这个赛道上
各大AI公司也展现出不同的战略选择
OpenAI的核心策略是"数据混合"，
通过多环境批处理技术
在同一个模型中融合代码、数学、搜索等多个领域的训练数据
避免传统的权重合并所导致的能力下降
这种方法虽然对基础设施的要求极高
但是能够保持模型的全面性
比如GPT-4o在多次更新后
持续提升了对复杂任务的推理能力
而Anthropic则走上了一条更加聚焦的道路
将资源集中在代码任务优化方面
他们的Claude 4
之所以在SWE-Bench等编程基准上表现突出
也是因为通过强化学习专门提升了代码生成的逻辑性和效率
这种策略的优势是在特定领域建立自己的技术壁垒
但是风险是可能在通用的能力上落后
谷歌的路径则更偏向于硬件-软件的协同优化
比如它在自家TPUv6芯片的设计中
就直接融入了强化学习的需求
使得它在处理大规模推演的时候
效率更高
同时
谷歌在机器人领域的强化学习应用
比如Gemini Robotics
也展现出对多模态环境的探索
说明他们正在尝试构建一个跨领域的强化学习生态
值得注意的是
小模型领域也出现了很多不同的技术路线
DeepSeek团队（应该是Qwen）就发现
对于参数规模较小的模型来说
知识蒸馏其实比强化学习更加高效
蒸馏通过让小模型模仿大模型的输出分布
在更低的算力下就能实现不错的性能
但是它的缺点是可能出现所谓的"尖峰效应"，
也就是在某些任务上表现优异
而在其他任务上则表现平庸
这说明强化学习也并不是适合用于所有的场景中
而是需要根据模型的定位
来选择合适的技术方案
回到我们一开头提到的OpenAI的新模型
它的出现
应该说标志着强化学习从一个"辅助工具"升级为了"核心的驱动力"。
传统的预训练奠定了模型的基础智能
而强化学习则负责将这种智能
转化为更具体的任务能力
并且支持持续迭代
比如o3模型在发布之后
就通过强化学习新增了多步骤的工具调用能力
这在预训练时代是难以想象的
从行业趋势来看
强化学习也正在重塑AI研发的优先级
过去衡量模型能力的核心指标
大多都是参数得规模和算力的投入
而现在则转向了奖励函数的设计、环境的构建和数据质量
这种转变
意味着即使没有超大规模的算力
通过精细化的强化学习
也能够在特定领域实现突破
这就给中小公司留下了较大的竞争空间
但是呢 文章也指出
我们也要看到
强化学习并不是万能的
在奖励不可验证的领域
比如创意写作、战略规划等等方面
它的效果依然是有限的
需要结合人类反馈和一些更复杂的评判机制才行
此外
长期存在的幻觉问题、算力成本的高企
以及伦理方面的风险
也都是需要持续攻克的难题
好了
以上就是SemiAnalysis这篇文章的主要内容了
从OpenAI新模型的训练计划中我们可以看出
整个AI行业正在经历从蛮力计算向智能优化的转型
强化学习的崛起
意味着我们正在从训练一个无所不知的模型
转向"培养一个能够在特定环境中持续进化的智能体
这种范式的转变
可能不会像预训练那样带来轰动性的性能飞跃
但是会在代码生成、科学研究、自动化工具等实际场景中
催生更为深远的技术落地
对于作为旁观者的我们来说
不仅需要关注新模型的性能指标
也要理解它背后更多的研发逻辑
当算力增长放缓的时候
如何通过算法创新和系统优化来实现突破呢？
当通用智能遇到瓶颈的时候
又如何在专用领域构建起深度的能力呢？
这些问题的答案
或许就藏在强化学习与大模型的深度融合之中
如果大家对强化学习的发展有任何看法
欢迎在评论区留言
感谢大家的观看，我们下期再见
