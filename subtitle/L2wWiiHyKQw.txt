大家好，这里是最佳拍档，我是大飞
在科技日新月异的今天
我们总是追逐着最新的设备和技术
但是不知道大家有没有想过
那些被岁月尘封的“老古董”电脑
也许还会换发新的生命力呢？
今天我要给大家带来一个令人惊叹的科技实验
那就是让26年前的Windows98系统运行前沿的AI大模型Llama
而且还取得了相当不错的成果
让我们先来看看最终效果
在开始详细介绍这个实验之前
我们先来了解一下这个实验的背景
如今，AI技术发展迅猛
大部分的AI应用和模型都依赖于强大的数据中心和高性能的现代硬件
然而
有一个名为EXO Labs的团队却提出了不同的看法
他们认为前沿AI不一定非要依赖高端的数据中心和先进的硬件设施才能运行
于是
他们展开了这个极具挑战性的实验
试图在一台有着26年历史的Windows 98 Pentium II PC上
让Llama模型成功运行
向我们展示了科技的另一种可能性
接下来
我们深入了解一下这个实验的具体过程
首先是硬件设置方面
EXO Labs团队在eBay上花费了118.88英镑（折合人民币大约1087元）
购置了一台Windows 98 Pentium II PC
这台电脑配备了128MB的内存
当他们拿到这台电脑时
面临的第一个难题就是现代的USB键盘和鼠标与这台老电脑不兼容
在Pentium II的那个时代
PS/2接口才是标准的配置
所以团队经过思考
决定放弃现代USB外设
转而使用PS/2外设
但是这里又出现了一个有趣的小插曲
他们发现必须将鼠标插入PS/2的端口1
键盘插入端口2
否则系统无法正确识别这些外设
这就好像是在古老的科技迷宫中寻找出路
每一步都充满了挑战
但也正是这些挑战让这个实验变得更加有趣和有意义
解决了外设的问题之后
接下来面临的挑战是
如何将文件传输到这台老旧机器上
因为要在这台Windows 98设备上运行大模型
就必须传输模型的权重、分词器配置（tokenizer configs）和推理代码等重要文件
团队尝试了许多种现代的解决方案
但是都以失败告终
比方说
系统无法识别可重写磁盘（RW disks）
4TB的USB硬盘也因为FAT32文件系统的限制
无法使用
在经过多次尝试后
他们最终选择了经典的FTP协议
EXO团队在一台M4 MacBook Pro上运行了一个FileZilla FTP服务器
并且通过USB-C转以太网适配器
将它连接到了Windows 98机器上
在设置了静态IP后
就可以通过命令行进行文件传输了
为了确保两台设备能够正常通信
他们还进行了ping测试
结果显示Windows 98与MacBook的延迟小于1毫秒
这表明连接是稳定可靠的
不过
在文件传输过程中还有一个关键问题需要解决
那就是传输过去的可执行文件无法执行
经过一番研究
团队发现需要以二进制模式来传输文件
只需在FTP命令行中输入binary即可解决这个问题
在解决了文件传输的问题以后
编译代码又成为了一个巨大的挑战
团队最初希望尝试使用mingw
这是一个用来在Windows操作系统上开发原生应用程序的开发工具集
它包含了一些GNU工具（比如GCC编译器）
理论上可以为Windows 98和奔腾II编译现代的C++代码
但是遗憾的是
由于奔腾II处理器不支持CMOV（条件传送）指令
所以程序在编译时出现错误或者无法编译
这条路也不得不放弃
于是，团队转而采用更为复古的方案
使用26年前的Borland C++ 5.02
这个IDE虽然只能支持非常老旧的C/C++版本
无法支持现代C++，
但是好在C语言在近几十年间变化相对较小
最大的一次变更还是1999年的C99标准
这种旧版本C语言的主要限制
是无法在函数中的任意位置声明变量
所有变量都必须在函数开头声明
不过，尽管存在这些限制
但是Borland C++ 5.02依然承担起了编译的重任
在编译过程中
团队还借助了Andrej Karpathy开发的llama2
c
llama2
c是一个仅有700行的纯C代码的推理引擎
可以运行基于Llama 2架构的模型推理
不过
为了让它能够在奔腾II和Windows 98上顺利运行
团队对它进行了一系列的优化调整
比方说
他们将long long替换为DLONGWORD
把所有变量声明移动到了函数开头
简化了磁盘到内存的加载
并且使用GetTickCount()函数
替换了clock_gettime来解决时间戳的问题
经过这些优化之后
团队得到了一个llama 98
c项目
这个项目的代码已经在GitHub上开源
大飞我会把链接地址放到视频简介中
经过重重困难和不懈的努力
团队终于成功地将代码编译为了可以在Windows 98上运行的可执行文件
那么，实际的运行结果如何呢？
在奔腾II CPU上运行测试的时候
当运行一个260K参数量的Llama模型时
这套系统在Windows98上达到了每秒39 token的生成速度
虽然260K的模型规模相对较小
但是考虑到这是在一台350MHz的单核古董PC上运行
这样的速度已经相当可观了
然而，当升级到15M参数量的模型之后
生成速度降到了略高于每秒1个token
而当尝试运行Llama 3.2的1B参数量模型时
速度则降到了极其缓慢的每秒0.0093 token
尽管速度远不及如今的ChatGPT
但是能够让现代AI模型在25年前的CPU上运行
这无疑也是一个重要的里程碑
那么
EXO Labs团队为什么要进行这样的实验呢？
这家成立于2024年9月的组织
主要成员来自牛津大学的研究人员和工程师
他们有着一个伟大的愿景
那就是让每个人都能平等地获取AI技术
在他们看来
目前AI技术主要由少数几家巨头公司掌控
这对文化、事实真相
以及社会的其他基本方面都可能产生不利的影响
所以
他们希望构建一个开放的基础设施
用来训练前沿模型
并且让任何人能够在任何地方运行它们
通过这次Windows 98的AI实验
他们向大家展示了在有限的资源下
训练和运行AI能力的可能性
也为实现AI普惠化迈出了重要的一步
除此以外
EXO Labs团队还提出了一种新的人工智能模型架构
BitNet
与传统的深度学习模型使用浮点数来表示权重不同
BitNet的权重值只使用三个可能的值
分别是0、-1和1
每个权重只需要1.58比特的存储空间
这种简化的权重表示
可以显著减少模型的存储需求和计算量
从而使得模型能够在硬件资源有限的设备上运行
比如
70亿参数的BitNet模型只需要1.38GB的存储空间
虽然对于一台26年前的奔腾II来说
这可能仍然是一个巨大的负担
但是对于现代硬件、甚至是十几年前的设备而言
都已经不算个事了
而且，BitNet以CPU优先为设计理念
微软的BitCPP可以在M2 Ultra CPU上实现每秒52 token的生成速度
在Intel i7上生成速度为每秒18 token
更令人惊叹的是
100B参数的BitNet可以在单个CPU上
以每秒5到7个token的人类阅读速度运行
并且它比全精度模型能效高50%以上
尽管目前还没有任何基于BitNet的开源大模型
但是EXO Labs坚信
三值模型是实现普惠AI的未来
他们也希望看到更多的努力
在老旧硬件上能运行起AI模型
不过这方面还有大量的工程工作需要去探索和完善
比如优化内存使用、寻找更高效的新架构等等
通过这个实验
我们看到了科技的顽强生命力和无限可能性
即使是26年前的老旧设备
也能在经过巧妙的改造和努力后
运行现代的AI模型
这不仅让我们对科技的发展有了新的认识
也为未来AI的普及和发展带来了新的思考和方向
好了
以上就是对Exo Labs和这次实验的介绍了
希望可以带给大家更多的一些灵感
把家里的老旧设备再次利用起来
感谢大家的观看，我们下期再见
