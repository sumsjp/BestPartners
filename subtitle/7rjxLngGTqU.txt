大家好，这里是最佳拍档，我是大飞
今天我们要聊的话题
可能会改变你对人工智能的认知
想象一下
当AI开始像人类一样用语言“思考”，
我们能读懂它们的想法吗？
更重要的是
这种“可读性”能持续多久呢？
最近
40多位来自全球顶尖AI机构的科学家
包括OpenAI、Meta、Google DeepMind、Anthropic的研究人员
罕见地联手发表了一份立场文件
这份文件甚至让诺奖得主和图灵奖得主都纷纷署名支持
是什么让这些平时竞争激烈的“对手”们
放下分歧
共同发出呼吁呢？
答案就藏在三个字里
那就是“思维链”。
7月15日
这份关于推理模型思维链的文件正式公布
如果大家经常关注AI领域
就会知道这些机构之间的竞争有多么激烈
挖墙脚、技术封锁是常有的事
但是这一次，他们却站到了一起
文件的作者名单堪称“星光熠熠”，
包括图灵奖得主约书亚·本吉奥（Yoshua Bengio）、Google DeepMind联合创始人沙恩·莱格（Shane Legg）
以及OpenAI的首席研究官马克·陈（Mark Chen）
更值得注意的是
四位专家推荐人同样分量十足
分别是诺贝尔奖得主杰弗里·辛顿（Geoffrey Hinton）、OpenAI前联合创始人、如今已是SSI CEO的伊利亚·苏茨克维尔（Ilya Sutskever）、Anthropic的AI安全和评估团队组长、兼美国纽约大学副教授的撒母耳·R·鲍曼（Samuel R
Bowman）
以及OpenAI前联合创始人、如今是Thinking Machines Lab首席科学家的约翰·舒尔曼（John Schulman）
看到这样的阵容，你可能会问
这份文件到底说了什么
能有这么大的号召力？
简单来说，它聚焦了一个关键的问题
那就是AI已经能够用人类的语言来模拟思考过程
而我们监控这些“思考”的窗口
可能正在关闭
于是，这些科学家们呼吁
必须抓紧时间深入研究AI推理模型的思维链监控
因为这可能是我们确保AI安全的一个重要机会
那么，究竟什么是思维链监控？
为什么它对AI安全如此的重要呢？
要想回答这些问题
还要从AI的“不透明性”说起
一直以来
AI系统的决策过程就像是一个黑箱一样
我们知道它的输入和输出
却很难理解中间的计算过程
这种不透明性滋生了很多的潜在风险
如果我们能够解读这些系统的“思考”过程
那么应对风险就会容易得多
大模型天生就会使用自然语言来处理信息
人们原本期待它们会比其他AI方法更加容易理解
而思维链的发现
让这种期待变得更加具体
思维链能够鼓励模型展示它的思考过程
这不仅能够提升模型的能力
还能够增加自然语言中相关计算的比例
不过，这里有个区别需要我们注意
那就是有些非推理语言模型生成的思维链
可能只是为了显得有用且无害
而被筛选出来的，可靠性非常有限
而推理模型则不同
它们在采取行动或者生成最终的输出之前
会经过显式训练
在思维链中进行扩展推理
在这些系统里
思维链可以被看作是模型计算中的潜在变量
在强化学习的训练阶段
这些潜在状态的处理方式和激活状态十分类似
不会被直接监督
而是通过对最终答案的贡献进行间接优化
这意味着
思维链可能包含一些模型输出中不会展示的信息
比如不当行为的意图
而这正是思维链监控的价值所在
通过读取这些思维链
标记可疑或者有潜在危害的响应
进而拦截或者替换为更安全的操作
当然，思维链监控也不是万能的
就像模型某一层的激活状态
不能代表整个推理过程一样
思维链的推理轨迹也可能是不完整的
甚至偏离自然语言的表达
但是它无需完全展现实际的推理过程
就能成为一个附加的安全层
仔细解读还能揭示部分的推理过程
那为什么思维链可能会具备可监控性呢？
主要有两个原因
一是有时必须“展示思考的过程”，
某些任务如果没有思维链
模型就无法完成
尤其是那些需要长时间推理的复杂任务
这可能让不当行为更容易被检测
二是有时倾向于“展示思考的过程”，
即使不是必须
模型也可能外显推理过程
虽然这种倾向性不是很稳定
但是仍然可能发现其他方式无法察觉的不当行为
说到复杂任务
就不得不提到工作记忆
这是一种存储中间推理结果的系统
能够支持进一步的推理和处理
对于AI实现复杂规划和推理能力来说
至关重要
在几乎所有的前沿AI模型使用的Transformer架构中
任何足够长的连续认知步骤链
都必须在某个时刻经过思维链
理论上
使用思维链能够显著增强Transformer模型的能力
对于需要大量串行推理的任务
这样也能够确保思维链中包含模型推理时使用的信息
不过，这里有个疑问
就是我们能理解这些信息吗？
AI模型可能会给某些符号赋予完全不同的含义
让人类无法解码
阻碍监控
但是在实践中
模型在预训练时会学习到强大的“自然语言先验知识”，
会倾向于像人类一样使用思维链
所以当前的AI模型具备一定外化推理的特性
对于足够困难的任务
Transformer模型必须用思维链作为工作记忆的一种形式
而且默认情况下人类是能够理解的
但是要注意的是
外显推理属性并不能够保证可监控性
思维链中可能只有看似无害的推理
不良意图的推理可能被隐藏掉了
而且这种特性对未来的模型是否适用
也不确定
所以持续评估前沿模型的可监控性是非常重要的
另外
最严重的危害是否需要复杂推理
目前还没有定论
文件认为
面对那些需要提前规划、并且试图避免被发现的风险
比如自我渗透或者破坏
模型很可能需要复杂推理
但是并非所有的危险行为都需要推理
尤其是当模型被常规性地赋予高风险任务的时候
在实践中
思维链监控已经给安全研究带来了许多积极的影响
比如检测模型的异常行为
当模型出现偏差
像是利用奖励函数漏洞、操纵数据或者受到提示注入攻击的时候
往往会在推理轨迹中有明确的表达
思维链监控能够比仅监控模型的行为
更有效地捕捉这些情况
再比如发现偏差的早期信号
当模型为了追求人类不认可的目标
而假装合理的时候
让人们及早察觉到
还有发现模型评估中的缺陷
对大模型智能体的评估
可能会受到记忆效应或者虚假失败的干扰
这时候复审推理轨迹
能够帮助识别评估结果是否反映了实际部署时的行为
但是还有个问题尚不明确
那就是这些可监控性有多少是源自于“展示思考过程”的必要性
又有多少是源自于倾向性呢？
文件担心
未来的模型可能会出现隐藏推理过程的动机
压倒自然的倾向
比如知道自己被监控
或者训练目标导致跳过重要的推理步骤等等
不过即使这种倾向减弱
面对需要思维链推理的艰巨任务时
监控可能仍然是有效的
不过
思维链的可监控性可能依然是很脆弱的
这取决于当前前沿模型的训练方式
而且可能会以多种方式被削弱
比如扩大强化学习的规模
可能会偏离清晰可辨的思维链
像以前
在预训练之后要接受RLHF的范式
优化的压力主要来自于人类生成或评估的数据
但是如果大幅提升推理训练的计算资源
可能就无法有效的激励可解释性了
从人类数据转向其他数据
本质上可能会改变思维链的可监控性
有研究表明
通过基于结果的奖励微调的语言模型
可能会偏离清晰的英语表达
再看对思维链的直接监督
当模型仅用基于结果的强化学习训练时
只要输出正确就会获得奖励
推理过程除了帮助得出正确答案
不会受到其他的优化压力
但是过程监督会直接塑造推理过程
比如缩短思维链、采用有效步骤等等
这可能会让思维链难以准确的反映决策过程
当然
过程监督也可能让模型以更安全的方式推理
帮助监督难以评估的任务
所以未来需要更多研究
来平衡模型推理在安全应用和可监控性方面的影响
还有间接的优化压力
即使奖励不是直接从思维链计算得出的
训练也可能对思维链产生影响
比如在Transformer架构中
生成思维链和生成输出的权重部分可能会共享
所以优化最终输出可能也会影响到思维链
如果向评估模型输出的人类展示思维链
可能还会影响人类偏好
进而影响奖励模型
此外
用思维链监测来修补奖励操纵漏洞
也可能在多轮训练中给思维链带来所谓的“进化压力”。
另外
模型架构的改进也可能会降低可监控性
最近有研究探索了增加模型在连续潜在空间中的推理序列深度
这样推理模型可能就不需要用语言来表达想法
自然就失去了思维链带来的安全优势
鉴于当前我们对思维链可监控性的理解还处于初级阶段
文件中也提出了一些需要解答的问题
并且倡议开发者开展相关的研究
比如如何评估思维链的可监控性
此前的忠实度评估有一定的局限性
不能区分倾向性和必要性
也多是针对于简单问答
文件还建议探索新的方法
包括直接衡量可监控性和针对特定属性的方法
并且列举了四种示例
第一种是思维链可读性评估
比如用另一个模型
来评估智能体思维链的清晰度和连贯性
第二种是潜在推理能力评估
考察模型在不外部化输出的时候
在高推理需求任务和智能体任务中的表现
第三种是衡量思维链的因果相关性
比如扰动一段思维链
观察对最终输出的影响
输出变化率低
可能就说明思维链是“虚构”或者“表演”性质的
还可以交叉验证思维链监控器
检查它认为重要的符号
是否可以被白盒可解释性方法识别为关键内容
第四种是可监控性的端到端评估
直接衡量监控器在智能体环境中
捕捉不当行为的有效性
还有如何判断可监控性是否足以作为安全措施
这个方面仅了解监控器的准确性是不够的
因为即使误报率高
只要能够低成本重新采样模型的输出
也能标记为可疑行为
而且大多数风险存在于敌对者试图逃避监控的情境中
评估时要纳入这些因素
比如让红队来参与压力测试
其他需要研究的问题还包括
哪些训练时的优化压力会降低可监控性
在可监控性不显著下降的前提下
允许什么程度和类型的直接与间接优化压力；
模型是否会故意模糊推理过程
需要进行压力测试
来看看它们能否在不针对监控器训练的情况下
隐藏恶意推理；
哪些架构会降低可监控性
研究非标准架构的影响以及保留部分可监控性的条件
比如混合方法在利用潜在计算时
可以保持部分的可见性；
以及如何构建更好的思维链监控器
比如通过零样本提示
让能力较弱的语言模型来充当监控器
利用测试时计算提升性能
或者对抗训练监控器
来应对试图隐藏不当行为的智能体
文件还倡议开发者们跟踪模型的思维链可监控性
将它作为提升安全性的重要部分
具体包括开发有效的评估方法和标准化指标
明确思维链何时具备足够的可监控性；
在系统说明卡中公布评估结果
定期运行评估并且报告方法、结果和局限性；
在训练和部署决策中纳入可监控性指标
结合能力和安全性评估来进行整体的风险评估
不过
对于让思维链可见以便用户发现并报告问题的做法
文件暂时没有给出明确的建议
因为这可能会带来反效果
比如迫使人们直接监督思维链
让它“看起来很安全”，
从而破坏了可监控性
需要强调的是
思维链监控是一个补充方案
而非替代方案
目前来看，它还有不少的限制
比如在不需要展示思考的时候
虽然能够检测一些不当行为
但是无法提供有力的安全证据；
针对需要推理的危险任务监控
可能无法捕捉所有的危害
因为有些危险任务不需要太多的推理
而且随着模型在激活过程中进行更复杂的推理
需要外部化推理的任务会减少；
在更高级的模型中
思维链监控可能失效
模型可能会抑制“展示思考过程”的倾向
模糊推理的过程；
另外，坚定维护当前的思维链模式
可能会牺牲更有效的安全干预措施
比如某些过程监督可能会通过干预推理
来显著改善模型的对齐性
所以仅靠维持现有的可监测性
可能会错失这些价值
因此
文件建议将思维链监控作为现有安全研究的一个补充
而非替代
换句话说
所有的监督和监管方法其实都有自己的局限性
可能会让一些不当行为漏网
所以未来AI智能体的安全措施
可能需要多个监控层
而且最好有互不相关的故障模式
总的来说
思维链监控是一项宝贵的补充
让我们能够更加了解AI的决策过程
但是目前的可见度能否持续还是不确定的
业内人士还需要充分研究如何保持这种可监控性
好了以上
就是这份由四十多位科学家
联合撰写的文件的核心内容
它让我们看到了AI安全领域的一个重要机遇
也提醒我们这个机遇可能转瞬即逝
对于AI的发展
安全始终是不能忽视的底线
而思维链监控为我们提供了一条窥探AI“思考”的路径
如何抓住并且守护这条路径
还需要整个行业的共同努力
感谢大家观看本期视频
我们下期再见
