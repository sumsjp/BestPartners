大家好，这里是最佳拍档，我是大飞
过去两年
整个 AI 行业因为大模型而狂踩油门
参数量从百亿推到万亿
算力预算也从几千万美金
飙升到几十亿美金
然而，我们也能看到
最近一段时间新出的模型
能力增长逐渐放缓
简单粗暴地堆砌算力和数据
带来的边际效益不断递减
很多人又开始怀疑起
大模型的Scaling Laws是否已经失效
难道，我们真的撞上数据墙了吗？
DatologyAI 的创始人阿里·莫科斯 Ari Morcos 给出的答案是
数据是 AI 研究中影响最大、但是投入却最少的领域
在Latent Space的播客节目中
他反复强调的一个核心观点是
模型吃进去什么，就会成为什么
他认为
与其无休止地堆砌算力、陷入规模化收益递减的陷阱
还不如回归本源
通过极致的Data Curation
让模型能吃得更好，学得更聪明
Data Curation这个词其实还挺难翻译
我看有叫数据管护、数据策展、数据保管、数据监护等等的
感觉都差那么点意思
所以今天就暂时用英文了
那么，为什么数据如此重要呢？
在算法、算力、数据的三驾马车中
为什么阿里会认为
数据才是那个最被低估的变量呢？
今天大飞就来带大家回顾一下这期访谈的内容
看看对于如今的大模型来说
数据究竟有多重要
要想理解阿里为什么押注于数据
要先回顾一下他的学术经历
阿里的博士背景不是计算机
而是神经科学
他的博士课题是训练小鼠「数数」，
然后记录成千上万个神经元的活动
尝试解释计数行为背后的动力学
并以此来研究智能的生物学基础
这段经历让他养成了用经验科学来思考问题的习惯
也就是先通过实验去理解系统
再利用这种理解去改进系统
为了处理这类高维数据
他开始学习并且深入机器学习的领域
2011 年之后
AlexNet、DQN 等里程碑接连出现
让他下定决心转向 AI
带着经验科学的思维
阿里想要为深度学习建立一套「可解释的科学框架」，
让大家不仅知道什么好用
更要理解为什么好用
因此，他理想中的论文
前半段应该是解释原理
后半段再根据原理来改进模型
不过，现实却很快给他泼了一盆冷水
因为弄清楚为什么有效其实并不算难
真正难的
是用这种理解去把系统做得更好
这让他意识到
很多看似与性能相关的可解释指标
一旦直接去优化
往往优化的只是相关性，而非因果性
这种挫败感在 2020 年到达了顶峰
他也意识到
数据，才是其中的决定性因素
当时他研究的是归纳偏置
也就是如何通过改动模型的架构或目标
把先验知识注入模型
让模型在小数据场景下能够学得更好
这也是当年的主流方向
其中
他的一个工作是把 ViT 权重等效初始化成 CNN
这样既能拥有 CNN 的归纳偏置
又能在训练中逐步忘掉这种偏置
结果却耐人寻味，他发现
在小数据场景下
比如小于50万张图的时候
软性归纳偏置很有用
这类方法后来在火山预测等数据稀缺的科学任务上
被频繁的引用
但是一旦数据量上来之后
这种优势就逐渐消失了
尤其当样本超过百万数量级的时候
精心设计的偏置甚至开始拖后腿了
相比之下
Transformer 也自带了较少的归纳偏置
却在超大数据上表现亮眼
于是阿里开始反思
他研究了六年的偏置
在大数据时代似乎并不怎么关键
这正是苦涩的教训（The Bitter Lesson）在他身上的真实写照
能更好利用算力与数据的通用方法
最终会胜过依赖人类专家知识的特定技巧
他在访谈中说道
这对我来说是一个非常痛苦的时刻
我花了六年职业生涯研究归纳偏置
但是好几篇论文同时告诉我
我一直在做的事情，其实没那么重要
冷静之后，他的眼前只剩下两条路了
要么把 GPU 转得更快
要么去研究数据
对于一名非硬件工程师而言
答案自然是后者
另外，数据研究最吸引他的一点在于
科学上有趣的问题
往往和实践中有用的问题会高度重合
比如，理解一个数据点为什么有用
几乎可以立刻指导数据集的改造
从而提升模型的性能
这种看上去的知行合一
让他选择把职业筹码压在了数据上
在进入这个研究领域之后
阿里发现了一个残酷的事实
那就是相对于数据的影响力来说
数据在 AI 研究中其实是长期投入不足的
而他也看到
这背后其实是科研文化、激励机制和历史惯性
所共同作用的结果
首先是文化偏见
数据工作往往被视作二等工作
既是脏活累活苦活
也被是看做是所谓的管道工程
缺乏顶级科学家所追求的荣耀感
所以很多人觉得
数据清洗是一种无聊的、重复性的劳动
但是其实不少的一线研究者也会承认
要想做出好的结果
第一件事就是把数据搞明白
模型终究只是它所见过的数据的镜像
只是这种琐碎又关键的工作
在文化上并没有得到应有的尊重
其次是研究激励的错位
长期以来机器学习的范式都是
给定一个数据集
比如ImageNet 或者 Kaggle
然后去优化测试集的表现
在这种逻辑下
数据往往被当作一种常量
创新也自然集中在模型与算法两个方面
最后则是时代背景的变化，2019 年前
业内的主流是监督学习
那是一个数据稀缺的时代
有标签的数据成本高
而且因为有人类的参与
质量至少有下限
但是，自从自监督学习崛起之后
游戏规则被彻底改写了
无论是语言里的预测下一个词
还是视觉里的对比学习
模型都开始从无标注的数据里进行自我学习了
随之而来的
就是数据规模从百万级跃迁到万亿级
增长了百万倍
这也导致 AI 的核心矛盾
从数据稀缺开始转为数据过多
也带来了相应的一系列后果
比如由于数据多到模型永远学不完
导致模型更容易欠拟合
抓取来的互联网数据充满了冗余、低质甚至有害的信息
导致数据的质量下限消失
还有就是
过去的Scaling Laws都是基于一个理想化的假设
那就是所有的数据都是独立同分布的
也就是假设所有数据点的价值都是相等的
但是这一点已经开始失效了
于是，当我们来到数据过多的时代后
如何去除冗余数据
又该如何度量信息的增益
这些过去不显眼的问题就成了头号的难题
显然
我们对于模型垃圾进、垃圾出（Garbage in
Garbage out）的朴素常识理解
与前面这些假设是不兼容的
因此，Data Curation的重要性
被推到了一个前所未有的高度
阿里提到
很多人会把数据工作等同于数据的筛选或者清洗
但是在他看来
这些工作只是冰山一角
真正的Data Curation其实是一个更系统的工程
它涉及到多个环节的步骤
首先是过滤(Filtering)，
我们需要识别并且剔除掉低质量的、低信息增益的数据
然后是重均衡（Rebalancing）
由于现实中的数据经常会呈现长尾形态
如何对它进行上/下采样
让模型学到完整的分布而不是只学到头部的模式
第三是序列化（Sequencing）
因为喂给模型的先后次序很关键
这让课程学习（Curriculum Learning）重焕生机
在一个永远欠拟合的时代
合理的编排顺序
能够让模型的开发人员用更少算力达成同等的效果
第四是合成数据（Synthetic Data）的使用
需要考虑如何用模型来生成高质量的合成样本
从而增强原始的数据集；
最后是批处理（Batching）
如何组织和处理批次
同样会影响学习的速度
此外，对于Data Curation
阿里认为
要重点理解冗余和自动化两个概念
我们先来说数据冗余
在我们提过的Data Curation的几个环节中
过滤的核心挑战之一就在于如何处理冗余
完全去除冗余显然是错误的
因为它会损害模型的泛化能力
但是无限的冗余，同样也是一种灾难
阿里用了一个形象的比喻
那就是大象和狗
大象的形态差异较少
主要分为亚洲象与非洲象
让模型学会大象的概念
不需要海量的样本
太多重复也只会是浪费
但是狗则完全不同，狗有数百个品种
每个品种之间的体型、毛色、形态
都差异巨大
要让模型真正理解狗这个概念
所需要的数据量和冗余度
显然要远高于理解大象的概念
而一个优秀的Data Curation系统
需要在无监督的条件下
自动发现成千上万个类似大象和狗的概念
评估各自的复杂度
再决定每个概念应该保留多少冗余
这对于人类来说是一个很难胜任的任务
有人可能会说
请一些专家让他们挑出来不就好了吗？
阿里却认为
这套复杂的系统必须是自动化的
甚至要刻意排除人的干预
他引用了斯坦福的 DCLM（DataComp for Language Models）项目作为例子
在这个项目中
大约 30 位顶尖博士生用两年的时间
搭建了一个自动筛选高质量网页文本的系统
最后做了个测试
让这些刚研究完筛选策略的专家
去预测系统会保留还是剔除掉某条样本
测试的结果是
这些专家的预测准确率
和随机猜测根本没有什么区别
为什么会这样呢？
阿里的解释是，一个数据点的价值
其实并不是由它本身决定的
而是由它与训练集中所有其他数据点的关系所决定的
举例来说
你就算有一万篇《哈姆雷特》的剧情摘要
每篇的质量都很高
但是模型真的需要一万篇吗？
人类无法在脑中装着整个数据集
来进行这种全局的权衡判断
但是算法可以，机器可以
因此，Data Curation必须是自动化的
不仅是因为规模
更因为人类在对这类问题的判断上并不可靠
除了过滤以外
在Data Curation的技术环节里
合成数据也是眼下最热门的方向之一
但是也充满了争议
尤其是关于对模型坍塌Model Collapse的担忧
为了解释这部分的原因
阿里将合成数据分为了两种截然不同的范式
第一种是「从无到有」。
这种方法能让一个大模型凭空生成新的知识
但是这是危险的
因为它极容易导致模型坍塌
生成模型会倾向于过拟合数据分布的众数(modes)，
而欠拟合长尾(tails)。
如果用它生成的数据再进行训练
会加剧导致多样性的不断丧失
最终模型只会输出千篇一律的内容
第二种是「转述或重写」，
这种方法更为安全
它的核心思想是
知识来源于原始数据
而非生成模型
模型扮演的角色
仅仅是将原始数据中的信息
用一种更清晰、更结构化、或更符合下游任务的形式
重新来组织一遍
因此，对于一个模型来说
它只需要知道如何转述就行了
甚至不需要理解内容本身是什么
这意味着
你可以用一个相对较弱的模型
去生成能教会一个更强模型的数据
这个观念打破了传统知识蒸馏中
学生无法超越教师的天花板
因为知识的源头是高质量的原始数据
而非教师模型
Datology AI近期的Beyond Web论文
就系统总结了他们在合成数据上的七点体会
第一点
合成数据并不等同于知识蒸馏
简单摘要虽然也可以把单位 token的信息密度提上去
做出类似用生成来驱动数据集的效果
但是精心设计的改写策略
通常能够走得更远
第二点，要想真正的破墙
得靠好的数据
如果只让模型续写网络上的文本
相当于重复的数据
收益很有限
真正能够突破质量瓶颈的
是那些可以填补原始分布空白的合成样本
第三点，高质量的种子很重要
但是不是全部
以好的数据为源头
改写后的质量显著会更好
但是只靠好的源头还不够
还要有合适的策略组合
第四点，风格匹配虽然有用
但是天花板很快出现
互联网上的对话体不到 5%，
可对话是主要的交互方式
因此
适当提高对话比例确实有一定的收益
但是这个收益也会迅速饱和
第五点
数据的多样性决定了持续收益
单一的改写
比如把内容全部都转成问答形式
在训练的早期可能有效
但是很快会出现瓶颈；
只有采用多样的策略
万亿 token 的长程训练才能够持续提升性能
第六点，改写模型本身的影响不大
即使用不同系列的模型
比如Llama、Mistral、OLMo来当改写器
产出的合成质量也相差不多；
而且改写器自身的性能强弱
与它产出数据的最终价值
也并不是线性相关的
第七点，小模型其实也够用
改写器从 1B 升到 3B 时的收益明显
但是从 3B 升到 8B 则会趋近于饱和
也就是说
做高质量的合成数据不一定需要参数规模很大的模型
从而显著降低了门槛
基于这些经验
BeyondWeb 在 8B 模型实验中给出了亮眼的结果
用 BeyondWeb 数据来进行训练
速度比普通的网络数据快了7.7 倍
甚至在 BeyondWeb 上训练的 3B 模型
表现能够超过在其他数据集上训练的 8B 模型
这无疑展示了Data Curation的杠杆效应
回到自己的公司Datology
阿里用三个词来概括了DatologyAI 对客户的价值
那就是更快、更好、更小
（Faster/Better/Smaller）
首先是Datology的训练更快
这不只是节省了几百万美元的单次训练费用
更关键的是迭代速度的提升
原本需要 10 天的训练
如今可能一夜就能完成
这样也让实验的迭代次数
可以随之呈指数级的上升
其次是Datology训练出来的模型性能更好
因为好的数据就如同算力的倍增器
在同样的预算下
用经过Curation的数据
效果会更好
所以对于想用一千万的预算
做出过去一亿预算水平的团队
这点最具有吸引力
最后就是Datology训练出来的模型更小
对于走在AI落地前沿的企业而言
推理的成本是 TCO 的大头
所以一个“参数减半但是效果不减”的小模型
相比通用大模型来说会更具商业上的优势
举个例子
如果一年在推理上花 5000 万美元
但是部署的模型比所需要的大了一倍
那就等于白烧了 2500 万
而重新训练一个同等性能的专用小模型
可能只需要两三百万
这笔账显然很好算
更何况
很多企业需要的不是一个能写诗、能聊天的通用大模型
而是一个一英寸宽、一英里深的专家模型
这样它才能以 99.999% 的可靠性
用尽可能低的成本
去完成那一小撮的核心任务
阿里更是指出，过去
阻碍企业训练自己模型的有两大障碍
分别是训练的基础设施和数据
而如今
像 MosaicML、Together AI 等公司
已经大大降低了训练的门槛
因此，Datology 的使命
就是推倒另一座大山
也就是数据的屏障
他还举了一个具体的案例
那就是他们与 RC 基础模型的合作
他们从 25 万亿 token的原始池子起步
经过Curation
筛选到 7 万亿 token 的高质量集合
结果不仅模型的性能变得更强了
达到同等水平的训练速度也得到了显著的提升
这说明
Data Curation的收益其实是可以叠加的
即便从当下最好的开源数据出发
依然能够进一步挖出可观的增量
好了
以上就是阿里这次访谈的主要内容了
其实，我们从Datology 公司的名字上
就可以看出阿里他们的野心
Datology就等于Data加上Ology
相当于专注于Data Curation这门学问上
而阿里的愿景
也正是把这门新的学科自动化、工具化
让曾经只在顶级实验室里口口相传的数据秘笈
变成任何想训练自有模型的团队
都能够触手可及的基础设施
我们不妨也换个角度来想想
当行业还在模型和算力上疯狂内卷的时候
真正能够改写游戏规则的
或许正是对数据的重新认识
无论如何，AI的尽头
未必是更大的模型
但一定是更好的数据
一个属于Data Curation的时代
也许正在到来
感谢大家收看本期视频
我们下期再见
