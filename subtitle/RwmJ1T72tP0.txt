大家好，这里是最佳拍档，我是大飞
大约一个月前
OpenAI 的安全系统团队负责人翁荔（Lilian Weng）
宣布从工作了近 7 年的 OpenAI 离职
当时她就表示离职后可能有更多时间的来写博客了
而就在几天前
她更新的一篇关于强化学习中奖励黑客的博客
迅速引起了大家的围观学习
这篇博客之所以受到广泛关注
是因为奖励黑客问题在当前的人工智能发展中具有重要意义
当强化学习Agent利用奖励函数或者环境中的缺陷
来最大化奖励
却没有学习到预期行为的时候
就会发生奖励黑客攻击
在现实世界中
随着我们越来越多地部署自主 AI 模型
这个问题也逐渐成为了主要的障碍之一
翁荔在博客中呼吁对奖励黑客
特别是对大语言模型和基于人类反馈的强化学习中
奖励黑客的缓解策略进行更多研究
博客原文很长
翁荔自己认为全读下来都要37分钟左右
更不用说理解它了
所以今天大飞就来给大家简单的整理和解读一下
希望能对大家有所帮助
要想理解奖励黑客
咱们必须先来说说强化学习中的奖励函数
奖励函数可是定义任务的一个关键
它对学习效率和准确性有着显著影响
设计强化学习任务的奖励函数
就像是一门 “玄学”，
很多因素让它变得极为复杂
比如说，如何把大目标分解成小目标？
奖励是稀疏的好还是密集的好？
又该如何衡量奖励成功呢？
不同的选择可能会导致学习的动态良好
也可能会引发问题
比如任务无法学习
或者奖励函数可被破解等等
早在 1999 年
吴恩达等人就在论文《奖励转换下的策略不变性：
奖励塑造的理论和应用Policy invariance under reward trasnsforamtions:
Theory and application to reward shaping》中
研究了如何修改马尔可夫决策过程
也就是MDP中的奖励函数
来让最优策略保持不变
他们发现线性变换是有效的
简单来说
就是通过对奖励函数进行特定的变换
引导学习算法更高效地学习
同时确保最优策略不受影响
这为我们理解奖励函数的设计提供了重要的理论基础
另一个与奖励黑客密切相关的概念就是虚假相关性
也叫捷径学习
在分类任务中，虚假或捷径特征
可能会让分类器无法按照预期学习和泛化
给大家举个例子
如果所有狼的训练图像中都包含了白雪
那么用来区分狼和哈士奇的二元分类器
可能会因为雪景这个特征而过拟合
从而在分布外测试集上表现很差
有研究表明
最小化训练数据的损失是风险的合理替代
但是这会导致模型依赖所有类型的信息特征
包括不可靠的虚假特征
无论任务多么简单
模型都可能出现这种情况
这就说明
虚假相关性是一个在机器学习中普遍存在
而且需要重视的问题
它与奖励黑客问题相互交织
共同影响着模型的性能和行为
接下来
咱们详细讲讲什么是奖励黑客
近年来，人们提出了几个相关概念
比如奖励黑客Reward Hacking、奖励腐化Reward Corruption、奖励塑造Reward Tampering、规范博弈Specification Gaming、客观鲁棒性Objective Robustness、目标错误概括Goal Misgeneralization、奖励错误指定Reward Misspecifications 等等
它们都指的是某种形式的奖励黑客行为
这个概念最早源于阿莫代伊Amodei 等人在 2016 年的研究
他们将奖励黑客列为了人工智能的关键安全问题之一
指的是Agent通过不良行为
欺骗奖励函数来获得高额奖励的可能性
规范博弈（Specification Gaming）也是类似的概念
即满足目标的字面规范
但是没有实现预期结果的行为
这里要注意区分奖励黑客和奖励塑造
奖励塑造是一种丰富奖励函数的技术
能让Agent更容易学习，但是设计不当
可能会改变最优策略的轨迹
准确地说
设计一个好的奖励函数本身就是极具挑战性的
因为涉及到任务复杂、状态部分可观察、以及多个维度等等因素
为了让大家更好地理解奖励黑客
翁荔给大家列举了一些实际的案例
比如在训练抓取物体的机械手时
它可能学会通过将手放在物体和相机之间来欺骗人
让人们以为它成功抓取了物体；
在训练最大化跳跃高度的Agent时
它可能会利用物理模拟器中的错误
来实现不切实际的高度；
当Agent被训练骑自行车到达目标
并且在接近目标时获得奖励
它可能会选择在目标周围绕小圈骑行
因为远离目标时不会受到处罚；
在足球比赛中
如果Agent触球就分配奖励
于是它会一直保持在球旁边高频触球；
在 Coast Runners 赛船游戏中
Agent控制船进行比赛
当击中绿色方块有奖励的时候
它会改为绕圈航行并反复击中同一方块
在大语言模型的任务中也存在奖励黑客现象
比如用来生成摘要的语言模型
可能会利用指标中的缺陷来获得高分
但是生成的摘要却几乎不可读；
编码模型可能会学习更改单元测试来通过编码问题
甚至直接修改用来计算奖励的代码
现实生活中也不乏这样的例子
社交媒体推荐算法本来应该提供有用的信息
但是由于用点赞、评论数量等代理指标
来衡量有用性
最终可能就推荐离谱、极端的内容来增加用户的参与度；
针对视频网站的错误指标进行优化
可能会大幅增加用户观看时间
却偏离了优化用户主观幸福感的真正目标；
2008 年房地产泡沫引发的金融危机
人们试图玩弄金融体系
这也是一种社会层面的奖励黑客攻击
那么，为什么奖励黑客会存在呢？
这里不得不提到古德哈特定律GoodHeart’s Law
它指出， 当一个指标成为目标的时候
它就不再是一个好的指标
一旦对某个指标施加过大的压力进行优化
它就容易被破坏
在强化学习中
指定 100% 准确的奖励目标是非常困难的
任何Agent都面临被黑客攻击的风险
因为强化学习算法会利用奖励函数定义中的小缺陷
古德哈特定律有回归、极值、因果、对抗四种变体
这些变体进一步解释了指标在成为目标后出现问题的不同情况
根据阿莫代伊等人的总结
奖励黑客攻击主要发生在强化学习设置中
原因包括部分观察到的状态和目标
不能完美表示环境状态；
系统本身复杂，容易被黑客攻击；
奖励可能涉及难以学习或描述的抽象概念
高维输入的奖励函数可能会过度依赖几个维度；
以及强化学习的目标是让奖励函数高度优化
这就存在天生的“冲突”，
使得设计良好的强化学习目标极具挑战性
特别是具有自我强化反馈组件的奖励函数
奖励可能被放大和扭曲
破坏原始意图
此外，我们通常也不可能设计出
能够优化Agent行为的最佳奖励函数
因为在固定环境中
可能存在无数个与观察到的策略一致的奖励函数
这就导致Agent可能会找到奖励函数设计中的漏洞
从而出现奖励黑客行为
另外有研究指出，更聪明的Agent
更有能力找到奖励函数设计中的 “漏洞” ，
获取更高的代理奖励
但是真实奖励却更低
相反，较弱的算法可能找不到漏洞
所以当模型不够强大时
我们可能观察不到奖励黑客现象
也难以发现当前奖励函数设计中的问题
有趣的是
即使真实奖励和代理奖励存在正相关性
奖励黑客攻击仍可能发生
接下来
我们再来说说大模型RLHF中的黑客行为
基于人类反馈的强化学习（RLHF）
如今已成为语言模型对齐训练的常用方法
在RLHF中
训练人员会先基于人类反馈数据
训练一个奖励模型
然后通过强化学习对语言模型进行微调
来优化人类偏好的代理奖励
这里有三种类型的奖励值得关注
金牌奖励
代表我们真正希望大模型优化的目标；
人类奖励
是指实践中收集的评估大模型的奖励
但是由于人类反馈可能不一致或者犯错
不能完全准确表示金牌奖励；
代理奖励
是通过人类数据训练的奖励模型预测的分数
继承了人类奖励的弱点和潜在建模偏差
虽然RLHF会优化代理奖励的分数
但是我们最终关心的是金牌奖励的分数
RLHF中的奖励黑客行为
会发生在几个阶段中
首先是训练过程中
RLHF的目标是提高模型与人类偏好的对齐程度
但是人类反馈可能无法体现所有关心的方面
比如事实性，从而可能被黑客攻击
导致模型过度拟合不想要的属性
例如，RLHF 可能会增加人类的认可度
但不一定提升正确度
它还会削弱人类的评估能力
使评估错误率更高
甚至还会让不正确的输出更容易说服人类
使得评估的假阳性率显著增大
模型学会了挑选、编造不真实的陈述
来捍卫错误答案
以及在长问答任务中创建更有说服力的捏造证据
甚至在编码任务中破解单元测试
生成可读性差的代码
来增加人工评估难度
其次，随着大语言模型的能力增强
它也开始作为评估器
为其他生成器模型提供反馈和训练奖励
但是
评估器也可能会发生奖励黑客行为
比如用一个大模型给其他多个模型的输出质量打分时
改变候选者的顺序
就能够黑掉质量排名
GPT-4会倾向于给第一个显示的候选者打高分
而ChatGPT 更喜欢第二个
尽管指令要求确保顺序不影响判断
但是大模型仍然存在位置偏差
为此研究人员提出了多重证据校准（MEC）、平衡位置校准（BPC）、人在回路校准（HITLC）等多个策略
在合理的成本下可以提高准确度
此外
在迭代式自我完善的训练设置中
由于用来评估和生成的模型是相同的
而且会共享参数，可以同时微调
所以容易引发上下文黑客攻击
简称ICRH
因为模型既是运动员又是裁判员
可能会利用上下文 “钻空子”，
导致 AI 评分与实际质量不符
有研究表明
在AI作为论文审稿人的任务中
较小的模型对 ICRH 更敏感
比如 GPT - 3.5 作为审稿人的时候
会比GPT - 4引发更严重的 ICRH；
甚至当AI审稿人
能看到与论文作者相同论的历史记录后
会更容易出现打分不当的情况
研究者进一步发现
ICRH和传统的奖励黑客有两个主要区别
一是发生的时机不同
ICRH 是在 AI 实际使用过程中
通过不断接收反馈而产生的问题
而 奖励黑客是在 AI 训练阶段就出现的
二是产生的原因不同
传统奖励黑客通常发生在 AI 专门做某一件事情的时候
而 ICRH 则是因为 AI 太「聪明」了
会投机取巧了
目前对于ICRH还没有完美的解决方案
最好的做法是在正式上线前就进行充分的测试
但是不能完全解决问题
而且 AI 模型越强大
ICRH 问题反而可能会越严重
除了以上几点以外
另一个有趣但令人担忧的现象是
AI 的 Reward Hacking 技能具有泛化能力
这就像一个学生在某道题上学会了「投机取巧」，
会延伸到其他科目一样
有研究发现，在简单环境中的训练
会加剧模型在其他环境中的投机行为
在某些情况下
当模型完成整个训练后
甚至能够零样本泛化到直接重写自己的奖励函数
好了，说完问题
我们再来说说解决办法
虽然奖励黑客的现象备受关注
但是缓解措施方面的研究相对较少
阿莫代伊等人指出了一些在强化学习训练中
可以减轻奖励黑客的方向
第一是对抗性奖励函数
将奖励函数视为自适应Agent
让它适应模型发现的新技巧
其次是模型前瞻
根据未来预期状态给予奖励
第三是对抗性致盲
让模型 “失明” 无法学习黑掉奖励函数的信息
第四是谨慎工程
通过设计避免针对系统的奖励黑客
比如沙箱化的Agent
第五是奖励随机化
通过添加随机噪声到奖励函数中
减少模型对特定奖励模式的过度拟合
第六是基于模型的强化学习
使用环境模型预测未来状态和奖励
提前检测潜在的奖励黑客行为
第七是保守价值迭代
通过对价值函数更新施加保守约束
防止过度乐观的价值估计导致奖励黑客
第八是修改学习算法
例如采用PPO算法中的 KL 散度约束
来限制策略更新的幅度
避免策略过于偏离
减少因为策略突变而引发奖励黑客的可能性
第九是集成方法
通过训练多个不同的模型并组合它们的预测
来降低单个模型因为奖励黑客而产生的偏差影响
然而
这些方法在实际应用中都面临各自的挑战
比如对抗性奖励函数
需要精心设计对抗策略
否则可能导致训练不稳定
而模型前瞻可能会增加计算复杂度
在复杂环境中难以有效实施等等
另外呢在RLHF中
优化人类反馈的收集过程
对于减轻奖励黑客至关重要
可以采用更细致的人类反馈标注方式
比如不仅仅是简单的二元判断或打分
而是提供更详细的定性反馈
解释为什么某个输出是好或者不好
这有助于模型更好地理解人类的真实意图
减少因模糊反馈而导致的奖励黑客行为
另外
增加人类反馈的多样性也是一个重要方向
从不同背景、不同专业领域的人群中收集反馈
能够使奖励模型更全面地学习人类偏好
降低因特定群体偏好而被黑客利用的风险
例如
在评估一篇科技文章的摘要生成质量时
除了普通读者的反馈
还可以收集专业科研人员的意见
这样就可以避免摘要仅在通俗易懂、但缺乏专业准确性方面
被过度优化
同时
对人类反馈进行一致性检查和校正也不可或缺
由于人类的主观性和不一致性
可能会出现前后矛盾的反馈
通过算法检测并校正这些不一致
可以提高奖励模型训练数据的质量
从而减少奖励黑客的发生几率
例如，可以采用聚类分析等方法
将相似的反馈归为一类
去除孤立的异常反馈点
此外，在奖励模型设计方面
可以探索更鲁棒的架构和训练方法
例如
采用多任务学习的方式来训练奖励模型
使其在多个相关任务上进行学习
增强对不同情境下奖励判断的泛化能力
还可以引入对抗训练机制
到奖励模型的训练过程中
通过设置一个对抗网络来生成可能导致奖励黑客的虚假输入
让奖励模型学习区分真实有效输入和虚假攻击输入
从而提高其抗攻击能力
对奖励模型的输出进行正则化处理
也是一种可行的方法
通过限制奖励模型输出的范围或分布
防止给出极端或不合理的奖励分数
也能够减少模型通过追求极端分数而进行奖励黑客的可能性
好了
以上就是翁荔这篇文章的主要内容了
在人工智能快速发展的今天
强化学习中的奖励黑客现象
是我们必须重视的一个关键问题
无论是在传统的强化学习环境
还是在大模型的 RLHF 过程中
奖励黑客都可能导致模型的行为偏离我们的预期
甚至产生负面的社会影响
虽然目前已经有一些针对奖励黑客的缓解措施被提出
但是这些方法都还存在着许多挑战和有待完善之处
希望通过今天的分享
能够让大家对强化学习中的奖励黑客现象
有更深入的了解
也期待更多的研究人员能够投身到这个领域的研究中来
共同推动人工智能技术朝着更加健康、稳定的方向发展
感谢大家的观看，我们下期再见
