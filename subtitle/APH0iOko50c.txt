大家好，这里是最佳拍档，我是大飞
前两天
著名的SemiAnalysis分析师迪伦·帕特尔（Dylan Patel）和丹尼尔·尼什博尔（Daniel Nishball）
又来爆料行业内幕了
而整个AI社区
再次被这次的消息所震惊
那就是OpenAI的算力比起谷歌来
只能说是小儿科
谷歌的下一代大模型Gemini
算力已达GPT-4的5倍
现在手中没有足够GPU的人
在商业化战争中会铁定出局
根据帕特尔和尼什博尔的说法
此前屡屡被爆料
将成为GPT-4大杀器的谷歌Gemini
已经开始在新的TPUv5 Pod上进行训练了
算力高达10的26次方FLOPS
比训练GPT-4的算力还要大5倍
如果按照手中的TPUv5数量
谷歌已经成为了算力王者
比OpenAI、Meta、CoreWeave、甲骨文和亚马逊拥有的GPU总和还要多！
虽然TPUv5在单个芯片性能上不如英伟达的H100
但是谷歌最可怕的优势在于
他们拥有高效、庞大的基础设施
这个文章发表出来之后
也引来了Sam Altman围观
当然他略有一些嘲讽
说道谷歌竟然让那个叫semianalysis的家伙
发布了他们的内部营销/招聘图表
太搞笑了
这显然是对于谷歌算力超过OpenAI的说法的不满
毕竟谁都会不爽自家的产品被人给比下去
不过此前
迪伦·帕特尔参与的两篇稿件
无一例外都被证实
而且也引发了业内的轩然大波
包括谷歌的内部文件泄漏事件
也就是「我们没有护城河
OpenAI也没有」那篇文章
我们也做过一期视频介绍
这个关于谷歌护城河的真实性
被谷歌DeepMind的首席执行官黛米斯·哈萨比斯（Demis Hassabis）在一次采访中确认了
另外就是关于GPT-4的架构、参数等内幕消息的大泄密
也后来被证实
那么下面就让我们来仔细看看
这次SemiAnalysis的第三弹爆料
又将带来多少重磅内幕消息
文章的标题就叫做《谷歌Gemini吞噬世界
Gemini的算力是GPT-4的5倍》，
其中第一次提出了GPU-Rich和GPU-Poor
也就是GPU富人和GPU穷人的概念
在新冠疫情之前
谷歌发布了一个名叫Meena模型
该模型在短时间内是世界上最好的大型语言模型
与当时最先进的生成模型GPT-2相比
Meena的模型容量提高了1.7倍
训练数据量增加了8.5倍
使用了GPT-2 14倍的FLOPS来训练
不过仅仅几个月之后
OpenAI就发布了GPT-3
参数量增加了65倍以上
令牌数量增加了60倍以上
FLOPS增加了4000倍以上
提出Transformer开山之作「Attention is all you need」的作者之一
同时也是LaMDA和PaLM的关键参与者诺姆·沙泽尔（Noam Shazeer）
曾经受到MEENA模型的启发
写过一篇名为《Meena吞噬世界》的内部文章
在这篇文章里
他准确地预言了ChatGPT的诞生给全世界带来的改变
大语言模型会越来越融入我们的生活
吞噬全球的算力
诺姆写的这篇文章远远领先于他的时代
但是却被谷歌的决策者忽略了
甚至遭到了嘲笑
当时大家的感觉是
谷歌拥有王国的所有钥匙
但是他们却弄丢了装钥匙的袋子
不过现在，沉睡的谷歌已经开始醒来
他们向前迭代的速度已经无法阻挡
在2023年底
谷歌的算力将达到GPT-4预训练FLOPS的五倍
而考虑谷歌现在的基础设施
到明年年底
这个数字或许会达到20倍
谷歌是否会在不削减创造力、不改变现有商业模式的基础上
在这条路上继续深耕？
目前无人知晓
现在，手握英伟达GPU的公司
可以说是掌握了最硬的硬通货
像OpenAI、谷歌、Anthropic、Inflection、X、Meta这些巨头或者明星初创企业
手里有2万多块A100或者H100芯片
平均下来
每位研究者分到的计算资源都很多
有些个人研究者
大概有100到1000块GPU
也可以玩一玩手头的小项目
到2024年底
这些公司GPU总数可能会达到十万块以上
现在在硅谷
最顶级的机器学习研究者自豪的谈资
就是吹嘘自己拥有或者即将拥有多少块GPU
而且在过去4个月内
这股风气越刮越盛
以至于这场竞赛已经被放到了明面上
谁家有更多的GPU
大牛研究员就去哪儿
于是Meta已经把自己拥有世界上第二多的H100 GPU
直接拿来当作招聘策略了
与此同时
数不清的小初创公司和开源研究者
正在为GPU短缺而苦苦挣扎
因为没有足够多虚拟内存的GPU
他们只能虚掷光阴
投入大量时间和精力
去做一些根本没有帮助、甚至没有意义的事
他们只能在更大的模型上来微调一些小模型
来想办法挤进基准排行榜
但是这些模型的评估方法也很支离破碎的
更强调的是风格
而不是准确性、有用性
他们通常不知道
只有拥有更大、更高质量的预训练数据集和IFT数据
才能让小的开源模型在实际工作中得到改进
OpenAI联合创始人安德烈·卡帕希（Andrej Karpathy）曾经这样感慨到
谁将获得多少H100，何时获得H100
现在都是硅谷的顶级八卦
没错，如何高效使用GPU非常重要
许多GPU穷人却忽视了这一点
他们不关心规模效应所带来的效率提升
也没有有效地利用自己的时间
到明年
世界就会被350万张H100所淹没
而这些GPU穷人，将彻底与商业化隔绝
他们只能用手中较弱的游戏GPU来进行学习、来做实验
大部分GPU穷人仍然在使用密集模型
这还是因为扎克伯格慷慨的开放了Llama系列模型
大多数的开源项目情况会更糟
如果他们真的关心效率
尤其是客户端的效率
他们应该选择MoE这样的稀疏模型架构
并且在更大的数据集上进行训练
以及像OpenAI、Anthropic、Google DeepMind这些前沿的大语言模型实验室一样
采用推测解码
处于劣势的公司
应该把重点放在如何提高模型的性能
或者减轻token到token的延迟
提高计算和内存容量要求
减少内存带宽
这些才是边缘效应所需要的
他们应该专注于如何在共享基础架构上
高效地提供多个微调模型
而不必为小批量模型付出可怕的成本代价
然而，事实却恰恰相反
他们过于关注内存容量限制或者量化程度过高
却对模型实际质量的下降视而不见
总的来说，现在的大模型排行榜
已经完全乱套了
虽然闭源社区还有很多人在努力改进这一点
但是这种开放基准已经变得毫无意义
出于某种原因
人们对大语言模型排行榜有一种病态的痴迷
并且为一些无用的模型起了一堆愚蠢的名字
比如Platypus等等
我们希望今后开源的工作能够转向评估、推测解码、MoE、开放IFT数据
以及用超过10万亿个token清洗预训练数据这些方面
否则
开源社区根本无法与商业巨头相竞争
现在
大模型之战的世界版图已经很明显了
美国和中国会持续领先
而欧洲因为缺乏大笔投资和GPU短缺
已经明显落后
即使有政府支持的超级计算机儒勒·凡尔纳也无济于事
而多个中东国家也在加大投资
为AI建设大规模基础设施
当然，缺乏GPU的
并不只是一些零散的小初创企业
即使是像HuggingFace、Databricks
以及Together这种最知名的AI公司
也依然属于「GPU贫困户」。
事实上
如果我们仅看每块GPU所对应的世界TOP级研究者
或者每块GPU所对应的潜在客户
他们或许是世界上最缺乏GPU的群体
虽然它们拥有世界一流的研究者
但是所有人都只能在能力低几个数量级的系统上工作
虽然他们获得了大量融资
买入了数千块的H100
但是这并不足以让他们抢占大部分市场
在内部的各种超级计算机中
英伟达拥有着比其他人多出几倍的GPU
其中
DGX Cloud提供了预训练模型、数据处理框架、向量数据库和个性化、优化推理引擎、API以及英伟达专家的支持
帮助企业定制用例并调整模型
如今
这项服务也已经吸引了来自SaaS、保险、制造、制药、生产力软件和汽车等垂直行业的多家大型企业
即便不算上那些未公开的合作伙伴
仅仅是由Amgen、Adobe、CCC、ServiceNow、埃森哲（Accenture）、阿斯利康（AstraZeneca）、盖蒂图片社（Getty Images）、
Shutterstock、晨星（Morningstar）、Evozyne、Insilico Medicine、Quantiphi、InstaDeep、
Oxford Nanopore、Peptone、Relation Therapeu、Okmab Therapedics和Runway等巨头组成的
这份比其他竞争对手要长得多的名单
就已经足够震撼了
考虑到云计算的支出和内部超级计算机的建设规模
这些企业从英伟达这里购买的GPU数量
似乎比HuggingFace、Together和Databricks所能够提供的服务加起来还要多
作为行业中最有影响力的公司之一
HuggingFace需要利用这一点来获得巨额投资
建立更多的模型、定制和推理能力
但是在最近一轮的融资中
过高的估值让他们并没有得到所需的金额
Databricks虽然可以凭借着数据和企业关系迎头赶上
但是问题在于，如果想要为超过7
000个客户提供服务
就必须增加支出数倍的成本
不幸的是
Databricks无法用股票来购买GPU
他们需要通过即将开始的私募或者IPO
来进行大规模融资
并进一步用这些现金来加倍购买硬件
从经济学的角度来看有些奇怪
因为他们必须先建设
然后才能引来客户
而英伟达同样也在为他们的服务一掷千金
不过，这也是参与竞争的前提条件
这里的关键在于
Databricks、HuggingFace和Together明显落后于他们的主要竞争对手
而后者又恰好是他们几乎所有计算资源的来源
也就是说，从Meta到微软
再到初创公司
实际上所有人都只是在充实英伟达的银行账户
那么
有⼈能把我们从英伟达的奴役中拯救出来吗？
是的，有⼀个潜在的救世主
那就是谷歌
虽然谷歌内部也在使用GPU
但是它的手中却握着其他的王牌
其中，最让业界期待的是
谷歌下一代大模型Gemini
以及下一个正在训练的迭代版本
都得到了谷歌⽆与伦⽐的⾼效基础设施的加持
早在2006年
谷歌就开始提出了构建人工智能专用基础设施的想法
并且在2013年将这一计划推向高潮
他们意识到
如果想大规模的部署人工智能
就必须将数据中心的数量增加一倍
因此
谷歌开始为3年后能够投入生产的TPU芯片去做准备
最著名的项目Nitro Program是在13年发起的
专注于开发用来优化通用CPU计算和存储的芯片
主要的目标是重新思考服务器的芯片设计
让它能够更加适合谷歌的人工智能计算工作
自2016年以来
谷歌已经构建了6种不同的AI芯片
TPU、TPUv2、TPUv3、TPUv4i、TPUv4和TPUv5
谷歌主要设计这些芯片
并与Broadcom进行了不同数量的中后端协作
然后由台积电生产
在TPUv2之后
这些芯片还采用了三星和SK海力士的HBM内存
在介绍Gemini和谷歌的云业务之前
爆料者先分享了关于谷歌疯狂扩张算力的一些数据
包括各季度新增加的⾼级芯⽚总数
对于OpenAI来说
他们拥有的总GPU数量将在2年内增加4倍
而对于谷歌来说，所有人都忽视了
谷歌拥有TPUv4、TPUv4 lite
以及内部使⽤的全系列GPU
此外
这里还没有把TPUv5 lite算进去
尽管它可能是推理较⼩语⾔模型的主⼒
实际上
谷歌拥有的TPUv5比OpenAI、Meta、CoreWeave、甲骨文和亚马逊拥有的GPU总和还要多
并且
谷歌能够将这些能力的很大一部分
出租给各种初创公司
当然，就每个芯片方面的性能来说
TPUv5与H100相比有显著的差距
即使撇开这点不说
OpenAI的算力也只是谷歌的一小部分
与此同时
TPUv5的构建能够大大提升训练和推理能⼒
此外
谷歌全新架构的多模态大模型Gemini
一直在以令人难以置信的速度迭代
据称Gemini可以访问多个TPU Pod集群
具体来讲呢
就是在7+7共14个Pod上进行训练
爆料者表示
初代的Gemini应该是在TPU V4上训练的
而且呢这些Pod并没有集成最大的芯片数
即4,096个芯片
而是使用了较少的芯片数量
以此来保证芯片的可靠性和热插拔
如果所有14个Pod
都在合理的光刻掩模板利用率MFU下
使用了大约100天
那么训练Gemini的硬件FLOPS将达到超过10的26次方
作为参考
爆料者在上次介绍GPT4架构的文章中
曾经详细介绍了GPT4模型的Flops
是略高于2乘以10的25次方
而谷歌模型的Flops利用率呢
在TPUV4上非常好
即使在大规模训练中
Gemini的第一次迭代也远远高于GPT4
尤其是在模型架构具有优越性的方面
比如说增强度模态更是如此
真正令人震惊的
是Gemini的下一次迭代
他已经开始在基于TPU V5的Pod上进行训练了
算力高达10的26次方FLOPS
这比GPT4的训练呢要大5倍
据消息称
第一个在TPU V5上训练的Gemini
在数据方面存在一些问题
所以不确定谷歌是否会发布
而这个10的26次方的模型应该就是谷歌公开称为Gemini的模型
不过呢这还并不是谷歌的最终形态
军备竞赛已经开始了
而谷歌呢有着巨大的优势
如果他们能够集中精力并付诸实施
至少在训练前的计算规模扩展和实验速度方面
他们终将会胜出
他们可以拥有多个比OpenAI最强大的集群还要强大的集群
目前呢谷歌已经拥有的基础设施
不仅可以满足内部的需求
连Authropic等新兴的大模型公司
和一些全球最大的公司
也将使用TPU V5进行内部模型的训练和推理
谷歌也计划将TPU迁移到云业务部门
并且重新树立了商业意识
这也让他们赢得了一些大公司的青睐
未来几个月我们将会看到谷歌的胜利
许多知名的公司会开始采购他们的TPU
以上呢就是这篇爆料的全部内容
其实我们都知道在算力方面
OpenAI并不是最大的
很多大公司的储备呢都比他多
而谷歌表面上这次被OpenAI和微软联手打的比较惨
但是实际上他的财报增长反而要更好
这次谷歌开始整合内部的算力资源
以GPT4 5倍的算力来训练Gemini
也凸显了Google Brain和DeepMind合并后的优势
而且呢就在前两天
谷歌还突然对外披露了公司新一代AI加速器Cloud TPU V5E
这是专门为提供中大型规模的训练和推理所构建的
能够提供更好的成本效益和性能
此外呢谷歌还宣布
基于Nvidia H100 GPU的GPU超级计算机A3 VM
也将于下个月全面上市
关于这些内容呢
我们会专门做一期视频来介绍
只是从目前来看
在经历了Bard的挫折之后
谷歌现在是把宝全部都压在了Gemini身上
既希望于Gemini让谷歌再次走在世界前沿
我们也可以期待一下
看看GPT4 五倍的算力
能训练出一个什么样的AI出来
另外呢就是作者还认为
在这样的巨头竞争下
本质上呢还是算力的竞争
一些初创AI公司如果没有足够的资本和资源
来获得更多的GPU
可能很快就要被市场淘汰了
好了本期的视频就到这里
感谢大家的观看
我们下期再见
