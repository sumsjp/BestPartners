大家好，这里是最佳拍档，我是大飞
今天我们来聊聊OpenAI连续12天圣诞发布会中
可能是最值得关注的产品
也可能是迄今为止
最强的推理模型、o1的升级版，o3
之所以我中间的发布会没有去做节目
主要是内容确实乏善可陈
也就是最后一天的这个o3
还能让人提得起一些精神
更是让一些人看到了接近AGI的曙光
所以我们今天来聊一聊
在今年9月份
OpenAI发布了o1模型之后
仿佛打开了推理模型的大门
随后许多国内外的大模型企业相继都推出了很多推理模型
因为可能会跟英国的电信运营商O2重名
所以这回OpenAI直接把模型的名字
从o1升到了o3
和前代的o1模型一样
o3模型也是利用思维链进行思考
通过逐步解释逻辑推理的过程
总结出它认为最准确的答案
而且这次o3模型推出了完整版和mini版
新的一项功能是可以将模型的推理时间
设置为低、中、高三种模式
当然模型思考时间越长，效果越好
mini版会更加精简
并且针对特定任务进行了微调
将在1月底推出
而o3完整版也将在之后不久推出
也就是说
今天我们介绍的o3，还是期货
不过这并不妨碍我们今天先给大家介绍一下它的纸面性能
这次o3最引人瞩目的
应该还是它在ARC-AGI基准测试下的性能表现了
ARC-AGI是一项用来评估AI系统
推理首次遇到的、极其困难的数学和逻辑问题能力的基准测试
由Keras之父弗朗索瓦·肖莱François Chollet在2019年的论文《论智力的评估》中提出
这个基准测试已经推出了 5 年时间
但是一直没能被攻克
我们可以看一个具体例子感受一下
像这种题目
AI 需要根据配对的输入输出示例来寻找规律
然后再基于一个输入来预测输出
这种图形推理问题很像国内的公务员考试
而在这次的ARC-AGI测试中
o3 已经达到了优良的水平
成为首个突破 ARC-AGI 基准的 AI 模型
具体来说
o3在高推理能力设置下取得了87.5%的分数
在低推理能力设置下的分数也达到了o1的3倍
要知道，之前GPT-3的评测结果为0%，
GPT-4 为2%，GPT-4o为5%，
即便是o1的Pro模式
也仅仅达到了大约50%的成绩
而o3一举将成绩提升到87.5%，
确实令人瞠目
与之前的大模型相比
o3能适应以前从来没遇到过的任务
可以说已经在接近人类的水平
弗朗索瓦·肖莱这次也发布了o3的完整测试报告
o3在两个ARC-AGI数据集中进行了测试
并在两个具有可变样本量的计算级别上进行了测试
分别是高效率的6个样本和低效率、但是计算量为172倍的1024个样本
其中
75.7%的高效率分数在ARC-AGI-Pub的10000美元预算规则范围内
87.5%的低效率分数成本则相当昂贵
但是测试结果仍然表明
新任务的性能确实会随着计算量的增加而提高
不过，报告也指出
ARC-AGI 并不是对 AGI 的严格考验
通过 ARC-AGI 并不等同于实现了 AGI
肖莱也表示，他认为 o3 还不是 AGI
o3 在一些非常简单的任务上仍然会失败
这表明它与人类智能存在着根本的差异
他还对o3 模型的具体工作原理进行了一些推测
o3 模型的核心机制
似乎是在 token 空间内进行自然语言程序的搜索和执行
在测试的时候
模型会在可能的思维链空间中搜索
这些思维链描述了解决任务所需要的步骤
这种方式可能与 AlphaZero 风格的蒙特卡洛树搜索（Monte-Carlo tree search）有着相似之处
实际上
o3可能是由某些深度学习模型引导的搜索
值得注意的是
DeepMind的Demis Hassabis 在 2023 年 6 月的一次采访中暗示
DeepMind其实对这个概念已经研究了很长时间
目前来看
o3模型的使用还不是很经济
用户需要以每项任务大约5美元
折合人民币约36元的价格来支付
而在低效率推理模式下
o3完成每个任务需要花费17-20美元
折合人民币约124到145元
OpenAI明年还将与ARC-AGI背后的基金会
一起合作构建下一代的基准测试
从早期的数据点来看
即将推出的 ARC-AGI-2 基准测试
仍然将对o3构成重大的挑战
即使在高计算量下
o3的得分也可能会降低到 30% 以下
而聪明人在不经过任何培训的情况下
仍然能够得分超过 95%。
不过，在其他的基准测试中
o3的表现也远远超过了竞品
在由真实世界软件任务组成的SWE-Bench Verified基准测试中
o3模型的准确率约为71.7%，
比o1模型高出20%以上
在编程竞赛Codeforces中
o1的分数是1891
而o3在低推理设置下的分数已经超过了o1
在高推理设置下
分数甚至可以达到2727
这个分数转化为人类的智商大约在157
如果按照大家普遍认为的
爱因斯坦的智商在160以上
估计也就他可以跟AI较量一下了
其实我们更应该关注提升的速度
OpenAI今年发布GPT-4o的时候
智商也只有115
属于正常人范围
o1预览版模型达到了123
相当于博士级水平
从完整版的o1开始达到惊人的135
o3mini则突破140大关
也就是说
OpenAI只用了短短7个月的时间
就把AI的智商提升了42分
而如果按照弗林效应Flynn Effect
也就是智商测试的结果会逐年增加的理论来看
人类想把智商提升42分的话
大约需要140年才能完成
而从Codeforces排行榜来看
o3的成绩也能够排到第175名
超过了全球99.8%的程序员
在数学基准测试AIME 2024中
o3的准确率达到惊人的96.7%，
只漏掉了一个问题
而o1的准确率为83.3%，
gpt-4o仅有13.4%。
在衡量博士级科学问题的严苛基准测试GPQA Diamond中
o3的准确率高达87.7%，
比o1的78%提高约10%，
而专业博士通常在自己的强项领域
也只得到70%的成绩
此外
o3还在陶哲轩等60余位全球数学家共同推出的
号称业界最强数学基准的EpochAI Frontier Math中创下了新的纪录
分数达到25.2%，
而其他模型甚至都没有超过2%。
相比来说
o3 mini 是一个更经济高效的 o3 版本
专注在提升推理速度、降低推理成本的同时
兼顾模型性能
通过「自适应思考时间」（adaptive thinking time）机制
o3 mini能够根据任务难度
自动调整推理的深度
它支持三种不同的推理强度选项
分别是低、中、高
与o1模型相比
o3-mini 在 Codeforces 上的性能具有显著的成本效益
比如在中强度模式下
o3-mini（medium）已经超了满血版o1的表现
虽然o3-mini（high）在高强度模式下
仍然落后于o3
但是几乎差别不大
除了代码成绩亮眼
在AIME 2024数学竞赛测试中
o3-mini（low）也已经接近o1 mini的水平
o3-mini（medium）则以78.2%的准确率超越了o1
而o3-mini（high）则进一步提升了性能
在延迟方面表现
o3-mini（low）大幅降低了延迟
不到1秒，足以媲美GPT-4的即时响应
而o3-mini（medium）的延迟比o1-mini快一半
为了满足开发者的需求
o3-mini这次也提供了全套的API功能
包括函数调用、结构化输出、开发者消息等等
难能可贵的是，在这些功能上
o3-mini的性能不仅完全对标o1
并且在多数评测中取得了更好的表现
另外，在GPQA数据集测试中
o3-mini也展现出稳定的性能
即便是在低强度的模式下
o3-mini（low）也达到了62%的准确率
OpenAI的研究科学家任泓宇
还在现场演示了一个使用o3-mini来实现Python代码生成和执行的示例
只用了30多秒
o3-mini就写出了一个自己的ChatGPT UI
通过发送请求来调用API与自己对话
然后再让o3-mini在这个UI中编写并且执行一个脚本
评估自己在GPQA上的表现
结果脚本正确返回了61.62%的数值
与正式的评估结果相近
而且这个低推理能力的模型运行极快
整个评估过程只用了一分钟
虽然o3系列模型不会立即发布
但是OpenAI已经开始向安全研究人员
开放了o3的访问权限
申请截止日期是1月10日
在官方博客中
OpenAI还透露了o3使用的、新的对齐策略的更多技术细节
我们这里也简单介绍一下
我们都知道
现代大语言模型基本都会使用监督微调（SFT）和基于人类反馈的强化学习（RLHF）
来进行安全训练
但是仍然存在很多安全缺陷
OpenAI研究人员认为
其中许多的失败是由于两个限制造成的
一、模型必须立即响应用户的请求
导致没有足够时间来推理复杂和边缘的安全场景；
二、大模型必须从大量标注样本中间接推断出所需的行为
而不是直接学习自然语言中的基本安全标准
这就迫使模型必须从示例中对理想行为进行逆向工程
导致数据效率和决策边界不佳
在这个基础上
OpenAI提出了审议对齐（Deliberative Alignment）的新训练方法
结合基于过程和结果的监督
让大模型在产生答案之前
明确地通过安全规范来进行复杂推理
从而克服前面说的两个问题
相比之下
其他在推理时优化响应的策略
会将模型限制为预定义的推理路径
并且不涉及对学习的安全规范的直接推理
我们再来看一下审议对齐的具体步骤
首先研究人员会训练一个只针对o系列模型
没有任何与安全相关的数据集
然后通过构建一个含有（提示、补全）（prompt
completion）对的数据集
其中补全中会引用思维链
并且在系统提示词中为每个对话
插入相关的安全规范文本
生成模型然后从数据中删除系统提示
接下来
再对这个数据集执行增量监督微调
让模型学习到安全规范的内容
以及如何通过推理来生成一致的响应
最后
使用强化学习来训练模型更有效地使用思维链
同时引入奖励模型
让它可以通过访问安全策略
来提供额外的奖励信号
这个策略会分为两个核心阶段来进行
第一阶段
通过对思维链引用规范的示例进行监督微调
来教会模型在思维链中直接推理安全规范
第二阶段
研究人员会使用高计算强化学习
来训练模型更有效地进行思考
并且引入使用指定安全规范的裁判大模型
来提供奖励信号
值得注意的是
OpenAI的训练程序不需要人工标注
只需要依赖模型生成的数据
就能够实现精确的规范遵守性
这就解决了标准大模型安全训练中
严重依赖大规模人工标注数据的难题
从结果来看
研究人员在一系列内部和外部安全基准中
比较了o1与GPT-4o、Claude 3.5 Sonnet和Gemini 1.5 Pro的安全性
可以看到
o1模型通过了一些较难的安全评估
并且在拒绝不足等方面实现了帕累托改进
关于审议对齐
最近Anthropic的团队也做了一期播客访谈
有机会我们会再做一期详细介绍的视频
好了
以上就是对o3和o3-mini的基本介绍了
由于目前还没有发布
所以具体使用情况
还得等明年正式公开之后大家再来评测
不过，在o3发布前不久
OpenAI GPT系列论文的主要作者亚历克·拉德福Alec Radford刚刚宣布离职
将会转向独立研究
外媒也刚刚披露，GPT-5的开发受阻
已经难产了10个月
现在还很难说，o3到时候会不会跳票
至少目前来看
通往AGI的全球竞赛还在加速进行
感谢大家观看本期视频
我们下期再见
