大家好，这里是最佳拍档，我是大飞
经常接触大模型的朋友
一定有过这样的经历
当你向 AI 提出一个复杂的问题时
它不仅给出了答案
还给出了详细的、一步一步的、看起来逻辑严密的解题过程
那一瞬间，你是不是感觉
屏幕对面的不再是一堆冰冷的代码
而是一个真正能够“推理”的智能体呢？
但是有时候
你换一个同样复杂、但是略有不同的问题
它又会给出一个错得离谱的答案
让你觉得它根本什么都不懂
只是一个更高级的“复读机”。
这种体验上的巨大反差
正是当前 AI 领域最核心的谜题之一
大语言模型展现出的“推理能力”，
究竟是一种真正的智能涌现
还是一种基于海量数据训练出来的、更高级的“模式匹配”呢？
它是在进行逻辑推导
还是在“模仿”它在网上看过的无数解题步骤呢？
关于这个问题
学术界和工业界争论不休
但是争论的意义
远不如我们去搞清楚
这种所谓的“推理能力”到底是怎么来的
以及我们如何才能稳定地、可靠地驾驭它
而要解答这个问题
有一个人的名字我们绕不开
他就是来自谷歌 DeepMind 的丹尼·周（Denny Zhou）
他和他的团队
可以说是奠定了我们今天理解和使用大语言模型推理能力的基石
他们开创性地提出了像“思维链提示”（Chain-of-Thought Prompting）和“自洽性”（Self-Consistency）这样的关键技术
并且深度参与了谷歌 Gemini 模型推理能力的构建
最近
丹尼·周在斯坦福大学做了一场讲座
系统性地梳理了从他创立谷歌大脑的推理团队开始
到今天我们所看到的强大 AI
这条技术路线是如何一步步演进的
这场讲座信息量巨大
它不仅揭示了 AI 推理能力的本质
更是对过去几年中所有相关技术的一次“祛魅”。
所以今天这期视频
我们就将以丹尼·周的这场讲座为蓝本
带着大家从最基础的概念出发
层层递进
彻底搞懂大语言模型“思考”的秘密
我们会看到
那些看似神奇的技术背后
往往遵循着一些极其简单而深刻的原理
相信看完这期视频
你再看待大语言模型的时候
会有一个全新的、更加清晰的视角
在深入探讨之前
我们必须先明确一件事
当我们谈论大语言模型的“推理”时
我们到底在谈论什么？
丹尼·周一上来就抛出了一个非常清晰且可操作的定义
这个定义也成为了整个领域的共识
他说
关于模型到底会不会推理的哲学辩论
他从不参加
因为没有一个明确的定义
大家都是在自说自话
而在他的团队里
“推理”有一个非常具体的含义
那就是，在模型的输入
也就是你的问题
和最终输出，也就是答案之间
生成的所有“中间步骤”（intermediate tokens）
这个定义非常关键
它把一个模糊的、哲学层面的“思考”概念
转化成了一个具体的、工程上可以实现和优化的目标
为了让我们更好地理解这一点
丹尼·周设计了一个非常巧妙的任务
叫做“末尾字母拼接”（last letter concatenation）
这个任务听起来很简单
比如我问模型
请拼接‘artificial intelligence’这两个单词的末尾字母
如果我们直接让模型输出答案
它可能会凭借着语言的惯性
直接猜一个答案，比如 "LE"。
这时它只是在预测下一个最可能的字符
而不是在执行一个多步骤的逻辑操作
但是
如果我们引导模型先生成“中间步骤”，
它的输出就会变成这样
artificial的最后一个字母是l
intelligence的最后一个字母是e
将l和e拼接起来，得到le
就是丹尼·周所定义的“推理”。
它把一个复杂任务
分解成了一系列简单的、可执行的子任务
最终导出了正确的答案
你可能会觉得
这不就是我们人类解决问题的方式吗？
先思考，再作答
但是丹尼·周提醒我们，作为研究者
必须时刻记住，大语言模型不是人类
它们只是概率模型
把它们拟人化，虽然有助于我们理解
但也很容易让我们走入歧途
一个有趣的故事是
丹尼·周最初尝试的是“首字母拼接”，
但是他发现当时所有的模型都能做得很好
为什么呢？
因为互联网上有大量的缩写词（initiatives）
模型在预训练阶段已经“背”会了如何拼接首字母
于是他换成了“末尾字母拼接”，
结果当时所有的模型都失败了
这恰恰说明
模型并没有真正“理解”拼接这个动作
而只是记住了某种常见的模式
那么
为什么要如此执着于生成这些“中间步骤”呢？
仅仅是为了模仿人类吗？
当然不是
这背后有非常坚实的理论依据
丹尼·周提到了他们和斯坦福大学教授滕尚华（Shang-Hua Teng）团队合作的一项理论研究
这个研究得出了一个非常强大的结论
对于任何一个可以被大小为 T 的布尔电路解决的问题
一个常数大小的 Transformer 模型
可以通过生成 O(T) 长度的中间步骤来解决它
这句话听起来有点过于技术
我们来把它翻译一下
布尔电路可以被看作是执行逻辑运算的基本单元
任何复杂的计算任务
比如运行一个大型软件
本质上都可以被分解成一个巨大规模的布尔电路
这里的“大小 T”，
就代表了问题的计算复杂度
这个理论告诉我们
哪怕是一个相对简单的 Transformer 模型
只要你允许它生成足够长的“思考过程”，
也就是中间步骤
它就有潜力解决几乎任何可计算的问题
反过来说
如果我们强迫模型直接蹦出最终答案
就相当于要求这个模型的网络深度本身要能够模拟整个复杂的计算过程
这要么需要一个巨大到不切实际的深度
要么就根本无法解决问题
所以，让模型“思考”，生成中间步骤
不是一个可有可无的选项
而是在计算原理上
解锁模型解决复杂问题能力的一把“金钥匙”。
这彻底改变了我们训练和使用大语言模型的范式
从单纯地追求“答案”，
转向了追求“过程”。
好
既然我们知道了“推理过程”如此重要
那么下一个问题自然就是
如何让模型来生成这个过程呢？
这里
丹尼·周提出了一个颠覆了当时很多人认知的观点
当时普遍认为
一个普通的、只经过预训练的大语言模型
是不会推理的
你必须通过像思维链提示（CoT prompting）这样的高级技巧
或者进行专门的微调
才能教会它推理
但是丹尼·周说，这个观点是错的
而且大错特错
他认为
预训练模型早就已经准备好进行推理了
我们所需要做的
仅仅是改变“解码过程”（decoding process）
这又是一个非常深刻的洞察
为了证明这一点
他举了一个经典的数学应用题
我有3个苹果
我的爸爸比我多2个苹果
我们总共有多少个苹果？
如果你把这个问题直接输入给一个原始的预训练模型
比如早期的 GPT-3 或者 LLaMA
然后使用默认的“贪婪解码”（Greedy Decoding）方式
会发生什么呢？
“贪婪解码”的意思是
模型在生成每一个词的时候
总是会选择当前概率最高的那一个
在这种模式下
模型很可能会直接输出一个看似合理、但却是错误的答案
比如，5个苹果
因为它看到了“3个”和“多2个”，
就直接联想到了“5”。
这是模型的一种直觉反应
或者说是一种系统思维
但是，模型的强大之处在于
它的输出概率分布中
并不仅仅只有这一个选项
在生成第一个词的时候
“5个”可能是概率最高的
但是还有第二、第三、第四高的选项
如果我们不那么“贪婪”，
而是去探索一下那些概率稍低一些的“岔路”，
奇迹就会发生
接着
丹尼·周向我们展示了这些隐藏的“候选答案”。
比如，候选二，可能以“我”字开头
模型会生成，我有3个苹果
我爸爸比我多2个，所以他有5个苹果
3 + 5 = 8
所以我们总共有8个苹果
这是一个完美的推理链，答案也正确
候选三，可能以“我们”开头
模型会生成，我们总共有8个苹果
虽然没有过程，但是答案也对了
候选四，可能以“你”字开头
模型会生成，你有3个苹果
你爸爸有 3 + 2 = 5 个苹果
你们总共有 3 + 5 = 8 个苹果
这同样是一个清晰的推理链
看到了吗？
正确的推理路径
其实一直都存在于模型的输出空间里
它们就像是隐藏在主干道旁边的小路
默认的“贪婪解码”因为只看了眼前最宽的路
所以错过了它们
这个发现被称为“思维链解码”（Chain-of-Thought Decoding）
它告诉我们
推理能力不是被“注入”到模型里的
而是模型在学习海量文本中蕴含的逻辑关系后
自然“涌现”出来的
于是，我们的任务
从“教会”模型推理
变成了如何引导模型把它已经知道的东西
以正确的形式表达出来
那么，问题就变成了
在这么多候选的输出里
我们怎么知道哪一个是最好的呢？
一个简单的想法是看长度
带有思考过程的回答，通常更长
但是丹尼·周的团队发现了一个更可靠的指标
那就是“答案置信度”（answer confidence）
他们观察到一个惊人的现象
对于那些包含了正确思维链的回答
模型在生成最终答案那个词
比如数字“8”的时候，其内部的置信度
也就是概率，会异常地高
在这个苹果的例子里
模型预测“8”这个词的概率可能高达 98%。
这是一个非常强的信号
因为对于一个拥有巨大词汇表的模型来说
通常每个词的概率都接近于零
这就像一个人在经过深思熟虑后
对自己得出的结论会非常笃定一样
所以，“思维链解码”的核心就两步
一，超越贪婪解码
生成并且检查更多的候选输出
二、选择那个对“最终答案”置信度最高的候选
这个方法虽然简单有效
但还是需要写一些代码
对于普通用户来说不够友好
于是，研究者们开始思考
我们能不能用更自然的方式
比如自然语言
来“重塑”模型的输出概率分布
让那些带有思考过程的优秀答案
能够“自动”排到第一名
这样我们用最简单的贪婪解码就能直接得到它呢？
这就引出了
我们后来耳熟能详的
一系列提示工程技术
首先呢最著名的就是思维链提示
它的做法非常直观
在你提出你的问题之前
先给模型看一两个类似的、从问题到思考过程
再到答案的例子
比如
你想让模型解决前面那个苹果问题
你可以先给它一个例子，问题
一个农民有5个香蕉
他又买了6个
后来吃了2个，还剩几个？
答案，农民开始有5个香蕉
买了6个后，他有 5+6=11个
然后他吃了2个
所以他剩下 11-2=9个
答案是9
然后，你再提出你的问题
我有3个苹果，我爸爸比我多2个苹果
我们总共有多少个苹果？
神奇的事情发生了
模型会“模仿”你给出的例子的风格
自动地开始一步步分析
生成详细的解题步骤
最后给出正确答案
从概率分布的角度看，你给出的例子
极大地提升了模型生成类似“思考过程”的句式的概率
从而把原本隐藏在后面的正确推理路径
推到了最前面
但是这种方法有个问题
你需要为不同类型的任务
手动编写高质量的示例
这很麻烦
而且如果你自己都知道怎么解决一个类似的问题
那你为什么还要问 AI 呢？
于是，一个更加“神奇”的提示出现了
它就是“让我们一步步思考”（Let's think step-by-step）
丹尼·周坦言，这篇论文刚出来的时候
他以为这是个玩笑
怎么可能在问题后面加上这么一句简单的话
模型就会自动开始思考了呢？
他当时就在谷歌内部的 PaLM 模型上做了测试
他非常清楚 PaLM 的训练数据里
绝对没有针对这个“咒语”做过任何的优化
结果，他震惊地发现，它真的有效！
模型真的开始输出一步步的解题过程了
这个发现极大地启发了他
尽管“Let's think step-by-step”这种零样本提示
效果通常比不过提供具体示例的少样本思维链提示
但是它证明了
我们可以用非常通用的方式
来激发模型的推理潜能
然而，无论是哪种提示方法
都感觉有点“奇怪”。
想象一下，你问一个聪明人问题
还必须得在后面加上一句“请一步步思考”，
否则他就不会思考了
这显然不符合我们对一个真正智能体的期望
所以
我们需要一种更稳定、更内化的方法
让推理能力成为模型固有的一部分
而不是需要外部“咒语”来触发
这就把我们带到了下一个阶段，微调
我们先说“监督微调”（Supervised Fine-Tuning
SFT）
它的思路非常直接
我们不就是希望模型能生成
从问题到思考过程再到答案
这样的数据吗？
那我们就雇佣一批人
针对大量的问题
手写出高质量的、一步步的解题方案
然后
我们把这些“标准答案”喂给模型
让模型去学习
这个方法在机器学习里叫“最大似然估计”，
简单来说
就是让模型生成的序列
跟人类专家写的序列
尽可能地一模一样
这个想法其实很早就有了
丹尼·周提到，早在 2017 年
DeepMind 的一篇论文就在做类似的事情
他们收集了一批数学应用题和人类手写的解题步骤
来训练一个序列模型
后来在 2021 年，OpenAI 更进一步
构建了一个更大、更著名的数据集
也就是 GSM8K
包含了八千多个小学水平的数学题和详细解法
用来微调 GPT-3 模型
这种方法训练出来的模型
在你给它一个新问题的时候
确实能够生成不错的解题步骤
看起来问题似乎解决了
一旦模型训练好，就可以随处部署
不再需要复杂的提示了
然而，在 2021 年夏天
丹尼·周的团队发现了一个严重的问题
那就是SFT 训练出来的模型
泛化能力很差
它在那些和训练数据很相似的问题上表现很好
但是一旦遇到一个新的、类型稍微不同的问题
就很容易失败
他们尝试了“大力出奇迹”的方法
扩大数据规模
找更多的人，标注更多的数据
可惜结果却是，无论如何扩大规模
这个问题始终存在
丹尼·周在这里给出了一个重要的教训
不要盲目地扩大规模
当你的范式本身是错误的时候
再多的数据也无济于事
那么，SFT 的范式，错在哪了呢？
问题又出在流程里的哪一步呢？
丹尼·周给出的答案可能会让你大吃一惊
他说
错误，出在“人”身上
这个转折点，来自于自我提升
后来也被称为“Self-Improve”或者 STaR（Self-Taught Reasoner）的方法
当他第一次听到“机器生成的训练数据
可能比人类专家写的还好”这个想法的时候
他自己也感到非常惊讶
这个新范式的流程是这样的
首先我们仍然从一批问题开始
但是我们不再找人类去写解题步骤
我们让一个已经比较强大的大语言模型
自己去针对这些问题
生成大量的、多样的解题步骤
最关键的一步是
我们用一个“验证器”（Verifier）
去检查模型生成的这些解题步骤
看哪个最终得出了正确的答案
比如对于数学题，我们知道标准答案
就可以直接判断
于是，我们只保留下来那些“过程多样
但结果正确”的生成结果
把它们当作新的、高质量的训练数据
然后用这些由模型自己生成、并且经过验证的“好数据”，
再去微调模型自己
这个过程可以不断地迭代
一个微调后变得更强的模型
又可以去生成质量更高、更复杂的解题步骤
用来进一步训练自己
这就形成了一个“自我进化”的闭环
丹尼·周提到
一篇在 2024 年 1 月发表的、来自字节跳动的论文《Reasoning with Reinforced Fine-Tuning》，
是他在学术界看到的、最早公开阐述类似思想的出版物之一
他相信，在 OpenAI 等多个机构内部
大家可能都独立地发现了这个简单而又极其有效的思想
现在，我们必须回答那个核心问题
为什么模型自己生成的数据
会比人类专家手写的数据
在训练上效果更好呢？
这背后
其实蕴含着机器学习的一个第一性原理
那就是直接优化你想要的东西
在 SFT的范式里，我们优化的目标是
让模型的输出模仿人类的解题步骤
我们假设人类的思维过程就是最优的
但实际上，人类的思维方式千差万别
充满了跳跃和不一致
而且人类专家写的“标准答案”，
对于模型来说
可能并不是最容易学习和泛化的路径
而在新的范式里，我们的目标变了
我们不再关心模型的解题过程
是否和人类一模一样
我们只关心一件事
它最终的答案是否正确
我们用“最终答案的正确性”这个指标
相当于强化学习里的奖励信号
来指导模型的学习
这在数学上
就等同于我们要求解一个策略梯度（Policy Gradient）问题
模型需要调整自己的参数
使得生成能够获得高奖励的序列的概率最大化
丹尼·周强调
我们不需要用“激励模型去思考”这种拟人化的、神秘的语言来描述这个过程
本质上就是三件事
定义你的目标（Metric）
计算梯度（Gradient）
然后反向传播（Back Propagation）
这就是机器学习的全部
通过这种方式
模型会自己去探索什么样的“思考过程”，
能够最稳定、最泛化地导向正确答案
这些过程可能看起来跟人类的思维不完全一样
但是它们更符合模型自身内部结构的学习路径
这个范式的转变，威力是巨大的
它也让我们明白
在整个“自我进化”的循环中
最最关键的环节
不是什么花哨的强化学习算法
而是那个“验证器”。
一个可靠的、能够自动判断答案好坏的验证器
也是整个新范式的基石
这让他想起了加拿大计算机科学家、强化学习之父理查德·萨顿（Richard Sutton）在 2001 年写的一篇文章标题
《验证是通往人工智能的关键（Verification is the Key to AI）》。
二十多年前的洞见
在今天的大语言模型时代
得到了完美的印证
通过这种“自我进化”的方式训练出来的模型
推理能力达到了一个前所未有的高度
它所展现出的“智慧”，
与经典的人工智能有着本质的不同
丹尼·周在这里引用了一句名言
来自国际象棋大师加里·卡斯帕罗夫（Garry Kasparov）在 1997 年输给 IBM 的“深蓝”后说的话
他说，深蓝的智能
就像你给闹钟编程让它准时响起一样
是程序化的智能
卡斯帕罗夫说得没错
深蓝的强大
来自于穷举式的搜索（exhaustive search）
它会暴力计算未来几步甚至几十步棋的所有可能性
然后选择最优解
这是经典 AI 的核心思想
但是，大语言模型的推理完全不同
它是一种类人的、启发式的推理过程
是从海量的语言数据中“涌现”出来的
而不是依赖于任何显式的、暴力的搜索
为了展示这一点
丹尼·周分享了一个令人拍案叫绝的例子
这个例子来自谷歌内部的一个模型
问题是这样的，请使用数字 1 到 10
每个数字只能用一次
通过加法和乘法运算，得到结果 2025
这是一个非常难的组合优化问题
如果用传统方法
你需要写一个程序去进行暴力搜索
尝试各种组合
但是让我们看看这个 Gemini 模型是怎么“思考”的
丹尼·周展示了模型在生成最终答案之前
内部的思考过程
模型首先判断
2025 是一个相对较大的数字
这表明乘法将在其中扮演重要的角色
这是一个非常像人类的启发式判断
然后
模型突然冒出了一个惊人的洞察
值得注意的是
2025是45的平方
丹尼周坦言
他自己出这道题的时候
都完全没有意识到这一点
这给解决问题提供了一个巨大的线索
接下来模型的思考继续深入
目标很大
我们应该考虑
如何得到较大的中间乘积
我们的目标是构建一些乘积
让它接近2025的平方
也就是45
在经过一长串
类似这样的自我对话和推理之后
模型最终给出了答案
并且他的答案
完美的遵循了自己的思考路径
他将1-10的数字分成了两组
每组呢都通过运算得到了45
第一部分
10x4+5=45
第二部分9x3+8+7+2+1=45
最后模型将两个45相乘
得到了 2025
整个过程，没有任何穷举搜索
模型就像一个顶尖的数学家
通过洞察、启发式思考和目标分解
一步步逼近了答案
这个例子有力地回应了理查德·萨顿在他著名的文章《苦涩的教训》（The Bitter Lesson）中提出的观点
萨顿在看到 AlphaGo 的成功后总结道
人工智能领域几十年的研究表明
最终能够规模化（scale）并且取得成功的
只有两种方法
学习（Learning）和搜索（Search）
但是丹尼·周在这里
对这个“苦涩教训”提出了一个更进一步的看法
也许，我们只需要学习就足够了
一个通过大规模学习训练出来的模型
它的内部涌现出的推理能力
本身就可以完成过去需要依赖搜索才能完成的任务
当然，这并不是说搜索完全没用
搜索可以作为一种外部工具被模型调用
就像我们使用计算器一样
但是在构建模型的核心推理能力时
重点应该放在“学习”上
通过强化学习微调训练出来的模型已经非常强大
但这还不是终点
丹尼·周接着介绍了两种在“推理时”（inference time）进一步压榨模型性能、提升结果可靠性的前沿技术
第一项技术是聚合（Aggregation）与自洽性（Self-Consistency）
我们首先要回到一个根本性的问题上
大语言模型在生成答案时
它的数学本质是什么？
我们前面提到，默认的“贪婪解码”，
是选择思考过程+ 答案
整个序列联合概率最高的那一个
但是，我们作为用户
真的关心它的推理过程是不是最优美、最高概率的吗？
不，我们只关心一件事
哪个“最终答案”本身是正确的
显然
这两个数学目标是完全不一样的
后一个目标
需要把所有可能导向这个答案的“推理过程”的概率
全部加起来
这个过程在数学上叫做“边际化”（marginalization）
直接计算这个值非常困难
因为可能的推理路径是无穷的
但是我们可以用一个非常简单的方法来近似它
这个方法就是“自洽性”（Self-Consistency）
它的操作极其简单
那就是我们不再使用确定性的贪婪解码
而是开启“随机采样”（random sampling）
让模型针对同一个问题
像掷骰(口误,tou2)子一样
生成许多个不同的、多样的、从推理过程到答案的序列
你会看到
模型可能会因为一些微小的随机扰动
走上完全不同的推理路径
得出不同的答案
比如对于一个数学题
它可能在 30 次生成中
得出答案是 18
在 20 次中得出答案是 26
其他答案则五花八门
最后一步
我们完全忽略掉所有的推理过程
只看最终的答案
我们进行“投票”，
哪个答案出现的次数最多
我们就认为哪个是最终的正确答案
在这个例子里，18 出现了 30 次
我们就选择 18
这个简单的“投票”过程，在经验上
就是对“边际化”的一个很好的近似
它背后的直觉是
如果一个答案是正确的
那么通往这个答案的“道路”应该有很多条
即使模型在某条路上犯了小错误
它在另一条路上可能就走对了
正确的答案会在多次尝试中
反复、稳定地出现
这个看似简单的技巧
带来的性能提升却是惊人的
丹尼·周用 GSM8K 这个基准测试举例
经过微调的 GPT-3 模型
准确率大约是 33%。
OpenAI 使用一个额外的“验证器”模型来筛选
可以提升到 55%。
谷歌的 PaLM 模型加上思维链提示
达到了 58%，
已经非常接近验证器的水平
然而，当在这个基础上
再用上“自洽性”技术后
准确率直接飙升到了 75%，
相对提升接近 50%。
后来在更强的 PaLM 2 上
这个数字甚至达到了 92%。
这充分说明
模型的单一输出可能存在偶然性
但是它多次输出的“共识”，
则具有高度的可靠性
丹尼·周还回答了两个关于自洽性的常见问题
首先是，如果模型不生成中间步骤
直接输出答案，用自洽性还有用吗？
答案是没用，而且没必要
因为在这种情况下
我们直接就可以看到每个答案的概率
选择概率最高的那个就行了
自洽性
是专门为“推理”这种包含隐藏变量的场景设计的
其次是，我能不能不进行多次采样
而是让模型一次性生成多个不同的答案呢？
答案是这样做没有意义
因为这违背了自洽性背后的数学原理
即通过独立采样来近似概率分布
当然，自洽性也有局限
它要求答案的形式是唯一的
比如一个数字
对于那些答案形式不唯一的开放性问题
比如
请列出亚洲最大的三个国家
模型可能会生成“中国、印度、日本”或者“印度、中国、日本”，
顺序不同但是内容一样
为了解决这个问题
他们还提出了“通用自洽性”（Universal Self-Consistency）的方法
让模型自己去判断哪个回答是与其他回答“最一致”的
第二项技术是检索（Retrieval）
关于大语言模型
另一个永恒的争论是
它到底是在“推理”，还是在“检索”？
也就是说，它是在进行逻辑推导
还是仅仅从它庞大的记忆库里
找到了一个最相似的已知答案呢？
丹尼·周对此的态度非常务实，他说
作为从业者，我只关心性能
为什么要在两者之间做选择呢？
把检索和推理结合起来
效果就是更好
他用“类比推理”（Analogical Reasoning）的例子来说明
比如这样一个几何问题
求四个顶点坐标分别为 (-2
2), (2, -2), (-2
-6)和(-6, -2)的正方形的面积
当他直接把这个问题扔给当时的 GPT-3.5 等模型时
模型失败了
但是
他仅仅在问题前面加了一句提示
请先回忆一个相关的问题
然后再解决这个问题
神奇的事情再次发生
模型在解决问题前
先自己生成了一段话
一个相关的问题是
如何在坐标平面上计算两点之间的距离
距离公式是，巴拉巴拉
然后
它利用这个自己“检索”出来的知识
先计算出正方形的边长
再计算出面积，最终成功解决了问题
另一个例子叫“退一步思考”（Step-Back Prompting）
在解决一个具体的、复杂的物理问题前
先提示模型，退一步
思考一下解决这类问题所需的基本物理原理是什么？
模型会先总结出相关的定律和公式
然后再用这些“检索”到的原理来指导具体的解题过程
这些方法
其实就是现在非常火热的“检索增强生成”（RAG）技术的思想雏形
都是将大模型的推理能力与外部强大的信息检索能力结合起来
所以
不必再纠结于推理和检索的二元对立
一个强大的推理系统
必然是一个开放的、懂得如何利用外部知识的系统
讲座的最后
丹尼·周对整个大语言模型推理的技术演进
做了一个精炼的总结
我们可以把它看作是四条经过实践检验的黄金法则
一、有推理优于无推理
生成中间步骤
是解锁复杂问题解决能力的基础
二、强化学习微调优于 SFT
让模型在“正确答案”的引导下自我进化
远比单纯模仿人类更有效
三、聚合多个答案优于单次生成
利用自洽性等方法
汇集模型的“集体智慧”，
可以大幅提升可靠性
四、检索+推理优于纯推理
将模型的内部推理与外部知识库相结合
是未来的方向
这四条法则
清晰地勾勒出了从一个原始的预训练模型
到我们今天看到的像 Gemini 这样强大的推理系统的完整技术路径
在展望未来的时候
丹尼·周也指出了当前面临的巨大挑战
我们今天讨论的所有技术
尤其是强化学习微调和自洽性
都严重依赖于一个前提
任务的答案是可以被自动验证的（automatically verifiable）
比如数学题有唯一答案
代码题可以通过单元测试
但是在现实世界中
大量更有价值的任务
并没有这样的“验证器”。
比如创意写作
如何判断一首诗写得好不好？
代码设计
我们不只关心代码能不能运行
更关心它的架构是否优雅、可读性是否高、是否易于维护
以及战略规划
如何判断一份商业计划书是否可行？
在这些没有唯一正确答案、充满主观性和复杂权衡的领域
我们该如何定义“奖励”？
又该如何构建“验证器”呢？
这可能是下一代人工智能需要突破的最大瓶颈
同时，他也呼吁
我们应该把更多的精力
从在基准测试上“刷分”，
转移到构建真正能够解决实际问题的应用上来
因为所有的基准测试
都很快会在模型的能力提升下达到“饱和”。
最后
丹尼·周引用了他非常欣赏的物理学家理查德·费曼（Richard Feynman）的一句话
来结束了演讲，那就是
真相
最终总是比你想象的要简单（The truth always turns out to be simpler than you thought）
回看整个历程
无论是思维链、自洽性
还是强化学习微调
它们背后的核心思想，都惊人地简单
甚至可以说是回归了机器学习最本源的原理
这或许就是科研最大的魅力所在
我们穿过层层迷雾，最终发现的
往往不是一个无比复杂的屠龙之术
而是一个简单、深刻、足以改变一切的真理
好了
今天这期视频的内容就到这里了
希望通过这次的梳理
能帮助大家对大语言模型的“推理”能力
有一个更清晰、更本质的认识
感谢收看本期视频，我们下期再见
