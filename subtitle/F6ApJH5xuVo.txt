大家好，这里是最佳拍档，我是大飞
前几天，我们用较为通俗的方式
讲解了一下大语言模型的内部运行原理
今天我们再来讲解一下Stable Diffusion的生成原理
应该说
这个Stable Diffusion算法是这一波AIGC浪潮崛起的基础
无数的AIGC工具都在此基础上衍生出来
本视频内主要会分为三个部分
第一部分介绍Stable Diffusion的原理
包括什么是扩散(Diffusion）？
如何稳定的控制扩散？
以及介绍CLIP和UNET的原理
理解VAE的编解码过程
第二部分我们会介绍模型训练的相关原理
包括机器是如何认识图片的
以及如何训练模型
第三部分
我们会介绍大模型的微调技术
包括为什么要进行微调
微调要解决的问题
以及常见的微调技术及其相关原理
OK，进入正题
我们先来了解一下什么是扩散
Diffusion
首先，要跟大家明确一下
Stable Diffusion是一个算法
我们说的Stable Difusion web UI是基于这个算法的一个工具
Stable Diffusion的算法名称很直接
Stable是稳定的
Diffusion是扩散
所以Stable Diffusion简单理解起来
就是一种稳定的扩散的算法
在图像领域中
扩散算法指的是通过一定规则去噪或者加噪的过程
其中去噪也称为反向扩散
加噪也称为正向扩散
拿提示语 a red flower 为例
当我们要生成一朵红色的花时
扩散过程会从最开始的灰色噪点块、逐渐去噪
直到最终清晰显示一朵红色的花
那么对于Stable Diffusion算法来说
这个扩散过程
怎么能够被稳定的控制住呢？
我们以文生图为例
给大家展示一下Stable Diffusion的原理
这里的过程
大家可以把它抽象理解为一个大的函数
函数名称是Fsd，函数的参数是prompt
也就是说
我们输入一段自然语义prompt
经过一系列的函数运算和变化
最终输出一张图的过程
这个过程中会经过几个部分
我们一一来拆解讲解一下
我们先来看第一个部分
我们输入了一段咒语
这段咒语是如何起作用的呢？
实际上，让我们的咒语能够起作用的
是一个叫做CLIP的算法
CLIP是Text Encoder算法的一种
Text Encoder
大家从字面意思也能够理解
是把文字转化为编码的一种算法
他的主要功能，是把自然语义prompt
转变为词特征向量Embedding
比如我们输入了一个prompt
cute girl
也就是可爱的女孩
CLIP算法在进行自然语义处理的时候
会根据之前被程序员调教的经验
大概感知到可爱的女孩可能具有哪些特征
比如她们可能有”大大圆圆的眼睛“，
可能有”白皙的肌肤“，
可能有”可爱的神态“等等
然后这些可能的特征
会被转化为77个等长的token词向量
其中每个词向量包含768个维度
这就是我们说的Embedding
这时可能有人会说，诶，不对啊
你看同样的一个关键词
为啥有的生成出来就这么好看
有的生成出来就很丑？
那是因为，当我们输入同一个prompt
我们的text encoder过程是一样
也就是你得到的词向量是一致的
但是后面的去噪算法依赖的模型不同
所以生成的效果千差万别
因此
接下来我们就来讲一下Stable Diffusion里面
最重要的UNET算法
UNET，是一种基于词向量的扩散算法
他的工作原理是这样的
我们刚刚说完，之前的Clip算法
会根据我们输入的prompt
输出对应机器能识别的词特征向量Embedding
这个Embedding大家只需要把它粗浅的理解成是一个函数
包含（Q、K、V）三个参数就好了
这三个参数会根据我们输入的对应的扩散步长
在UNET去噪算法的每一步发生作用
比如我设置了去噪步长为20步
大家能看到这张图逐渐扩散生成的效果
这里需要注意的是
实际上UNET去噪的时候
原理比我们这里描述的复杂很多
它其实并不是一步一步去噪就能得到对应效果的
如果仅仅是一步一步的去噪
效果往往很差
并不能精确得到描述文本的图片
所以这里我会稍微讲得深入一些
介绍一下Classifier Free Guidance引导方法
为了保证我们prompt最终的精确性
在UNET分步去噪的时候
比如我设置的去噪步长为20
他会在每一步都生成一个有prompt特征引导的图
和一个没有prompt特征引导的图
然后把两者相减
就得到了每一个去噪步骤中
单纯由文字引导的特征信号
然后将这个特征信号放大很多倍
这就加强了文本引导
同时，在第N+1步去噪结束后
它还会用第N+1步去噪的信息特征
减去第N部的特征
然后继续放大很多倍
这样可以保证prompt
在每一步都能有足够的权重比参与运算
用通俗的大白话来说
就是这个方法加强了prompt的权重
这个方法在Stable Diffusion web UI中被直译为提示词相关性
是一个很常用的参数
他的数值决定了生成的图
与提示词的相关程度
说完了文生图
我们大概也说一下图生图
咱们在Stable Diffusion web UI上
使用图生图功能的时候
往往是给一张图
然后再输入一段prompt
比如我们还是设置扩散步数N=20
这时候
它的原理是先把我们提供的图进行逐步加噪
逐步提取图片信息
使它变成一张完全的噪点图
再让prompt起作用
结合上面的UNET算法逐步去噪
得到既有素材图片特征、也有prompt特征的最终效果图
最后我们简单来理解一下VAE编解码的过程
VAE全称变分自编码器
我们其他视频里也多次提到过
这里大家不需要做太多理解
只需要知道他是一个先压缩后解压的算法就好了
需要注意的是
我们上面写的UNET算法不是直接在图片上进行的
而是在”负空间“进行的
大家可以理解为在编码层面即可
VAE的原理是这样的
假如我们要生成的图是512x512的
VAE算法在一开始的时候
会把它压缩到八分之一，变成64x64
然后在经过UNET算法的时候
会把图形数据带在噪点图中
这个过程叫Encoder
然后等走完UNET算法后
我们就得到了一个带有所有图片特征的噪点图
此时VAE再进行Decoder的过程
把这张图解析并放大成512x512
大致的过程就是这样
其实讲到这里
Stable Diffusion的原理也就讲完了
大家感觉应该还好吧？
其实也没那么难，对吧？
接下来我们要讲一下模型训练
看看模型是如何被训练出来的
首先
我们先来介绍一下机器是如何认识图片的
相信大家都听说过
最初计算机视觉训练认识物体的时候
采用的是成对训练的方法
通过“图”+“对应描述”，
成对得进行训练
然后运用图像识别、自然语义处理与卷积神经网络等一系列技术
让计算机能够识别这个图形
打个比方，如果要识别狗
我们会不停的给计算机喂成千上亿张狗的图
然后告诉他，你看这是狗，这也是狗
这还是狗
然后机器会不断归纳狗的特征
重复学习上亿遍后，它就认识狗了
好了，假设经过我们的投喂
机器已经能够识别万物了
但是这时候由于学习的图片风格都不同
生成的图片有可能不满足我们的期望
举个例子
假设程序员经过十年的努力后
让机器正确认识了小女孩的特征
不会出现三个眼睛两个嘴巴
当我们输入一个提示
a cute girl的时候
生成的是图A
但是A的风格与我们想要的完全不同
比如我们想要的是图B这种风格
这时候程序员们就会不断调整函数中的各种算法和参数
使得产出的图形A无限接近于图像B
然后停止训练
此时整个Fsd（x）函数的所有调整的参数
就被保存为一个
ckpt的文件
这个叫checkpoint的文件
就是程序员训练好的、可供调用的AI绘图大模型
他能保证我们每次生成的图
都偏向于某种特征集合
了解过Stable Diffusion的应该都知道
Stable Diffusion可用的模型有两类
一种是safetensors
这种文件是用numpy 保存的
这意味着它们只包含张量数据
没有任何代码，加载
safetensors 文件会更加安全和快速
第二种是ckpt
这种文件是用pickle序列化过的
这意味着它们可能包含恶意代码
如果你不信任模型的来源，加载
ckpt 文件可能会带来安全风险
因此，我们下载模型的时候
建议优先下载
safetensors
我们再来讲讲大模型的微调技术
首先我们要问一个问题
为什么要进行大模型微调呢？
刚刚我们说过
Unet模型是SD中最重要的模型网络
内部包含了上亿个参数
要训练这样一个超大的模型
大概需要15亿个图像文本
使用256张A100显卡
跑15万个GPU小时
大概成本是60万美元
一般的设计师也不能像程序员那样
动不动就调函数参数来训练模型
那怎么办呢？
因为UNET模型的泛化性极强
泛化性极强就会带来风格化不足
就可能无法满足特定风格的需要
所以我们往往会对UNET大模型进行微调
让它更符合需要的使用场景
那么接下来就要介绍一下模型微调技术了
也就是大家在Stable Diffusion里常用的训练方法
不过我们今天不会讲具体怎么训练
我们先把原理讲明白
网上有很多自训练的方法
大家学起来也会更容易
所有的大模型微调技术
都是为了要解决两个问题
1
如何减少训练参数
提高训练效率和生成图像的质量
2
如何解决模型泛化性差的问题
泛化性指的是模型对新数据的适应能力
他具体会表现为过拟合和欠拟合两种
其中，过拟合（Overfitting）
是指微调模型对原始模型的语义理解程度
发生了变化
整体训练出现语义漂移的现象
比如有一只猫，名字叫jojo
我拍了他的几十张照片
一直用a jojo cat这个名字
来强化模型对这只猫的认知
等以后我输入a jojo cat的时候
确实能出现我想要的这只猫
但是我输入a cat的时候
出现的画面却很怪异
这就说明cat这个词被污染了
出现了过度拟合的现象
另一种，欠拟合（Underfitting）
是指无法识别这个特征
比如它就完全识别不出jojo和cat的关系
这往往是训练样本量不足
或者训练样本质量较差等因素导致
属于训练无效
那么我们来看下常见的大模型微调技术都有哪些
以及他们是如何解决上面两个问题的
我们常见的大模型微调技术大概有这四种
Dreambooth、LoRA、Embedding、Hypernetwork
我们一个一个来看
首先是Dreambooth
它是怎么来解决过拟合的问题呢？
Dreambooth要求我们在训练过程中
“特征词+类别”和“类别”要成对出现
比如之前提到的
我如果想训练这只叫JOJO的猫的模型
又不能让它跟“猫”这个词过度拟合
我们可以采用的这样的输入方法
这样的话AI就能识别到
JOJO cat is a cat named JOJO
Dreambooth是google在2022年8月提出的一种新的图像算法
这种方法可以完整的获得
你想要的模型的视觉特征
它的提出既不是为了训练人物也不是为了训练画风
而是为了能在少量训练图像的基础上
完美的还原细节特征
在这张图上
大家注意原图闹钟右侧有一个”黄色的3“，
Dreambooth将他完整还原了
这说明使用Dreambooth确实可以完整的获得
你想要的模型的视觉特征
所以当时发布的时候还是很牛的
Dreambooth虽然叫大模型微调技术
但是实际上它调得一点都不微
他基本把UNET算法每一层的内部参数都调了一遍
就是图上红色的部分
所以他的优点和缺点都很明显
优点是可以将视觉特征完美融入
缺点是需要调整UNet所有内部参数
训练时间长
模型体积也很大
那讲完了Dreambooth之后呢
我们再来讲一下LoRA
相信大家应该都听过这个词了
中文名呢叫做大模型的低秩适配器
如果要讲清楚LoRA的原理呢
还是得回到之前的UNET算法
这张图呢
就是Stable Diffusion核心算法UNET的工作原理
其中UNET的神经网络运算结构
是由很多很多的计算层叠加组成的
它内部的每一个计算层
大家可以把它看成是一个一个的小的函数
前一个函数的输出等于后一个函数的输入
随着层数的叠加
整个模型就能够理解
你所输出的数据的特征了
前面说到的Dreambooth
相当于把unet里的每一层函数都要进行微调
所以我们说它计算量大训练时间长
模型体积大
但是LoRA就不同了
LoRA的目的呢是减少模型的训练参数
提升模型的训练效率
因此LoRA建议冻结预训练模型的权重
并将训练参数注入到Transformer函数架构的每个层中
它的优点呢就是不破坏原有的模型即插即用
像图中红色部分所显示那样
正因为LoRA它的插入层较少
相较于Dreambooth
可以把训练参数降低1,000倍
而且对CPU的要求呢也会下降3倍
所以训练出来的LoRA模型就会非常小
一般大家下载过呢就知道
往往呢它们只有几十M
而一个大模型呢往往有几个g
甚至十几个g
所以呢它在我们日常工作中来说就会变得非常常用
通俗点来说呢
大家可以把LoRA理解成在原有的大模型上添加了一个滤镜
让这个底下的大模型往我们期望的效果走
比方说呢当我们的prompt不变的情况下
在一个偏动漫的底层模型revAnimated上
加了一个盲盒效果的LoRA
就相当于是给这个底层模型上了一个滤镜
让整个底层模型出来的效果
往盲盒的方向上去靠
接下来呢我们再讲一下Embedding
Embedding也叫Text inversion
它反映的是一个prompt与对应向量的映射记录关系的算法
可以用来训练特定的人或者是物体
还记得前面我讲我们的咒语是如何起作用的时候
用到的这张图吗
我们说过
CLIP模型是text encoder算法的一种
text encoder主要是把自然语义prompt转变为词特征向量Embedding
所以我们可以针对从prompt到向量这个映射过程进行训练
去修改它们之间的映射记录关系
从而达到训练特定的人或者物体的效果
由于它生成呢是一套纯文字的映射记录
所以体积非常小
一般呢只有几百k
举个实际的例子
比如说下面这个人物
是游戏守望先锋里的Dva
如果我们要通过描述她的特征
去把她完整的描绘出来
可能就需要几千个prompt tag才行
那如果我们每次想生成Dva的时候
都要再输入一次这么多的prompt
肯定是不科学的
所以这个时候呢
我们可以把这一串的tag
打包映射成一个新的词汇叫做OW_Dva
由于这个词呢是我们自创的
在clip映射集合里是找不到对应的映射关系的
所以Clip会默认给他创建一个新的映射空间
经过一系列的训练之后
我们后续就可以通过输入一个简单的词汇
来完成一系列tag的打包效果
最后呢我们来看一下hypernetwork
它作为一种即将要被Lora淘汰了的技术
我们也只是会大概讲一下它的原理
我们先来总结一下
三种技术的原理和在UNET中的使用范围
其中Dreambooth调整了整个UNET的函数和参数
所以它体积最大适用范围最全
但是训练难度和训练耗时和成本也都最大
而LoRA只是将训练参数注入到了部分的Transformer函数中
所以它不改变原来的模型即插直用
模型大小也可控
而hypernetwork大家从图上可以看出
它是新建了一个单独的神经网络模型
插入到原来的UNET模型的中间层
在训练过程中啊他会冻结所有的参数
只训练插入的部分
从而使输出图像与输入指令之间产生关联关系
同时呢只改变原来模型的一小块内容
从这个原理的描述上大家应该就能发现
Hypernetwork这种方法更适合用于训练某种画风
比如说像素画之类的
当然了也不是说它不能训练别的
它也是可以训练人物的
就是比LoRA呢麻烦一点
总之呢它在国内几乎是一个要被LoRA淘汰的技术了
关于hypernetwork的相关paper也都是2022年以前的了
所以我们也就把它当做一个滤镜来理解就好了
那以上呢就是对整个Stable Diffusion算法以及相关内容的介绍
最后呢还是想声明一下
由于我们这个是一个通俗版的介绍
为了方便大家理解
所以我对算法原理和数学运算进行了一定的简化
内容呢也不可能面面俱到
有想深入了解一些技术细节的可以在评论区留言
我们找时间再深入的介绍
感谢大家的观看
我们下期再见
