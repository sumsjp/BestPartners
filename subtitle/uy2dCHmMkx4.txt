大家好，这里是最佳拍档，我是大飞
真是没想到
科技圈的吃瓜能吃到arxiv上
最近
为了回应EDA行业对于AlphaChip的质疑
谷歌的首席科学家Jeff Dean专门发表了一篇论文
并且提示大家注意
这属于是同行竞争，恶意诋毁
今天我们就来聊聊这到底是怎么一回事
说实话，AlphaChip这事搞得有点蹊跷
按理说
芯片设计系统AlphaChip的论文已经登上了Nature
有实验，有论文，也有工业应用
甚至还有一篇作为受邀论文发在了ISPD 2023上
却多次遭受到行业的质疑
说难听点
同系列的兄弟AlphaFold都已经拿诺奖了
AlphaChip还搁这辟谣呢
可能是实在受不了了
Jeff Dean就在x平台上表示
你们不是都发论文质疑AlphaChip的Nature论文么
那我也发一篇论文质疑你们
于是就有了我们现在看到的这篇论文
Jeff Dean认为，这些毫无根据的怀疑
在很大程度上是由加州大学发的这篇论文所导致的
更是直接指出这是一篇存在严重缺陷的、未经同行评审的论文
为了方便起见
我们后面就简称这篇挑起争端的论文为加州大学论文
总的来说
Jeff Dean认为加州大学论文
虽然声称复制了AlphaChip的方法
但是实际上有好几个方面并没有遵循原论文内容
比如
作者并没有进行非常重要的预训练环节
剥夺了基于强化学习从其他芯片设计中学习的能力；
其次，减少了20倍的计算量
并且没有进行收敛训练
这就像是评估一个以前从未见过围棋的AlphaGo
然后得出结论
AlphaGo不太擅长围棋
这些问题我们会在后面详细一一展开来说
另外Jeff Dean等人还回应了Synopsys架构师伊戈尔·马尔科夫Igor Markov
在CACM 2024年11月刊上发表的分析文章
Jeff Dean表示
马尔科夫在发论文的时候
根本没表明自己是Synopsys的高级员工
而Synopsys是商业EDA软件
AlphaChip是开源的
明显就是有针对性
而且
马尔科夫的论文分析中还引用了另一篇没发表的匿名PDF
但是实际上也是马尔科夫写的
Jeff Dean更是说到
马尔科夫的文章提出了隐晦的指控
但是所有这些都是完全没有根据的
而且已经被Nature证明过了
我很惊讶Synopsys想与此扯上关系
我也很惊讶CACMmag认为有必要在没有证据的情况下
发表这类指控
何况除了这两篇有缺陷的、未经同行评审的文章之外
没有任何技术数据能够证明指控
为了更好说明事件的来龙去脉
Jeff Dean直接在自己的论文介绍部分
拉了个时间表出来，时间跨度长达4年
2020年4月
AlphaChip发布Nature论文的arXiv预印本
2020年8月
TPU v5e中流片了10个AlphaChip布局
2021年6月，正式发表了Nature文章
2021年9月
在TPU v5p中流片了15个AlphaChip布局
2022年1月 - 2022年7月
谷歌开源了AlphaChip
而另一个团队独立复制了Nature论文中的结果
2022年2月
谷歌内部独立委员会拒绝发表马尔科夫等人的观点
因为数据不支持他们的主张和结论
2022年10月
在最新的公共TPU Trillium中
流片了25个AlphaChip布局
2023年2月
Cheng等人在arXiv上发表论文
声称对谷歌的方法进行了「大规模重新实现」。
2023年6月
马尔科夫发布了他的「meta-analysis」文章
2023年9月
Nature启动了第二次同行评审
2024年3月
Google 在Axion处理器中采用了7个AlphaChip布局
2024年4月
Nature完成了调查和出版后审查
发现完全对谷歌有利
2024年9月
MediaTek的高级副总裁宣布扩展AlphaChip
来加速他们最先进芯片的开发
2024年11月：
马尔科夫重新发表了他的「meta-analysis」文章
简单来说，Jeff Dean在论文中认为
AlphaChip已经在谷歌自家服役这么长时间了
联发科也用了，Nature也调查过了
而且作为谷歌内部不同的部门
TPU团队需要有足够的信任才会使用AlphaChip
因为他们没必要承担不必要的风险
而对于反方的马尔科夫
Jeff Dean在论文评价道
马尔科夫的大部分批评都是这种形式
那就是在他看来
我们的方法不应该奏效
因此它一定不起作用
任何表明相反的证据都是欺诈
而说起欺诈这件事
正反两方都谈到了内部的吹哨人
马尔科夫在他的文章中是这样记载的
论文的两位主要作者
抱怨他们的研究中不断出现欺诈指控
2022 年，谷歌解雇了内部的吹哨人
并且拒绝批准出版谷歌研究人员撰写的一篇批评米尔霍塞尼Mirhoseini等人的论文
吹哨人也起诉谷歌的不当解雇
而Jeff Dean在论文中表示
这位吹哨人向谷歌的调查员承认
他怀疑这项研究是欺诈性的
但是没有任何证据
反正目前这件事肯定还是存疑的
需要更多深入的调查
才能确认吹哨人的举证是否属实
我们再来看看Jeff Dean论文中对加州大学论文内容的逐条回应
首先是认为对方没有进行预训练
AlphaChip是一种用AI来设计芯片的方法
它基于神经网络和强化学习
这也意味着预训练阶段的训练数据集越大
模型学习到的放置新区块的方法就越好
而在加州大学论文中
作者等人根本没有进行预训练
因为没有训练数据
这就意味着模型以前从来没见过芯片
必须学习如何从头开始为每个测试用例执行布局
AlphaChip的作者在Nature原论文中详细讨论了预训练的重要性
并且实证证明了
预训练可以提高芯片放置质量和收敛速度
在开源的Ariane RISC-V CPU上
非预训练的RL需要48个小时
才能够接近预训练模型在6小时内可以产生的值
因此在Nature原论文中
作者针对主数据表中的结果进行了48小时的预训练
而加州大学论文预训练了 0 小时
Jeff Dean更是指出
对方试图通过暗示谷歌的开源存储库不支持预训练
来为他们缺乏预训练找借口
但是这是不正确的
预训练本来就是在多个样本上运行的
除此以外，在加州大学论文中
提供的强化学习体验收集器减少了20倍
相比于原论文的512个，只提供了26个
同时GPU数量也减少了一半
从16个减少到了8个
Jeff Dean指出
使用较少的计算可能会损害性能
或者需要运行相当长的时间
才能实现相同的性能
从原论文中，我们也可以看到
在大量GPU上进行训练
可以加快收敛速度
并且产生更好的最终质量
再有一点就是
加州大学论文中的强化学习方法
没有训练到收敛状态
我们都知道
随着机器学习模型的训练
损失通常会减少，然后趋于平稳
这就是所谓的收敛
表示模型已经了解了它正在执行的任务
在机器学习的训练中
训练到收敛状态也是标准的做法
但是在加州大学论文中
作者没有为任何一个用例进行收敛训练
除了BlackParrotNG45和Ariane-NG45没有提供图以外
其他四个具有收敛图的块
Ariane-GF12、MemPool-NG45、BlackParrot-GF12 和 MemPool-GF12
都在相对较低的步数就停止了训练
最后一点就是，在Nature的原论文中
作者报告的张量处理单元TPU块的结果
来自小于7nm的技术节点
这是现代芯片的标准制程
而相比之下，加州大学论作者等人
则采用了旧的技术节点尺寸
45nm和12nm的结果
这从物理设计的角度来看
就存在很大的不同
比方说，在低于10nm的时候
通常会使用多重图案技术
导致在较低密度下会出现布线拥塞的问题
因此，对于较旧的技术节点大小
AlphaChip可能需要调整奖励函数的拥塞或密度分量
而AlphaChip的所有工作都是在7nm、5nm和更新的工艺上进行的
根本就没有考虑旧的工艺制程
好了
以上就是Jeff Dean这篇回应论文的主要内容了
其实我为什么说这件事很蹊跷呢
就是因为AlphaChip是完全开源的
在Github仓库上可以完全复现Nature论文中描述的方法
其中强化学习方法的每一行都可以进行免费的检查、执行或者修改
并且提供源代码和二进制文件
来执行所有预处理和后处理步骤
按理说，只要有足够的资源
验证这个事情非常容易
那到底这个事情后面还有没有其他的隐情呢？
究竟是AlphaChip真的有问题
还是有人对于开源项目眼红
认为影响了自己的商业利益呢？
我们还需要拭目以待
大家对这个事情怎么看呢
欢迎在评论区留言
感谢大家的观看，我们下期再见
