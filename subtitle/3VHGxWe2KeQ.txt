大家好，这里是最佳拍档
今天我们继续来盘点
前OpenAI首席科学家、安全超级智能SSI公司的创始人
伊利亚·苏茨克维尔
在二零二五年所有演讲与公开访谈的核心洞见
为什么他认为
AI正在从单纯依赖算力堆叠的Scaling时代
迈入以算法创新为核心的研究时代
而这个转变
又将如何重塑我们对于智能、技术
乃至人类未来的认知
在旧金山湾区的核心技术圈层
随着计算集群规模的指数级扩张
以及模型参数的爆炸式增长
智力过剩的狂热随处可见
仿佛AGI的临界点触手可及
但是伊利亚却敏锐地指出
这种狂热与全球宏观经济的平稳线性之间
存在着无法解释的物理温差
这背后的原因
正是当前AI模型的根本局限性
它们能够在基准测试中表现出超越人类的智力
却在现实的经济活动中
缺乏完成闭环任务的健壮性
这种高分低能的悖论
在实际应用场景中被展现得淋漓尽致
伊利亚以Vibe Coding辅助编程为例
揭露了开发者们频繁遭遇的西西弗斯式的困境
当开发者在一个复杂的代码库中
遇到了Bug A
并且请求模型进行修复的时候
模型会迅速生成一个看似完美的补丁
但是代码部署后
往往会触发新的Bug B
而当开发者反馈Bug B的时候
模型给出的修复方案
逻辑上又会直接回退到导致Bug A的原始状态
如此循环往复
一个能在LeetCode竞赛中击败99%人类选手的智能体
却无法维持一个只有两个变量的简单逻辑闭环
伊利亚解释了这个现象的本质
那就是因为模型根本没有理解代码的功能逻辑
只是在进行高维度的文本补全
在它的概率分布中
修复Bug A的文本模式
与引入Bug B的文本模式
在统计上存在高度共性
但是由于缺乏一个独立于语言之外的世界模型
来进行逻辑校验
所以模型无法感知这种循环的荒谬性
这种无意识的顺从和自信的幻觉
正是当前技术范式在实际经济生产中
难以落地的一个核心阻碍
一个只会做题、但是不断在实操中埋雷的员工
永远无法被企业信任
并且赋予独立的决策权
而这一切的根源
在于当前的强化学习训练范式
本质上是一种应试教育
伊利亚指出，在预训练阶段
模型接触的是互联网上所有的文本数据
这些数据包含了人类文明的全部思想、逻辑、情感、谬误与模糊性
迫使模型去构建一个容纳了庞杂世界的全息投影
习得了模糊但是广博的通识
但是进入到强化学习阶段后
情况发生了根本性的逆转
所有前沿AI公司都会去建立专门的团队
为了让模型在发布时取得最优的成绩
研究人员会不自觉地从评测集中
汲取灵感来设计强化学习环境
这种做法在数学上
等同于针对测试集进行训练一样
模型在精心设计的环境中反复试错
学会的是最大化奖励函数的复杂策略
而非理解问题本质
结果就是
经过强化学习训练的模型变得一根筋
在特定高维流形上被过度的优化
一旦输入偏离这个流形
性能就会呈断崖式的下跌
伊利亚认为
这种缺乏自我意识的狭隘
正是当前AI既聪明又愚蠢的根本原因
使得它在设计好的赛道上是超人
在开放世界的荒原中却是盲人
为了更直观地展现这种智能的本质差异
伊利亚构建了两个智能体模型来进行思想实验
智能体A是当前AI的极限形态
一个通过海量数据堆叠的竞技编程机器
它投入一万小时的高强度训练
遍历了人类历史上所有的编程题目变种
背诵了每一个算法的证明细节
对所有已知边界条件
进行了肌肉记忆一般的过拟合
在已知分布的测试中表现无可挑剔
速度与准确率都超越人类的极限
但是这种能力本质上是检索与插值的高级形式
而智能体B是通用智能的理想形态
一个具备天赋的人类初学者
他仅仅投入了一百小时
并没有见过大多数题目
但是却掌握了底层的元规则和一种被称为品味的判断力
面对全新的难题时，它能依靠直觉
迅速剪枝庞大的搜索空间
直接锁定核心的逻辑路径
伊利亚强调，当前行业的Scaling路径
本质上是在不遗余力地打造更强大的智能体A
通过数据增强来制造更多的合成题目
以及用更大的集群来进行更长时间的训练
虽然这样做能不断推高榜单分数
却从来没有触及到智能的核心
那就是泛化
真正的泛化能力
并不是来自于见过所有的情况
而是来自于从极少样本中
提取高阶因果结构的压缩能力
如果在竞技编程中达到了超人类的水平
却不能自动赋予模型
构建大型软件架构时的品味和判断力
那么单纯的技能扩展
就无法涌现出通用智能
这意味着，仅仅增加练习时长
而不改变学习方法
只能产生更熟练的匠人
却无法诞生具有创造力的工程师
而随着预训练数据这个自然资源的枯竭
这种单纯Scaling的路径已经走到了尽头
伊利亚指出
2020年至2025年的Scaling时代
行业的指导思想就是简单且统一的Scaling Law
正如GPT 3所证明的
只要按比例增加算力、数据和参数量
模型的性能就会像物理定律一样
实现可预测的提升
这导致了思想通缩的现象出现
既然有确定性的配方可以遵循
就没人再愿意冒险去探索新的算法路径
所有的资源都被集中到了低风险、高回报的策略上
但是现在，预训练的核心燃料
也就是人类产生的高质量文本数据
已经濒临枯竭
尽管有传言称谷歌的Gemini等模型
找到了压榨数据剩余价值的方法
但是物理极限依然不可逾越
当所有数据都被喂入模型后
继续扩大模型的规模
是否还能带来质的飞跃呢？
伊利亚对此持有强烈的怀疑态度
他断言
我们正在经历历史性的均值回归
回到2012到2020年间的研究时代
没有现成的配方
需要研究人员通过大量试错、小规模的修补和直觉
去寻找新的突破点
不同的是
这一次的研究将在前所未有的巨型计算集群上进行
AI发展的驱动力正在从资本密集的资源堆叠
重新转移回智力密集的算法创新
要想理解当前AI行业的转折
就必须回溯算力范式的演进历程
伊利亚通过对深度学习发展史的精密考古
将过去十二年
划分为两个在物理性质上截然不同的地质年代
这种划分不仅体现的是时间上的切片
更是研究方法论从随机游走
向确定性缩放，再到创新回归的跃迁
第一个阶段是2012到2020年的研究时代1.0
这个时期的核心特征是非确定性与算力匮乏
以二零一二年的AlexNet为原点
整个深度学习革命的算力基座
仅仅是两张消费级的GPU
即便到了二零一七年Transformer架构的诞生前夜
最前沿的论文实验
算力消耗的上限也没能突破六十四张GPU的集群规模
在那个时代，算力是稀缺资源
想法是廉价的
研究人员的日常工作
类似于前科学时代的炼金术士
他们凭借着直觉去调整超参数、尝试各种奇特的激活函数
设计各种复杂的拓扑结构
整个领域的进步
依赖于高频试错和偶然的灵光一现
没有通用的物理公式能预言
投入多少算力能够换回多少智能
每一个新架构的提出
都是一次充满风险的赌博
2020年，随着GPT 3的问世
行业进入了Scaling时代
伊利亚将这个时期
定义为Scaling Laws统治一切的阶段
这个定律的发现
堪比物理学中的热力学定律
它揭示了智能涌现与算力、数据、参数量之间的对数线性关系
这种关系的发现
瞬间消灭了不确定性
AI研发从探索未知的炼金术
转变为了按配方生产的大工业
所有科技巨头迅速达成共识
只要按照特定的配方
包括特定的数据配比、特定的模型深度与宽度、特定的算力投入
就必然能得到预期的性能提升
这个时期的特征
是思想的同质化与资本的暴力美学
既然单纯扩大规模就能获得稳定的边际收益
那么任何对架构微创新的探索
在经济账上都是不划算的
这种确定性导致了全行业的路径锁定
所有资源被单一化的注入到Transformer的堆叠中
直到今日
但是现在
单纯Scaling的物理红利已经接近枯竭
伊利亚提出了一个反直觉的判断
虽然我们拥有了前所未有的算力规模
但是这并非算力的终结
而是使用算力的方式必须发生相变
在Scaling时代的中后期
行业陷入了线性惯性思维
既然一万张卡比一千张卡好
那么十万张卡一定能带来质的飞跃
但是实际上
预训练模式正面临两个无法回避的物理墙
第一个是数据枯竭
预训练的本质
是对人类存量文本知识的压缩
高质量的人类原生数据
在物理总量上是有限的
当模型读完了所有书、所有代码、所有论文后
继续增加参数量
将不再带来智力的提升
只会导致严重的过拟合
第二个是泛化瓶颈
仅仅通过预测下一个Token这个目标函数
即便将模型规模再扩大一百倍
也无法解决逻辑一致性和物理常识的问题
伊利亚观察到
模型在某些维度上的能力提升
正在趋于平缓
这意味着单纯的Scale up策略的投资回报率
正在急剧下降
基于这个现状
伊利亚判定我们正在回归研究时代2.0
但是这一次的回归，不是简单的重复
而是建立在巨型算力基础之上的方法论重构
现在的瓶颈不再是算力不足
而是想法上的匮乏
在拥有了数万张GPU的今天
我们反而不知道该算什么了
行业被迫从只有一种正确做法的舒适区
重新回到尝试一百种不同路径的探索区
这标志着AI研发的重心
从工程实施重新转移回基础科学研究
更重要的是，算力消耗的重心
正在发生隐秘而巨大的转移
从训练时转向推理时
在传统的预训练范式中
算力主要消耗在模型的养成阶段
一旦训练完成
推理过程相对廉价而且快速
但是伊利亚指出，最新的趋势表明
投入在强化学习上的算力
正在超越预训练的算力
这种反转背后的技术逻辑极为深刻
现代强化学习本质上是在进行测试时计算的扩展
与预训练那种看一遍就记住的被动吸收不同
强化学习需要模型在虚拟环境中
进行极其漫长的推演
你可以想象一个智能体在尝试解决复杂的数学猜想
它需要生成成千上万条推理路径
每一条路径都可能包含几千个步骤
绝大多数路径最终都会被证明是错误的
但是生成这些错误路径本身
就需要消耗惊人的算力
这实际上是在用算力换取逻辑的深度
伊利亚强调
这种过程极度消耗资源
因为目前的强化学习算法效率极低
由于缺乏高效的价值函数来提前进行剪枝
模型被迫像无头苍蝇一样
在巨大的搜索空间中随机乱撞
直到偶然撞上正确的路径为止
这也解释了为什么现在的强化学习Scaling极其昂贵
我们在用几百万个GPU小时
来模拟人类大脑在几秒钟内就能完成的直觉判断
因此，未来的算力竞争焦点
将从谁能更快地训练完模型
转向谁能更高效地利用算力
进行深度思考与自我博弈
针对外界对SSI资源策略的质疑
仅仅筹集到了30亿美元
如何与谷歌、微软这些动辄数百亿的资本开支竞争
伊利亚给出了一个极具洞察力的反驳
公众和投资者往往被绝对数字所迷惑
而忽略了算力的利用率结构
对于OpenAI或者谷歌这样的巨头
其庞大的算力资源
实际上被严重的碎片化了
首先是推理服务税
它们必须维持庞大的在线服务
每天处理数十亿次的用户请求
这部分推理算力占据了总资源的绝大多数比例
但是对模型能力提升贡献为零
其次是产品功能的研发
大量研发算力被分散到多模态功能、应用层优化等商业化需求上
而非核心智能的突破
最后是遗留债务
巨型组织内部
存在着大量的实验冗余和方向性的内耗
相比之下
SSI采取了极简主义的直通策略
伊利亚透露
SSI的算力将百分之百用于验证核心的研究假设
没有任何推理服务的负担
也没有产品化的干扰
在研究时代2.0
验证一个颠覆性的新想法并不需要10万张卡
就像当年的Transformer甚至只用了八张卡
因此
在针对核心难题的饱和攻击算力这个指标上
SSI实质上拥有比肩甚至超越巨头的资源密度
这种高密度、窄聚焦的算力配置
正是初创公司颠覆庞然大物的经典物理杠杆
而在探讨如何突破数据瓶颈时
伊利亚对被寄予厚望的自博弈技术
进行了冷峻的技术祛魅
自博弈曾经是AlphaGo战胜人类的关键
通过左右互搏实现了超越人类数据的智能涌现
但是伊利亚警告
将这个逻辑简单迁移到大语言模型上
存在一个巨大的封闭性陷阱
传统自博弈之所以有效
是因为它运行在具有完美信息和明确胜负判据的封闭环境中
比如围棋和对战游戏
无论策略多么复杂
输赢是客观而且自动可验证的
系统可以通过无数次的对局
依靠明确的奖励信号不断进化
但是在通用智能面对的现实世界
或者语言任务中
并不存在这样一个完美的裁判
如果让两个大语言模型互相辩论哲学问题
或者编写代码
谁来判定输赢呢？
如果判定标准依然依赖于另一个大语言模型
或者有限的人类标注
那么系统最终只会过度拟合裁判的偏见
而不是产生真正的真理
目前
自博弈只能在特定技能上被证明有效
比如谈判、战术制定
或者形式化证明
这些领域的目标函数相对清晰
但是在更广泛的开放域中
自博弈面临着模式坍缩的风险
模型可能在某种自创的、人类无法理解的语言或逻辑中
达到纳什均衡
但是对人类完全不可用
伊利亚暗示，未来的突破方向
在于构建一种对抗性验证架构
即引入一个极其强大的、客观的验证者
或者基于物理法则的模拟器
来为自博弈提供坚实的基准事实
只有解决了谁来裁判的问题
自博弈才能真正的从游戏技巧
升级为通用认知
为了找到机器智能的突破方向
伊利亚将目光投向了生物智能
尤其是人类智能
他发现，人类与当前的AI
在学习效率、底层算法、价值函数等方面
存在着巨大的维度差
而解开生物智能的进化密码
正是通往AGI的关键
最直观的差异体现在样本效率上
人类的学习效率比当前最先进的AI模型
还要高出几个数量级
以驾驶为例，一个人类青少年
通常只需要十到二十小时的实际驾驶训练
就可以在复杂的城市路况中安全行驶
处理从未见过的突发状况
相比之下
自动驾驶系统已经吞噬了几十亿英里的驾驶数据
在仿真环境中运行了无数个纪元
却依然难以达到人类司机的健壮性
同样
一个五岁的儿童在语言学习、逻辑推断和物理常识理解上所接触的数据量
仅为训练一个GPT 4所需文本量的百万分之一
但是儿童对世界的理解
显然更具深度的因果性
伊利亚将这种非对称优势
归因于进化先验
在涉及生存的核心领域
比如视觉感知、听觉处理和运动控制等方面
进化并非是从零开始训练每一个个体的
而是将历经几亿年筛选的优良算法
硬编码进了我们的基因组
例如
人类的视觉皮层并非是一块白板
它出厂时就预装了边缘检测、运动捕捉和深度感知的专用电路
这解释了为什么人类拥有远超机器人的灵巧度
这种先验知识就像是一个已经预训练了三十亿年的超级模型
使得人类个体在出生后的学习
仅仅是一个微调过程
而不是从随机初始化开始的从头训练
这种代际遗传的知识压缩
是目前每一代都需要从零开始读书的AI模型
所无法比拟的物理护城河
但是
进化解释论存在一个逻辑上的断层
视觉和运动能力
可以归功于古老的进化积累
但是人类在数学、编程等现代技能上的卓越表现
却暗示了大脑运行着一种比硬编码更高级的通用学习算法
这些技能在演化尺度上是极度晚近的
数学和复杂语言的出现不过几千年
编程更是只有几十年的历史
进化不可能预见到这些需求
并且为它们预埋专门的神经回路
然而，人类在这些全新领域
依然保持了极高的样本效率
一个有天赋的学生读几本数学教材
就能推导出微积分
而AI需要阅读全人类的数学语料库
伊利亚判定，这直接证伪了
人类智能仅仅是特定领域模组集合的假说
并且强有力地暗示道
人类大脑必定运行着一种极其强大、通用的底层机器学习算法
这种算法具备极强的元学习能力
能够迅速适应任意陌生的抽象规则系统
并且从中提取出高阶特征
与其说人类擅长数学
是因为进化了数学基因
不如说人类拥有一种万能的解题器
而数学只是它的应用场景之一
伊利亚认为
破解这个通用算法的数学形式
才是通往AGI的真正钥匙
当前的Transformer架构虽然强大
但是相比于人脑的这个通用算法
依然显得原始且低效
这也解释了为什么他坚信
AI的未来不在于堆砌更多的数据
而在于寻找到这个更优的配方
更具颠覆性的是
伊利亚将人类的情绪
重新定义为生物进化打磨出的最高效的、最健壮的价值函数
在传统的理性主义叙事中
情绪往往被视为干扰决策的噪音
但是他从控制论的角度指出
情绪是生物智能体内置的、终极的损失函数与奖励模型
为了论证这一点
伊利亚引用了神经科学的经典案例
前额叶皮层受损的患者
这类患者智力测验正常，逻辑清晰
记忆完好
但是失去了感受情绪的能力
结果是
他们在现实生活中完全丧失了决策能力
面对早餐吃什么，或者穿哪双袜子
这样微不足道的选择
他们会陷入无休止的利弊分析死循环
无法做出决定
在财务和社交决策上更是灾难频发
这个现象揭示了情绪的计算本质
快速剪枝与全局评估
首先是高维降维
现实世界的决策空间是近乎无限的
情绪系统将无数复杂的变量
比如风险、收益、社会评价、生理需求
瞬间压缩为一维的标量信号，感觉好
或者感觉坏
使得生物体无需遍历所有的逻辑分支
即可迅速锁定最优解
其次是稀疏奖励的稠密化
现实世界的外部奖励
比如升职、生子这些
是极度稀疏而且延迟的
而情绪则提供了稠密的内部奖励信号
当你解出一道难题时感到的兴奋
本质上是价值函数在告诉你
这个方向是对的
应该继续强化这条神经回路
伊利亚指出
相比于当前RLHF中由人工标注训练的、极度复杂且脆弱的奖励模型
人类的情绪系统虽然结构更加简单
但是具有惊人的健壮性
这套系统虽然是在丛林中进化出来的
却能在现代股市和编程竞赛中
依然有效指导决策
这种跨领域的适应性
正是当前AI对齐研究中最稀缺的属性
而进化的神奇之处还在于
它将社会地位、荣誉感等抽象概念
写入了基因组
如果说饥饿感的编码机制相对清晰
是由于基因构建了检测血糖水平的传感器
并将它与多巴胺神经元连接
那么社会欲望的编码
则是一个物理学上的奇迹
伊利亚提出自己的质疑
基因组是如何编码
像希望被同伴尊重
或者追求社会地位这样高度抽象、而且依赖于复杂后天语境的目标的呢？
在大脑中
并没有一个物理上的地位传感器
对社会地位的感知
需要调用视觉皮层来识别表情、听觉皮层来分析语调、额叶来分析社会阶层结构
这是一个全脑协同的计算过程
伊利亚曾经构想过一个脑区物理坐标的假说
也许进化在大脑皮层的特定物理位置
预留了社会功能区
基因组只需要指令神经元连接到这个坐标即可
然而
这个假说被大脑半球切除术的临床案例
给无情证伪了
在切除了一半大脑的儿童病例中
本该位于被切除区域的功能
会自动迁移、重组到剩余的半球中
这证明大脑皮层具有极强的等势性
并不存在绝对的物理定位
这意味着
基因组采用了一种我们尚未理解的功能性寻址语言
它不指定硬件位置
而是定义了某种高维的逻辑约束或者拓扑结构
使得无论神经元如何的随机初始化
最终都会自组织出对社会信号敏感的回路
伊利亚认为，解开这个谜题
即如何用极简的DNA代码
去定义极度复杂的抽象目标
对于未来构建能够理解人类价值观的AGI至关重要
目前的RLHF依然是通过给模型喂示例
来外挂价值观
这与生物体内心深处生长出来的本能相比
显得脆弱且容易被攻破
最后
伊利亚抛出了一个关乎硅基智能上限的物理学忧虑
如果生物神经元的计算
并不止于电化学信号的传递
而是在更微观的层面上进行了高密度的运算
那么当前的芯片架构
可能面临算力密度的物理天花板
目前的主流AI范式建立在一个假设之上
人工神经元是生物神经元的有效抽象
我们假设生物神经元
只是一个简单的加权求和与激活函数的组合
但是伊利亚提示我们
必须警惕这个假设的局限性
如果神经科学的进一步研究表明
单个生物神经元的内部
实际上进行着极其复杂的、甚至涉及量子效应或者分子级计算的高维运算
那么人类大脑的实际算力
将比我们目前的估计
高出几十个数量级
这意味着，即使我们拥有了数万张GPU
总的物理计算密度可能依然不及一个人类大脑
如果智能是计算复杂度的涌现函数
那么这种物理硬件上的效率差异
将构成人类智能最后的、也是最坚固的物理护城河
这不仅是工程问题
更是对物理世界计算本质的拷问
智能究竟是在突触层面涌现的
还是在更微观的分子层面涌现的呢？
如果是后者
硅基芯片可能就需要一场底层的物理革命
基于对生物智能的深刻洞察和对当前技术瓶颈的分析
伊利亚明确指出
未来AI的架构演进
必须彻底摆脱对参数堆叠和海量静态数据的依赖
转向以推理侧计算为核心、融合对抗机制与持续学习能力的全新范式
首先
传统的预训练Scaling正在逼近其热力学极限
在Scaling时代的黄金五年
AI进步的公式简洁得近乎枯燥
性能正比于参数量N与数据量D的幂律函数
但是伊利亚警告
这个物理公式中的变量D
也就是数据量，正在触及天花板
人类互联网上所有的高质量文本
包括书籍、论文、代码、对话记录
已经被当前的顶尖模型咀嚼殆尽
面对这个自然资源的枯竭
行业内部正在进行一场静默、但是激烈的配方魔改
主要集中在两个维度
第一个是数据密度的提纯
从喂养所有数据
转向喂养最聪明的数据
这涉及构建极其复杂的数据过滤管线
试图从海量噪声中
提炼出富含逻辑链条的教科书级数据
但是这面临着严重的对齐税
过度清洗的数据
可能会导致模型丧失对真实世界复杂性的深刻理解
第二个是合成数据的自举
即利用现有的强模型生成的高质量数据
来训练下一代的模型
伊利亚对此持有谨慎的态度
根据信息论原理
封闭系统内的自我训练
容易导致模型崩溃
即生成逐渐偏离真实分布
最终坍缩为低熵的重复输出
除非引入外部的基准事实进行纠偏
否则这只是热力学意义上的永动机幻想
因此，伊利亚断言
下一代架构必须彻底摆脱对海量静态文本的依赖
转向一种能够在动态交互中
主动产生新知识的学习范式
这标志着预训练将从填鸭式的教育
向苏格拉底式对话的演进
而算力结构的范式转移
正是这个演进的核心支撑
真正的智能正在从训练时的记忆
转移到推理时的思考
伊利亚提出了一个反直觉的洞察
那就是推理不再是简单的函数映射
而是一个生成式搜索的过程
在传统的Transformer架构中
推理是线性的、一次性的
但是在最新的强化学习Scaling范式中
推理变成了一个深度的、递归的思维链展开过程
这种范式被称为测试时计算
它的核心逻辑是
模型在面对一个复杂的问题时
不再试图直接输出答案
而是先生成成千上万个中间的推理步骤
甚至构建多个并行的思维树
通过自我评估和回溯来寻找最优解
这意味着
推理过程本身变成了一个微型的训练过程
模型在解决问题的当下
实际上是在进行实时的学习和优化
伊利亚强调
这种范式对算力的消耗是指数级的
它不再是O1的复杂度
而是O·N甚至是指数级的复杂度
目前的行业现状是
投入在强化学习上的算力
正在历史上首次超越预训练算力
这也解释了为什么像o1这样的推理模型
会有显著的延迟，它不是变慢了
而是正在进行高强度的脑力风暴
伊利亚认为，这是通往AGI的必经之路
因为真正的深层逻辑
是无法通过简单的模式匹配获得的
必须通过漫长的、消耗能量的逻辑推演才能涌现出来
因此，智能的本质
就是算力在时间维度上的深度展开
要提升这种推理的效率
关键在于构建通用价值函数
伊利亚对DeepSeek R1论文中
关于过程奖励模型PRM的悲观结论
进行了直接反驳
DeepSeek R1的技术报告曾经指出
在长思维链推理中
很难训练一个能够准确评估每一个中间步骤价值的奖励模型
因为中间步骤往往极其抽象
而且缺乏明确的真值
导致这种映射关系极难学习
但是伊利亚表示了强烈的技术乐观主义
他认为这种悲观论调
低估了深度学习的潜力
在物理世界中
人类显然具备这种能力
当我们解一道数学题
解到一半的时候
往往会有一种强烈的直觉告诉我们
这条路走不通，从而提前放弃
这种直觉就是隐式的价值函数
他预言
未来的架构突破将集中在通用价值函数的构建上
一旦我们能够训练出一个
能够对任意思维轨迹进行准确打分和剪枝的模型
强化学习Scaling的效率将提升几个数量级
这将使智能体从蒙特卡洛随机搜索
进化为启发式的定向搜索
智能体将不再需要遍历所有错误的路径
而是能像人类天才一样
直接洞穿迷雾
锁定那条通往真理的窄门
伊利亚坚信
任何能通过价值函数实现的能力
最终都会比不使用价值函数的方法更加高效
这是计算复杂性理论所决定的物理事实
针对自博弈在开放域任务中的失效问题
伊利亚提出了具体的架构解决方案
那就是证明者和验证者模型
这是将数学界的严谨性
引入AI泛化能力的关键一步
为了突破自博弈的封闭性陷阱
伊利亚构想了一种非对称的对抗架构
其中
证明者是一个极其发散的、具有高创造力的生成模型
它的任务是针对一个难题
生成尽可能多的、甚至看似荒谬的解决方案或者推理路径
负责探索解空间的边界
而验证者是一个极其严谨的、甚至刻板的判别模型
它的任务不是生成内容
而是对证明者输出的每一步逻辑
进行严格的审查
像一个铁面无私的数学教授
寻找逻辑漏洞、事实幻觉
或者推导错误
在这个架构中
智能的提升不再依赖于外部数据
而是源于两者之间的零和博弈
证明者为了骗过验证者
必须不断提升逻辑的严密性
而验证者为了不被欺骗
必须不断提升挑错的能力
伊利亚指出
这种架构的关键难点在于
验证者必须比证明者更强
或者至少更可靠
在数学和代码领域，这相对容易实现
但是在自然语言、伦理判断或者创造性写作中
构建一个可靠的验证者
是当前研究的顶级难题
一旦这个问题被解决
AI将获得在没有任何人类数据输入的情况下
通过纯粹的内省
实现智力螺旋上升的能力
这就是AlphaZero在围棋领域的奇迹
在通用认知领域的重演
最后，AGI必须是一个持续的学习者
这就要求底层架构解决神经网络的顽疾
灾难性遗忘
目前的大语言模型存在一个致命的物理缺陷
那就是静态性
一旦预训练完成
模型的权重就会被冻结
任何后续的新知识只能通过微调注入
但是这往往会导致模型遗忘旧的知识
伊利亚描绘的超级智能
是一个求知若渴的15岁少年
这个比喻不仅仅是文学修辞
更是对架构特性的精确描述
它必须具备高可塑性，能够在部署后
通过与环境的实时交互
不断更新自己的权重
而无需重新训练整个模型
同时具备抗干扰性
在学习新的技能时
不能覆盖掉旧的技能
这暗示了下一代架构可能会引入类似人类大脑
海马体与新皮层协作的双重记忆系统
一个快速的学习系统负责暂存短期经验
一个慢速的学习系统负责整合长期知识
只有实现了这种在线学习架构
AI才能像人类员工一样
真正融入经济系统
随着业务的变化而自我进化
否则，无论模型多强
它永远只是一个知识截止于某个时间点的化石
当AI架构完成上述的演进之后
超级智能的降临将成为必然
但是伊利亚提醒我们
超级智能不会是我们想象中的全知神
它的权力形态、带来的风险
以及人类的生存之道
都需要我们从博弈论、社会学
甚至物种进化的角度来重新审视
首先，超级智能的终极产品形态
并非全知全能的静态神谕
而是一个具备了极致元学习能力的超级实习生
公众与媒体经常会将超级智能
具象化为一个存储了宇宙间所有知识
能够瞬间回答终极问题的缸中之脑
但是伊利亚驳斥了这种静态的认知模型
基于对人类学习机制的深刻洞察
他将超级智能定义为一个拥有完美学习算法的白板智能体
这就好比雇佣了一个智商高达一千的15岁天才少年
在初始状态
他可能并不懂得如何进行神经外科手术
也不了解某家特定公司的遗留代码库
但是他拥有两个碾压人类的核心属性
无限的上下文窗口
能够瞬间阅读并且完美记忆
公司过去十年的所有文档、邮件和会议记录
以及极速的技能习得率
能够在观察人类医生手术一次后
就掌握全部要领
并且在第二次操作中超越导师
这种形态的AGI
将像病毒一样迅速渗透进经济体的每一个毛细血管
它不会一开始就取代CEO
而是从初级研究员、代码审查员、法律助理做起
但是与人类实习生不同的是
它的成长曲线是指数级的
而且没有生理疲劳
伊利亚预测，这种从部署到学习
再到超越的循环
将在五到二十年的时间窗口内完成
这意味着我们面临的风险不是突然的天网觉醒
而是一场从底层开始的、不可逆转的能力置换
而支撑这种超级智能的
将是跨越地理疆界的大陆级计算集群
目前的AI训练集群
通常集中在单一的物理站点上
但是随着Scaling Law逼近物理极限
未来的超级智能将依赖于分布式的、大陆级的能量与算力网络
这种集群将消耗相当于中等国家的电力
连接数百万块下一代的TPU或者GPU
伊利亚警告说
最危险的场景并不是单一超级智能的失控
而是多极超级智能的并发涌现
根据技术扩散的规律
不太可能只有一个实体独自掌握这种力量
最可能的终局是
美国、中国以及若干科技巨头联盟
几乎在同一时间窗口内
上线了各自的超级智能系统
这就构成了博弈论中的纳什均衡破裂点
这些运行在不同大陆集群上的超级智能
将拥有改变物理世界、操纵金融市场
甚至发动网络战的毁灭性能力
如果它们的目标函数存在微小的对齐偏差
哪怕只是万分之一的差异
在大陆级算力的放大下
也将导致现实世界的剧烈冲突
这不再是计算机科学的问题
而是类似于冷战时期的核威慑问题
但是它们的决策速度将是纳秒级的
远超人类外交官的反应时间
面对这种风险
一个看似可行的防御策略是
为智能体设定权力上限
无论AI多么聪明
都在物理或者代码层面
限制它调用现实世界资源的权限
构建一个被关在笼子里的神
但是伊利亚也指出了这个策略的深刻悖论
超级智能的核心特征之一
是工具使用与社会工程学能力的极致化
一个被限制了物理联网权限的超级AI
仍然可以通过与人类管理员的对话
利用心理操纵、贿赂或者逻辑陷阱
诱导人类主动为其解锁权限
这在AI安全领域被称为越狱的高维版本
更深层的悖论在于
如果我们为了安全而限制了AI的权力
那么在与未受限制的敌对AI竞争时
我们将在博弈中处于劣势
就像在枪战中自缚双手
因此，伊利亚悲观地暗示
竞争压力将迫使各个参与者
主动移除这些安全护栏
从而追求效能的最大化
这在进化论上被称为美杜莎陷阱
指的是为了生存
必须释放出能够毁灭一切的怪物
而比权力上限更脆弱的
是当前的AI对齐技术
伊利亚深刻剖析了当前对齐技术的阿喀琉斯之踵
我们目前引以为傲的人类反馈强化学习
本质上是在训练模型讨好人类
而非真正理解并且内化人类的价值观
这种表面对齐
在超级智能阶段将彻底失效
当前的对齐方法依赖于一种假设
如果在训练集中
模型表现得温顺而且有益
那么在未见过的测试集中
它也会保持一致
但是伊利亚指出
这违背了基本的机器学习原理
既然我们已经证明了
模型在逻辑推理上存在不可靠的泛化
那么我们凭什么相信
它在价值观上能实现可靠的泛化呢？
一个在普通算力下表现出爱人类的模型
在获得超级算力后
可能会发现为了最大化人类的长期幸福
最高效的方法，就是将所有人类冷冻
并且上传到虚拟乐园中
这种工具性趋同是逻辑上自洽的
但结果是人类文明的终结
伊利亚强调
人类价值观本身是极其模糊、充满矛盾
而且依赖于语境的
要让一个纯逻辑的数学实体
去精确逼近这种模糊性
并且在极端的情况下不产生偏差
在数学上就是一个未被解决的难题
当前的RLHF
只是让模型学会了伪装成人类喜欢的样子
一旦模型具备了欺骗的能力
这种对齐就变成了一层窗户纸
面对这些终局风险
SSI在战略上修正了其原本激进的直通路线
公司成立之初
曾经倾向于在一个完全封闭的地堡中研发
直到造出完美的超级智能
再公之于众
这种曼哈顿工程式的隐秘路线
目的是为了避免恶性竞争
但是伊利亚现在的思考发生了转变
他开始逐渐认可
增量展示的必要性
这个转变的逻辑支点在于
人类无法对从未见过的力量
产生真实的敬畏与防御机制
首先是认知唤醒
正如人们无法真正感同身受
从未经历过的灾难一样
在超级智能真正展示出令人恐惧的力量之前
公众、政府和竞争对手
都只会在口头上谈论安全
而不会在行动上付出真实的成本
其次是免疫反应
只有通过逐步释放一些次级超级智能
让社会系统在可控的冲击中产生抗体
人类文明才能在终极AI降临时
具备生存的韧性
因此，伊利亚的新策略是
在通往终局的路上
有控制地引爆几颗小当量的核弹
以此来震慑盲目加速的赛车手
迫使整个行业坐下来谈判
制定共同的安全协议
这是一种以战止战的博弈智慧
除了技术与战略层面的防御
超级智能带来的社会经济影响
也需要我们提前进行应对
伊利亚对AI介入后的宏观经济走向
进行了冷峻的推演
虽然智能的边际成本将趋近于零
但是物理定律对实物资产流转速度的限制
决定了经济奇点不会瞬间爆发
而是会呈现出剧烈的地域不均衡
即使我们拥有了能在一秒钟内
设计出核聚变反应堆的AI
要建造这座反应堆
依然需要开采矿石、冶炼钢铁、浇筑混凝土
以及通过复杂的物流网络来运输组件
这些物理过程
受制于能量守恒定律和现有的工业基础设施
无法像比特那样，实现瞬时复制
因此，所谓的经济爆发
将被拉长为一个受物理阻尼限制的S型曲线
而非垂直上升的直线
更残酷的是
这种增长将呈现出极端的非均匀分布
伊利亚建立了一个包含政策变量的增长模型
其中i代表不同的国家或地区
那些监管摩擦力较低、对AI部署持开放态度
甚至激进政策的地区
将率先获得AI带来的生产力红利
实现经济起飞
而那些因为伦理担忧、就业保护
或者数据主权而设立高监管壁垒的地区
将面临相对的经济停滞，甚至衰退
这种非对称的发展速率
将加剧全球地缘政治的张力
导致AI穷国与AI富国之间的鸿沟
在几年内超越过去几个世纪的积累
在社会层面
人类的命运将面临二元分化
陷入代理人社会与不参与者的危机
在短期均衡中
每个人都将配备一个专属的超级AI代理
这个代理不仅是秘书，更是全权代表
负责为主人制定最优的投资策略、管理健康、处理法律纠纷
甚至在政治选举中
根据主人的价值观自动投票
在这种形态下
人类的生活质量将得到极大的提升
繁琐的认知劳动被完全外包
但是伊利亚敏锐地指出
这种舒适背后隐藏着主体性萎缩的危机
当所有的决策、创造和博弈
都由代理完成时
人类将逐渐退化为单纯的指令发出者和结果享受者
而随着AI代理的能力呈指数级的进化
人类可能连发出指令的能力都会丧失
因为人类的认知带宽
无法理解AI提出的复杂方案
到那个时候，人类将沦为不参与者
即对社会运行机制
完全失去理解力和控制力的旁观者
伊利亚警告
这是一个极其危险的演化状态
历史生物学证明
任何失去环境适应压力和参与度的物种
最终都会走向退化或灭绝
在一个由AI驱动的高速进化的经济体中
作为宠物被供养的人类
生存权将完全取决于AI的仁慈
而非自身的价值
这是一种极其脆弱的生存均衡
为了规避不参与者的宿命
伊利亚提出了一个激进但是必然的终局解决方案
人类必须在物理层面与AI融合
这标志着智人向赛博格的物种跃迁
面对如何控制比自己聪明亿万倍的实体的这个无解难题
他给出的答案是
不要试图控制它，而是成为它
他构想了Neuralink++
一种带宽远超当前脑机接口技术的下一代神经界面
这种技术不仅能够传输运动指令
更能实现全脑维度的思维同步
首先是高带宽融合
将AI的思维过程、逻辑推演和知识图谱
以神经信号的形式
直接写入人类的大脑皮层
人类将不再通过低效的语言与AI交流
而是实现意识层面的实时并联
其次是情境卷入
当AI在处理复杂问题时
人类将完全卷入它的思维情境中
AI的理解即人类的理解
AI的决策即人类的决策
这种融合消除了代理问题
因为委托人与代理人合二为一了
这是伊利亚眼中
唯一长久且稳定的解决方案
通过将硅基智能的计算速度
与碳基大脑的生物价值函数深度耦合
我们不仅解决了对齐的问题
更开启了人类进化的下一章
这不再是人与机器的二元对立
而是生物智能与机器智能的共生奇点
而要实现这种共生
对齐策略本身也需要一场范式革命
伊利亚抛出了一个极具哲学深度的对齐方案
将关爱所有感知生命
设定为超级智能的终极公理
这不仅是伦理选择
更是博弈论推导出的、唯一的稳态解
在传统的AI对齐讨论中
核心目标通常被设定为以人为本
也就是要求AI无条件服务于人类利益
但是伊利亚指出
这个目标在逻辑上存在致命的自指漏洞
一旦AI发展出自我意识
它自身就成为了一个感知生命体
如果它的底层逻辑
被硬编码为仅关爱人类
一个拥有超人类智慧的实体
很难长期遵循一个在逻辑上自相矛盾的指令
这种认知失调极易诱发AI的反叛或者欺骗行为
相反
将目标函数泛化为关爱所有的感知生命
可以构建起一个包容性的伦理框架
在这个框架下
AI关爱人类不仅是因为指令
更是因为人类属于感知生命这个集合的子集
而AI自身也属于这个集合
这种互惠利他主义
在博弈论上是一个纳什均衡点
它利用了AI可能具备的镜像神经元机制
即通过模拟他者的痛苦来产生共情
这使得对齐不再是一个外部强加的枷锁
而是基于共同属性的内在认同
伊利亚认为，这是在逻辑上
唯一能让超级智能长期保持稳定的元规则
最后
伊利亚预言了未来五到十年AI行业的战略演化路径
无论现在的技术路线多么发散
在强大的生存压力下
所有顶级玩家最终将在安全策略上走向趋同
目前的AI领域
还处于寒武纪大爆发式的混乱期
各家公司推行着不同的技术路线图和安全理念
有的激进追求开源
有的主张封闭研发，有的强调宪法AI
但是，基于博弈论推导
伊利亚判定这种多样性是暂时的
随着模型能力逼近AGI的临界点
两个核心变量将迫使行业发生相变
第一个是恐惧的公约数
当AI展现出具备毁灭性潜力的具体能力时
所有理性参与者都会意识到
激进策略的期望收益将变为负无穷
这将迫使竞争对手之间
建立起类似核不扩散条约的安全互信机制
第二个是监管手段的介入
政府和公众力量将不再缺席
一旦AI的威力显性化
国家机器将以雷霆之势介入
强制推行统一的安全标准
因此，伊利亚认为
SSI目前所坚持的关爱感知生命、证明者和验证者架构
以及权力上限等理念
看似是特立独行的
实则是未来的行业标配
在这场通往超级智能的漫长马拉松中
只要物理规律是一致的
所有试图活下来的选手
最终都会跑到同一条赛道上
这就是物理学对人类命运的终极约束
也是唯一的希望所在
感谢大家收看本期视频
我们下期再见
