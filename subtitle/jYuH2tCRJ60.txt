大家好，这里是最佳拍档，我是大飞
最近呢
图灵奖得主、前Meta首席AI科学家Yann LeCun
在The Information Bottleneck栏目里
用将近两小时的时间
把当前硅谷追捧的AI发展路径
批了个体无完肤
他直言
靠扩大大语言模型的规模、喂合成数据、搞强化学习微调
就想通往超级智能，完全是胡扯
甚至说，AGI的概念本身就站不住脚
更重磅的是，这位65岁本来可以退休
安享晚年的AI泰斗
选择离开效力12年的Meta
从零创办一家新的公司
押注一套全新的技术路线
要重新定义AI的未来
这已经不是一场普通的吐槽了
而是AI领域路线之争的一次公开宣战
一边是硅谷巨头们扎堆押注的算力、数据、参数竞赛
另一边是杨立昆坚持的认知与感知优先的世界模型
今天咱们就来回顾一下这期访谈
看看这位AI先驱
为什么敢逆势而为
他口中的世界模型到底是什么
以及这场技术革命
可能给AI行业带来怎样的颠覆
很多人的第一反应可能会是
杨立昆已经功成名就了
图灵奖、女王奖在手
在Meta主导了AI研究12年
推动了深度学习、计算机视觉等多个领域的发展
完全可以退休享受荣誉
但是他偏偏在65岁这个年纪
选择创业
背后到底是什么原因呢？
杨立昆在访谈中坦言
他离开Meta创办先进机器智能公司AMI（Advanced Machine Intelligence）
核心原因是现在的AI投资热潮
让长期研究型的创业成为了可能
放在以前
这类需要长期投入、短期内看不到回报的基础研究
只能依托IBM、贝尔实验室
这样的垄断型大企业
或者Meta、谷歌、微软这些科技巨头的研究院
但是现在情况变了
包括谷歌、OpenAI甚至Meta在内的很多实验室
都从曾经的开放研究，转向了封闭
因为大家更看重短期产品的落地
而非真正的技术突破
杨立昆点出了当前工业界研究的痛点
在他看来，真正的研究必须公开发表
接受学界的检验
否则内部过度追捧的项目
很可能只是一种自嗨式错觉
因为你永远不知道
别人已经做出了更出色的工作
所以AMI从一开始就明确了定位
既要坚持开放研究
所有上游的核心成果，都会公开发表
又要最终推出实际产品
围绕世界模型和规划技术
成为未来智能系统的主要供应商之一
杨立昆之所以有这样的底气
是因为他已经在这个方向上钻研了将近20年
再结合纽约大学和Meta的研究积累
现在终于到了
可以把构想变成现实的时刻了
而选择创业的另一个深层原因
是杨立昆对当前主流AI发展路径的彻底否定
他认为
现在硅谷所有的公司都在扎堆做同一件事
那就是扩大大语言模型的规模
堆砌数据和算力，优化强化学习微调
这种技术单一化的现象非常危险
就像所有选手都挤在同一条赛道上
拼命的往前冲，却完全忽视了
可能从另一个方向而来的颠覆性技术
而他要做的，就是那条少有人走的路
构建一个能够理解和预测世界的世界模型
杨立昆直言
当前基于大语言模型的架构
虽然在语言的处理上表现尚可
但是构建的智能体系统
存在着致命的缺陷
那就是需要海量的数据来模仿人类行为
而且可靠性极低
为什么会这样呢？
杨立昆指出了核心问题
大语言模型本质上就是一个记忆型系统
而不是理解型的系统
要训练一个性能还不错的大语言模型
需要用到几乎整个互联网的文本数据
再加上合成数据和授权数据
两三年前
主流模型的预训练规模就已经达到了30万亿token
相当于10的14次方字节的数据量
这么大的数据量
本质上是让模型记住文本中孤立的事实
然后在生成内容时进行复述
但是文本数据的冗余度很低
缺乏真实世界的结构信息
这就决定了大语言模型永远无法真正理解世界
更关键的是
大语言模型完全无法处理高维度、连续
而且含有噪声的数据模态
比如图像、视频、物理世界的感知数据
虽然现在有些大模型会结合视觉模块
但是这些视觉能力都是分开训练的
并不属于大语言模型架构的核心部分
杨立昆做了一个很直观的对比
10的14次方字节的文本数据
是大语言模型的训练基础
但是同样体量的视频数据
只相当于1.5万小时的内容
这大概是YouTube半小时的上传量
也差不多是一个四岁孩子一生中
清醒时间看到的视觉信息总量
而视频数据的结构远比文本丰富
这种丰富的冗余结构
正是自监督学习的关键
如果数据是完全随机的
自监督学习根本无法进行
杨立昆一直坚持一个观点
仅靠文本训练
永远不可能达到人类水平的智能
因为真实世界的理解、预测和行动能力
远比生成流畅文本要复杂得多
而现有以语言为核心的模型
从未真正触及到这个问题的本质
在他看来，当前AI行业最大的误区
是把逼近人类级别的智能
当作了目标
却忽略了一个更基础、更困难的门槛
那就是让机器具备狗的智能水平
狗能理解物理世界的基本规律
物体不会凭空消失、东西会下落、不能同时出现在两个地方
也能根据环境变化调整行为
进行简单的规划和预测
而这些能力
恰恰是大语言模型完全不具备的
它们只是被微调到给出看起来正确的答案
这是复述，不是理解
杨立昆毫不客气地说
那些宣称一两年内实现AGI的说法
完全是脱离现实的幻想
真实世界的复杂度
远不是通过对世界进行token化
再喂给语言模型就能解决的
既然大语言模型走不通
那么杨立昆押注的世界模型到底是什么？
很多人会误以为
世界模型是对现实世界每一个细节的完整复刻
比如像《星际迷航》里的全息甲板
那样的高度逼真模拟器
但是这恰恰是杨立昆极力反对的错误认知
世界模型不需要是现实的逐个像素模拟器
而是在抽象表征空间中
只模拟与任务相关的那部分现实
杨立昆举了一个很经典的例子
如果问100年后木星在哪里
你根本不需要关于木星的全部信息
只需要6个数字
三个位置坐标和三个速度分量
其余的细节都无关紧要
这个例子背后
是抽象层级的核心思想
在真实世界中
我们对事物的理解都是分层抽象的
从粒子、原子、分子
到细胞、器官、个体、社会
每一层的抽象都会忽略下层的大量细节
而正是这种忽略
让我们能够进行更长期、更稳定的预测
比如用流体力学来模拟飞机周围的气流
不会逐个分子的模拟
而是把空间切成小立方体
记录速度、密度、温度等关键变量
解偏微分方程
这就是对物理过程的高度抽象
既高效又有效
所以，世界模型的核心逻辑
就是学习这样的抽象表征空间
先滤除输入中大量无法预测的细节
然后在这个简化后的表征空间内
预测世界的演化规律
它关注的不是生成看起来像什么
而是世界将如何变化
从而为机器提供更接近真实认知的基础能力
这正是当前AI缺失的前额叶皮层功能
对应着规划、预测和行动能力
与主流生成模型相比
世界模型的本质区别在于
生成模型是在像素或文本层面直接输出的
试图复现表面的统计相关性
而世界模型是在抽象表征层面进行预测
捕捉的是世界的底层动力学规律
杨立昆强调
一个视频生成模型可能看起来很炫酷
但是它并不一定理解世界的底层逻辑
而世界模型的目标
就是让AI真正看懂世界的运行规则
要实现这样的世界模型
杨立昆团队的核心技术架构是联合嵌入预测架构
JEPA（Joint Embedding Predictive Architecture）
JEPA的核心思想
就是放弃像素级或文本级的直接预测
转而在抽象表征空间中进行预测
同时通过一系列技术手段避免模型坍缩
也就是模型为了最小化预测误差
直接输出恒定表征来作弊的问题
回忆起来，杨立昆对世界模型的执着
并不是一时兴起
而是跨越近20年的研究沉淀
在这条路上
他经历过方向的迷茫、技术的瓶颈
也见证了整个领域的起起落落
早在近20年前，杨立昆就确信
构建智能系统的正确途径
是某种形式的无监督学习
21世纪初
他就开始朝着这个方向探索
当时的主流思路
是训练自编码器来学习表征
编码器将输入转化为表征
再解码还原
确保表征包含输入的全部信息
但是后来他发现
这种表征必须包含全部信息的直觉是错误的
并不是构建智能的有效方法
那段时间，他和团队尝试了多种方案
包括受限玻尔兹曼机、去噪自编码器等等
而杨立昆自己主攻稀疏自编码器
通过高维稀疏表征来构建信息瓶颈
限制表征中的信息量
他的学生
包括后来成为DeepMind首席技术官的科拉伊·卡武库奥卢（Koray Kavukcuoglu）
都围绕这个方向做了不少博士研究
核心目标就是通过自编码器预训练
来搭建非常深的神经网络
但是事情出现了转折
随着归一化、ReLU激活函数等技术的出现
以及数据集规模的扩大
研究人员发现
在完全有监督的方式下
也能成功训练相当深的网络
于是，自监督/无监督学习的想法
被暂时搁置
直到2015年残差网络（ResNet）的出现
基本解决了训练极深架构的问题
也是在2015年
杨立昆重新回到了如何迈向人类级别AI的初心
他意识到
强化学习等方法在样本效率上极低
难以扩展，于是关于世界模型
即系统能够预测自身行动后果
并且进行规划的想法，开始真正成型
2016年，他在NIPS的主题演讲中
公开阐述了这个核心主张
随后和学生开始在视频预测等领域
进行具体的研究
但是当时整个领域都犯了一个根本性的错误
那就是试图在像素级别进行预测
在视频这样的高维连续空间里
这种做法几乎是不可能的
因为预测本质是非确定性的
模型需要潜变量来表征未知信息
而像素级预测的复杂度
远超当时的技术能力
杨立昆和团队实验了多年
探索了扩散模型、基于能量的模型等
训练非确定性函数的方法
始终没有突破
直到后来，他才领悟到根本的出路
那就是放弃像素级预测
转向抽象表征层面的预测
但是这个方向也面临一个重大的难题
坍缩
早在九十年代，杨立昆就发现
如果只是简单训练两个共享权重的神经网络
让它们对同一对象的略微不同版本
输出相同的表示，系统很快就会塌缩
学不到任何有用的东西
为了解决这个问题
1993年，杨立昆引入了对比项的核心思路
除了相似样本对
还引入不相似的样本对
通过训练让系统在相似样本上拉近表示
在不相似样本上拉远表示
形成相似吸引、不相似排斥的代价函数
这个想法最初来自一个非常实际的需求
为信用卡磁条设计小于80字节的手写签名编码
用于签名验证
最终技术上大获成功
但是商业上却被用PIN码替代的方案否决
这也让杨立昆得到一个深刻的教训
技术可行，不代表商业上会被采纳
到了2000年代中期
杨立昆和两位学生拉娅·哈塞尔（Raia Hadsell）以及苏米特·乔普拉（Sumit Chopra）
重新回到对比学习方向
提出了新的目标函数
正样本对应低能量
负样本对应高能量
能量本质上就是表征之间的距离
他们在2005年和2006年的CVPR会议上发表了相关论文
让对比学习重新活了过来
但是当时的效果并不理想
比如在图像任务中学到的表征维度很低
在ImageNet上训练后
表征的有效维度只有两三百
远达不到实际应用的需求
真正的突破发生在大约五年前
杨立昆在MIT的博士后斯特凡诺（Stefano）
提出了一个他最初并不看好的想法
直接最大化编码器输出的信息量
杨立昆之所以怀疑
是因为早在1980年代
杰弗里·辛顿就做过类似的尝试
信息量本身很难最大化
因为通常只有上界
没有可计算的下界
但是斯特凡诺提出的方法居然奏效了
这就是后来以理论神经科学家命名的巴洛双胞胎（Barlow Twins）方法
这个突破让杨立昆意识到
这个方向值得深入推进
随后
他们又提出了方差-不变性-协方差正则化
VICReg（Variance–Invariance–Covariance Regularization）
结构更加简单，效果反而更好
最近
杨立昆和兰德尔（Randall）还讨论了一个可以进一步工程化的方案
信号正则化（SigReg）
核心思想是约束编码器输出的向量分布
接近各向同性高斯分布
杨立昆对这个领域的未来充满信心
他认为未来一两年内还会有显著的进展
而这条技术路线
正是训练能够学习抽象表征模型的关键
而抽象表征，恰恰是世界模型的核心
在访谈中，杨立昆最颠覆认知的言论
莫过于AGI是个彻头彻尾的谎言
但是他并不是要否定AI能达到高的水平
而是认为通用智能这个概念
本身就站不住脚
它本质上是以人类智能为参照定义的
但是人类智能本身是高度专用化的
杨立昆解释道
我们擅长在现实世界中行动、与他人互动
但是在下棋等任务上表现糟糕；
而很多动物在某些方面远胜人类
比如狗的嗅觉、猫的灵活性
我们之所以误以为自己是通用的
只是因为我们只能理解
自己能够想象的问题
所以在他看来，与其讨论通用智能
不如讨论人类水平的智能
比如机器是否会在所有人类擅长的领域
达到或超过人类？
答案是肯定的
而且在某些领域已经发生了
比如机器可以在上千种语言之间进行双向翻译
这是任何人类都无法做到的
但是这个过程不会是一个突发事件
而是一个渐进的过程
杨立昆给出了一个非常保守但是务实的时间表
如果一切顺利
没有遇到尚未意识到的根本性障碍
最乐观的情况是，在5到10年内
我们或许能看到接近人类
或者至少接近狗水平的智能系统
但是这只是最乐观的估计
历史告诉我们
AI发展中总会出现新的瓶颈
所以可能需要20年
甚至更久才能够突破
而更令人意外的是，杨立昆认为
从当前的AI水平到狗水平智能
比从狗水平到人类水平更难
一旦你达到狗的智能阶段
绝大多数核心要素就已经具备了
他解释道，从灵长类到人类
真正新增的关键能力可能主要是语言
而语言在大脑中只占据极小的区域
我们现在的大语言模型已经在这方面做得相当不错
某种意义上，未来的语言模型
可能会扮演人脑中布罗卡区和韦尼克区的角色
而我们当前真正缺失的
是相当于前额叶皮层的能力
也就是世界模型的规划与行动能力
杨立昆认为
智能的核心不在于记忆和复述
而在于能够预测自身行动的后果
并且用于规划
狗之所以具备基础智能
就是因为它能理解物理世界的基本规律
能根据环境预测接下来会发生什么
然后调整自己的行为
而当前的AI
恰恰缺少这种预测和规划的闭环
这也是为什么
即便大语言模型能生成流畅的文本
却依然无法在真实世界中自主行动
随着AI能力的提升
安全问题一直是业界争论的焦点
尤其是AI灭世论的说法
让很多人担心AI未来会失控
杨立昆作为AI领域的权威
也分享了他的独到见解
他既不认同暂停AI发展的极端观点
也反对忽视风险的盲目乐观
杨立昆坦言
他亲身经历过AI恐惧带来的真实伤害
有一次在纽约大学的校园
他遇到一名情绪严重不稳定的人
携带危险物品被警方带走
而这个人的精神状态
就受到了AI灭世论的影响
还有高中生给他写信
说他们被这类言论吓到
甚至不再上学
但是他同时强调，历史告诉我们
任何强大的技术都会带来利弊
关键不在于是否发展技术
而在于如何通过工程和治理来控制风险
他用汽车来举例
早期的汽车极其危险
但是通过安全带、溃缩区、自动刹车系统等技术演进
如今已经大幅降低了死亡率
欧盟强制配备的自动紧急制动系统
已经被证明能减少40%的正面碰撞事故
AI也是如此，它既可能带来风险
也已经在医疗影像等领域
挽救了大量生命
对于是否需要暂停AI发展来专注安全的问题
杨立昆的答案非常明确
安全必须与发展同步进行
而不是先停下来等着绝对安全
他用喷气发动机作比喻
第一代喷气发动机根本不安全、不可靠
但正是在不断工程改进中
才达到了今天这种可以连续飞行17小时的可靠性
所以他认为，AI也会走类似的路径
我们会逐步构建具备规划与行动能力的系统
同时在底层引入明确的安全约束
比如家用机器人必须始终避开人类、不能伤害人
手持刀具时必须限制动作幅度
这些都可以通过低层的规则来明确约束
所谓回形针最大化的极端案例
在工程上其实是非常容易避免的
而对于大语言模型容易被越狱、绕过安全限制的问题
杨立昆认为
这正是依赖大语言模型的根本缺陷
我们不应该指望通过微调或内容过滤来解决安全问题
而应该转向目标驱动（objective-driven）的AI架构
这种架构的安全设计是先天的
而非事后修补
它具备三个关键能力
第一，拥有世界模型
能够预测自身行为可能带来的后果
第二
可以规划一系列的行动来完成任务
第三，也是最关键的
受到一整套硬性约束的限制
确保无论采取什么行动、预测到什么世界状态
都不会对人类造成危险
换句话说
它的输出不是靠过滤坏的内容
而是通过在满足约束条件的前提下
优化目标函数得出的
从结构上就不具备逃逸的可能性
对于那些通过暴力搜索来提升安全性的方法
比如让模型生成大量的候选输出
再用过滤系统挑出最不糟糕的结果
杨立昆直言极其荒谬
这种方法的计算成本高得离谱
无法规模化
除非有真正意义上的目标函数或价值函数
能在生成过程中就把系统引导到高质量、低风险的输出
访谈中
主持人还问到了Meta内部的AI布局
以及Alex Wang是否在接替杨立昆角色的问题
杨立昆也公开回应了这些内斗传闻
首先，杨立昆明确表示
Alex Wang并不是在接替我
他解释道
Alex Wang负责的是Meta所有AI相关的研发与产品整体运作
而不是科研本身
Alex Wang并不是研究员或科学家
而是一个负责全面统筹的管理者
在Meta的超级智能实验室体系下
AI相关工作大致分为四个部分
第一是人工智能基础研究实验室（FAIR）
负责长期基础研究
第二是TBD Lab，主要做前沿模型
几乎完全聚焦大语言模型
第三是AI基础设施，包括软件和硬件
第四是产品部门
把前沿模型做成真正可用的产品
比如聊天机器人
并且集成到WhatsApp等应用中
Alex Wang的职责是统管这四个方向
而杨立昆本人是FAIR的首席AI科学家
在访谈时他已经明确表示
还会在Meta待三周左右
之后就会正式离职
对于FAIR的定位变化
杨立昆也毫不避讳地表示
FAIR目前由他在纽约大学的同事罗布·弗格斯（Rob Fergus）领导
在乔尔·皮诺（Joel Pineau）离开后
FAIR被明显推向更短期、更偏应用的研究方向
发表论文的重要性下降
更多是为TBD Lab的大模型工作提供支持
这也意味着Meta整体正在变得更封闭
有些原本属于FAIR的研究团队
也被重新归类到产品部门
比如做SAM（Segment Anything）模型的团队
因为他们的技术更偏向对外、实用型
更适合直接对接产品落地
这种重应用、轻基础研究的转向
或许也是杨立昆最终选择离开的重要原因之一
毕竟，他一直坚信
基础研究的开放和沉淀
才是AI长期发展的核心动力
作为世界模型领域的先驱
杨立昆也点评了当前其他试图构建世界模型的公司
其中既有肯定，也有尖锐的批评
首先是伊利亚的SSI
杨立昆直言它已经成了行业笑话
几乎没人知道他们在干什么
包括他们自己的投资人
不过他也强调这只是传言
不确定真假
对于Physical Intelligence公司
杨立昆表示了解他们的大致方向
他们主要做几何一致的视频生成
即场景具有持久的三维结构
转身再回来，物体不会凭空变化
但是杨立昆认为
这仍然是生成像素的思路
而他早就已经明确
生成像素本身是个错误方向
因为这种方法无法让模型
真正理解世界的底层动力学
只是学会了表面的统计相关性
在所有同行中
杨立昆相对认可的是总部位于牛津的Wayve公司
他本人也是这家公司的顾问
Wayve在自动驾驶领域构建了一个世界模型
核心思路是先学习一个表征空间
再在这个抽象空间中做时间预测
杨立昆认为他们做对了一半
对的地方在于
预测应该发生在表征空间
而不是像素空间，但是问题在于
他们的表征空间仍然主要通过重建训练得到
这一点在杨立昆看来是错误的
不过即便如此
Wayve的系统整体效果非常好
在自动驾驶领域已经走得相当靠前
此外
NVIDIA和Sandbox AQ公司也在探索类似的方向
Sandbox AQ的首席执行官杰克·希达里（Jack Hidary）提出了大型定量模型（Large Quantitative Models）的概念
而非语言模型
本质上就是能够处理连续、高维、噪声数据的预测模型
这与杨立昆的主张高度一致
谷歌（Google）也做了很多世界模型相关的研究
但是主要仍然是生成式路径
杨立昆认为
达尼贾尔·哈夫纳（Danijar Hafner）的Dreamer系列模型
其实走在了正确的道路上
只可惜哈夫纳已经离开谷歌创业了
从这些点评可以看出
杨立昆判断一个世界模型公司是否走对路的核心标准
就是是否放弃了像素级/文本级的直接生成或重建
转向了抽象表征空间的预测
这也正是他自己多年研究得出的核心结论
杨立昆之所以选择创办一家全球性的公司
在巴黎、纽约等地布局
而不是扎根硅谷，核心原因之一
就是硅谷存在严重的技术单一化问题
在硅谷，竞争极端激烈
所有公司都在被迫做同一件事
如果你走一条不同的技术路线
就有掉队的巨大风险
杨立昆解释道
现在OpenAI、Meta、Google、Anthropic几乎所有的硅谷巨头
都在扎堆做大语言模型
拼命扩大规模、堆砌数据和算力
在这种环境下
没有人敢轻易尝试不同的技术路线
因为一旦失败，就可能被市场淘汰
更严重的是
硅谷还形成了一种大语言模型洗脑的文化
很多人坚信
只要不断扩大模型的规模、生成更多合成数据、加强强化学习微调
就一定能走向超级智能
但是杨立昆认为这是彻底错误的
因为大语言模型根本不擅长处理连续、高维、噪声数据
比如在视频、物理世界感知等领域
大语言模型的尝试几乎都失败了
大家拼命都在同一条战壕里向前冲
却很容易被来自完全不同方向的技术突破所颠覆
杨立昆的这句话
其实是对整个行业的警示
他透露，其实在硅谷的大公司内部
也有不少人私下认同他的观点
只是迫于公司的战略方向
无法公开表达或者践行
而他现在正在把这些人招入自己的新公司AMI
最后
主持人问到了一个很多人关心的问题
对于已经65岁、功成名就的杨立昆
为什么还要选择从零开始创业？
他的回答非常简单，因为使命感
杨立昆认为，提升世界上的智能总量
是一件内在正确的事情
智能是这个世界上最稀缺、最被需要的资源
这也是为什么人类会投入如此多的成本去教育
无论是帮助人类更聪明
还是用机器来增强人类智能
本质上都是在服务同一个目标
强大的技术必然会伴随风险
但那是工程和治理的问题
而不是不可逾越的根本障碍
杨立昆一生的研究、教学、公共传播
几乎都围绕着让人类变得更聪明这件事
而机器智能
本质上也是这个目标的一部分
他之所以在65岁的年纪还要创业
就是因为他相信
自己多年坚守的世界模型路线
能够真正推动AI的本质进步
为人类带来更有价值的智能工具
当然，回顾职业生涯
杨立昆也有遗憾，最让他后悔的
是没有花足够时间把自己的想法写下来
结果经常被别人抢先
反向传播算法（Backpropagation）就是一个例子
他其实很早就有了类似的思路
但是没有及时完整的发表
不过杨立昆并没有纠结于此
因为他明白
科学思想几乎从来不是孤立产生的
从想法到论文、到理论、到应用、到产品
本身就是一个漫长而复杂的链条
就像世界模型这个概念一样
其实它也并不新了
早在1960年代
控制论和航天工程就已经在使用世界模型来规划火箭轨道
所谓系统辨识
更是1970年代的老概念
但是真正的难点
从来不在谁最早提出
而在于把一个想法真正变成可工作的系统
而这正是杨立昆现在要做的事情
杨立昆的这场访谈
与其说是一次个人观点的表达
不如说是一场AI行业的路线宣言
一边是硅谷巨头们主导的算力+数据竞赛
追求短期产品落地和商业价值
另一边是杨立昆坚守的认知+感知路线
执着于基础研究和长期技术突破
到底谁是对的？
现在还无法给出答案
但可以肯定的是，杨立昆的存在
为AI行业提供了一种宝贵的多元视角
如果所有顶尖人才和资源都扎堆在同一条赛道上
行业很可能会陷入路径依赖
错过真正的颠覆性突破
而杨立昆的创业
恰恰为这种突破提供了可能性
也许正如他所说
智能的核心应该是预测和规划
而不是记忆和复述
未来的AI，终将走出实验室
走进真实世界
成为能够理解、预测、行动的智能体
而世界模型
很可能就是通向这个未来的关键钥匙
无论最终结果如何
这位65岁仍在追梦的AI泰斗
用自己的坚守和勇气
为我们展现了科学家的初心和使命感
而这场关于AI未来的路线之争
也必将深刻影响整个行业的发展方向
感谢收看本期视频，我们下期再见
