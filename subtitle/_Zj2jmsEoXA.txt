大家好这里是最佳拍档
我是大飞
前段时间Meta发布了一个
号称可以分割一切的SAM模型
可以为图像或者视频中的任何物体
生成一个mask然后去把它们给分割出来
我们也专门做了一期节目来介绍
在这之后就陆续有一些工作
在这个分割的基础上
又结合了目标检测图像生成等等这些功能吧
不过大部分的研究
还是基于一些静态图像来做的
不过现在
一群来自康奈尔大学
谷歌研究院和UC伯克利的研究人员
联合提出了一种完整而且全局一致的运动表征
叫OmniMotion
并且提出了一种新的test time的优化方法
可以对视频中的每一个像素
进行准确完整的运动估计
跟踪视频中任意物体的运动轨迹
从该研究发布的Demo来看呢
它的运动跟踪的效果还是非常不错的
比如说去跟踪一个跳跃袋鼠的运动轨迹
或者是女孩荡秋千时的运动轨迹
以及人物跳跃时候的运动轨迹
火车转弯时候的运动轨迹
马术选手和马同时在运动的运动轨迹
以及这个蝴蝶煽动翅膀时的运动轨迹等等
而且即使这个物体被遮挡了
也能够跟踪到它的运动轨迹
比如说狗在跑动的过程中被树遮挡
或者是小孩在玩耍的时候被铁链所遮挡等等
而且在官方提供的这个交互式的Demo里
你还可以自己去设置跟踪点
然后逐帧的去看每个点的运动过程
注意在被物体遮挡的时候
这个跟踪点会显示成一个加号
那在计算机视觉领域
常用的运动估计方法有两种
一种是稀疏特征跟踪
一种是密集光流
但是这两种方法呢各有各的缺点
比如说稀疏特征跟踪呢
它不能够建模所有像素的运动
而密集光流的方法
它无法长时间的去捕获运动的轨迹
但是这项研究提出的全局运动表征OmniMotion
使用了quasi-3D的规范体积来表征视频
并且通过局部空间和规范空间之间的双摄
bijection对每个像素进行跟踪
这种表征呢能够保证全局的一致性
即使在物体被遮挡的情况下
也能够进行运动的跟踪
并且可以对相机和物体运动的任何组合进行建模
具体来说
就是将帧的集合与成对的噪声运动估计
比方说光流场作为输入
从而形成整个视频的完整全局一致的运动表征
然后呢它又添加了一个优化的过程
使它可以用任何帧的任何像素来查询这个表征
从而在整个视频中产生平滑准确的运动轨迹
那值得注意的是
这个方法可以识别画面中的点
何时被遮挡住了
甚至是可以穿过遮挡来跟踪点
传统的运动估计方法
当这个物体被遮挡的时候
就会失去对这个物体的跟踪
那为了在没有显式动态3D重建的情况下
能够继续准确追踪到真实世界的运动
OmniMotion将视频中的场景
表示为规范的3D体积
通过局部的规范双射
映射成每个帧中的局部体积
局部规范双射被参数化为一个神经网络
并且在不分离两者的情况下
去捕获相机和场景的运动
在这种方法下
视频就可以被看作是一个
来自固定静态相机局部体积的渲染结果
由于OmniMotion没有明确的区分相机和场景运动
所以它形成的表征
不是物理上准确的3D场景重建
所以他们的研究人员
将这个称为一个叫quasi-3D的表征
翻译过来应该就是准3D或者是伪3D
因为OmniMotion保留了
投影到每个像素的所有场景点的信息
以及它们的相对深度顺序
所以这就让画面中的点
即使暂时被遮挡住了也能够对它进行跟踪
那OmniMotion的实际效果怎么样呢
我们可以看一下它跟其他模型的对比
那我们基本可以看到
相比于RAFT、TAP-NET、FLow-Walk、PIPs等等这些其他的方法
OmniMotion对物体的跟踪效果都会更好
无论是快速的移动
遮挡远近实体啊这些情况
它的效果都还不错
不过呢研究人员也提出了
这种方法的一些局限性
就是在某些快速非刚性的运动下
会出现跟踪失败的情况
目前这项研究的代码还没有开放出来
不过估计也快了
好了本期的分享呢就到这里
呃欢迎大家订阅我们的频道
我们下期再见
