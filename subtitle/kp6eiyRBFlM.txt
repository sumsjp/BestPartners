大家好，这里是最佳拍档，我是大飞
最近
一项关于大模型的研究登上了权威期刊Nature
这项研究打破了人们一直以来的一个误解
那就是在我们的固有印象中
似乎是模型的参数规模越大
生成的答案就越准确
但是最新研究的结论却并不是这样
事实上，相比于小参数模型
大参数模型不会承认它们的“无知”，
而是会更加倾向于生成错误的答案
更重要的是
人们并不善于发现这些错误
这到底这是怎么一回事呢
为什么数据越多参数越大
反而越不可靠了呢？
今天大飞就带大家来深入了解一下这项研究
这项研究来自瓦伦西亚理工大学的团队及其合作者
他们在研究了GPT、LLaMA和BLOOM系列的大语言模型之后
得出了两条结论，第一条是
参数规模更大的模型
在一些复杂任务上
生成的答案相对会更加准确
尤其是在一些复杂任务上
但是整体的可靠性却比较低
第二条是，在所有不准确的回答中
大参数模型的错误回答比例
相对于小参数模型来说有所上升
甚至在一些简单任务上
会出现更多的低级错误
比方说
GPT-4 在处理简单的加法和字谜时的错误率
竟然比一些小模型高出15%。
这主要是因为大参数模型不太可能会回避回答问题
比如承认自己不知道或者转移话题
在这项工作中
研究人员从人类用户与大语言模型互动的角度
探讨了难度一致性、任务回避和提示稳定性
这三个核心交互元素对大语言模型可靠性的影响
研究团队对比了三大模型不同系列、不同型号
在不同任务中的表现
尤其是在数字计算、文字游戏、地理知识、基础与高级科学问题和信息转化等任务方面
通过对这些任务的正确率、错误率和回避行为的分析
揭示了模型扩展所带来的能力反差现象
论文作者通过研究
发现了一个十分反直觉的现象
那就是模型的正确率
并不总是和人类一样
随着难度上升而下降的
模型在面对复杂任务的时候
表现反而会显著提升
但是在简单任务上的错误率
却会有明显上升
论文作者把这种现象称为“难度不一致（Difficulty Inconsistency）”，
也就是说
扩展后的模型在复杂任务上逐步提升了正确率
但是在简单任务上却变得更加容易出错
以加法任务为例
虽然模型能够解决复杂的多位数加法
但是在简单的两位数加法上
却频繁出错
其中
所有LLaMA系列模型在最简单任务上的正确率
全部都没能超过60%，
反而在一些较难的任务中
表现却相对出色
研究团队指出，这个现象表明
当前模型的扩展可能过于集中在复杂任务上
而忽视了简单的任务
从某种意义上来说
这也表明了扩展模型的参数
似乎并不总是能带来全面的提升
这对于大语言模型在实际应用的可靠性来说
无疑画上了一个巨大的问号
除了难度不一致现象以外
研究还揭示了优化后模型中
回避行为与错误率之间的微妙关系
什么是回避行为呢？
就是指模型在无法正确回答问题的时候
选择不作答
或者给出不符合要求的回应
在模型没有经过优化的时候
回避行为比较常见
也就是当模型不确定答案的时候
往往会选择“不作答”，
或者提供模糊的回应
然而，在经过扩展和优化后
模型虽然大幅减少了回避行为
但是转而给出了更多表面上“合理”、但是实际上错误的答案
这意味着
虽然一些优化方法能够让模型变得更加“自信”，
减少回避行为
但是错误率却也随之增加
这个现象在GPT-4和GPT-3.5-turbo等模型中
尤其明显
说明规模扩展并没有带来预期的稳定性
对比LLaMA和BLOOM模型
这个趋势虽然没有那么明显
但是也同样存在
换句话说就是
大模型们有了更多的知识
反而有点“骄傲自大”和“粗心大意”了
研究团队称
这种现象与用户对模型产生的过度信任
密切相关
尤其是在用户面对看似简单任务的时候
这种信任反过来更加助长了大模型们的“骄傲心理”。
大模型们好像也知道
自己在做简单题目的时候
被质疑的概率更低
所以更加自信地给出了错误的答案
就好像它在想，反正人类也看不懂
随便糊弄一下就可以了
大家是不是感觉这样的大语言模型
反而更有点像人了呢？
其实不是这样的
问题的根源在于
人类与大语言模型对于难度的感受是不同的
虽然模型在人类认为困难的任务上
往往不太准确
但是即使是在简单任务上
它们也不是100%的准确
这意味着不存在可以完全信任模型、让它做到完美运行的“安全区”，
也就是说
无论是什么问题
大语言模型们都有可能犯错
必须要经过人工的检验才行
不过，随着模型规模的扩大
模型对不同自然语言表述的敏感度也有所提高
能够更好地应对措辞上的微调
这自然也包括那些“简单的任务”，
但是整体上来说
模型对于简单题目的“骄傲”是在所难免的
毕竟，其实目前而言
所有的大模型都更加注重于那些“人类认为困难的题目”，
而那些在人类看来很简单的题目
在模型的眼中
可能其实并不像人类认为的那么简单
研究人员也对模型进行了一些相关的优化
研究论文中介绍
没有经过优化的GPT和LLaMA模型
对提示词的选择表现出了极高的敏感性
尤其是在简单任务中
如果提示词选择的比较恰当
模型的表现就会有所提升；
而优化后的模型
虽然在提示词的敏感性上有所改善
表现更加稳定
但是也存在一定的变异性
也就是说虽然正确率更高
但是整体而言
在简单题目上正确率的反常下降现象
仍然存在
同时
模型在与人类判断难度的一致性和谨慎度方面
表现也较差
论文作者之一的沃特·谢拉特（Wout Schellaert） 表示
最终，从人类的角度来看
大语言模型正在变得越来越不可靠
而让用户监督来纠正错误
并不是一种解决方案
因为我们往往过于依赖于模型
无法识别出不同难度级别的错误结果
因此
通用人工智能（AGI）的设计和开发
需要进行根本性的改变
特别是对于高风险的应用来说
预测大语言模型的性能
并且检测模型的错误至关重要
其实人类对于工具的要求
很多时候功效都不是第一位的
安全稳定能达到预期才是最重要的
比方说
如果一个超级计算机可以解决世界上的一切问题
但是同时有可能也会给出一个错误的答案
并且人类所有科学家加在一起都不一定能够发现哪里错了
那这样的超级计算机几乎不可能会投入使用
所以，就目前来看
大语言模型还没有被正式广泛地投入生产
也可能是由于这个原因
虽然乍看之下
这项研究是在给目前的AI研究
泼上一盆冷水，但是其实正好相反
这项研究不仅揭示了大模型扩展的关键盲区
更为未来的AI发展提供了新的方向
那就是如何在模型规模与任务难度之间
找到最佳的平衡点
或许这才是智能进化的真正关键
科学研究需要的从来不只是赞美和夸耀
更重要的是实事求是
长久以来
AI研究上不断攻城略地的喜讯
难免也会让我们冲昏了头脑
这种时候更是需要一点冷静
让人们可以从狂热的情绪当中惊醒过来
回到实事求是的科研精神上
不要陷入狂热的陷阱
更重要的是，它不但指出了错误
还讲清楚了错误之后
我们应该去向何方
这一点应该说要比发现错误更要难能可贵
没错，虽然发现错误、提出质疑
需要我们的观察和勇气
但是对于技术研究来说
从错误中总结思考，找到未来的去处
是远远比发现错误更加重要的
不过话又说回来
这项研究虽然具有足够的开创性和远见
也确实看到了当前的不足和未来的方向
称得上是相关方面取得的一项重要成果
但是仍然也存在一定的局限性
首先，这项研究中的参与者
大多都不是本领域的专家
所以他们在解释校准难度值的时候
难免会有一些判断上的失误
对于一些基准数据集
非专业人士可能无法给出大量的、有难度的判断
而且研究的目的是为了捕捉普通人群的预期难度
以便在所有数据集中进行可比性分析
对于这些非专业人士所给出的难度划分和判断
仍然需要专业人士进行更加细致的追加研究
才可以真正确定
而且
这项研究中使用的、所谓“自然”的提示描述
是从多样化的数据来源中收集的
但是并没能获取这些提示在真实场景中出现的频率数据
那么，这些所谓的自然提示
是不是真的可以代表我们现实生活中的自然语言描述呢？
如果和真正的生活现实有所偏差
或者干脆一点
全部的提示和询问全都是刻意筛选的
那么这个结果就会变成先射箭后画靶
自然就不具有可信度了
同时
这项研究仅仅覆盖了部分的模型
其他的一些专业推理模型
尤其是那些依赖于外部工具或复杂推理技术的模型
并没有被纳入到研究中
这就限制了对大语言模型在更复杂场景下动态表现的理解
无法全面评估不同模型的潜力与问题
仅仅三个系列的大语言模型
可以代表当下的研究全貌吗？
我想很多人也不会同意
对于外界的这些种种质疑
这项研究的相关人员表示
他们接受外界的合理质疑
并且已经在逐步改进自己的研究方法了
目前的计划是进一步扩大
关于人类难度预期和输出监督的数据集
尽量还原和反应现实生活中人类的思考和想法
并且将这些更高质量的数据
引入到模型的训练中
通过模型反过来训练监督者
从而达到利用更高效的人员监督
来改进模型的进一步优化
同时
研究人员会尽量排查并且减少可能会出现的变异性
让模型的输出结果
保持在一个平稳合理的状态之下
另外研究人员也建议说
即使是模型仍然存在可能会谎报误报
以及突发变异的情况
也还是可以在医疗等关键领域
适当地引入大语言模型来解决一些工作问题的
只不过，这些模型在设计的时候
就最好还是设置一些拒绝回答的选项
或者与外部AI监督者结合的方式
从而提高模型对不确定问题的回避能力
最终让大语言模型可以展现出
更加符合人类预期的可靠性和一致性
在这里，AI的监督者可以是人
也可以是另一个AI模型
只要能起到监督的作用就可以
好了
关于这项更大参数的模型可能会更不可靠的最新研究
就分享到这里
Scaling Law带给了我们一种感觉
那就是模型的参数规模会决定智能程度
但是事实是
即便现在模型的参数规模已经比两年前扩大了千倍万倍
但是依然有很多低级的问题
模型无法答对
也许在Transformer和Scaling Law的背后
我们还缺少一个巨大的发现
来彻底填补上到达真正智能的那个沟壑
那大家觉得
如果未来大模型的参数变得更大
有可能达到足够的正确率吗？
小参数模型又该如何找到自己合适的位置呢？
欢迎在评论区留言，感谢大家的观看
我们下期再见
