大家好，这里是最佳拍档，我是大飞
就在18日的下午
马斯克刚刚发布了Grok 3之后
DeepSeek团队就在X平台上发布了一项重磅研究成果
瞬间吸引了大量用户围观
新研究刚发布四个小时后
也就是在写这篇稿子的时候
就有超过六十万的浏览量
有意思的是，在论文作者中
出现了DeepSeek的创始人兼CEO梁文锋的名字
说明他深度参与到了一线的研究工作中
另外必须要提的是
论文的第一作者Jingyang Yuan
他在完成这项研究的时候还是一名实习生
以实习生的身份主导如此重要的研究并且取得成果
这一方面展现了他自身的科研能力
另一方面也说明DeepSeek为年轻人才
提供了广阔的发展空间和实践机会
让新人能够在前沿研究中崭露头角
好了，话不多说
今天大飞就来为大家简单解读一下这篇论文的内容
简单来说
DeepSeek 的这篇新论文提出了一种新的注意力机制
Native Sparse Attention，简称NSA
中文翻译过来应该是原生稀疏注意力
这是一个用于超快长上下文训练和推断的、本地可训练的稀疏注意力机制
并且还具有与硬件对齐的特点
有望大幅提升下一代大语言模型处理长文本的能力
同时还能兼顾效率
在论文一开始
研究人员就先交代了NSA诞生的背景
那就是在人工智能领域
长文本建模是下一代语言模型的关键能力
随着模型应用场景的不断拓展
处理长序列文本的需求越来越迫切
像是在一些文档分析、长篇故事生成等场景中
长文本的理解和处理至关重要
然而在这些情况下
传统的注意力机制却成了拦路虎
传统注意力机制在处理长序列时
由于计算量与序列长度的平方成正比
所以导致计算复杂度极高
比方说，在解码64k长度的上下文时
注意力计算消耗就占据了总延迟的70%到80%。
为了解决这个问题
稀疏注意力机制应运而生
它的原理是通过选择性地计算关键的查询键值对
从而减少不必要的计算开销
只不过虽然想法是好的
但是现有的稀疏注意力方法却存在着不少的缺陷
比如
有些方法只能在自回归解码阶段应用稀疏性
预填充阶段仍然需要进行密集计算
这就好比一个运动员
在比赛的后半程知道节省体力
但是在前半程却浪费了太多精力
所以无法实现全程高效
还有些方法只关注预填充阶段的稀疏性
这导致在某些工作负载下无法实现全阶段加速
同样无法发挥出最大效能
另外
部分的稀疏方法无法适应现代高效的解码架构
比如多查询注意力MQA和分组查询注意力GQA
这就如同给迈巴赫装上了拖拉机的零件
即使迈巴赫本身的性能再好
也无法充分发挥自身的优势
而且，现有的稀疏注意力方法
大多只能在推理阶段应用稀疏性
缺乏对训练阶段的支持
综合考虑这些问题
那么开发一种更加高效
而且能适应多种场景的稀疏注意力机制
就迫在眉睫了
而NSA正是在这样的背景下诞生的
DeepSeek推出NSA主要希望解决两大问题
一是事后稀疏化导致的性能退化
比如预训练模型的检索头容易被剪枝；
二是现有稀疏方法难以应对长序列训练的效率需求
存在着非可训练组件和低效反向传播等问题
阻碍了高效训练和长上下文模型的发展
NSA的设计十分精妙
它有三大核心组件
分别是动态分层稀疏策略、粗粒度token压缩
以及细粒度token选择
这三个组件相互配合
就像一个精密的齿轮系统
共同推动着NSA的高效运转
与此同时，NSA还做了两大创新
分别是算术强度平衡的算法设计与硬件优化
以及支持端到端可训练
这意味着它不仅在推理阶段十分高效
还能减少预训练的计算量
同时不牺牲模型的性能
应该说，论文中
最重要的就是这张NSA的架构概览
在左侧
NSA将输入序列通过三个并行的注意力分支进行处理
分别是压缩注意力（compressed attention）、选择性注意力（selected attention）和滑动窗口注意力（sliding attention）
右侧是对每个分支产生的不同注意力模式的可视化
绿色区域表示需要计算注意力分数的区域
而白色区域表示可以跳过的区域
我们先看压缩注意力
它通过将键（key）和值（value）聚合成块block
表示来捕捉粗粒度的语义信息
并且对每个块内的所有 token 进行聚合
再通过一个带有位置编码的可学习 MLP
将一个块内的信息融合成一个单一的压缩表示
这种方式不仅捕捉了块级的全局语义信息
并且大幅减少了后续注意力计算所需处理的 token 数量
举个例子
我们在阅读一篇长篇文章时
首先会快速浏览各个段落
把握每段的大致主题
这就是一种粗粒度的理解
压缩注意力就像是这种快速浏览
它能捕捉到文本中更粗粒度的高层语义信息
同时减轻注意力计算的负担
让模型在处理大量信息时不会被细节淹没
但是，仅靠压缩注意力还不够
因为这样可能会丢失一些重要的细粒度信息
就像我们在浏览文章后
还需要精读一些关键的段落
才能够掌握全部的信息
所以
DeepSeek又引入了选择性注意力
它通过块选择机制来保留重要的细粒度信息
具体做法是给每个块分配重要性分数
根据分数来选择排名前n的块
并将这些块中的标记
用于注意力的计算
这样一来
让模型在保留关键信息的同时
又显著降低了计算负担
在注意力机制中，还有一个问题
就是局部模式通常会快速适应并且主导学习过程
这可能会阻碍模型从压缩和选择token中进行有效学习
比如我们在学习新知识的时候
如果过于关注局部的细节
可能就会忽略整体的逻辑框架
滑动窗口注意力就可以解决这个问题
它通过在输入序列中维护一个固定大小的窗口
对窗口内的 token 进行常规的注意力计算
确保模型能够敏感地捕捉到近邻之间的细节和依赖关系
防止模型在全局稀疏化处理时遗漏局部信息
从而平衡好全局和局部的关系
这三个注意力组件的输出
通过一个门控机制进行加权融合
形成最终的注意力输出
为了最大化效率
NSA还针对现代硬件进行了优化
首先
DeepSeek在Triton上实现了硬件对齐的稀疏注意力内核
考虑到多头自注意力MHA的内存密集且解码效率低
于是研究人员专注在了共享KV缓存的架构上
比如分组查询注意力GQA和多查询注意力MQA
这些架构与当前最先进的大语言模型是一致的
为了实现近乎最优的计算强度平衡
DeepSeek还采用了不同的查询分组策略
其中有几个关键的特性
首先是进行以组为中心的数据加载
在每个内循环中
加载组内所有头的查询
及其共享的稀疏KV块索引
这就好比把相关的物品放在一起
方便取用，减少了寻找的时间
其次是共享KV加载
在内循环中连续地加载KV块
从而最小化内存加载
进一步提高了效率
第三是网格循环调度
由于内循环长度在不同查询块中几乎相同
所以将查询/输出循环放在Triton的网格调度器中
从而简化和优化了内核
就像给复杂的工作制定了一个合理的流程
让一切变得有条不紊
从理论上看，NSA做了很多的优化
那么实际应用中又如何呢？
为了测试NSA机制在实际训练、推理场景中的表现
DeepSeek做了一系列严谨的实验
他们采用了目前最先进的大语言模型的常见实践
使用了一个结合分组查询注意力GQA和混合专家MoE的骨干架构
作为样本模型
这个模型的总参数量为270亿
其中30亿为活跃参数
在多个通用基准测试中
采用NSA的模型尽管具有一定的稀疏性
但是总体性能优于所有的基线模型
包括全注意力模型
在9项指标中有7项表现最佳
这说明NSA在实际应用中
不仅能够减少计算开销
还能提升模型的综合性能
即使在较短序列上
NSA可能没有能够充分发挥它的效率优势
但是性能依然强劲
在推理相关的基准测试中
NSA取得了显著提升
这表明NSA的预训练机制
有助于模型开发专门的注意力机制
能够让模型专注于最重要的信息
过滤掉无关注意力路径中的噪声
从而潜在地提升了性能
在长上下文任务中
NSA更是展现出了强大的实力
在64k上下文的大海捞针测试中
NSA实现了超强的检索精度
这个结果要归功于它的分层稀疏注意力设计
通过粗粒度的压缩token
来实现高效的全局上下文扫描
再通过细粒度的选择标记
来保留关键信息
就如同就用广角镜头先观察整个场景
然后再用长焦镜头聚焦到具体的目标
从而在全局感知和局部精确性之间
取得了完美平衡
在LongBench上
NSA在多跳QA任务和代码理解任务中也表现优异
优于所有的基线模型
显示出了在复杂长文本推理任务上的优势
此外，NSA机制还能与推理模型结合
适配前沿的后训练方式
DeepSeek通过从DeepSeek-R1蒸馏知识和监督微调的方式
让采用NSA的模型在32k长度的数学推理任务上
获得了链式数学推理能力
在具有挑战性的AIME 24基准测试中
NSA的稀疏注意力变体NSA-R
和全注意力的基线模型全注意力-R进行了对比
结果显示，在8k和16k上下文设置下
NSA-R均显著优于全注意力-R
分别高出0.075和0.054
进一步验证了NSA在复杂推理任务中的优势
在计算效率方面
DeepSeek在8-GPU的A100 系统上
对NSA和全注意力机制进行了对比
在训练速度上
随着上下文长度的增加
NSA的加速效果越来越显著
在64k上下文长度时
NSA的前向传播速度提升了9倍
反向传播速度提升了6倍
这主要得益于NSA的硬件对齐设计
块状的内存访问模式
通过合并加载最大化了Tensor Core的利用率
内核中精细的循环调度
则消除了冗余的KV传输
这就像是优化了工厂的所有生产流程
让每一个环节都能够高效运转
在解码速度方面
注意力机制的解码速度主要受限于KV缓存加载的内存瓶颈
随着解码长度的增加
NSA的延迟表现出了显著的降低
在64k上下文长度时实现了高达11.6倍的速度提升
而且这种内存访问效率的优势
随着序列长度的增加而更加明显
这对于需要处理长文本的应用场景来说
无疑是一个巨大的利好
尽管NSA取得了显著的成果
但是DeepSeek研究团队依然保持着严谨的科研态度
也指出了一些可能的改进方向
比如进一步优化稀疏注意力模式的学习过程
让模型能够更智能地选择关键信息；
还有探索更高效的硬件实现方式
充分挖掘硬件潜力
使NSA能够在未来发挥更大的作用
就像DeepSeek之前发布的所有技术报告一样
这篇介绍NSA的论文内容详实
对NSA机制中涉及的技术细节阐释十分清晰
具有很强的可操作性
再次为AI研究贡献了重要的成果
也让我们再次看到了DeepSeek团队的优秀工程能力
期待他们带来更多的、更先进的研究成果
感谢大家观看本期视频
我们下期再见
