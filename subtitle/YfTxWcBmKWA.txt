大家好
这里是最佳拍档
我是大飞
最近呢
看到一个跟阿里AI专家的会谈记录
主要介绍了阿里大模型的研究进展
我觉得呢很有收获
从中可以一窥国内的各大公司
在这方面的能力和情况
也能够清楚国内现在跟国外的差距
所以在这里呢跟大家分享一下
首先呢阿里的大模型
主要是由阿里巴巴的达摩院
来牵头去做的
大概是在22年的时候
达摩院已经发布了阿里自己的
自然语言理解的大模型
一个呢叫M6一个呢叫plug
M6的话呢
基本上是能够
支持多模态的这种AI模型
比如说文字生成图片
文字生成语音
文字生成视频这种模型
但是呢
阿里的大模型相比于GPT3.5来说呢
在参数量和整体规模上呢
其实都还是要小一些的
所以呢目前
这种模型也就只能叫做是中模型
因为达摩院的这个顶层领导下了命令
所以现在
整个团队大概有100个人
开始在做相关大模型的迭代和升级
一方面呢是自己在复现GPT的水平
另一方面呢
是要把M6和plug
迭代到能够对标到GPT的水平
目前来看呢M6的自然语言理解能力
还不能对标到GPT3.5
跟GPT3.5大概还得相差
有一年半左右的差距
预计呢
今年在下半年的云溪大会上的时候
应该会发布M6的一些最新的模型进展
可能大概也就能到GPT2.5左右的水平
这其中一个很大的原因呢
就在于语料积累不足
现有的这个大语言模型
在文本清洗和筛选方面
存在着一些限制
因此需要对现有的模型
进行升级和迭代
从而让他能够适应
不同领域的数据需求
比如说要将这个模型应用到军事
旅游文化政治等其他领域
就需要进行更多的数据收集
和人工标注
才能达到更高的准确度和效率
那谈到国内的算力储备呢
目前阿里是国内AI算力储备
应该说是最多的
然后依次为字节百度腾讯
阿里云呢
现在在云上至少
应该有上万片的A100
整体至少应该能够达到10万片
阿里集团呢差不多会是阿里云的5倍
像达摩院天猫淘宝的这个算力资源
都是作为集团的内部资源来使用的
阿里云这块今年的增长呢会有30%到50%
有个别八九个客户呢
会有复现GPT的需求啊
也提出了大规模AI算力的需求
这个到时候
会通过阿里云的方式给到他们
百度年初呢紧急下单了
3,000台8卡的A800服务器
这就是2.4万张卡
预计全年呢
百度会有A800或者是H800
共5万张的需求
阿里云的需求不会有这么多
因为去年已经采购了2万多张
今年呢采购量可能会下降啊
预计云上就1万张左右
其中呢6,000张是H800
此外呢阿里云也会用到像平头哥
这种自研的芯片
每年大概3,000张采购量
除此之外呢
阿里云也会选择
一家国产的芯片去合作
目前选择的是寒武纪 MLU370
主要是性能比较过关
大概是A100的60%到70%
而且检测是合格的
厂家的服务态度也很积极
愿意对接啊贴身服务
今年会采购大概2,000张这样的数量
主要用在一些CV等小模型的训练
或者是推理上
寒武纪的MLU370没有供货的风险
但是后续的MLU590
也许就会有供货的风险了
对于壁仞等等其他厂家呢
虽然宣传上做的不错
但是实际上拿不到实测的卡
流片大概都要到今年的4-6月份
量产半年以后
而且壁仞4月要流片的卡呢
不能支持FP64 啊
互通带宽呢也不支持8卡
最多只能支持到4卡
采用的是NV bridge的方式
能够达到180GB的水平
而8卡呢用的是PCIe的方式
只能做到32GB
弱点比较明显
阿里云内部也有技术人员看好海光的
他的CUDA的兼容性也会更好一点
但是海光呢
可能因为产能方面的原因
更侧重满足国有算力那边需求
所以跟海光对接的时候呢
有各种各样的问题
支持的力度也不好
所以就没有跟海光测起来
另外也考虑
海光是被拉入了这个黑名单的
所以也担心阿里云引火烧身
总体上呢
在其他的国产AI芯片的竞争上
海光好像也不是很在意云的这个市场
那谈到大模型的计算架构呢
阿里云的公有云
目前都采用了阿里云自研的产品
叫做DPU
已经迭代到了3.0或者4.0的这个水平
支持双口100GB
那DPU的功能呢
是用来在云上开发弹性的裸金属
因为如果要把物理服务器
做成云上的云服务器
中间呢是要有一层虚拟化的开发
那虚拟化之后呢
这个物理机上资源呢
其实是有一定的损耗的
包括像CPU的核心数啊
内存的容量以及网络带宽
硬盘这个存储容量呢
都会有所影响
所以搞这个DPU呢
就是为了把这些虚拟化的资源
都能够让它加载到这个DPU上去
使得这个云上的云服务器的资源
跟线下的物理级的资源是整体的
资源数量也是一模一样的
是没有变化的
现在研发这个DPU呢
应该已经是第四代的了
它里面主要集成了ERDMA的这个能力
这个RDMA呢
就是远程内存直接访问的意思
然后呢
这个主要场景
实际上是用在这个HPC的这个场景
也就是高性能计算
然后这个e呢就代表的是elastic
实际上就是弹性的RDMA啊
类似于高性能计算的一个协议
他能够把一些这个
高性能计算的场景支持起来
所以在云上呢
阿里云也不太会考虑
英伟达提出的一些一些光模块架构
但是像百度云以及字节的火山云
可能会采用英伟达的架构
也取决于数量
在模型优化方面呢
除了之前所说的大模型之外
包括对stable diffusion这种文生图的模型
也一直在做优化
以前一个推理任务呢一张A100
现在呢
可以算力降级到一个推理任务
一张V100
对于像阿里这种巨头而言呢
V100的存货还是非常多的
同时
还会有一些针对于模型的优化
或者是加速软件
从而加快模型的训练与推理
最后呢还可以对模型进行降级
降低这个模型的精准度要求
比如从FP16降级到FP8
那谈到国内几家竞争对手呢
首先是百度的文心一言
目前在国内呢他可能还是算是第一名
不过呢还是有很大的提升空间
文心一言呢虽然有一定的实用性
但是离达到GPT3的水平
还有一定的差距
不过呢目前至少可以达到GPT2.5的水平
如果未来能够持续的迭代和优化呢
有可能会达到GPT 3.5的水平
数据积累呢
对于百度来说应该是一个优势
尤其是在搜索领域
百度在知识库方面呢有很多年的积累
包括百度知道这些产品
而且文心一言的这个模型架构呢
是基于BERT的
那么如何能够实现更智能的迭代
可能是一个挑战
腾讯呢目前是在迭代混元这个模型
有大概100人左右的团队
在做GPT的复现
以及这个模型的迭代
预计呢大概8月份会推出
但是现在应该只支持文生文的场景
华为的这个盘古大模型呢
效果还有待考证
而且它并没有明确的去对标GPT
而是往B端去做啊
同时因为那个华为本身的限制
只能用自己的昇腾的芯片
虽然昇腾910大概也有A100 70%的水平吧
比寒武纪要好一点
但是算力的限制
可能会制约它大模型的发展
360呢最近向英伟达下了上千块A800的货
而且也发布了自己的大模型
360在语料方面的积累呢
可能会比阿里强一点
但是具体的效果呢还需要验证
字节的大模型是比较看好的
因为他有数据有算力有场景
但是还是等待他的一些发布
阿里目前呢对于这个AI大模型呢
其实是一个稳扎稳打的状态
因为现在整个集团拆分后
各个事业部都要自负盈亏
压力还是蛮大的
像ChatGPT的一个推理的任务
大概所需要消耗的这个能力是5张A100
在两秒之内要做一次推理
所以大规模应用起来呢
这个成本还是很高的
冲击也很大
而且阿里如果想要追赶百度的话呢
可能会有两种方案
一种是
阿里推出的这个模型
效果如果相对比较好的话
打算与集团内部的产品先做结合
比如说天猫淘宝高德地图等等
以及这个搜索业务
此外呢可能还考虑把这个大模型的API
向这个合作伙伴或者渠道商输出
以及收费
目前呢已经有一些厂商在尝试
将阿里的API集成到他们的产品中
但是进展不如预期的快
主要还是用在电商
搜索推荐等等这些特定的领域上
好了今天的分享就到这里
感兴趣的小伙伴们
欢迎关注我们的频道
我们下期再见
