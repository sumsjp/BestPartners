大家好，这里是最佳拍档，我是大飞
华罗庚先生曾经说过，学习
是一个“先把书读厚
再把书读薄”的过程
如今
这句话也要在大语言模型上应验了
2024年八月的第一天
谷歌DeepMind发布了一个只有2B参数的小模型Gemma 2 2B
2B这样的小参数
放在以前来说简直就是来搞笑的
这点参数，AI恐怕连话都说不利索
然而
Gemma2 2B的测试结果却让人大跌眼镜
在Chatbot Arena的测试上
Gemma2 2B直接干碎了比它大87倍的、OpenAI的GPT-3.5 Turbo！
有网友直呼，这是开挂了么？
谷歌这次是往模型里塞了多少黑科技？
说到黑科技
谷歌这次其实是甩出了三板斧
包括使用了NVIDIA TensorRT-LLM进行优化的Gemma 2 2B
专门用来检测有害内容的安全分类器ShieldGemma
以及用了稀疏自编码器（SAE）
来分析Gemma 2内部决策过程的Gemma Scope
今天大飞就来给大家一一介绍
看看是否给我们带来了什么意想不到的惊喜
首先
Gemma 2 2B 无疑是这次发布内容中「最耀眼的仔」。
就像我们前面提到的
它在大模型竞技场 LMSYS Chatbot Arena 中的结果令人眼前一亮
仅凭 20 亿参数，就跑出了 1130 分
这个数值不仅吊打了 GPT-3.5-Turbo（0613）
还顺带把比它大40倍的 Mixtral-8x7b
也踹到了一边
这个成绩也意味着
Gemma 2 2B 将成为端侧模型的最佳选择
至于为什么这个小模型这么刚猛
答案其实和GPT4o mini差不多
那就是蒸馏技术
谷歌使用自家的TPU v5e
在2万亿个 token 上训练了这个模型
虽然不知道具体使用了哪个大模型
但是根据谷歌的报告
Gemma 2 2B 轻量级模型
确实是从蒸馏而来的
而且效果出乎意料地好
当然了
这个好不是说它推理能力比最新的大模型要更强
而是说他的部署灵活以及经济效益比很高
由于模型占用的空间小
所以特别适合设备端的应用程序
想想这东西可只有2B大小
手机上都能跑
苹果机器学习研究团队的研究科学家奥尼汉农 Awni Hannun
展示了 Gemma 2 2B 4bit 量化版本
跑在 iPhone 15 pro 上的情况
结果显示速度是相当得快
实际上，不只是苹果
Gemma 2 2B能够在各种终端设备
包括手机、笔记本
甚至是使用Vertex AI和Google Kubernetes Engine（GKE）之类的云端服务
都能够进行部署
为了让模型的推理加速
Gemma 2 2B还通过NVIDIA TensorRT-LLM完成了优化
在NVIDIA NIM平台也可以使用
优化后的模型可以适用于各种平台的部署
包括数据中心、云服务、本地工作站、PC和边缘设备等等
它还可以支持RTX、RTX GPU、Jetson等模块
从而实现边缘化的AI部署
此外
Gemma 2 2B还无缝集成了Keras、JAX、Hugging Face、NVIDIA NeMo、Ollama、Gemma
cpp等等框架
并且很快将与MediaPipe集成
从而简化应用的开发
由于小参数量大大降低了研究和开发的门槛
使得Gemma 2 2B能够在Google Colab免费的T4 GPU服务上流畅运行
从而为用户带来了灵活而且成本效益较高的解决方案
不过，要注意的一点是
Gemma 2 2B的上下文长度只有8K
可能会影响多轮对话中的表现
而像Mixtral 8x7B有32k的上下文长度
在代码、数学和一般语言任务上会表现得更加出色
从图中也可以看出，在特定领域
大模型仍然保持着明显优势
讲完了Gemma2 2B
让我们把目光转向ShieldGemma
正如其名
ShieldGemma是一个最先进的安全分类器
目的是确保AI模型输出的内容具有吸引力、安全性和包容性
能够检测和减少有害的内容输出
ShieldGemma的设计专门针对于四个关键的有害领域
分别是仇恨言论
骚扰内容
露骨内容，和危险内容
这个开源的安全分类器
是对谷歌现有的、负责任的AI工具包中
安全分类器套件的补充
其中包括了一种基于有限数据点
构建针对特定策略分类器的方法
以及通过API提供的、现成的Google Cloud分类器
ShieldGemma也是基于Gemma 2构建的
提供了各种模型参数规模
包括2B、9B、27B
而且都经过英伟达得推理优化
在各种硬件中可以高效的运行
其中，2B非常适合于在线分类任务
而9B和27B版本则为对延迟要求较低的离线应用
提供了更高性能
如这张表所示，ShieldGemma 模型
无论是2B、9B 还是 27B
表现都优于所有得基线模型
包括 GPT-4
这也标志着AI安全性能的又一次迈进
以后越狱AI的难度又会提升不少
最后，就是这次发布的另一大亮点
开源稀疏自编码器，Gemma Scope了
相信不少人都听这个说法
大语言模型的内部是一个黑盒子
没人知道它是怎么运作的
即使对于训练它们的研究人员
也是如此
大语言模型的可解释性问题
为什么这么难？
这还要从模型的运行原理说起
当你向大语言模型提出问题的时候
它会将你的文本输入转换为一系列神经网络的「激活」。
这些激活映射了输入的词语之间的关系
帮助模型在不同词语之间建立联系
然后生成答案
在模型处理文本输入的过程中
神经网络中不同层的激活
又会逐渐发展出多个高级的概念
这些概念被称为「特征」。
比方说
模型的早期层可能会学习到乔丹打篮球这样的事实
而后期层可能会识别出更为复杂的概念
比如文本的真实性
然而
研究可解释性的人员却一直面临着一个关键的问题
那就是模型的激活
其实是许多不同特征的混合物
在研究的早期
研究人员希望神经网络激活中的特征
能够与单个神经元对齐
但是不幸的是，在实践中
神经元对许多无关的特征都很活跃
这也就意味着，没有什么明显的方法
能判断出哪些特征属于激活的一部分
而这恰恰就是稀疏自编码器的用武之地
要知道
尽管大语言模型可能会稀疏地检测到
数百万甚至数十亿个特征
一个特定的激活只会是少数特征的混合
比方说
大语言模型在回答关于爱因斯坦的问题时
会想到相对论
而在写关于煎蛋卷的时候
会想到鸡蛋
但是可能就不会想到相对论了
稀疏自编码器就是利用了这个事实
来发现一组潜在的特征
并将每个激活分解为少数几个特征
研究人员希望
稀疏自编码器完成这项任务的最佳方式
就是找到大语言模型实际使用的基本特征
重要的是，在这个过程中
研究人员并不会告诉稀疏自编码器要寻找哪些特征
这样他们就能够发现之前没有预料过的丰富结构
于是这一次
谷歌DeepMind的研究人员
在Gemma 2 2B和9B每一层和子层的输出上
都训练了稀疏自编码器
这样构建出来的Gemma Scope
总共生成了超过400个稀疏自编码器
获得了超过 3000万个特征
有了这件秘密武器
Gemma Scope就仿佛是一个强大的显微镜
通过大量的疏自编码器 (SAE)，
让模型呈现前所未有的透明度
方便研究人员研究特征在整个模型中的演变方式
深入了解Gemma 2模型的决策过程
以及特征在模型中是如何相互作用
如何组合形成更复杂的特征的
简单来说
Gemma Scope就是数百个适用于Gemma 2 9B和Gemma 2 2B的、免费开放的稀疏自动编码器集合
此外
Gemma Scope使用了最新的、最先进的JumpReLU SAE架构进行了训练
因为原来的稀疏自编码器架构
在检测特征存在与估计强度这两个目标之间
往往难以平衡
而JumpReLU架构
能够更加容易地实现二者的平衡
并且显著减少误差
当然，训练如此多的稀疏自编码器
也是一项重大的工程挑战
需要大量的计算资源
在这个过程中
研究人员使用了Gemma 2 9B训练计算量的大约15%，
这还不包括生成蒸馏标签所需要的计算
将大约20 Pebibytes的激活保存到了磁盘
大约相当于一百万份英文维基百科的内容
总共生成了数千亿个稀疏自编码器参数
以上就是本次Google发布的全部内容
在今年 6 月底
谷歌开源了 9B、27B 版本的 Gemma 2 模型
并且自从亮相以来
27B 版本迅速成为大模型竞技场 LMSYS Chatbot Arena 中排名最高的开放模型之一
在真实对话任务中
一度比它两倍规模以上的模型表现还要好
与此同时
从这次谷歌 Gemma 2 2B 的性能表现
也可以看到一种趋势
即「小」模型逐渐拥有了与更大尺寸模型匹敌的底气和优势
随着今年AI的不断发展
大模型的光环似乎正在逐渐褪去
而如何将模型做小
正在成为今年语言模型发展的重要趋势
这种趋势也引起了一些业内人士的关注
比如Lepton AI 的创始人贾扬清就提出了一种观点
大语言模型的模型大小可能正在走 CNN 的老路
在 ImageNet 时代
我们曾经看到参数规模快速的增长
然后转向了更小、更高效的模型
而大语言模型，会遵循同样的趋势吗？
各位观众是怎么看的呢？
欢迎大家在评论区里留言
感谢大家的观看，我们下期节目再见
