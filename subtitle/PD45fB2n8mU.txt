大家好，这里是最佳拍档，我是大飞
因为有很多观众在评论区要求
所以今天我们来聊聊最近引起大家广泛关注的一个模型
DeepSeek V3
它以出色的性能和相对较低的成本
在国际上崭露头角
网上一些简单的介绍内容其实很多了
我也不再过多复述
今天主要还是基于官方发布的DeepSeek V3技术报告
从性能、架构、工程、预训练和后训练五个方面
来给大家深度剖析一下
DeepSeek V3是如何炼成的
首先
让我们大概来了解一下 DeepSeek-V3 的性能表现
在众多权威测试集上
它都展现出了强大的实力
比如在 MMLU-Pro、GPQA-Diamond、MATH 500、AIME 2024、Codeforces (Percentile) 和 SWE-bench Verified 等测试中
涵盖了知识理解、逻辑推理、数学能力、代码生成以及软件工程能力等多个关键维度
特别是在数学推理方面
像 MATH 500 和 AIME 2024 这样的测试
它大幅超越了许多其他模型
这表明它在处理复杂数学问题上有着独特的优势
在与其他开源基础模型
比如 DeepSeek-V2-Base、Qwen-2.5 72B Base 和 LLaMA-3.1 405B Base 进行对比的时候
DeepSeek-V3-Base 的表现也十分亮眼
在 BBH、MMLU 系列、DROP、GSM8K、MATH、MGSM、CMath 等几乎所有任务上
它都取得了最佳成绩
而且经过指令微调之后
它的性能进一步提升
在与 GPT-4o、Claude-3.5-Sonnet 等顶尖模型的较量中
在 MMLU、MMLU-Redux、GPQA-Diamond、Codeforces、AIME 2024、MATH-500等任务上
都展现出了不逊色甚至更优的性能
而且，实现这样优秀的性能
按照租用H800每GPU小时2美元来计算的话
总成本只用了大约550 万美金
这在模型训练领域可以说是一个相当惊人的成绩
接下来
我们来介绍一下DeepSeek V3 在架构方面做出的三项重要创新
第一个是多头潜在注意力MLA
它通过将 Key (K) 和 Value (V) 联合映射到低维潜空间向量上
有效地降低了 KV Cache 的大小
在 DeepSeek-V3 中
MLA 的 KV 压缩维度 (dc) 设置为 512
Query 的压缩维度 (d') 设置为 1536
解耦 Key 的头维度 (dr) 设置为 64
这种独特的设计在保证模型性能的同时
大大减少了显存占用和计算开销
使得模型在处理长文本时更加高效
第二个是 DeepSeekMoE 架构
它采用了细粒度专家、共享专家和 Top-K 路由策略
实现了模型容量的高效扩展
每个 MoE 层包含了 1 个共享专家和 256 个路由专家
每个 Token 选择了 8 个路由专家
最多可以路由到 4 个节点
这种稀疏激活的机制
使得模型能够在不显著增加计算成本的情况下
拥有庞大的模型容量
为处理复杂任务提供了强大的支持
第三个是无额外损耗的负载均衡策略
DeepSeek V3通过引入了一个可以动态调整的偏置项 (Bias Term) ，
来影响路由的决策
从而避免了传统损失对模型性能的负面影响
这个偏置项的更新速度 (γ) ，
在预训练的前 14.3T 个 Token 中
设置为 0.001
剩余 500B 个 Token 中设置为 0.0；
而序列级平衡损失因子 (α) 设置为 0.0001
通过实际数据对比可以发现
使用这个策略的训练模型分工更为明确
也更好地释放了 MoE 的潜力
此外，在工程方面
DeepSeek-V3 也进行了多项优化
在流水线并行上
它采用了 DualPipe 策略
与传统的单向流水线不同
DualPipe 采用了双向流水线设计
同时从流水线的两端来进行micro-batch
这样能显著减少流水线气泡 (Pipeline Bubble)，
提高 GPU的 利用率
此外
它还将每个 micro-batch 进一步划分为更小的 chunk
然后对每个 chunk 的计算和通信进行精细的调度
通过巧妙地编排计算和通信的顺序
实现了两者的高度重叠
从实际的调度示例中可以看到
在 8 个 PP rank 和 20 个 micro-batch 的双向流水线调度下
流水线气泡可以得到显著减少
GPU 利用率得到极大的提升
在流水线气泡数量和激活内存开销等方面
双向流水线设计要好于1F1B 和 ZeroBubble 等方法
在通信优化方面
由于跨节点的 MoE 训练存在着巨大的通信开销
所以DeepSeek-V3 也采取了一系列措施
通过节点限制路由 (Node-Limited Routing)，
只允许将每个Token最多路由到 4 个节点
从而有效限制了跨节点通信的范围和规模；
然后通过定制化 All-to-All 通信内核
充分利用了Infiniband和 NVLink 的带宽
并且最大程度地减少了用于通信的 SM 数量；
此外
通过Warp 专业化 (Warp Specialization)，
将不同的通信任务分配给不同的 Warp
并且根据实际的负载情况
来动态调整每个任务的 Warp 数量
从而实现通信任务的精细化管理和优化；
最后，通过自动调整通信块的大小
来减少对 L2 缓存的依赖
从而降低对其他计算内核的干扰
进一步提升通信效率
在内存管理上
DeepSeek-V3 也做到了极致
在反向传播过程中
它会重新计算 RMSNorm 和 MLA 上投影的输出
而不是将这些中间结果存储在显存中
虽然这会略微增加一些计算量
但是能显著地降低显存占用；
另一方面
DeepSeek-V3 将模型参数的 EMA (Exponential Moving Average) 存储在了 CPU 内存中
进行异步更新
从而避免了在 GPU 上存储 EMA 参数所带来的额外显存开销；
而在 MTP 模块中
通过将 Embedding 层和 Output Head 与主模型进行共享
进而减少了模型的参数量和内存占用
Deepseek V3在训练精度上
DeepSeek-V3 采用了 FP8 混合精度训练
不过对于模型中对精度较为敏感的组件
比如 Embedding、Attention 等等
仍然采用 BF16 或 FP32 进行计算
来保证模型的性能
同时，它还采用了细粒度的量化策略
对激活值采用 1x128 tile-wise 量化
对权重采用 128x128 block-wise 量化
这种策略能更好地适应数据的分布
减少量化误差；
为了减少 FP8 计算过程中的精度损失
DeepSeek-V3还将MMA (Matrix Multiply-Accumulate) 操作的中间结果
累加到 FP32 寄存器中
并且将激活值和优化器状态
用FP8 或 BF16 的格式进行存储
这样在保证模型精度的同时
可以大幅降低显存占用
并且提升训练速度
我们再来看DeepSeek-V3 的预训练过程
它的预训练语料库达到了 14.8 万亿 Token的规模
这些数据经过了严格的筛选和清洗
保证了高质量和多样性
与前代模型相比
它的数据构建策略有了很大的改进
在数据内容上
大幅提升了数学和编程相关数据在整体数据中的占比
这使得模型在相关基准测试中的表现突出；
在多语言数据方面
进一步扩展了覆盖范围
超越了传统的英语和中文
大大提升了模型的多语言处理能力
为了保证数据的质量
DeepSeek 还开发了一套完善的数据处理流程
在最小化数据冗余的同时
保留了数据的多样性
DeepSeek 还借鉴了Document Packing方法
将多个文档拼接成一个训练样本
避免了在传统方法中
由于截断导致的上下文信息丢失
确保模型能够学习到更完整的语义信息
对于代码数据
则借鉴了V2 中采用的 Fill-in-Middle (FIM) 策略
通过“填空”的方式
来迫使模型学习代码的上下文关系
从而提升代码生成和补全的准确性
在分词器与词表方面
DeepSeek-V3 采用了基于字节级 BPE (Byte-level BPE) 的分词器
并且构建了一个包含 128K 个 token 的词表
为了优化多语言的压缩效率
团队对预分词器 (Pretokenizer) 和训练数据还进行了专门的调整
跟之前的DeepSeek-V2 相比
新的预分词器引入了将标点符号和换行符组合成新 token 的机制
虽然这种方法能够提高压缩率
但是在处理不带换行符的多行输入时
可能会引入 token 边界偏差
所以团队又在训练过程中
以一定概率
随机地将这些组合 token 拆分开来
从而让模型能够适应更加多样化的输入形式
提升模型的鲁棒性
在模型配置与超参数上
DeepSeek-V3 也经过了精心设计
它的 Transformer 层数设置为 61 层
隐藏层维度为 7168
所有可学习的参数都采用了标准差为 0.006 的随机初始化
在 MLA 结构中
注意力头的数量 (nh) 设置为 128
每个注意力头的维度 (dh) 为 128
KV 压缩维度 (dc) 为 512
Query 压缩维度 (d') 为 1536
解耦的 Key 头的维度 (dr) 为 64
除了前三层之外
其余的 FFN 层都被替换为 MoE 层
每个 MoE 层包含 1 个共享专家和 256 个路由专家
每个专家的中间隐藏层维度为 2048
每个 Token 会被路由到 8 个专家
并且最多会被路由到 4 个节点
多 Token 预测深度 (D) 被设置为 1
而且还添加了额外的 RMSNorm 层
并且附加了额外的缩放因子
在训练超参数方面
采用了 AdamW 优化器
β1 设置为 0.9，β2 设置为 0.95
权重衰减系数 (weight_decay) 设置为 0.1
最大序列长度设置为 4K
学习率采用组合式的调度策略：
在前 2K 步
学习率从 0 线性增加到 2.2 × 10^-4；
然后保持这个学习率直到模型处理完 10T 个 Token；
接下来，在 4.3T 个 Token 的过程中
学习率按照余弦曲线
逐渐衰减至 2.2 × 10^-5；
在最后的 500B 个 Token 中
学习率先保持 2.2 × 10^-5 不变
然后切换到一个更小的常数学习率 7.3 × 10^-6
Batch Size 采用动态调整的策略
在前 469B 个 Token 的训练过程中
先从3072增加到15360
在训练中保持 15360 不变
为了实现 MoE 架构中的负载均衡
DeepSeekV3还采用了无额外损耗的负载均衡策略
将偏置项的更新速度 (γ) 在预训练的前 14.3T 个 Token 中设置为 0.001
在剩余的 500B 个 Token 中设置为 0.0
序列级平衡损失因子
设置为0.0001
多头肯预测MPT损失的权重拉姆达
在前10 TIK TOKEN中设置为0.3
在剩余的4.8 TK TOKEN中设置为0.1
在长上下文扩展与多 Token 预测方面
DeepSeek 采用了两阶段的训练策略
将模型的上下文窗口从 4K 逐步扩展到 128K
第一阶段从 4K 扩展到 32K
序列长度设置为 32K
Batch Size 设置为 1920
学习率设置为 7.3 × 10^-6；
第二阶段从 32K 扩展到 128K
序列长度设置为 128K
Batch Size 设置为 480
学习率设置为 7.3 × 10^-6
在大海捞针“Needle In A Haystack”测试中
可以看到 DeepSeek-V3 在处理长文本方面的卓越能力
此外
DeepSeek-V3采用的多 Token 预测 (MTP) 策略
要求模型在每个位置预测未来的多个 Token
而不仅仅是下一个 Token
从而增强模型的预见能力
并且提供了更加丰富的训练信号
最后是 DeepSeek-V3 的后训练阶段
包括监督微调 (SFT) 和强化学习 (RL) 两个重要步骤
在监督微调 (SFT) 阶段
DeepSeek-V3 在一个包含150万条指令响应对的高质量数据集上进行微调
涵盖了多种任务类型和领域
并且采用了不同的数据构建策略
对于数学、代码、逻辑推理等需要复杂推理过程的任务
采用了基于 DeepSeek-R1 模型生成的高质量推理数据
对于非推理类任务
比如创意写作、角色扮演、简单问答等
则利用 DeepSeek-V2.5 来生成响应
然后由人工进行标注和校验
确保数据的准确性和可靠性
在强化学习 (RL) 阶段
为了让 DeepSeek-V3 能够更好地对齐人类偏好
团队构建了基于规则的奖励模型和基于模型的奖励模型相结合的奖励机制
对于可以通过明确规则来进行判别的任务
如数学题、编程题等
就采用基于规则的奖励模型
对于难以通过规则进行判别的任务
比如开放式问答、创意写作等
则采用基于模型的奖励模型
在强化学习的过程中
团队采用了 GRPO（Group Relative Policy Optimization ) 算法
与传统的 PPO 算法不同
GRPO 不需要一个单独的 Critic 模型来估计 Value 函数
而是通过比较一组样本的奖励来估计 Advantage值
因此，在 RewardBench 测试中
DeepSeek-V3 在多个方面能够超或者与GPT-4o 和 Claude-3.5-sonnet相当
充分展示了它在对齐人类偏好方面的能力
好了
以上就是对DeepSeek v3技术报告的一些解读了
建议对模型感兴趣的观众
还是尽量去阅读一下原文
相信会有更多的收获
也欢迎大家在评论区分享自己的模型使用体验
感谢大家的观看，我们下期再见
