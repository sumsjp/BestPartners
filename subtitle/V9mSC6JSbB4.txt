大家好，这里是最佳拍档
如果说过去几年是大语言模型的爆发期
那么二零二六年的今天
这个领域已经进入了深度沉淀和精准突破的新阶段
我们见证了技术路线从野蛮生长向理性深耕的过渡
今天
我们将基于AI领域的著名研究员、《从零开始构建大语言模型》一书的作者
塞巴斯蒂安·拉施卡近期在The Mad的播客访谈
和大家一起聊聊二零二六年大语言模型的技术版图
从架构之争到后训练革命
从推理优化到行业落地
看看他对当前的大模型技术有着怎样的洞察
以及对未来趋势的预判
从二零一七年Transformer架构诞生至今
已经走过了近九个年头
这九年里
无数的研究者都在追问同一个问题
Transformer的统治地位会被取代吗？
尤其是在二零二四到二零二五年
Mamba模型、状态空间模型等替代方案层出不穷
让这场架构革命的讨论愈发热烈
但是站在二零二六年的时间节点上
拉施卡给出了一个明确的答案
Transformer仍然是构建Sota模型的首选
真正的颠覆者还没有出现
Transformer能够持续领跑的核心原因在于
它的生成质量至今无人能及
虽然替代架构都在试图攻克Transformer昂贵且庞大的顽疾
但是技术的选择始终存在权衡
以文本扩散模型为例
它在某些场景下运行成本确实更低
但是要达到和Transformer同等的生成质量
就必须增加大量的去噪步骤
最终会导致成本上的剧烈增加
再看Mamba模型和状态空间模型
它们在特定任务上展现出了效率优势
但是在通用生成的能力上
依然难以和Transformer匹敌
不过
这并不意味着Transformer的架构停滞不前了
当前的技术创新
更多的是围绕着Transformer进行的优化、微调和策略性的改造
比如通过线性注意力变体来精简基础组件
以及在不牺牲性能的前提下降低计算成本
而更具代表性的则是混合专家模型的全面普及
混合专家模型的核心优势是
在扩大模型容量的同时
不会显著增加推理的成本
它就像一个高效的专业团队一样
不同专家模块负责处理不同类型的任务
既保证了模型的规模优势
又控制了实际的运行成本
拉施卡提到
混合专家模型并不是什么新鲜事物
早在谷歌的Pathways论文和Mixtral的模型中就已经出现
但是在二零二四年底DeepSeek V3模型的引爆下
这个技术才真正成为行业主流
目前
很多企业都在直接采用DeepSeek V3的架构
比如kimi将它的参数规模
从六千七百亿提升到了一万亿
依然保持了高效运行
欧洲的Mistral AI也在最新的Mistral 3模型中采用了DeepSeek V3架构
实践证明这套方案的效果非常出色
值得注意的是，拉施卡提到
当前很多被视为架构突破的改进
本质上只是稳定训练流程的工程技巧
而不是范式转移
比如将RMSNorm放到不同的位置
可能会略微优化损失函数曲线
但是这更像是给汽车换个高性能的空气滤芯
并没有改变引擎本身的核心逻辑
拉施卡甚至提到，只要几行代码
就能把一个GPT-1或者GPT-2模型
改造出最新版DeepSeek V3.2架构的雏形
这足以说明
当前架构创新的核心是迭代而不是颠覆
当然
研究者们从来没有停止探索下一代架构的脚步
文本扩散模型、Mamba模型等依然在持续演进
它们各自的技术路线都有着独特的价值
但是至少在二零二六年
如果你想构建一个在生成质量、通用性上都处于顶尖水平的模型
Transformer依然是不可替代的选择
在架构没有本质突破的情况下
训练目标的革新就成为推动大模型性能提升的重要方向
其中
世界模型和内部状态预测这两个概念
正在重塑我们对于大语言模型学习方式的认知
世界模型的核心逻辑是
让模型内部具备一套对现实世界的模拟机制
能够在内部模拟外部环境的运行规律
打个比方
如果一个下棋模型内置了国际象棋的模拟器
它就能够更好的预测每一步走法带来的后续局势
这种能力在机器人领域已经展现出了巨大的潜力
因为机器人需要精准理解物理世界的规则
才能够完成复杂的动作
而在大语言模型领域
Meta最近的一篇论文提出了一个全新的训练目标
彻底跳出了传统的下一个Token预测框架
对于传统的代码生成来说
模型本质上还是在通过统计规律
预测下一个最可能出现的代码字符
但是Meta的做法是
让模型在训练时尝试预测变量的内部状态
具体来说
当模型执行Python代码的每一轮迭代时
如果有人单步执行代码
模型需要准确指出某个变量的具体数值
这种训练方式看似简单
却带来了本质上的变化
它强迫模型去真正理解训练数据背后的逻辑
而不仅仅是通过暴力破解或者统计规律来预测结果
这非常符合人类的思维方式
我们在读代码的时候
大脑里也会自动模拟循环中变量的变化过程
很多人刚学编程的时候
还会用铅笔在纸上
逐行记录变量在每次循环中的状态
Meta的这种训练方法
本质上就是在大语言模型上实现类似的逻辑推演
拉施卡认为
虽然这种训练方式的成本更高
但是它是推动模型性能进一步提升的关键路径
因为它让模型从表面模仿走向了深度理解
能够处理那些需要复杂逻辑推理的任务
而不仅仅是生成符合语法规范的文本
这种对内部状态的理解能力
正在成为区分顶尖模型和普通模型的重要标志
另外，自从二零二五年以来
小型推理模型成为大模型领域的一大热点
它的出现
打破了只有大模型才能够解决复杂问题的固有认知
这类模型虽然体积很小
但是在Arc基准测试中表现惊人
甚至能够和Gemini、ChatGPT等巨型模型一较高下
首先我们来了解一下Arc基准测试
它不同于传统的文本类测试
更像是逻辑谜题或者智商测试
要求模型根据一组符号阵列来预测缺失部分
它的特点是跳出了传统互联网文本的范畴
专门测试模型在面对训练中从未见过的新任务时的泛化能力
而且难度还在不断升级
能够在Arc测试中取得好成绩
意味着模型已经具备了真正的逻辑推理能力
而不是单纯的记忆和模仿
小型推理模型之所以能够实现小体量、高性能
核心在于它独特的工作机制
本质上
它依然是基于Transformer的架构
但是引入了递归机制
通过潜空间存储向量
在多次迭代中不断的精炼答案
和传统模型一次生成最终答案的模式不同
它会先生成一个中间的答案
然后自我审视、反复打磨
就像一个科学家不断验证自己的假设一样
这种方式虽然并不廉价
因为迭代过程需要消耗一定的计算资源
但是模型本身的运行成本要低得多
它让人们意识到
解决复杂难题不一定需要百亿、千亿参数的大型模型
专用的小型推理模型同样可以胜任
不过，拉施卡也提醒我们
不能因此忽视了通用大型模型的独特价值
通用模型的优势在于全能而且门槛非常低
即便是从来没有接触过AI的人
只要输入简单的提示词就能上手
无论是分析图片、咨询代码
还是撰写文案，它都能够处理得很好
但是全能的代价就是巨大的模型体量和高昂的运营成本
在处理单个简单任务的时候
动用像ChatGPT这样的巨型模型显然不够经济
因此，小型推理模型的出现
更多是填补了专用场景下的高效、低成本解决方案的空白
对于企业来说，当你明确了具体任务
并且希望极致优化成本的时候
用这种更便宜的专用模块来替换掉昂贵的通用模型
是非常有意义的选择
甚至可以把大语言模型当作大脑
通过工具调用的方式
来驱动这些专业的小模型
从而形成通用大脑加专用工具的协作模式
拉施卡认为
小型推理模型代表了一个非常有前景的方向
但是它和顶级大语言模型并不属于同维度的竞争
前者是专才，后者是全才
各自在不同的场景下发挥着价值
未来
这种通用模型加专用小模型的组合
可能会成为很多企业的首选方案
如果说架构和训练目标的革新是量变
那么后训练范式的演进
就是二零二五到二零二六年大模型领域的质变了
拉施卡提出了一个核心观点
那就是预训练尚未终结
但是已经显得乏味
当前性能改进的核心动力
已经不可逆转的从架构设计转向后训练
而这个转变的核心
就是RLVR和GRPO算法的崛起
要理解RLVR的革命性
我们首先要回顾传统的人类反馈强化学习RLHF
RLHF是大语言模型从聊天机器人走向有用工具的关键一步
它让模型学会了理解人类的偏好
比如在两个答案中选择更安全、更易懂的那个
但是这种方式存在两个致命问题
一是成本非常高
需要雇佣大量人工对模型的多个答案进行排序
二是需要同时训练奖励模型和价值模型
再加上原本的基础模型
训练时内存里至少要塞进三个模型
内存开销非常惊人
而RLVR的核心创新
就是用自动化验证取代了人工评分和奖励模型
以数学题为例
传统的RLHF需要人工判断哪个答案更好
而RLVR会直接利用数学解析器或者Wolfram Alpha
将模型的解法和标准答案进行算法比对
根据结果的正确性直接给予奖励
无需人工干预
也无需训练单独的奖励模型
与此同时
GRPO算法则解决了价值模型的冗余问题
它通过组内相对比较的方式
省去了对每个响应分配绝对价值的需求
简单来说
GRPO不需要判断一个答案绝对有多好
只需要判断在一组答案中哪个相对更好
这就大大简化了计算的过程
同时保证了奖励信号的有效性
这两者的结合产生了惊人的效果
不仅去掉了传统RLHF中两个庞大的模型
让训练成本下降了十倍以上
还极大提升了模型的推理能力
拉施卡分享了自己的实验
他在新书《从零开始的推理》中
对千问 3模型进行了仅五十步的RLVR训练
它在Math 500测试集上的准确率
就从百分之一点五飙升到了百分之五十
拉施卡解释道
模型并不是在这五十步训练中学会了新的知识
这些知识早已存在于预训练数据中
RLVR其实是解锁并且激活了预训练数据中潜藏的推理能力
预训练数据中包含了大量的、具有思维链格式的推理数据
但是模型在预训练阶段
并没有学会如何运用这些数据进行逻辑推理
而RLVR通过强化学习的方式
教会了模型如何思考
如何将已有的知识转化为解决问题的能力
更令人惊喜的是
RLVR的应用门槛极低
你甚至可以拿一个基座模型
跳过监督微调和人类反馈强化学习
直接进行RLVR训练
就能够得到一个非常出色的推理模型
既然生成中间步骤有助于提升准确率
那么过程奖励模型自然成为了研究者们关注的焦点
结果奖励只看最终答案对不对
而过程奖励关注的是模型通往答案的路径
也就是推理解释过程的质量
大家都在想
为什么要扔掉模型生成的中间内容而只看终点呢？
如果能优化解释过程
理论上能够进一步提升模型的准确率
而且，已经有研究证明
生成中间步骤本身就和更高的准确率正相关
无论解释本身是不是完全准确
这个行为都会促使模型进行更深入的思考
从而更容易得出正确的答案
因此，过程奖励模型的目标
就是根据解释的质量来奖励模型
引导模型生成更优的推理过程
但是拉施卡指出
过程奖励模型目前的实际效果并不理想
主要原因是奖励黑客的问题
要给过程打分
你需要另一个模型来充当评委
但是这个评委模型很容易被钻空子
模型可能会生成看似逻辑清晰、实则毫无意义的解释
来欺骗评委模型获得高分
而这种解释并不能真正提升推理的准确性
根据DeepSeek-R1的论文
这种方案的投入产出比并不高
目前还不是最优的选择
不过
这并不意味着过程奖励模型没有未来
拉施卡提到
在二零二五年底DeepSeek-Math-V2的论文中
就有一个非常有趣的探索
他们设计了一个套娃式的架构
用三个模型协同工作
第一个模型生成答案
第二个模型给答案和中间步骤评分
第三个模型则负责给第二个评分者打分
判断这个评委是否称职
虽然听起来很繁琐
但是这套架构的整体表现非常出色
他们通过增加自我改进的迭代步数
在数学基准测试中达到了顶尖水平
尽管有人质疑存在数据污染
但不可否认的是
这种关注过程的训练方式
确实为整个训练流程增加了价值
因此，拉施卡认为
未来会看到更多这类进展
虽然它对算力和训练量的要求更高
但这才是未来获得巨大收益的方向
而非单纯地堆砌参数规模
在讨论了训练阶段的许多创新后
我们不得不关注另一个核心趋势
推理扩展
拉施卡认为
推理扩展是二零二六年大模型性能提升的最大驱动力之一
它意味着在不改变AI权重的前提下
通过在推理阶段投入更多的计算资源
来实现与训练规模扩展类似的性能增幅
OpenAI的o1模型提供了一个完美的范例
它展示出，扩展训练与扩展推理
都能带来显著的性能提升
两者的区别在于，训练是一次性投入
而推理扩展由于生成了更多Token且步骤更复杂
运行成本会随之增加
但是这种成本的增加往往能带来明显的体验改善
推理扩展的形式多种多样
最常见的是生成更多的Token
来换取更准确的答案
也就是模型通过更长的思维链展开推理
考虑更多的可能性
从而降低出错概率
另一种是并行采样，即对同一个问题
进行多次询问并进行多数投票
这种方法虽然让成本翻了几倍
但是能显著提升准确率
尤其适用于对结果可靠性要求高的场景
此外
使用裁判模型评估结果、自我精炼迭代等手段
也属于推理扩展的范畴
二零二六年一月发表的一篇论文就提出了一种创新方式
将复杂的提示词切分成多个子任务
由AI自主决定拆分逻辑并且分步执行
这种将单次请求转化为多次调用的方式
能让模型更有条理地处理复杂任务
避免因为信息过载导致的推理失误
拉施卡特别强调
很多工程细节在某种程度上被低估了
它们虽然不像热门论文那样引人关注
但却是提升大语言模型性能的核心驱动力
比如提示词清理
与其让模型费力学习如何处理拼写错误、语法混乱的输入
不如直接在输入端进行预处理
这样能更有效地提高准确率
此外
上下文管理、历史记录维护等细节
也在很大程度上影响着用户体验
这也解释了为什么同样的模型
在ChatGPT等平台上运行的表现
会比在本地运行好得多
因为这些平台不仅仅是在单纯运行大语言模型
背后还集成了一系列的工程优化
正是这些看似微小的工程技巧
共同推动了用户能直观感受到的技术进步
在分享了这么多技术细节后
拉施卡提到了二零二五年的一个重要教训
那就是AI的进步并非源于某项单一的突破
不存在能解决所有问题的魔法杠杆或银弹
真正的进步
是由无数微小的技巧、调优
以及对系统稳健性的打磨累积而成的
回顾我们前面讨论的所有技术
Transformer架构的持续优化、混合专家模型的普及、RLVR与GRPO的组合、过程奖励模型的探索
等等等等
没有哪一项是独立的革命性突破
但是它们共同推动了大模型性能的飞跃
如果说当年的突破
源于Transformer架构这个从零到一的创新
那么现在的重点
则是对现有体系的从1到N的极致精炼
包括后训练技术的升级、预训练质量的提升、架构细节的微调
以及算法的持续优化
这种全方位的持续迭代
离不开行业的分工协作
大公司的团队分工非常明确
预训练和后训练团队各司其职
不同团队并行推进各自的优化方向
最后再将所有改进整合进AI系统中
在实际工作中
没有人需要精通所有的细节
但是每个人的微小贡献
最终都会汇聚成推动行业进步的巨大力量
不过，拉施卡也指出了一个挑战
基准测试已经快要不够用了
性能提升也变得越来越难衡量
未来
衡量AI进步的标准可能不再是基准测试中的分数
而是AI在复杂任务闭环中的表现
比如给定一个复杂目标
看AI的自主运行能力和时长
或者在真实场景中的落地效果
这种转变
也会促使行业在创新方向上更加注重实用性和落地性
最后
我们来聊聊拉施卡本人的工作方式
作为一名产出惊人的AI研究员和作家
他是如何高效吸收知识的呢？
AI在他的日常工作流中
又扮演着什么样的角色呢？
拉施卡表示，他没有什么秘诀
核心就是热情
当他对某件事感到兴奋时
研究和写作就会变得非常高效
他基本上是凭直觉在工作
而幸运的是
他感兴趣的内容刚好也是大家关注的热点
他从不强迫自己去写那些所谓的必看内容
如果发现了一个有趣的递归语言模型
他会直接去读论文并把心得写下来
这种好奇心驱动的学习方式
让他能够保持极高的专注度和效率
而他理解一个事物的最好方法
就是亲手实现它
在写书时
他非常注重代码实现和基础架构
虽然书里的系统并不是为了直接用于生产环境
而是为了教学
但是通过代码
读者可以清晰地看到训练数据是如何格式化的
损失函数是如何运作的
以及模型参数是如何更新的
这种学习方式非常有成就感
也比单纯的理论讲解更透彻
至于AI在工作流中的角色
拉施卡表示
他会用GPT-5 Pro来辅助自己的工作
但是并非完全依赖
比如在写完文章后
他会让AI帮他检查是否存在错误或者拼写问题
虽然自己也能检查
但交给AI会快得多
AI还能帮他发现表达不清的地方
并且提供优化建议
由于英语并非他的母语
有时在遣词造句上会遇到瓶颈
而AI建议的表达方式通常很地道
他强调
自己是在利用AI提升工作质量
而非让AI完全取代自己
虽然从长远来看
AI最终可能无所不能
甚至达到AGI
但是他太热爱这种钻研的过程
不想把一切都交给机器
这种人与AI协同的工作方式
或许也是未来很多专业人士的主流选择
好了
以上就是这期访谈的主要内容了
希望能带给大家一些启发
感谢收看本期视频，我们下期再见
