大家好，这里是最佳拍档，我是大飞
随着各种大模型技术的井喷
多模态、计算机视觉领域的相关研究
可以说是迈入了一个新的时代
有了GPT-4V 等强力的多模态模型
我们惊讶地发现
通过大语言模型的辅助
可以在一些传统计算机视觉难以解决的问题上
取得很好的效果
6 月 15 日
在智源大会的「多模态大模型」论坛上
纽约大学助理教授谢赛宁
从哲学的角度出发
分享了针对大语言时代的视觉表征研究
他和他的团队为我们带来了两篇最新的研究论文
分别从探索多模态大型语言模型的视觉缺陷、基于视觉搜索引导的多模态大模型
以及真实世界中的虚拟智能落地等方面
介绍了团队的最新工作
今天大飞想通过谢赛宁的分享
让我们一睹计算机视觉研究领域的最前沿研究成果
谢赛宁的第一篇论文《大开眼界？
探索多模态大语言模型的视觉缺陷》Eyes Wide Shut？
Exploring the Visual Shortcomings of Multimodal LLMs
专注于探索多模态大语言模型中存在的缺陷
他们盯上了OpenAI在21年发布的CLIP
一种多模态视觉和语言模型
它可以用来实现图像文本相似性和零样本图像分类
CLIP 使用了类似 ViT 的转换器
来获取视觉特征
并且使用因果语言模型来获取文本特征
在过去很长的一段时间里面
虽然出现了许多越来越强、越来越大的语言模型
但是对于视觉编码器而言
大家不约而同地还是在使用 OpenAI 发布的 CLIP ViT
以及它公开的权重
那么问题来了
在多模态技术井喷的当下
三年前的CLIP 现在还够用吗？
对于语言理解来说
现有的视觉表征学习系统足够好吗？
为了搞清楚这一点
谢赛宁和他团队开始系统性地收集一些 GPT-4V的失败案例
比方说
AI对左上角照片中的狗狗判断明显出了问题
它很难判断狗狗的嘴到底是朝向左侧还是右侧
而谢赛宁收集这些错误数据的目的
是通过「CLIP-blind Pairs」方法
来构建一个新的名为「MMVP」的对比基准
首先，他们要从一些现有的数据集
比如ImageNet、LAION中
找出一些成对的图像
例如两张相似度很高的小狗照片
谢赛宁会用 CLIP 和通过自监督方式训练的纯视觉模型
分别得到各自特征空间中的图像对嵌入特征
接下来
团队需要分别在两个特征空间中
度量图像对中两张图像的嵌入距离
谢赛宁希望这两张图像在 CLIP 嵌入空间下的相似度非常高
而在纯视觉模型的特征空间下相似度非常低
得到满足上述约束的图像对后
他们会以「人为回路」的方式
要求人类标注者写出两张图之间具体的视觉差异
由于之前机器完成了自动的筛选工作
所以这样的标注比较容易
在构建了对比基准之后
研究人员相当于获得了一份针对视觉表征学习系统的“考卷”，
可以利用它来评价各种开源或者闭源的多模态大语言模型
只有当模型对两幅图的回答都正确的时候
才加1分
拿着这份考卷
谢赛宁的团队开始一个接一个地测试市面上的模型
最终获得了一个惊人的结果
相信大家会第一眼看到的是
在谢赛宁考试的得分上
人类以95.7的超高分
遥遥领先于任何大模型
因为他们惊讶地发现
人类可以很容易地找出两张图像之间的视觉差异
正确回答问题
但是大多数现有的多模态大模型
性能却很差
为了搞清楚背后的原因
研究团队又统计了AI犯的所有错误
总结出了 9 类多模态系统会犯错的典型模式
然后又重新构建了一个难度更高的图文匹配任务
尽管谢赛宁他们并不知道 GPT-4V 具体采用了哪一个模型
但是仍然可以将 CLIP 作为一个很强的视觉编码器
研究团队认为
CLIP 也很有可能会出现这些多模态系统在视觉方面出现的错误
基于上述观察
谢赛宁尝试通过向 CLIP 编码器得到的特征中
加入一些通过自监督方法训练的纯视觉编码器的特征
比如附加式混合特征Additive MoF和交错式混合特征Interleaved MoF 等等策略
在使用附加式混合特征策略的时候
随着视觉特征越来越多
模型在 MMVP 上的性能不断增长
而在 LLaVA 上性能会变差
这可能是因为这种直观的 token 混合策略
破坏了原有特征的性质
而在使用交错式混合特征策略时
可以在其他 VQA 任务性能持平的情况下
在 MMVP 上取得很大的性能提升
当使用 DINOv2 之外的自监督视觉模型实现MoF的时候
在 MMVP 测试任务上也可以取得很大的性能提升
也就是说
纯视觉模型的特征可以作为CLIP特征的补充
根据这项研究的结果，谢赛宁指出
研究社区急需比 CLIP 更强的继任者
在保持CLIP优点的情况下
弥补它的不足
况且
视觉自监督学习仍然具有很高的研究价值
视觉基础对于语言理解和语义表示也十分重要
谢赛宁团队的另一篇论文V*，
引导式视觉搜索作为多模态大语言模型的核心机制「V*：
Guided Visual Search as a Core Mechanism in Multimodal LLMs」，
并没有太过关注模型的缺陷上
而是从另一个角度研究了视觉与语言模型的融合
我们前面提到CLIP 也并非现代多模态模型的唯一短板
对于现代的多模态语言模型
视觉信息的瓶颈在于
人们还是使用在小规模图像数据上预训练的编码器
换句话说，AI还是不能像人类一样
将自己的“注意力”集中在需要重点关注的关键视觉信息上
比方说
如果我们给GPT-4V 一张带有星巴克logo的花哨图片
然后再去问多模态大模型应该去哪里买带这样logo的杯子
GPT-4V 会被其他部分的视觉信息所干扰
从而回答错误
而为什么人类就不会犯这样的错误呢？
这还得从对人类的视觉和认知研究说起
人的视网膜上存在着中央凹结构
它只占视网膜 1% 不到的尺寸
但是却会激活大脑中超过 50% 的视觉皮质
换句话说
我们视觉和认知系统的能力是有限的
因此人类需要通过视觉搜索
重点关注一些目标
同时选择性忽略一些背景和不重要的部分
著名实验，房间里的大猩猩
就展现了人类删选自身注意力的过程
总之
我们无时无刻不在进行着视觉搜索
如果我们想要知道
图中塑料吸管是什么颜色的？
现有的视觉系统就会从左上角开始扫描
然后对图中所有信息进行编码、处理
对于人类来说
这样做的认知负担过高
效率太低
由于吸管很可能会出现在桌子上的咖啡杯里
人类就会先看看桌子上有没有吸管
如果没有发现
就会优先再查看下一个桌子
这是一个很自然也很必要的过程
认知科学家和心理学家对这个问题也有很多的研究
为了执行视觉搜索
人类可能会考虑以下五种引导信息
1、自底向上的显著性引导
也就是显著性的视觉特性
2、自顶向下的特征引导
也就是目标物体的已知特征
3、场景引导
比如语义信息和世界知识
4、基于先前的搜索历史引导
5、基于感知到的某些物体或特征的价值引导
这些对于人类视觉和注意力的探索
也启发了机器学习领域
研究者在认知科学的基础上
对视觉搜索进行了大量的研究
许多研究者关心如何更好地模仿人类注视的结构和轨迹
但是仍然很难准确的定位目标
计算机学者们总是习惯于
优先处理图像分辨率之类较为清晰的指标
在研究数据上严重依赖统计相关性
换言之
他们最终的研究成果泛化能力都比较差
为了设计更好的视觉搜索模型
谢赛宁指出
可以借鉴前文中提到的自底向上的引导、自顶向下的引导、以及场景引导等思路
与过去相比
现在的研究者可以利用大语言模型提供的、丰富的世界知识编码
尽管大语言模型没有视觉基础
也不一定可信
但是仍然可以为研究者提供很好的引导
给出一些头脑风暴式的实验思路
在大语言模型以及多模态技术的基础上
谢赛宁的团队提出了 SEAL 框架
分别表示Show、sERach 和 telL
他们希望借助这个框架
将视觉搜索能力融入到多模态大模型中
SEAL 是一个元架构
代表了一套使用大语言模型的思想方式
从长远来看，这种架构是很有必要的
SEAL 架构包含以下 3 个部分
1、VQA大语言模型
2、视觉工作记忆VWM
3、视觉搜索模块
首先，VQA是人与系统交互的接口
当我们没有看到自己需要的视觉信息时
就会激活视觉搜索模型
获取需要的视觉信息
并将其填充到视觉工作记忆中
接着
VQA的大语言模型会从视觉工作记忆中
获取相关的信息，回答用户的问题
视觉工作记忆可以包含各种内容
比方说
原始问题、全局上下文、视觉搜索结果等等
基于视觉主干网络
谢赛宁的团队使用多模态语言模型来搜索视觉线索
接着
解码器会分别输出搜索到的线索和目标位置
这是现有的 CLIP 等系统所无法做到的
例如，如果问模型
橙色的行李最有可能在哪里？
模型会回答道
橙色的行李最有可能在人旁边
这样就提供了世界知识
接下来
AI会一步步找到城墙、再找到人
搜索到的线索会被存储在热力图中
然后可以通过这个搜索热力图找到概率最大的地方
再进行下一轮的搜索
最终找到目标物体
整个视觉搜索的部分
实际上就是递归地对图像做不停的切分
谢赛宁表示
这是一个“大力出奇迹”式的解决方案
谈不上什么工程上的巧妙
对于一个多模态大语言系统来说
它需要具备以下几点能力
1、明确知道初始化的视觉信息是否足够
知道自己有没有看到所需要的视觉信息
2、显式地列出所需要的额外视觉信息
3、在视觉搜索后
理解并且融合搜索的结果
4、为复杂任务分配更多的计算资源
谢赛宁指出
从先前的搜索历史中学习到先验知识
十分重要
此外，也许对于如今的互联网图像
这种视觉搜索能力并不一定是必须的
我们也许只需要通过视觉编码器进行统一的编码
但是对于处理以后的长视频数据、3D、类人智能体的数据而言
SEAL 架构带来的视觉搜索能力
也许会非常关键
好了
对于谢赛宁的两篇论文的介绍就先告一段落
我再给大家简单介绍谢赛宁团队其他的一些有趣成果
比如说
谢赛宁团队发布了一个名为 V-IRL 的系统（https://virl-platform
github
io/）
将虚拟AI角色搬到了我们的现实生活中来
目前，大多数与 Agent 相关的工作
还都是部署在沙盒游戏中
这些Agent可以探索沙盒中已有的地形和设施
用户可以通过和这些Agent互动
来测试它们的智能
然而，电子游戏毕竟还是虚拟的
缺乏如同现实世界一样的真实随机性
这最多只能体现出Agent在开放互联网环境下
对于周遭环境的感知
而不是在真正开放世界中的感知
为此，团队在 V-IRL 项目中
创建了不同的 Agent
他们有自己的行为、性格
并且在真实环境下进行部署
这些 Agent 上部署有大语言模型、视觉模型
是一套将语言与视觉融合到一起、较为复杂的系统
智能体可以在纽约的中央公园中数出有多少个垃圾桶
也可以通过合作的方式进行路线导航
还可以为人们提供每天的行动规划
谢赛宁认为
现实世界环境是一个很好的、衡量大模型性能的评测任务
实际上
在将 Agent 部署到真实世界中之后
他们发现了许多新的难题
例如
当部署场景的语言环境发生变化之后
Agent 的性能会变得很差等等
谢赛宁在大会上也表示
他和他的团队都期待该平台在机器人、3D、AR 等领域的应用
好了
以上就是谢赛宁在本次「多模态大模型」论坛上分享的内容
想了解更多的朋友可以去看一下原视频
感谢观看，我们下期再见
