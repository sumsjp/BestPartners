大家好，这里是最佳拍档，我是大飞
最近，李沐回到了母校上海交大
做了一场关于大语言模型和个人生涯的分享
本来我没打算做这一期
因为演讲内容和视频在网上都很容易找到
演讲也用的中文
但是后来有观众提到
也考虑到我们频道的观众很多是在美国和台湾地区
并不一定及时关注到这个信息
或者没有时间看原视频
所以还是稍微总结一下吧
在进入正题之前
我们先简单介绍一下这次演讲的背景
首先是上海交大
它可以说是中国比较具备创业土壤的高校之一了
像饿了么的张旭豪、米哈游蔡浩宇这都是一个宿舍一个宿舍的创业
还有像红杉中国的沈南鹏、华住集团的季琦、宁德时代的曾毓群、Shopee的李小冬、小红书的毛文超、第四范式的戴文渊、依图的林晨曦
都毕业于上海交大
而作为工科及计算机强校
上海交大在人工智能方面也毫不逊色
上交大的ACM班更是号称中国计算机人才的黄埔军校
曾经在2002、2005、2010年
三次获得ACM国际大学生程序设计竞赛的世界冠军
我们之前也做过一期专门的节目介绍
我们再简单介绍一下李沐
他2008年毕业于上交ACM班
毕业后先在百度担任高级研究员
后来进入卡耐基梅隆大学读博
师从亚历克斯·斯莫拉Alex Smola和戴夫·安德森Dave Andersen
2017年CMU博士毕业后
李沐加入亚马逊成为Senior Principal Scientist
也是深度学习框架MXNet的主要贡献者之一
2023年他与导师
当时亚马逊的副总裁、杰出科学家斯莫拉一起
离开了亚马逊，创办了Boson
AI
现在主要为B端客户提供定制化的大模型
李沐在国内有很好的群众基础
经常会做一些技术普及和分享
在B站有80多万的粉丝
在YouTube上也有频道
建议大家关注一下
好了
接下来我们来梳理一下李沐这次演讲的核心内容
首先李沐从算力、数据和算法三个方面
介绍了大语言模型的现在
以及他对未来情况的预测
现在来看，无论是大语言模型
还是机器学习模型
本质上就是把数据通过算力和算法压进模型里
让模型具有一定的能力
面对新的数据能找到原数据里相似的东西
然后做一定的修改
再输出想要的东西
如果说传统的机器学习是老中医
那么深度学习就更像是玄幻小说里的炼丹
其中数据就相当于炼丹的材料
这个找起来最花精力
而算力就相当于炼丹的设备
算法就是丹方，不过
丹方每年都在进步
而且对于细节的把控非常重要
就算别人把丹方给你
相比于深度学习
这次大语言模型的区别在于
前者只能解决某一个问题
而后者则是要解决很多问题
未来几年
算力、数据和算法都会有一些可以预见到的发展
在算力的硬件方面
第一个要解决的就是带宽
如今大模型的训练基本上都是分布式的
所以通常瓶颈就在带宽上
在GPU兴起之后
由于供电、散热等一系列的问题
原来可以放很多刀片服务器的机架
如今只能放两台GPU服务器了
GPU服务器通常会采用水冷散热
虽然水冷的散热效果更好
但是也有很多问题，比如容易漏水
不过一旦用了水冷之后
算力就可以更加密集
芯片可以压得更扁
每个芯片之间就更近了
虽然芯片之间可以直接用光纤互通
但是光纤距离还是会影响分布式训练的性能
另一个影响通讯是GPU和CPU之间的PCIe
它虽然每几年也在翻倍
但是相对会慢一些
其次是内存，现在的语言模型
核心是把整个世界的数据压进模型里面
所以模型就被搞得很大
经常几百个GB
在运行的时候，它的中间变量也很大
所以就需要很多的内存
即使现在可以在一个芯片里封装高达192GB的内存
但是也到了瓶颈了
如果内存工艺有没有突破
那么模型大小就会被限制住
在这方面，虽然英伟达是领先者
但是其实英伟达不如AMD
甚至不如Google的TPU
如果未来带宽和内存都能搞定
那么算力最大的挑战
就变成了供电
有时候自己造一个电厂的成本
可能比租用数据中心要付的电费还便宜
因为最大的一个芯片就要耗一千瓦
一千块芯片就是一兆瓦
整个大学都未必能用上一兆瓦的电
我们再看算力的价格，按理说
即使算力翻倍，但是由于充分竞争
算力的价格会保持不变
但是最近几年由于英伟达的垄断
导致算力翻倍后
价格可能会有1.4倍的提升
不过长期来看
摩尔定律还是会发挥作用
训练的成本会翻倍的下降
所以李沐认为
很多时候
大家不要光去想现在能搞多大的模型
因为一年之后，这个模型就会贬值
李沐坦言
大模型不是个特别有性价比的东西
从长期来看
还要看模型究竟能带来什么价值
讲完算力，再来看模型
现在大语言模型的预训练
基本上是用10T到50T的token
这个数量级已经差不多了
再多数据质量也没法保证了
所以模型大小差不多也就是100B到500B了
好一点的模型500B
超过500B就很难做serving了
以谷歌为例
历史上他们就没有让500B以上的模型上过线
OpenAI估计也差不多
不过这里指的是稠密模型
而不是MOE
因为受到内存和数据的限制
未来100B到500B的模型会是主流
即使是MOE模型，它的有效大小
也就是每次激活的大小也不会超过500B
接下来
李沐对于一些新模态的模型发展给出了自己的看法
语音模型的好处有两点
一是现在可以在语音中包括更多的信息
比如情绪、语调等等
也可以在输出中对它们进行控制
二是延迟时间更短了
可以做到300毫秒
这就可以在交互过程被打断
体验会更像真人
而视频模型其实还处于早期阶段
通用的视频生成还非常贵
因为数据数据非常难弄
视频模型数据处理的成本
很有可能会高于训练成本
所以市面上没有特别好的开源模型出来
主要问题还在于生成一张图片容易
但是保持一连串图片的一致性很难
显然
多模态已经成为了如今的一个趋势
将在文本上学到的能力
泛化到图片、视频和声音等模态上
这样做有两大好处
一是可以借助强大的文本模型进行泛化
而且文本数据相对更好获取
另一个是可以通过文本来定制和控制其他模态的输出
而不再需要专业的编程技能或者工具
在人机交互方面
语音对话显然是一个趋势
人们会逐渐开始习惯用很长的文本来解决问题
而不是像在手机上一样点点刷刷
但是长文字输入还是太麻烦
所以语音会更加方便
未来的语音控制系统能够处理更加复杂和具体的任务
自然性和便捷性也将显著提高
不过，养成用户的习惯还需要时间
上一波顶级的AI公司
基本上快死得差不多了
下一代killer APP是什么大家还不知道
可能还要等待技术更加成熟
在应用层面，李沐认为AI的本质
是辅助人类完成任务
给人类提供无限的人力资源
所以他将应用分成了三类
第一类就是文科白领
指的是用自然语言去跟人、跟世界打交道
包括个人助理、Call centers、文本处理、游戏、舆论以及教育等领域
这方面AI已经能够基本完成简单任务了
复杂任务需要继续努力
第二个是工科白领，比如说程序员
这方面简单任务还需要努力
复杂任务存在困难
最后一个是蓝领阶级的工作
这方面AI的机会是最大的
但是目前除了无人驾驶和像工厂这样的特定场景
AI连简单任务都还做不了
因为缺少足够的数据
估计至少还要五年时间才能做到
急也急不来
最后
李沐聊了聊自己这创业一年半来的感受
也分成技术和非技术两块
首先就是关于模型的预训练和后训练
之前大家会觉得预训练很重要
现在看来，预训练只是一个工程问题
后训练才是一个技术问题
在后训练过程中
高质量的数据和改进的算法
能够极大地提升模型效果
虽然大家对OpenAI的RLHF评价很高
但是李沐提出了自己的质疑
它的问题在于每个人用的数据不一样
导致目标函数对结构化问题的假设并不一定匹配
算法所适用的场景也并不一样
李沐建议创业公司不要把精力放在预训练阶段
因为太烧钱
而是应该把精力放在后训练部分
提升的效果更加明显
其次是关于垂直模型，李沐认为
通用模型存在一个指数问题
如果想要模型在通用维度上刚好满足你的需求
就需要指数级的数据
并且模型会变得很大
所以大家才开始要做垂直模型
但是做了一年多的时间
李沐发现这也是一个伪命题
实际上，并没有所谓真正的垂直模型
因为垂直模型的通用能力也不能很差
比如说考试成绩好的学霸，你会发现
他不仅某个学科能拿第一
其他学科也都差不到哪里去才行
另外就是评估
模型在实际场景中的评估其实非常困难
就像大家不断刷榜
但是实际使用的感觉却并不好
就是因为评估不到位
所以很多时候，评估是最重要的事情
要先把评估做好
由于自然语言有一定的二义性
所以我们很难评价它的正确性、逻辑性和风格
用人来评估比较贵
但是模型评估又会带来偏差
所以有一个好的评估
既可以帮你进行后续的优化
也说明你已经处理好了数据的问题
说到数据
李沐提到了行业的一句老话
那就是数据决定了模型的上限
算法决定了模型的下限
他认为我们目前离AGI还很远
因为现在的模型还是填鸭式的学习
而AGI应该能够做到自主学习
不过他觉得Anthropic的数据做的不错
得益于他们在数据上花了很多时间
总体来说，要想模型好
数据就得好
大家普遍在数据上都要花百分之七八十的时间
剩下就是算力，对于创业公司来说
也没有什么好的方案，就是买GPU
或者租GPU
即使自建机房也不会便宜太多
因为成本大头都被英伟达吃掉了
它把一块3000美金成本的卡
卖30000美金
利润90%，而且从不打折
在行业内几乎相当于一个奢侈品了
三年下来能吃掉创业公司50%的成本
所以大家经常说天下苦英伟达久已
并不是个玩笑
最后
李沐给如今的语言模型祛了祛魅
认为它还是在机器学习的范畴内
只是换了一个更大的架构
大家没必要神话它
只不过它的规模变成了以前的100倍大
数据和评估就变得很难
带来了很多工程问题
本质上还是算法上的探索不够
好了
以上就是李沐分享的一些技术内容
后面他主要讲了讲自己的工作经历和创业心得
这部分我们简单过一下
有兴趣的同学可以自己去看下原视频
李沐把自己的工作戏称为打卡式人生
他在上交、港科大、CMU、伯克利和斯坦福都待过
也去过百度、亚马逊这样的大企业
现在又创业，经历算很丰富
这几段工作领域，其实有不同的目标
比方说，去大公司当个打工人
是为了升职加薪
好处是晚上不用做噩梦
但是做久了会成为螺丝钉
读PhD目的是为了毕业
如果不是真心热爱研究
很难坚持下来
李沐提醒道，除了研究能力以外
写作和演讲的能力也很重要
读PhD的好处是你可以专心探索某个领域
但是坏处是很少有实验室能参与大项目的研发
而且要去适应研究课题和导师风格
而创业的目标是要能推出产品
上市或者卖掉
这个过程就像当海盗一样
既有「生死一瞬间」的刺激
也有「三小时醒一次」的痛苦
你要面临所有的困难，无法逃避
而且要学会延迟享受
在没有正反馈的情况下
也要学会给自己打鸡血
无论选择上面哪条路
我们都要有一个强烈的动机
它要么来自很深沉、很底层的欲望
要么来自很深的恐惧
有了动机之后就得想要解决什么问题
因为问题可能就是动机本身
比方说，如果问题有学术价值
那么你可以考虑去读PhD
如果有商业价值，你可以考虑去创业
如果两者都不强烈
但是至少有成长价值
那也可以先做做打工人
最后，李沐还跟在场的校友们
分享了一个持续提升自我的方法
那就是不断的复盘
复盘自己的动机有没有变化
目标有没有达成
中间的过程还有没有努力的空间
在李沐看来，这是一个最好的时代
新技术的出现将对未来影响深远
也是一个最坏的时代
时代红利接近消亡
大家都需要付出更多的努力
好了
以上就是李沐这次演讲的核心内容
建议大家有时间可以去看看原视频
感谢大家的观看，我们下期再见
