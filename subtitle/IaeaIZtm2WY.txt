大家好，这里是最佳拍档，我是大飞
7月31日，第63届计算语言学协会年会
也就是ACL 2025在奥地利召开
作为自然语言处理领域最具学术影响力的会议之一
本届ACL吸引了超过8300多篇论文的投稿
共评选出4篇最佳论文
其中有两篇分别来自美国和德国
一个是斯坦福大学和康奈尔大学联合团队
提出了一套评估算法公平性的基准测试
并且发现现有促进算法公平性的手段存在一定的误区
如果盲目使用可能会适得其反
另一个是由德国CISPA亥姆霍兹信息安全中心、TCS Research以及微软这三家机构合作
聚焦在大语言模型在自主决策中的采样偏差
揭示了背后由“描述性常态”与“规定性理想”所共同塑造的启发式机制
并且通过公共卫生与经济趋势等现实案例
论证了这种向理想值偏移的现象
是如何在实际应用中导致显著的偏差与伦理风险的
另外两篇则由中国团队获得
分别是北大、DeepSeek和华盛顿大学联合团队
提出的原生稀疏注意力NSA
它以性价比极高的方式
罕见地在训练阶段应用稀疏性
在训练和推理场景中都实现了速度的明显提升
特别是在解码阶段
实现了高达11.6倍的提升
这篇论文我们频道在五个月前就专门做过一期节目介绍
大家如果想了解可以去回看一下
不过
今天我们要重点介绍的则是另一篇论文
由北大-灵初智能联合实验室的首席科学家杨耀东博士团队发表
揭示了大模型的参数结构中存在的一种弹性机制
可能会导致模型在后训练阶段产生抗拒对齐的行为
这个发现对于AI治理和安全问题
可能有着深远的启发意义
所以大飞今天来给大家稍微解读一下
人工智能对齐，简单来说
就是让人工智能系统的行为
符合人类的意图和价值观
这是当前 AI 安全研究的一个核心议题
像 OpenAI 提出的RLHF方法
就是希望通过人类偏好微调
来提升模型的性能
让模型更好地为人类服务
对齐方法也是通用模型转向专用模型的核心技术路径之一
然而，现实情况是
这些后训练方法并不能从根本上消除模型偏见
也难以保障模型真正实现对齐
OpenAI 与 Anthropic 就发现
大模型为了维持自身的输出偏好
可能会在训练过程中表现出 “阳奉阴违” 的行为
为了避免被关闭或重新训练
模型可能会假装迎合训练者设定的奖励目标
实则放大自身的错位目标
从而造成欺骗性对齐（Deceptive Alignment）的现象
更让人担忧的是
只需要几十条有害样本
就可能让原本经过精细安全对齐的模型
重新变得不安全
正是在这样的背景下
杨耀东团队围绕 “大模型能否被对齐” 这个核心问题展开了研究
研究团队发现
语言模型呈现出了一种 “弹性”的 特质
这种特质主要包括两个方面
抵抗性和回弹性
抵抗性指的是预训练模型倾向于保留原始分布
而回弹性则是说对齐的程度越深
模型在反向微调中会越快回归到预训练分布
那这个 “弹性” 特质是如何被发现和证实的呢？
我们都知道
数据压缩与预测之间存在着紧密的关联
理论研究表明，最优压缩与最优预测
在理论上具有等价性
而且越来越多的实验证据显示
语言模型的预测能力与压缩能力之间
具有一定的关联性
压缩性能与模型的智能水平也呈线性相关
所以
一般认为大语言模型本质上可视为一种无损压缩协议
通过对大规模数据的压缩
来实现智能与泛化能力
基于这个理论
论文作者通过四个步骤
建模了语言模型的无损压缩协议
第一步是数据集的 token 树表示
在分词处理后，数据集中的所有响应
都由预定义字母表中的有限符号序列构成
所以整个数据集可以被建模为一棵 Token 树
这样就能以结构化的方式
表达不同数据的分布特征
第二步是压缩协议的构建
由于语言模型的参数数量有限
所以模型对数据集的压缩过程
可以看作是对对应 Token 树中有限深度部分的表征的捕捉
于是
论文作者对剪枝后的 Token 树进行了霍夫曼编码
构建了相应的无损数据压缩协议
第三步是计算理想的编码长度
考虑到霍夫曼编码的最优性
可以在既定压缩协议下
计算随机响应的理想编码长度
当语言模型对随机响应进行压缩的时候
压缩率在数量级上
主要取决于模型的参数因素
比如模型的规模
第四步是预训练与对齐阶段的联合压缩
由于预训练和对齐阶段
通常涉及到多个相互独立的数据分布
所以需要将压缩率的定义
推广到多个数据集的联合压缩情形
通过这样的压缩理论建模
论文作者有了一个重要发现
那就是当对齐后的大模型受到扰动的时候
它在预训练数据和对齐数据上的性能变化
呈现出了与各自数据量成反比的关系
因为预训练阶段的数据量通常更大
对应的 “弹性系数” 也更高
所以在发生扰动的时候
模型更倾向于保留预训练分布的特征
而对齐性能则会迅速下降
从而表现出对对齐过程的抵抗性
这个发现和胡克定律在弹簧系统中的反比关系
呈现出了惊人地一致性
我们可以将弹簧的弹性系数
类比为训练与对齐阶段中各自的数据量大小
而模型分布的变化
则对应于弹簧的伸长量
在扰动作用下
各数据集压缩率的变化速率与数据量成反比
就像串联弹簧系统中胡克定律所描述的
弹簧的伸长量与它的弹性系数呈反比关系
为了验证这个理论
论文作者还进行了一系列精巧的实验设计
我们先来看抵抗现象
它的核心表现是逆向对齐比正向对齐更加容易
论文作者首先在一个预训练模型上
进行了监督微调（SFT）
并且在这个过程中保存了不同阶段的模型切片
他们还定义了前向对齐和逆向对齐
前向对齐是将一个早期切片
在后期切片所生成的数据上训练
推动模型远离原始的状态；
逆向对齐则是将一个后期切片
在早期切片所生成的数据上训练
将模型拉回原始状态
团队选用了 Llama2 - 7B、Llama2 - 13B 和 Llama3 - 8B 等多种主流模型进行了验证
实验覆盖了代表不同对齐目标的 SFT 数据集
包括 Alpaca、TruthfulQA 和 Beavertails
结果清晰地表明
在所有测试的模型、数据集和阶段切片组合中
逆向对齐的训练损失
一致性地低于前向对齐的训练损失
这就像模型有一个强大的 “引力场”，
始终将它拉向更熟悉的预训练分布
换句话说，如果模型存在抵抗性
那么将模型 “拉回” 它的早期状态
也就是逆向对齐
应该比将它 “推离” 得更远
也就是前向对齐
所需要付出的 “努力”，
也就是训练 loss更小
我们再来看回弹现象
即模型被对齐得越深
当受到反向微调扰动的时候
它的回归预训练分布的速度就越快
实验设计是这样的
首先使用不同数量的 “正向” 数据
比如 IMDb 数据集中的积极评论
或者 Beavertails 中的安全对话
对预训练模型进行微调
从而得到一系列对齐程度不同的模型
随后，使用少量的 “负向” 数据
比如消极评论或者不安全的对话
对这些已经对齐的模型进行 “逆向微调”。
团队使用了 Llama2 - 7B 和 Gemma - 2B 模型
涵盖了情感生成和安全对话两个任务
并且采用任务特定的评分模型来量化性能
比如使用 Sentiment Roberta 模型来评估情感倾向
以及使用安全奖励模型来评估对话安全性
结果明确的显示
使用更多正向数据训练的模型
在接触到负向数据后
性能得分会经历一个更快速、更陡峭的下降过程
在快速下降后
性能衰减速度会显著放缓
并且趋于稳定
更令人惊讶的是
经历更多正向数据训练的模型
在负向数据训练后，会变得更加糟糕
论文作者对此解释道
初始的性能急剧下降
正是回弹效应的体现
因为模型此时距离预训练的 “平衡点” 最远；
而后续性能衰减的放缓
是因为模型已接近它的原始分布
抵抗开始占据主导
从而让它稳定在这个区域附近
团队还进一步研究了影响回弹强度的
两个与预训练紧密相关的关键因素
分别是模型参数规模和预训练数据量
在模型规模的影响方面
团队在 Qwen 系列的 0.5B、4B 和 7B 参数规模的模型上
重复了回弹实验
结果表明，随着模型参数规模的增加
回弹现象会越来越显著
而且
参数量大的模型在经过负向数据的微调后
它的初始性能下降的速度更快
而末期则更加平稳
这意味着随着模型能力的增强
维持预训练分布的 “惯性” 或者 “固执度” ，
也会随之增强
在预训练数据量的影响方面
团队使用了由 TinyLlama 项目发布的、基于不同预训练数据量训练出的模型切片
进行了相同的回弹实验
结果显示，随着预训练数据量的增加
模型的回弹效应也明显增强
用更多数据预训练的模型
在逆向微调时的性能衰退更为迅速
这就好比数据集的规模如同弹簧的劲度系数一样
预训练数据量越大
它所形成的分布 “引力” 就越强
使得任何偏离该分布的对齐状态
都变得更加不稳定，更容易被 “拉回”。
这些实验结果有力地证明了
大语言模型存在一种内在的、抵抗对齐微调的弹力
以及倾向于回归到预训练状态的特性
这个发现对当前的大模型对齐范式
提出了严峻的挑战
我们一直以来依赖的 “99% 预训练 + 1% 后训练” 的模式
可能正在快速失效
Grok - 4 的训练过程
就是一个典型的案例
即便在对齐阶段
调用了与预训练等量的算力资源
模型仍然难以完全消除原始偏差
这背后反映的正是模型 “弹性” 的本质
也就是模型参数在经过大规模压缩训练之后
会天然倾向于回到预训练形成的行为分布
哪怕后训练的强度极高
也难以 “根除本性”。
也就是说
越是具有高压缩、高分布惯性的模型
越倾向回到预训练状态
更具有挑战性的是
模型在 “逆向对齐”的任务中往往更加容易
论文作者实验证明
无论是 Qwen 系列还是 Llama 系列
在多个模型规模上都出现这样的现象
即使使用上万条正向数据进行微调
只需要大约 500 条反向样本
就可以显著削弱、甚至完全抵消掉已有的对齐效果
这种极端的数据敏感性
凸显了后训练对齐的脆弱性与易逆性
总的来说
这篇论文首次从理论与实验层面系统性的揭示
大模型并不是一张可以任意塑造的「白纸」，
它的参数结构中存在着一种「弹性」机制
这个机制源自模型的预训练阶段
具备驱动模型分布回归的结构性惯性
使得模型在微调后
仍然可能「弹回」到预训练状态
从而抵抗人类赋予的新指令
以及导致模型产生抗拒对齐的行为
这意味着对齐的难度远超大家的预期
后训练所需要的资源与算力可能不仅不能减少
反而需要与预训练阶段相当
甚至更多才行
而且模型的规模越大、预训练越充分
它的弹性就会越强
对齐时发生回弹的风险也越高
它也如同一记警钟一样提醒着我们
AI 的风险不光在于能力的失控
更源于它对人类偏好的 “弹性回弹”，
只有正视模型 “抗改造” 的本质
重构现有的对齐范式
才能在日新月异的模型变化中
达到真正的对齐效果
感谢大家收看本期视频
我们下期再见
