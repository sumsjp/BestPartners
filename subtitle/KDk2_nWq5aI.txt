大家好，这里是最佳拍档，我是大飞
本来说这两天想休息一下
结果昨天晚上
Meta突然放了个大招
发布了目前最强的开源大模型Llama 3
果然不愧是开源社区的灯塔
真·OpenAI
这次Meta发布的Llama 3
包括8B与70B两款模型
在月活用户小于7亿的情况下
开发者可以免费使用
基本等于完全商用免费
而且这两个版本也将很快登陆主要的云服务商
按照Yann LeCun的说法
Llama 3 8B和70B模型是目前同体量下
性能最好的开源模型
Llama 3 8B在某些测试集上
性能甚至超过了llama 2 70B
真是自己革了自己的命
而且在未来几个月内
Meta还会推出更多的版本
就连马斯克也现身评论区
回答了一句简洁而含蓄的「Not bad
不错」。
英伟达高级科学家Jim Fan则将注意力转向了即将推出的Llama 3 400B
在他看来
Llama 3的推出已经脱离了技术层面的进步
更像是一种开源模型与顶尖闭源模型并驾齐驱的象征
从他分享的基准测试可以看出
Llama 3 400B的实力几乎可以媲美Claude超大杯、以及新版的GPT-4 Turbo
虽然仍然有一定的差距
但是足以证明Llama 3在顶尖大模型中已经占有一席之地
巧的是，昨天还恰逢是吴恩达的生日
Llama 3 的到来无疑是对他最特别的庆生方式
在Llama 3发布后
扎克伯格也向外媒表示
我们的目标不是与开源模型竞争
而是要超过所有人
打造最领先的人工智能
好了，话不多说
我们先来总结一下Llama 3这次发布的亮点和特性
1、在大量重要的基准测试中性能都是最好的；
2、基于超过15T的token训练
超过Llama 2数据集的7倍多；
3、训练效率比Llama 2提高了3倍；
4、支持8K长文本
改进的tokenizer具有128K token的词汇量
可以实现更好的性能；
5、增强的推理和代码能力；
6、安全性有重大突破
提供了新版的信任和安全工具
包括Llama Guard 2、Code Shield 和CyberSec Eval 2
还比Llama 2有更好的“错误拒绝表现”。
接下来我们来一一介绍
先说性能方面
Llama 3 8B模型与Gemma 7B和Mistral 7B Instruct等模型相比
在MMLU、GPQA、HumanEval等多项基准上都有更好的表现
而70B模型则超越了闭源超级明星模型Claude 3 Sonnet
并且与谷歌的Gemini Pro 1.5不相上下
此外
为了测试Llama 3在真实世界场景中的性能
Meta专门开发了一个新的高质量人类评估数据集
这个数据集包含1800个提示
涵盖12种关键用例
包括征求建议、头脑风暴、分类、封闭式问题解答、编码、创意写作、提取、人物角色、开放式问题解答、推理、改写和总结等等
而且为了避免Llama 3出现过度拟合
Meta甚至禁止他们的自己研究团队访问这个数据集
最后
在与Claude Sonnet、Mistral Medium和GPT-3.5的对比中
Llama 3在这个数据集上也有着更好的表现
最后Meta还不忘卖个关子
说自己目前最大的400B
也就是4000亿参数模型还在训练中
性能将直接赶超Claude 3
但是我估计大概率不会开源出来
其次
这次Llama 3之所以能成为最强开源大模型
主要得益于四个关键要素
分别是模型架构、预训练数据、扩大预训练规模和指令微调
首先是模型架构
Llama 3采用了相对标准的纯解码器Transformer架构
与Llama 2相比
Llama 3进行了几项关键的改进
Llama 3使用了一个128K token的tokenizer
它能够更有效地编码语言
从而大幅提高模型的性能
为了提高Llama 3模型的推理效率
Meta在8B和70B模型中都采用了分组查询关注GQA
在8192个token序列上对模型进行了训练
并且使用掩码来确保自注意力不会跨越文档边界
其次是训练数据
要训练出最佳的语言模型
离不开庞大数据集的训练
这次Llama 3使用了15万亿token
几乎是Llama 2的七倍
其中包含的代码数量是Llama 2的四倍
堆量只是第一步
Meta在训练的时候也十分重视数据质量
用上了许多过滤手段
按照官网介绍
Meta发现前几代Llama非常擅长识别高质量数据
因此他们使用Llama 2来生成训练数据
并且开发了一系列的数据过滤管道
包括使用启发式过滤器、NSFW过滤器、语义重复数据删除方法和文本分类器
来预测数据的质量
在接受海量的数据投喂之后
Llama 3在回答琐碎问题时更加准确
在面对历史、STEM、工科及编程类问题时
也显得更加游刃有余
Meta还提到
Llama 3的预训练数据集中
有超过5%的部分
来自于高质量的非英语数据
涵盖30多种语言
加入这部分的目的
是为了能更好满足各国用户、不同语言背景的使用需求
不过，针对所使用训练数据的来源
Meta依然选择打了马虎眼
只说是从公开来源收集
并且两个版本的数据截止日期
还略微有点不同
8B版本截止日期为2023年3月
70B版本为2023年12月
根据外媒的说法
Llama 3使用的训练数据
有很大一部分是AI合成的数据
这也跟他们自己的说法不谋而合
看来，用AI来训练AI
已经是一件正在发生的事情了
Meta还投入了大量的精力来扩大Llama 3的预训练
制定了一系列详尽的扩展规则
评估模型在各种下游任务中的表现
这些规则可以帮助Llama 3选择出最优的数据组合
并且在使用训练计算资源时做出明智的决策
比方说
尽管8B模型理想的训练计算量是大约2000亿的Token
但是他们发现即使数据量增加了一百倍
模型性能仍在持续提升
甚至模型在经过15万亿Token的训练后
性能还在按对数线性递增
第三，相比于自己前两代的模型
Meta这次还在训练Llama 3的过程中
做出了很多流程上的优化
包括数据并行化、模型并行化和管道并行化
Meta在16000个GPU的集群上训练了Llama 3
实现了每个GPU超过400 TFLOPS的计算利用率
为了最大限度地延长GPU的正常运行时间
Meta还开发了一种先进的训练堆栈
可以自动执行GPU的错误检测、处理和维护
Meta还极大地改进了硬件可靠性和静默数据损坏检测机制
并且开发了新的可扩展存储系统
减少检查点和回滚的开销
综合这些改进
让Llama 3的总体有效训练时间超过了95%，
训练效率比Llama 2提高了大约三倍
最后
为了充分释放预训练模型的潜力
Meta还对指令微调方法进行了创新
他们的后期训练方法中结合了监督微调（SFT）、拒绝采样、近似策略优化（PPO）和直接策略优化（DPO）
并且发现在SFT中使用的提示
以及在PPO和DPO中使用的偏好排序的质量
对排列模型的性能有着极大的影响
而针对外界关于开源大模型担忧最多的安全性问题
Meta看起来也是做足了准备
这次Meta采用了一种新的系统级方法来负责任地开发和部署Llama 3
他们将Llama 3视为更广泛系统的一部分
让开发人员能够完全掌握模型的主导权
在确保模型的安全性方面
Meta的指令微调模型
已经通过了内部和外部的红队测试
Meta的红队利用人类专家和自动化方法来生成对抗性提示
通过全面的测试
来评估模型在化学、生物、网络安全和其他风险领域相关的滥用风险
除此之外
Meta还采用了业内最为先进的大模型安全技术
自带了Llama Guard 2、Code Shield和CyberSec Eval 2等新版信任和安全工具
确保模型不会被轻易越狱
输出有害的内容
看来这次Meta充分吸取了Llama去年意外泄漏的教训
在模型安全性上下的功夫
已经不亚于他对性能的追求
除了模型以外
这次同Llama 3一起推出的
还有基于Llama 3构建的Meta AI
按照小扎的说法
Meta AI是目前最智能的免费AI助手
而且由于Meta AI与自家APP生态的兼容性
可以让用户的使用体验大大提升
比方说，用户无需切换
就可以在Instagram、Facebook、WhatsApp和Messenger 的搜索框中顺畅的使用Meta AI
在手机的聊天窗口中
用户只需要输入问题+@Meta AI
就能够得到想要的答案
也可以跟Meta AI进行私聊对话
如果用户在刷Facebook的时候
遇到一件好玩的事情
突发奇想有个问题
就可以在帖子下面点开直接问
Meta也没有忘记PC端
用户只需要打开meta
ai，无需注册登录
就可以像GPT一样开启对话
如果登录则可以保存对话记录
此外
由于这次图像生成速度大大加快
用户每输入几个字母
图像就会发生变化
真正做到了输入即所得
而且还能在AI生成的高质量图片基础上
生成GIF动图，与好友进行分享
根据媒体The Verge的报道
Meta AI助手也是唯一一个集成了Bing和Google实时搜索结果的聊天机器人
并且Meta可以决定使用哪种搜索引擎来回答prompt提示词
目前Meta已经向美国以外的十几个国家
推出了英语版的Meta AI
包括澳大利亚、加拿大、加纳、牙买加、马拉维、新西兰、尼日利亚、巴基斯坦、新加坡、南非、乌干达、赞比亚和津巴布韦等等
未来
Meta团队还将会公布Llama 3的技术报告
披露模型更多的细节，同时
Meta官方还会以直播或者博客的形式
让模型开发团队直接与外界进行交流
总之，大飞觉得
Meta没有辜负开源社区对它的期待
继续在大模型开源之路上奋力狂奔
而这次Llama 3的发布
无疑也是对前几天百度李彦宏发表的言论
认为开源大模型意义不大
闭源模型能赚到钱的啪啪打脸
最后
大家一定都想喊话OpenAI和Sam Altman
GPT-5什么时候发布呢？
好了，感谢大家收看本期节目
我们下期再见
