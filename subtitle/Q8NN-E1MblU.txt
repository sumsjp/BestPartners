大家好，这里是最佳拍档，我是大飞
最近
人工智能领域迎来了一个重磅消息
被誉为“强化学习之父”的加拿大计算机科学家理查德·萨顿（Richard Sutton）和他的博士导师安德鲁·巴托（Andrew Barto ）
荣获了2024年度的图灵奖
这个奖项可以说是计算机领域的最高荣誉之一
表彰了两人在人工智能
尤其是强化学习方面的开创性贡献
关于萨顿我们频道也做过几期节目了
他的《苦涩的教训》如今已经成了AI行业从业者的必读经典
今天我们就来回顾一下萨顿在2022年的一个重要演讲
题目是《The Increasing Role of Sensorimotor Experience in AI》，
对强化学习的核心
也就是利用经验来学习的方法
进行了深刻的阐述
在探讨经验对AI的重要性之前
咱们先来了解一下萨顿对Agent与外部世界交互的看法
他认为，Agent在运行过程中
会不断地与外部世界发生交互
比方说，Agent会向外部世界发出动作
同时接收感知所带来的反馈
这种交互过程中产生的经验
在强化学习里是一种非常普遍的感知方式
而且
当Agent尝试去预测外部世界的变化的时候
也是通过这种经验交互来实现的
但是，这种依赖经验的方式
在当前最常见的机器学习类型
也就是监督学习中却并不多见
在监督学习里
机器学习模型不会从普通的经验中学习
它们学习的是特殊的训练数据
而且，在监督学习系统运行的时候
其实它根本就不学习新的经验
经验，简单来说
就是Agent与外部世界互动产生的数据
是Agent与外界沟通的重要途径
不过
经验本身如果不与其他经验建立联系
那它就没有什么实际意义
当然，这里面有一个例外
那就是经由特殊信号所表示的奖励
奖励代表着Agent追求的好目标
而Agent的目标就是最大化奖励
在演讲中
萨顿提出了一个非常核心的问题
智能最终是可以被什么来解释呢？
是用客观的术语，还是经验的术语呢？
客观术语包含了外部世界的状态、目标、人、地点、关系、空间、动作、距离等等
这些不在Agent内部的事物；
而经验术语则包含了感知、动作、奖励、时间步数等等Agent内部的事物
萨顿认为
虽然在日常交流和学术论文写作中
研究者们通常会思考客观概念
但是现在
我们更应该把关注点放在Agent与外部世界交互过程中产生的经验上
随着经验在AI发展中逐渐被重视
萨顿提出
这个过程一共会经历四个阶段
分别是智能体身份（Agenthood）、奖励（Reward）、经验状态（Experiential State）
以及可预测的知识（Predictive Knowledge）
经过这四个阶段的发展
AI会逐渐拥有经验
变得更加实际、可学习并且易于扩展
接下来
咱们就来详细了解一下这四个阶段
首先是智能体身份阶段
在人工智能发展的早期阶段
也就是1954年到1985年这个时期
大多数AI系统的功能比较单一
主要就是用来解决问题或者回答问题
它们既没有感知能力
也不会主动行动
传统的机器人系统只有启动状态和目标状态
在解决问题的时候
研究者们会设定一个从启动状态到目标状态的解决方案
这个方案就是一个行动序列
但是这里面并没有真正的感知和行动
因为当时人们认为整个外部世界是已知、确定、封闭的
不需要AI去感知和行动
研究者们觉得自己知道会发生什么
所以只需要构建一个解决问题的计划
让AI执行就可以了
他们相信这样就能解决问题
不过，在过去30年的发展中
人工智能的研究方向发生了很大的转变
开始关注构建Agent
这种转变在很多方面都有所体现
其中一个重要的标志就是人工智能的标准教科书
开始把Agent的概念作为基础内容
就拿1995年版本的《人工智能：
一种现代的方法》来说
这本书的统一主题就是介绍Agent的概念
从这个角度来看
AI的问题就变成了如何描述和构建Agent
让它们能够从环境中获得认知
并且采取相应的行动
随着研究的不断深入
如今标准、现代的方法
就是构建一个能够和外部世界交互的Agent
这也是萨顿看待AI的一个重要视角
接下来，我们进入奖励阶段
奖励
其实就是以经验的形式来描述AI的目标
这是目前一种比较有效的构建AI目标的方法
也是萨顿和他的合作者提出来的
现在有一种假说
认为智能及其相关的能力
都可以被理解为是服务于最大化奖励的结果
所以很多人觉得奖励对于Agent来说已经足够了
但是，萨顿却认为
这种思路是需要被挑战的
为什么呢？
因为奖励仅仅只是一个数字
一个标量
它并不足以解释智能的目标
大家想想，人类的目标是非常宏大的
比如照顾家庭、拯救世界、促进世界和平、让世界变得更美好
这些目标远远比单纯的最大化快乐和舒适要重要得多
而仅仅用一个来自Agent外部的单一数字来代表目标
显得太小、太简单化
甚至有点贬低了人类目标的价值
不过
我们也不能忽视通过奖励构建目标的优势
虽然奖励构建的目标比较小
但是它有一个很大的好处
就是目标可以被清晰、明确地定义
而且很容易学习
这对于通过经验构建目标来说
既是一个挑战
也是一个契机
实际上，回顾AI的发展历史
我们会发现AI原本对奖励并没有太大的兴趣
就算是现在，情况也没有完全改变
早期的问题解决系统
还有当前最新版的AI教科书
很多依然会将目标定义为需要达到的世界状态
而不是从经验的角度来定义
当然
现在最新的教科书中已经有章节提到强化学习
也提及了AI使用的奖励机制
如今在构建目标的过程中
奖励已经成为一种常规的做法
并且可以使用马尔科夫决策过程来实现
虽然杨立昆曾经批评
奖励不能够充分构建目标
但是他自己也承认
奖励已经是智能这块“蛋糕”顶端的“樱桃”了
这说明奖励还是很重要的
在了解了Agent和奖励这两个阶段之后
萨顿中间插入了个小插曲
来深入探讨一下到底什么是经验
为了让大家更好地理解
萨顿举了一个具体的例子
展示了一个Agent执行程序的输入输出信号阵列
在这个阵列里，第一列是时间步数
每一步我们可以认为是0.1秒或者0.01秒这样的瞬间
行动信号列采用二进制表示
用灰白两色来区分
后面跟着的是感知信号列
其中前四列是二进制值
同样用灰白两色表示
后四列则采用0 - 3的四种取值
分别用红黄蓝绿四种颜色表示
最后一列是连续变量，代表奖励
在实际的实验中
研究者们会把数字去掉，只留下颜色
这样更容易寻找其中的模式
萨顿认为
经验就是对感觉-运动经验的数据中发现的模式
产生的知识和理解
在这个例子里，就有很多典型的模式
比如说，行动的最后一位
和紧随其后的感知信号是相同的
如果某个时间步数的行动是白色的
那么它后面的第一个感知信号也是白色
灰色也是如此
还有，当出现红色时
紧随其后的一个时间步数是绿色
如果扩大数据的范围，还会发现
在红绿色先后出现后
隔一个时间步数就会出现蓝色
另外
数据的最后三列往往会出现一长串同样颜色
保持不变，形成条纹
除了这些
为了让AI能够预测特定的感知数据
还增加了返回值
这个返回值代表着对将会到来的奖励的预测
比如说
框中的绿色条带代表了随后的奖赏中
绿色会比红色多
这就是当前对于奖励的预测
特殊的阴影区域表示等待函数
在这个区域里会有绿色和红色的条带
而且研究者会给越早返回的、带有颜色的奖励更高的权重
当返回值随着时间移动时
我们就可以看到预测结果和实际奖励之间的颜色和值的对应变化
这种返回值本质上并不是从已经发生的事件中学习的
而是从时间差信号中学到的
其中最重要的信号就是价值函数
在这个例子里
返回值实际上就是一个价值函数
代表的是对于未来奖励的总和
如果想要一个更一般形式的、复杂的、能够指代未来值的函数
可以采用一般价值函数GVFs的方法
一般价值函数可以包含各种信号
不仅仅是奖励；
它可以是任何时间封包形式
还可以包括任何策略
能够预测的事情数量非常多
范围也很广
不过
用一般价值函数进行预测的时候
被预测对象的表达形式
需要设计成易于学习的形式
而且要有很高的计算效率
理解了什么是经验之后
我们再来看经验状态这个阶段
提到“状态”这个词
很多研究者可能会想到世界状态
这是一个属于客观概念的词语
世界状态指的是对客观世界的一种符号化的描述
能够和世界本身的情况相匹配
比如积木块的位置信息“C在A上”。
在最近一段时间，一些研究者
像朱迪亚·珀尔（Judea Pearl）提出了概率图模型
它表示的是世界状态的概率分布
用来描述像“外面下雨
草地是否是湿的”这类事件之间的概率关系
还有一种状态是信念状态
在这种概念里
状态是一种概率分布
表示的是离散世界的状态
对应的方法叫做POMDPs
也就是部分可观察马尔科夫决策过程
这里面存在着隐藏状态变量
其中部分是可被观察到的
可以使用马尔科夫决策过程来进行建模
不过，这些都属于客观的状态
和经验相距比较远
是研究者们一开始尝试描述世界状态的方法
而萨顿提出的经验状态则不同
他认为经验状态是指整个世界的状态根据经验来定义
它是过去经验的总结
能够用来预测和控制未来将会获得的经验
这种通过构造过去经验来预测未来的做法
其实在很多研究中都有体现
比如说在强化学习的任务之一
雅达利游戏中
研究者们会用游戏的最后四帧视频来构建经验状态
然后根据这个经验状态来预测之后的行为
还有LSTM网络中的一些方法
也可以被认为是从某种经验状态中进行预测
经验状态还有一个很重要的特点
就是它可以递归更新
经验状态是整个过去发生事情的总结函数
因为AI需要每时每刻访问经验状态
来实现对接下来发生事件的预测
所以经验状态的更新是递归式的
也就是说
当前时刻只访问上一时刻的经验状态
而上一时刻经验状态是对过去所有发生过的事件的总结
到了下一个时刻
同样只访问此时此刻的经验状态
而这个经验状态也是对过去发生的所有事件的总结
最后是预测性知识阶段
像“拜登是美国总统”、“埃菲尔铁塔在巴黎”这样的知识
都是对于外部客观世界的一种描述
它们并不是经验性的知识
但是
类似于“做某件事情预计要花费X小时”这类的知识
就属于经验知识
经验知识和客观知识之间存在着巨大的差异
这也是AI研究要面临的一个挑战
以往的AI研究
大多倾向于将知识视为一种客观选项
虽然近期已经有一些研究
开始从经验的角度来看待问题
但是早期的AI系统由于没有经验
根本无法进行预测
而更现代一些的AI
把知识看作是客观的存在
比如概率图模型
不过很多时候它研究的是两件同时发生的事情之间的概率
而预测面向的应该是一连串的序列事件
基于对序列事件的预测是一种具有明确语义属性的知识
如果AI预测某件事情会发生
就可以将预测和实际结果进行对比
而这种预测模型
可以被认为是一种新的世界知识
也就是预测性知识
在预测性知识领域
萨顿认为最前沿的成果就是一般价值函数和选择模型
萨顿把世界知识分为了两类
一类是关于世界状态的知识
另一类是关于世界状态转换的知识
比如说一个世界预测模型
不过这个预测模型
并不是初级形态的马科夫决策过程或者差分方程
它可以是抽象的状态
在经验状态中能够被抽取出来
因为预测是以整个行为为条件进行的
所以在选择模型中
Agent可以选择根据某种条件终止
也可以选择对某个行为后的状态进行预测
举个日常生活中的例子
假设你要去城里
那么就会对前往市中心的距离、时间进行一个预测
如果步行10分钟进城这个行为
超过了某个阈值
可能就会进一步预测出一个状态
比如疲惫等等
有了这种能够延伸行为的模型
知识所表示的规模就可以非常大
比如说
你可以根据一个行为来预测世界状态
然后根据状态再预测下一个行为
如此循环下去
不过萨顿也指出，在AI的发展历程中
经验虽然越来越受到重视
但是我们也不能忽视它的一些问题
对于人类来说
经验过于主观化和个人化
所以我们仍然不太喜欢用经验的方式去思考和表达
经验对我们来说
显得陌生、反直觉、短暂又复杂
而且，经验是主观、私人的
和他人进行交流或者进行验证
几乎是不可能的
但是呢 经验对于AI来说
却有着非常重要的意义
首先呢
经验来自于AI的日常运行过程
获取这些经验是无成本的、自动的
同时，AI有大量的数据可以用来计算
经验就像是一条通向了解世界的道路
如果世界中的任何事实都是经验性的
那么AI就可以从经验中学习对世界的认识
并且在经验中进行验证
最后，萨顿指出，回顾AI的发展历史
在过去70年里
AI领域逐渐在增加对经验的重视
从最初获得经验
到根据经验设定目标
再到根据经验获得状态和知识
每一个阶段都在不断进步
虽然目前在利用经验方面
AI还没有完成经验状态和预测性知识这两个阶段
也就是阶段三和四
但是这种发展的趋势会越来越明显
萨顿认为，将一切都归于经验
才是通向真正AI的可行路径
就像他自己在演讲最后一页提到的
并不是所有的一切都是从经验中习得的
而是所有的一切都与经验有关
好了
以上就是萨顿这次演讲的主要内容了
希望能对大家理解经验在AI中的重要性所有帮助
感谢大家观看本期视频
我们下期再见
