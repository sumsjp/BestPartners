大家好，这里是最佳拍档，我是大飞
今天要和大家聊的这个话题
可能是所有做大模型开发、应用的朋友都绕不开的一个“老大难”问题
那就是为什么有时候给大模型输入完全一样的内容
得到的输出却不一样呢？
哪怕你已经把随机种子都固定了
结果还是会“失控”。
而就在昨天
一家公司的研究成果直接揭开了这个问题的核心真相
这家公司就是由OpenAI前CTO米拉·穆拉蒂（Mira Murati）创立的Thinking Machines Lab
先给大家铺垫一下这家公司的背景
因为它从成立之初就自带“顶流”光环
今年7月
Thinking Machines Lab完成了一轮高达20亿美元的种子轮融资
公司估值直接冲到了120亿美元
要知道，在拿到这笔巨额融资的时候
这家公司还没有发布任何一款产品
这种“未出产品先获重投”的情况
在AI行业里都是极为罕见的
而投资方名单更是星光熠熠
领投方是知名风投A16z
跟投的包括英伟达、AMD、思科（Cisco）这些科技巨头
为什么资本市场愿意给一家“零产品”公司如此高的估值？
答案可能就藏在它的团队和今天发布的这篇研究里
9月11日当天
Thinking Machines Lab正式推出了他们的研究博客
名字叫Connectionism
翻译过来就是联结主义
而博客的第一篇文章
就直接瞄准了大模型推理中最让人头疼的“非确定性”问题
标题是《击败大语言模型推理中的非确定性》。
这篇文章不仅点破了很多人对大模型非确定性的误解
还给出了可落地的解决方案
甚至附上了完整的实验数据
今天我们就从现象到原理
再到解决方案
对这篇论文进行一个解读
首先，我们得先明确一个前提
大模型推理的非确定性到底是怎么回事？
可能很多做算法开发、工程落地的朋友都有过这样的经历
你给大模型输入了完全相同的提示词
比如
请介绍一下理查德·费曼（Richard Feynman）
并且在代码里把随机种子（random seed）设置成了同一个值
按照常理
这时候模型的输出应该完全一致才对
但是实际情况是，有时候你跑10次
可能会得到5、6种不同的结果
更让人困惑的是
哪怕你用的是vLLM、SGLang这些开源推理库
在自己的硬件上跑
这种情况依然会出现
面对这种现象
行业里最常见的解释是什么呢？
很多人会把这个锅甩给“GPU并发执行”或者“浮点数运算”。
比如有人会说，GPU是并行计算的
不同核心的计算完成顺序不一样
结果自然会有差异；
还有人会说
浮点数运算本身就有误差
叠加之后就导致输出不一样了
这些说法对吗？
不能说完全错
但是它们都没有触及问题的核心
至少在Thinking Machines Lab的研究里
这些都不是导致大模型推理非确定性的“真凶”。
那真正的原因是什么呢？
研究团队通过大量实验发现
核心问题在于大模型推理过程中的“批次不变性缺失”。
这里我需要先解释一下“批次处理”的概念
当你向大模型推理服务器发送请求的时候
服务器不会每次只处理你一个人的请求
它会根据当前的负载情况
把你的请求和其他用户的请求打包成一个“批次”，
也就是batch来处理
比如你在低峰期发送请求
服务器可能把你的请求和另外2个请求打包成一个“批次大小3”的任务
而在高峰期
可能会和另外15个请求打包成一个“批次大小16”的任务
而问题就出在这里，相同的输入
在不同批次大小下
会产生不同的输出
这就好比你去餐厅点一道番茄炒蛋
结果厨房告诉你，今天同时要炒3道菜
你的番茄炒蛋就会偏咸；
如果同时炒10道菜
你的番茄炒蛋就会偏甜
这听起来十分的荒谬
但是这就是现在绝大多数大模型推理系统的真实现状
而导致这种“批次依赖”的根本原因
还要从浮点数的一个特性说起
那就是浮点数的非结合性
什么是浮点数的非结合性呢？
简单来说，就是对于浮点数运算
(a+b)+c的结果并不等于a+(b+c)。
给大家举个具体的例子
比如计算(0.1 + 1e20) - 1e20
结果会是0
但是如果计算0.1 + (1e20 - 1e20)，
结果会是0.1
明明是同样的三个数
只是改变了加法的顺序
结果就完全不一样了
为什么会这样？
因为浮点数的设计初衷是“动态精度”，
它通过“尾数×10^指数”的形式
既能表示非常大的数
也能表示非常小的数
但是它的代价是
当两个数值尺度差异过大的时候
小数的精度就会被大数“吞噬”。
比如1e20是一个极其大的数
0.1和它相加时
0.1的精度会被完全忽略
结果还是1e20；
而1e20减1e20等于0
再加上0.1才会保留0.1的精度
这种特性在大模型的核心操作里会被无限的放大
在模型的推理过程中
矩阵乘法、RMSNorm
也就是均方根归一化和注意力机制这些关键的步骤
本质上都会涉及到大量的浮点数加法和乘法
而当服务器处理不同批次大小的请求时
这些操作的归约化Reduction的顺序
也就是浮点数相加的顺序
会发生变化
比如批次大的时候
内核会采用“分割归约”的策略
把大任务拆成小任务并行计算；
批次小的时候
可能会用“单核心完整归约”的策略
归约的顺序变了
最终的计算结果自然就不一样了
这里需要澄清一个误区
很多人会以为“GPU并发执行”是关键
但是实际上，哪怕是在GPU上
只要归约的顺序是固定
相同的计算任务重复执行1000次
结果也会完全一致
Thinking Machines Lab的团队做过一个实验
用PyTorch定义两个2048×2048的随机矩阵A和B
计算它们的乘积，然后重复1000次
每次的结果都完全相同的
就连每一位都是相等的
这说明什么？
说明GPU本身的并发能力并不会导致非确定性
真正的问题还是内核的归约策略会随着批次大小而变化
也就是我们刚才说的“批次不变性缺失”。
既然找到了核心原因
解决方案就明确了
那就是实现“批次不变的内核”，
让大模型的核心操作
比如RMSNorm、矩阵乘法和注意力机制
在任何批次大小下
都采用相同的归约顺序
接下来我们就具体说说
这三类操作分别是怎么实现批次不变的
先看RMSNorm
RMSNorm的计算公式是
输出 = 输入 × (1/根号(输入平方的均值)) × 权重
这里的关键是输入平方的均值
因为计算均值需要对输入的每个元素做平方后求和
也就是归约操作
常规的RMSNorm内核在批次大的时候
会采用数据并行的策略
每个核心处理一个批次元素
在核心内部完成完整的归约
这样归约的顺序就是固定的；
但是当批次变小的时候
核心数量多于批次元素
为了不浪费算力
内核就会切换到“分割归约”的策略
把一个批次元素的归约任务拆给多个核心
这就改变了归约顺序
Thinking Machines Lab的解决方案很直接
无论批次大小如何
都强制采用数据并行的策略
哪怕批次很小，导致部分核心闲置
也要保证每个批次元素的归约在单个核心内完成
这样一来，不管批次是3还是16
归约顺序都不会变
RMSNorm的结果自然也就批次不变了
再看矩阵乘法
矩阵乘法是大模型中计算量最大的操作
常规的内核为了追求性能
会根据批次大小来调整策略
比如当矩阵的维度（M、N）较小的时候
内核会采用“Split-K”策略
把矩阵乘法的归约维度（K）拆分成多个部分
并行计算后再合并结果
但是Split-K会改变归约的顺序
导致批次不变性缺失
另外，GPU里张量核心的指令选择
也会影响到归约顺序
比如批次小的时候
内核可能会用小尺寸的张量核心指令
比如m64n128k16；
批次大的时候，可能用大尺寸指令
而不同指令的内部归约顺序是不同的
针对矩阵乘法
Thinking Machines Lab的方案是固定内核配置
首先，禁用Split-K策略
无论矩阵维度如何
都不拆分归约维度
其次，固定张量核心指令的尺寸
比如统一使用128×128×32的块大小
哪怕批次小的时候会有算力浪费
也要保证归约顺序是一致的
实验数据显示
这种方案虽然会损失一定的性能
但是在批次较大的场景下
性能损耗只有20%左右
完全在可接受的范围以内
最后是注意力机制
这也是最复杂的部分
注意力机制涉及两次矩阵乘法
分别是Q×K^T和注意力权重×V
以及一次softmax
其中Q×K^T的归约过程是最容易受批次影响的
常规的注意力内核
比如FlashAttention2
在批次大的时候，会沿Q维度并行
每个核心处理一部分的Q
在核心内部完成Q与K^T的归约；
但是在解码阶段
Q维度的并行性会消失
内核会切换到Split-KV策略
也就是会把KV缓存拆分成多个部分
并行计算后合并
这就改变了归约的顺序
更麻烦的是，很多推理引擎
比如vLLM
会对KV缓存做“分块预填充”，
把一个序列的KV拆成多个块处理
这也会导致不同块的归约顺序出现不同
Thinking Machines Lab的解决方案是固定分割大小
也就是不按照分割数量来拆分KV缓存
而是按照固定分割大小来拆分
比如不管KV缓存的总长度是1000还是2000
都按照每个分割256个元素的大小来拆分
于是1000个元素就会拆成3个256的块和1个232的块
2000个元素拆成7个256的块和1个208的块
这样一来
无论序列长度、批次大小如何
KV的归约顺序都是由固定的分割大小决定的
自然就实现了批次不变
同时
他们还在注意力内核之前统一更新KV缓存的布局
确保预填充和解码阶段的KV数据格式一致
避免因为数据布局差异而导致的归约顺序变化
那么
这些解决方案的效果到底如何呢？
Thinking Machines Lab做了一组非常有说服力的实验
他们用的模型是Qwen/Qwen3-235B-A22B-Instruct-2507
提示词是“介绍一下理查德·费曼（Tell me about Richard Feynman）”，
温度设置为0，也就是贪婪采样
理论上应该输出唯一的结果
总共采样1000次
在使用常规内核的情况下
结果让人意外
1000次采样竟然产生了80个不同的完成结果
更有意思的是
所有结果的前102个token完全相同
都是“费曼出生在1918年五月11日（Feynman was born on May 11
1918, in）”，
但是从第103个token开始出现分歧
跑完992步输出的是纽约皇后区
而跑完8步输出的是纽约市
这种局部分歧
正是归约顺序差异的累积所导致的
而当启用“批次不变内核”之后
奇迹发生了
1000次采样的结果完全一致
没有任何差异
这说明只要解决了批次不变性的问题
大模型推理的非确定性是可以被彻底消除的
当然，性能方面确实有一些损耗
没有经过优化的批次不变内核
会让推理的速度下降大约2倍
但是经过优化
比如调整核心的调度、优化内存的访问之后
性能损耗已经控制在可接受的范围内了
这对于需要结果一致性的场景
比如金融AI、医疗诊断和强化学习训练等等
这种性能上的trade off是完全值得的
说到强化学习
这里还要特别提一下这项研究的一个重要价值
它让大模型的“在线策略强化学习”，
也就是On-Policy RL成为可能
之前为什么在线策略RL很难落地呢？
正是因为训练时的模型输出和推理时的模型输出不一致
训练时用的是小批次、甚至单批次
而推理时用的是大批次
两者之间的差异就会导致策略偏移
最终训练崩溃
而有了批次不变内核以后
训练和推理的输出完全一致
KL散度
也就是衡量两个概率分布差异的指标
可以保持在0，真正实现了在线策略
Thinking Machines Lab的团队在Bigmath上做了实验
没有批次不变内核的时候
不使用离线策略校正
比如重要性加权的RL训练
会在318步左右出现奖励崩溃
而启用了批次不变内核后
即使不使用校正，训练也能顺利进行
奖励的稳定性大幅上升
聊完技术方案
我们来总结一下这篇研究的意义
它不仅仅是解决了大模型推理非确定性的一个技术难题
更重要的是
它为大模型的“可重复性”和“可靠性”提供了科学的解决方案
在AI行业快速发展的今天
很多时候我们会被性能、参数规模这些指标吸引
却忽略了“可重复性”这个科学研究的基石
如果一个模型的输出无法复现
那么它的性能再强
也难以在需要严谨性的领域落地
而Thinking Machines Lab的研究
恰恰填补了这一空白
正如托马斯在社交媒体上的总结
大模型推理非确定性
不只是浮点数非结合性或者GPU并发执行的问题
核心的罪魁祸首是批次方差
服务器负载不可预测地改变了数值计算
批次不变内核解锁了真正的可重复性
终于让强化学习的在线策略变得可行
这句话也点出了这项研究的核心价值
它不是对现有技术的小修小补
而是从底层逻辑上解决了大模型落地的一个关键障碍
对于我们普通人来说
这项研究也意味着
未来我们使用的AI产品
会变得更加可靠
比如AI写的报告不会再因为服务器负载的变化
而出现不同的版本，AI辅助的决策
也不会因为计算顺序的差异而产生偏差
而对于AI从业者来说
这篇文章也提供了一个全新的思考视角
那在追求模型性能的同时
也要关注底层计算的确定性
这或许会成为未来大模型技术竞争的一个新的焦点
最后，还有一个小细节值得我们关注
那就是博客的名字“联结主义”。
这个词并不是凭空创造的
它其实是1980年代AI领域的一个重要子领域
核心研究方向是“神经网络与生物大脑的相似性”，
简单来说
就是试图让AI的神经网络像人脑的神经元一样工作
米拉·穆拉蒂在社交媒体上解释说
Thinking Machines Lab的使命之一
就是提高人们对AI的科学理解
与更广泛的研究社区合作
而推出这个博客
就是为了分享他们的科学见解
更有意思的是
按照联合创始人Lilian Weng的补充
那就是Thinking Machines Lab的第一代旗舰产品
就叫“Connection Machine”。
好了
今天关于这篇文章的解读就到这里
链接我会放在视频简介里
如果大家有任何疑问
或者想讨论更多技术细节
欢迎在评论区留言
感谢收看本期视频，我们下期再见
