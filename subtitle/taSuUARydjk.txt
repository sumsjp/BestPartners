大家好这里是最佳拍档
我是大飞
最近呢各行各业都在想着
怎么去跟大语言模型去结合
医疗行业呢也自然不例外
尽管像ChatGPT这样的语言模型
能够生成一些内容比较详实
表达也比较流畅逻辑比较清晰的回复
但是对于医学方面的问题
尤其是在回应患者
所描述的症状的时候
其实还是缺乏专业性
以及对患者输入的精确解读
ChatGPT的回复
常常会包含很多种可能性
并且以较为抽象的建议的形式来呈现出来
但是缺少更深入的上下文理解的能力
所以无法更具体的帮助到患者
相比之下在现实世界中
医生跟患者的互动数据
其实能够更准确的反映出
医疗情况的复杂性
并且提供准确无误的诊断建议
具有极高的专业性
但是由于时间的限制
医生的回应常常简洁到
已经不能充分的传达信息了
甚至有的时候呢会显得不连贯
如果仅仅依靠这些数据
来训练模型的话
那么得到的模型
也很难流畅的去应对各种指令
或者是对话
生成的回应也会较为短小表述不佳
而有时候信息还会含糊不清
这对于患者来说也并不友好
最近香港中文大学深圳校区
和深圳市大数据研究院
所在的王本友教授的团队
利用指令微调和强化学习
在ChatGPT和医生的回复中
找到了一个结合点
训练并且开源了一个新的医疗大模型
HuatuoGPT
这个模型致力于通过融合
ChatGPT生成的蒸馏数据
以及真实世界医生回复的数据
从而让语言模型
能够具备像医生一样的诊断能力
同时呢保持用户交互的流畅性
和内容的丰富性
相关的论文地址
代码地址和演示地址
我会放到视频的简介和评论区里
供大家参考
我们先来简单的介绍一下
这个论文的内容
首先这篇论文提出的语言模型训练方法
可以结合医生和ChatGPT的数据
充分的发挥他们之间的互补作用
既保留真实医疗数据的专业性和准确性
又借助ChatGPT的多样性
和内容的丰富性的特点
那HuatuoGPT使用了四种不同的数据集
第一个是蒸馏后的ChatGPT指令数据集
这个数据集受到了Alpaca模型
创建指令集的方法启发
从ChatGPT中提炼出医疗相关的指令
与之前工作不同的是
这个方法还加入了科室和角色的信息
根据采样的科室或者角色
来生成符合条件的指令数据集
第二个是真实的医生指令数据集
这个数据集来源于
真实的医生和患者之间的问答
医生的回复通常简洁而且口语化
因此通过这个方法可以进行润色
提高这些数据的可读性
第三个是蒸馏后的ChatGPT对话数据集
这个数据集通过为两个ChatGPT模型
提供共享的对话背景
让他们分别模仿医生和患者进行对话
第四个是真实的医生对话数据集
这个数据集来源于真实医生的对话
但是对医生的回复
使用了模型进行润色
这些数据集共同为模型
提供了一个统一的语言模式
医生诊断能力以及指令跟随的能力
为了进一步提升模型生成的质量
HuatuoGPT还使用了基于AI反馈的强化学习技术
也就是RLAIF
通过使用ChatGPT
对模型生成的内容进行评分
考虑内容的用户友好程度
并且结合医生的回答作为参考
将医生回复的质量纳入考量
然后再利用PPO算法
对模型的生成偏好
调整到医生和用户之间的一致性
从而增强模型生成丰富详尽正确的诊断的能力
在评估HuatuoGPT的性能表现上
团队成员采用了自动评估和人工评估
这两种方式相互来验证
在单轮问答场景和多轮交互式诊断场景中
分别进行了评估
针对单轮问答场景
团队的成员精心收集了涵盖
10个医疗领域意图的100个问题
并且利用GPT-4进行自动的评估
具体来说
就是团队提供了两个模型
对同一个问题生成回复
然后使用GPT-4对每个模型的回复
进行分析和打分
最终的测试结果显示
相较于基于LLaMA和ChatGLM
的开源中文医疗模型
HuatuoGPT的表现更加优秀
这个优势也得益于HuatuoGPT
同时使用了从ChatGPT蒸馏的数据
以及真实世界的数据来进行训练
并且借助来自于ChatGPT
和专业医生的混合反馈进行了优化
此外HuatuoGPT在总体性能上
甚至超过了GPT 3.5 Turbo
对于多轮的问诊场景
团队成员收集了涵盖20个科室的
100个多轮对话进行了评估
评估结果显示
HuatuoGPT不仅全面的优于
目前的开源中文医疗模型
而且在大部分科室的表现上
均优于GPT 3.5 Turbo
这也证明了HuatuoGPT
在处理更加复杂的多论问诊场景中
有更加优异的性能
在人工评估方面团队成员
使用了自动评估中的样本进行评估验证
团队成员邀请了专业的医生
为模型的输出结果进行人工评估
同样也分为单轮问答场景和多轮问诊场景
评估结果表明
无论是单轮的人工评测
还是多轮的人工评测
结果都与自动评估的结果保持了一致
这充分验证了模型的性能评估的
一致性和可靠性
那除了公开的论文
代码和演示环境之外
团队还开放了HuatuoGPT的前置工作
Huatuo-26M医疗问答数据集
注意啊
这个并不是HuatuoGPT的训练数据
总共2,600万的医疗问答数据
全部开源到了HuggingFace
如果你需要清理好的干净数据
你可以给他们发邮件申请
并且注明单位和承诺只用于科学目的
对于包括Huatuo-26M在内的
三个公开的医疗问答数据集
HuatuoGPT的Zero shot性能
都超过了GPT 3.5 Turbo ChatGLM
和已有的医疗GBT
甚至远好于全微调的
中等大小的T5和GPT
大飞我也简单试着问了一些
医学上面的问题啊
感觉回答的还行
但是因为我对医学这方面不是很懂
问的问题都是很笼统和简单的
如果有精通医学方面的朋友
可以深度的体验一下
希望能够把体验的感受发到评论区
跟大家共享一下谢谢
好了本期的分享就到这里
感兴趣的小伙伴们欢迎订阅我的频道
我们下期再见
