大家好，这里是最佳拍档，我是大飞
在人工智能算力军备竞赛的今天
NVIDIA的GPU几乎成为了全球数据中心的标配
而AMD的产品却似乎总是游离在主流视野之外
即便AMD近年来频繁发布MI300X、MI325X等重磅产品
甚至在部分技术参数上看似超越了竞品
但是市场的反馈却始终是不温不火
这背后究竟隐藏了怎样的技术博弈与市场逻辑呢？
最近
半导体分析机构SemiAnalysis耗时6个月的时间
完成了一份超长的深度报告
或许能为我们揭开谜底
这份报告不仅涵盖了两家公司推理性能的全方位对比
还深入探讨了总拥有成本、租赁市场生态等关键议题
甚至直言不讳地指出
为何除了超大规模云服务提供商以外
几乎没有企业愿意采用AMD的GPU呢？
今天，我们就来抽丝剥茧
解析一下这场AI算力战争背后的复杂图景
长久以来
市场上一直流传着“AMD的AI服务器
在总拥有成本下推理性能更优”的说法
为了验证这个假设
SemiAnalysis团队展开了一场横跨6个月的马拉松式测试
他们的目标很明确
那就是在真实的生产环境中
对比AMD和NVIDIA推理解决方案的实际表现
但是测试结果却远比团队预期的要复杂
因为在不同的任务类型下
两者的性能表现
呈现出了惊人的差异性
无论是聊天应用、文档处理还是专业的推理场景
硬件与软件的协同效应、延迟要求与工作负载的匹配度
都成为了影响最终结果的变量
SemiAnalysis这次测试的核心方法论
是要突破传统离线基准测试的局限性
转而聚焦在线的吞吐量与端到端延迟的动态平衡
简单来说
就是模拟真实用户的访问场景
通过逐步增加并发的用户数量
来观察系统在压力下的延迟变化
从而得出更贴近实际操作条件的吞吐量指标
这种“用户体验优先”的测试逻辑
让结论数据会更加具有现实的指导意义
在模型选择上
测试团队精心挑选了几个具有代表性的稠密架构和稀疏混合专家架构模型
前者以FP16精度的Llama3 70B和FP8精度的405B模型为代表
后者则采用了FP8精度的DeepSeek V3 670B模型
值得注意的是
无论从算术强度、近似活跃参数、总参数数量以及内存访问模式来看
DeepSeekV3 的模型架构与 OpenAI 的 4o/4.1/o1/o3 等前沿模型架构非常接近
也是用来测试 OpenAI 内部模型架构的最佳代理模型
这意味着它的测试结果
有着重要的参考价值
此外
为了反映实际推理场景和性能特征
团队还对三种不同的输入和输出 Token 长度组合
进行了基准测试
对于计算密集型的摘要任务
采用了4K输入和1K输出的设置
这个场景以大规模预填充通用矩阵乘法操作为主
对预填充的性能要求较高
对于平衡型的对话任务
采用了1K输入和1K输出的设置
平衡了预填充和解码的性能需求
对于内存带宽敏感型的推理任务
采用了1K输入和4K输出的设置
这种性能通常受到内存带宽而非计算能力的限制
可以说
这三种场景全面覆盖了不同业务需求下的性能痛点
推理引擎的选择同样十分考究
vLLM因为广泛的兼容性
成为了Llama3系列的主要测试框架
TensorRT-LLM则展现出了NVIDIA在自有硬件上的深度优化能力
而SGLang则因为其处理大规模模型的高效性
成为了DeepSeekV3的首选
值得一提的是
测试团队还特别评估了张量并行TP配置的影响
比如AMD MI300X支持TP=4和TP=8
而NVIDIA H100由于受到内存的限制
通常只能支持TP=8
这种底层架构的差异
在高并发场景下也会直接影响数据的通信效率
我们先来看一些纸面上的实力对比
在硬件规格上
AMD的MI300X和MI325X的数据令人瞩目
其中MI300X凭借192GB的HBM容量和每秒5.3TB的带宽
单节点理论的带宽可以达到每秒42.4TB
而MI325X更以256GB的HBM和每秒6 TB的带宽
刷新了纪录
相比之下
NVIDIA H200 144GB的HBM和每秒4.8TB的带宽
似乎稍逊一筹
但是Blackwell架构的B200
却能够以每秒8 TB的恐怖带宽后来居上
单节点的理论带宽甚至高达每秒64 TB
这种代际上的差距
或许能解释为何AMD在2025年Q1的市场份额
因为NVIDIA的新品发布而下滑
不过，我们也都知道
参数优势并不就直接等同于实际表现的优势
最终还要看具体的测试结果
在Llama3 70B FP16的测试中
场景差异导致出现了一些戏剧性的结果
在聊天和翻译这类1K输入和1K输出的平衡型任务中
低延迟场景下H100和H200凭借着vLLM轻松领先
但是当批处理的规模和并发度提升之后
MI325X的高带宽优势开始显现
甚至在TP=1的配置下实现反超
然而在1K输入和4K输出的内存敏感型任务中
H100因为受到带宽的限制表现垫底
每秒每 GPU处理的token数量很快就稳定在了大约900
MI325X虽然稳定的时间比较晚
但是在高延迟场景下吞吐量领先
而H200搭配TensorRT-LLM则展现出了全面的优势
尤其是内存管理方面的优化能力
让它能在全延迟范围内保持足够的高性能
表现无可匹敌
而在Llama3 405B FP8的大型稠密模型测试中
AMD的硬件优势则更为明显
在1K输入和1K输出场景中
MI325X和MI300X在40秒以内的延迟时间上
超越了所有NVIDIA的配置
凸显了大内存对大型模型的重要性
但是H200搭配TensorRT-LLM再次展现出了惊人的技术深度
对内存利用率的优化让它在高并发的时候
仍然能够保持接近每秒1000个token的吞吐量
在内存受限的1K输入和4K输出场景中
MI325X则全面压制了H200搭配vLLM的配置
但是经过TensorRT-LLM加持的H200
仍然以1.5倍的吞吐量领先MI325X
这表明推理引擎的优化潜力
甚至可能超越硬件本身的参数差异
而在DeepSeekV3 670B FP8的测试中
由于H100因为单节点的内存限制
甚至无法运行这个模型
而H200与MI300X/MI325X的竞争
也呈现出了明显的场景分化
在低延迟高交互性的聊天场景中
H200几乎全部胜出
MI325X 仅在 25 到 35 秒的小范围延迟内可与 H200 竞争
但是在高延迟的摘要任务中
MI325X的每美元性能比H200要高出20%到30%，
这或许也能够解释为什么部分的超大规模企业
愿意为AMD的长期成本优势来买单
整体测试显示，在大多数的场景中
MI300X与H200相比仍然缺乏足够的竞争力
尤其是在低延迟场景下
H200搭配TensorRT-LLM的表现
几乎可以碾压对手
但是在某些特定领域
AMD还是展现出了其独特的优势
比如在Llama3 405B和DeepSeekV3 670B等大型模型的测试中
MI300X的绝对性能和性价比
反超了英伟达的H100
这主要还是得益于AMD更大的内存带宽和容量
能够更高效地处理大规模参数的存储与调用
我们再来看看总持有成本TCO方面
从长期来看
AMD确实展现出了一定优势
MI300X的每小时总拥有成本为1.34美元
低于H100的1.58美元和H200的1.63美元
资本成本占比大约在70%左右
在Llama3 70B的超低延迟任务中
MI325X和MI300X的每百万token成本要优于NVIDIA
但是随着延迟的增加
NVIDIA的规模效应和软件优化
使得它的成本效率实现了反超
值得注意的是
MI325X在部分场景中的性能提升
并没有能够转化为成本优势
因为它价格上的涨幅超过了性能上的增益
这会对企业的采购决策构成现实方面的挑战
总的来说，对于超大规模企业而言
直接采购AMD GPU并且进行长期运营可能是更具有经济性的
尤其是在处理大型稠密模型的时候
硬件成本的节省能够抵消软件生态的不足
但是对于中小型企业或者有临时算力需求的企业来说
租赁市场的高成本和服务稀缺性
使得AMD几乎成为一个不可能的选项
这种“两极分化”的市场格局
本质上还是供应链生态与商业模式差异的集中体现
从测试结果中我们也可以看出
AMD的真正挑战并不在硬件参数上
而是软件生态的建设过于滞后
NVIDIA的CUDA生态经过数十年的积累
已经拥有超过200万的开发者和数万款应用
而AMD的ROCm平台在CI覆盖率、数值精度内核等方面
与CUDA的差距显著
比方说
TensorRT-LLM的开发者体验虽然饱受诟病
但是仍然在持续优化
而AMD的SGLang的市场覆盖率还不到NVIDIA的10%，
而且需要用户手动调整大量的环境变量
才能够接近峰值性能
这就对普通企业用户构成了极高的技术门槛
研发投入结构的差异也是一个值得关注的因素
AMD上个季度在股票回购上花费了7.49亿美元
而内部研发的集群资源投入只有1300万美元
这种“重股东回报、轻技术积累”的策略
导致AMD的软件生态建设进展相当缓慢
相比之下
NVIDIA将大量资源投入到了开发者工具和框架的优化中
尽管TensorRT-LLM的Python版本仍然不够完善
但是已经展现出了追赶的态势
而AMD在解耦解码/预填充等前沿技术上的缺席
进一步拉大了自己与竞争对手的差距
另外，租赁市场的生态差异
也成为了AMD难以跨越的一道鸿沟
目前有超过100家新的云服务商
提供NVIDIA GPU的中短期
也就是6个月以内的租赁服务
充分的市场竞争将H100/H200的租赁成本压到了非常低的位置
而AMD方面
只有极少数供应商提供MI300X/MI325X的短期租赁
而市场上的稀缺性
也导致AMD GPU的租金居高不下
比如MI300X的1个月的租金就超过每小时2.5美元
而同期H200也只需要每小时2.5美元左右
而MI325X 1 个月期的租赁合约几乎在市场上就没有
这种失衡使得中小型企业在无需长期持有硬件的情况下
几乎不可能选择AMD
而这恰恰是市场主流的需求所在
SemiAnalysis分析
AMD GPU 如果想要在租赁市场与 NVIDIA 竞争
就必须要将租金降至合理区间
比如MI300X要降到每小时1.9到2.4美元左右
MI325X要降到每小时2.5到3美元左右
才有可能
最后，SemiAnalysis认为
面对NVIDIA Blackwell架构的强势冲击
AMD的应对节奏显得稍显滞后
不仅MI325X的大规模出货比H200晚了一个季度
MI355X更是要等到2025年年底
而此时NVIDIA的B200早已占据了市场先机
不过，AMD也不能说是毫无机会
毕竟MI355X 288GB的HBM和每秒8.0TB的带宽
可以说是直接剑指B200
如果能够搭配上快速迭代的软件优化
还是有希望在2026年重塑一部分的竞争格局
不过，对于行业而言
这种双雄争霸的格局恰恰是技术进步的一个驱动力
NVIDIA的生态壁垒与AMD的成本优势
形成了一种微妙的平衡
超大规模企业的定制化需求
与中小企业的灵活采购模式并存
预示着未来算力市场将呈现出更为复杂的分层结构
而对于开发者和用户来说
我们要做的就是密切关注硬件的迭代与软件生态的协同进化
把握住技术发展的趋势变化
好了
以上就是这篇文章的主要内容了
说起来
文章作者迪伦Dylan真的是非常执着于对比NVDIA和AMD
之前已经专门写过两篇文章来分析AMD GPU的问题了
还被苏妈约谈过了一次
这次还是没忍住又写了一篇
那大家是否赞同SemiAnalysis在文章中对AMD的分析呢？
欢迎在评论区留言
感谢大家观看本期视频
我们下期再见
