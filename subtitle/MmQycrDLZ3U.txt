大家好，这里是最佳拍档，我是大飞
2月19日
Hugging Face发布了一本如何在GPU集群上
训练大语言模型的超大规模训练手册
这本手册耗时6个月完成
在多达512个GPU上进行了超过4000次的scaling实验
内容涵盖了从基础原理到实际操作的方方面面
对于想要深入了解大模型训练的人来说
是一份价值很高的参考资料
Hugging Face的联创兼CEO 克莱门特·德朗格（Clement Delangue）曾经表示
他理想中的世界
是所有的公司和组织
无论规模大小、财力是否雄厚
都有能力来训练属于自己的AI
而这份「超大规模实战手册」的发布
正是朝着这个理想迈进的重要一步
克莱门特希望通过分享这些宝贵的经验和技术
推动AI领域的民主化发展
让更多的人能够参与到这个领域中
而不是让AI训练的资源仅仅集中在少数大型企业和研究机构手中
今天咱们就一起来解读一下这本手册里的关键内容
我们先来了解一下大模型训练过程中面临的几个关键挑战
首先就是显存的占用问题
在训练大模型的时候
需要在显存中存储模型权重、模型梯度、优化器状态以及计算梯度所需的激活值
随着模型规模的不断扩大
这些数据所占用的显存也在急剧增加
比如说，当模型参数达到70B的时候
仅仅权重和优化器状态所需要的显存
就超过了典型GPU
比如H100的80GB显存容量
而一旦某个训练步骤所需要的显存超过了GPU的容量
训练就会被迫中断
这是训练过程中最直接、也是最棘手的问题之一
除了显存占用
计算效率也是一个重要挑战
我们都希望GPU能够在训练过程中充分发挥它的计算能力
尽可能多地执行计算任务
而不是把时间浪费在数据传输或者等待其他GPU完成任务上
然而，在实际的训练环境中
由于各种因素的影响
GPU的计算效率往往无法达到理想状态
比如说
数据并行在扩展到一定规模后
会因为通信开销而导致计算效率的下降
这就需要我们采取一些技术手段来优化计算过程
提高GPU的利用率
还有一个挑战就是通信开销
在多GPU的训练环境中
不同GPU之间需要进行大量的数据通信
比如模型梯度的同步、参数的传输等等
如果通信开销过大
就会导致GPU在等待数据传输的过程中处于空闲状态
这不仅浪费了计算资源
还会降低整个训练系统的效率
因此，如何减少通信开销
让通信和计算过程尽可能地重叠进行
充分利用节点内部和节点之间的带宽
就成为了提高训练效率的关键问题之一
面对这些挑战
Hugging Face在手册中介绍了一系列行之有效的技术手段
首先是激活值重计算技术
这项技术的基本思想是在前向传播过程中丢弃一些激活值
以此来节省显存空间
由于激活值在训练过程中占用了大量的显存
尤其是对于长序列的输入
所以通过丢弃部分激活值
可以显著减少显存的占用
当然，这样做会增加一些计算量
不过好在可以在反向传播过程中
动态地重新计算这些被丢弃的激活值
在实际应用中
通常会选择在模型架构的几个关键点来存储激活值
丢弃其余的部分
然后在反向传播的时候
从最近保存的激活值开始重新计算
现在，大多数的训练框架
比如大家常用的PyTorch
都集成了激活值重计算的优化策略
像FlashAttention技术
它原生就支持激活值重计算
这样就使得这项技术在实际训练中
变得更加容易应用
虽然激活值重计算会稍微增加一些浮点运算的次数
但是它减少了内存访问的开销
在GPU上
这种计算和内存的权衡往往是非常有利的
不仅计算速度更快
还能有效的降低显存占用
不过，需要注意的是
激活值仍然与批大小呈线性相关
当批大小不断增加的时候
激活值所占用的显存可能又会成为一个问题
为了解决激活值显存占用的问题
还有一种方法叫做梯度累积
梯度累积的原理是将批量数据拆分成多个微批次
然后依次进行前向传播和反向传播
这样做的好处是，在计算梯度的时候
不需要一次性计算整个批量数据的梯度
而是通过累积多个微批次的梯度
来得到最终的梯度
通过梯度累积
全局批大小可以通过“微批次大小×梯度累积步数”这个公式来计算
比如说
原本一个大的批量数据可能会导致显存爆炸
但是通过将它拆分成多个微批次
就可以避免这个问题
而且
梯度累积还可以和激活重计算技术结合使用
进一步减少显存的占用
这就像是给训练过程加上了双重保险
让我们在有限的显存条件下
能够更高效地进行训练
说完了激活值重计算和梯度累积
我们再来看看数据并行技术
数据并行是梯度累积的并行版本
它的核心思想是在多个GPU上同时运行训练任务
每个GPU并行处理不同的微批次数据
在这个过程中
每个GPU上计算得到的梯度是不同的
为了保证不同GPU上的模型能够保持同步
就需要使用all - reduce操作
对模型的梯度进行平均
举个例子，假设有4个GPU
每个GPU处理不同的微批次数据
在计算完梯度后
通过all - reduce操作将这些梯度进行汇总和平均
然后再更新模型的参数
为了提高数据并行的效率
通常会采用一些优化方法
比如将梯度同步与后向传播重叠进行、梯度分桶
以及和梯度累积相结合等等
这样可以尽可能地让GPU在计算和通信过程中保持忙碌状态
减少空闲时间
不过，当GPU数量超过一定限制的时候
数据并行的吞吐量就会开始显著下降
这是因为数据并行在扩展到一定规模后
会出现明显的通信开销瓶颈
为了应对数据并行的通信瓶颈问题
以及进一步减少内存冗余
就出现了ZeRO
也就是零冗余优化器的技术
ZeRO是DeepSpeed提出的一种优化技术
它的目的在于减少大模型训练过程中的内存冗余
在传统的数据并行中
每个实例都会简单地复制优化器的状态、梯度和参数
这就会引入大量的内存冗余
而ZeRO通过在数据并行维度上对这些数据进行分区
有效地消除了内存冗余
ZeRO分为三个优化阶段
分别是ZeRO - 1、ZeRO - 2和ZeRO - 3
其中ZeRO - 1阶段主要是对优化器状态进行分区
将优化器状态分成N_d等份
N_d是数据并行度，在优化步骤中
只有1/N_d的权重会被更新
同时对梯度执行reduce - scatter操作
和传统的数据并行相比
ZeRO - 1将all - reduce梯度通信替换为reduce - scatter操作
并且在优化器后添加了一个全参数的all - gather操作
ZeRO - 2阶段则在ZeRO - 1的基础上
增加了对梯度的分区，在反向传播中
不再对梯度执行all - reduce操作
而是仅执行reduce - scatter操作
只传播1/N_d的梯度
这样可以节省更多的内存
随着N_d的增加，最多可节省8倍内存
到了ZeRO - 3阶段
也就是大家常说的FSDP（Fully Sharded Data Parallel）
它将分区扩展到了模型参数
在这个阶段
模型的所有部分都被分布式存储
在前向传播和反向传播过程中
需要不断地进行all - gather操作来获取所需的参数
虽然ZeRO技术在减少内存冗余方面效果显著
但是它也有一定的局限性
比如说无法对激活值内存进行处理
这部分内存会随着序列长度和批大小的增加而增加
当ZeRO在处理激活值内存遇到瓶颈的时候
张量并行技术就要发挥作用了
张量并行是针对激活内存超预算问题的一种优化技术
它利用矩阵乘法的特性
通过按列或按行分区的方式
将张量分布到多个GPU上进行计算
比如说，在列线性计算中
需要广播输入矩阵、分割权重矩阵列
然后用all - reduce操作组合结果
而行线性计算则要分割权重矩阵行和输入
分别使用scatter和all - reduce操作
通过这种方式
张量并行可以减少矩阵乘法的激活内存
并且能够在多GPU间分布模型参数、梯度、优化器状态
这样一来，即使是7B参数的模型
也能够在单节点8个GPU上运行
不过，张量并行也有一些缺点
比如跨节点通信速度较慢
当张量并行度超过8个GPU的时候
通信开销就会变得非常明显
从TP = 8提升到TP = 16
再到TP = 32的时候，性能会显著下降
而且，像层归一化和随机失活等操作
仍然需要收集完整的激活值
这在一定程度上限制了张量并行的应用
为了解决这两个问题
序列并行技术应运而生
序列并行的优势在于它可以减少最大激活值的存储大小
在仅使用张量并行时
需要存储形状为(bsh)的激活值，而使用序列并行后
激活值的存储大小可以减少到(bsh / SP)
这里的SP表示序列并行度
这样有助于节省激活值的内存
从而能够增大批大小和序列长度
比如说，在70B参数的模型中
使用TP/SP = 16时
可以处理16k token的序列长度
这比单纯使用张量并行的情况要好很多
不过，随着张量并行度的增加
计算效率和显存容量之间需要进行权衡
从TP = 8提升到TP = 16时
性能会下降明显
这是因为涉及到节点内到节点间的通信转变
而且，当序列长度增加时
TP区域的激活值内存仍然会激增；
如果模型过大
即使TP = 8也可能无法适配
会因为节点间的连接导致速度大幅下降
针对这些问题
又出现了上下文并行和流水线并行技术
上下文并行技术借鉴了序列并行中
按照序列长度拆分的思路
对已经应用张量并行的模块
沿着序列长度和另一个维度进行拆分
并且在整个模型上应用序列拆分
而不是仅仅在模型的序列并行区域
对于大多数模块来说
这种方式并不会产生太大的影响
因为这些模块中每个token都是独立处理的
而且不需要像张量并行那样进行高成本的通信
只需要分割输入
不需要拆分权重矩阵
而是在计算梯度后
通过all - reduce操作同步上下文并行组内的梯度
在注意力模块中
每个token需要访问其他所有token的键/值对
由于上下文并行是按序列维度拆分输入的
所以注意力模块需要在GPU间进行全面通信
来交换键/值数据
为了高效处理这种通信
引入了环形注意力（Ring Attention）技术
以4个GPU和4个token的输入为例
每个GPU先异步地将自身的键/值对发送给其他GPU
在等待的过程中计算已有数据的注意力分数
理想状态下
在计算完成前能够收到下一个键/值对
这样就可以立即开始下一轮计算
不过，环形注意力的简单实现
会因为因果注意力矩阵的形状而导致GPU计算负载不均衡
为了解决这个问题
又出现了Zig - Zag环形注意力机制
它摒弃了向GPU顺序分配token的方式
采用混合排序
让每个GPU上都有早期和晚期的token
从而实现了计算在各个GPU上的平衡分布
前面介绍的这些技术呢
在一定程度上
解决了大语言模型训练中的各种问题
但是当模型权重无法轻松地在一个节点上存储时
就需要用到流水线并行技术了
像张量并行在扩展到超过单个节点的GPU数量时
通常是4或8个
就会受到节点间连接低带宽网络的影响
性能下降明显
对于70B参数以上的模型
单节点4 - 8个GPU很难承载其权重规模
这时流水线并行就派上用场了
流水线并行的原理是将模型的各层分布到多个GPU上
比如有8个GPU的时候
可以把第1 - 4层放在GPU1上
第5 - 8层放在GPU2上，以此类推
这样每个GPU只需要存储和处理部分的模型层
减少了单个GPU的内存需求
但是由于每个GPU仍然需要处理完整批次的数据
所以激活内存并不会因为层的划分而减少
而且激活张量需要在GPU间按照流水线顺序传递
在流水线并行中
由于数据的处理具有顺序性
这就导致GPU利用率不高
存在空闲时间
为了解决这个问题
出现了多种调度方法
全前向全反向（AFAB）调度是先进行所有前向传播
再进行所有反向传播
这种方法保留了模型训练代码的总体结构
易于实现
通过增加微批次数量
可以减小空闲时间的占比
提高效率
但是由于在反向传播前需要保存所有的激活值
这种方法会导致内存迅速耗尽
于是，又出现了一次前向一次反向
也就是1F1B调度
它交替执行一次前向和一次反向传播
尽早开始反向传播
这样可以减少激活内存占用
并且可以增加微批次来减小空闲时间
不过
当微批次数量等于或者小于流水线的并行度-1时
性能会比较低
并且会随着并行度的增加而下降
虽然使用更多的微批次有助于提升低并行度时的性能
但是在高并行度时仍然会受到限制
此外，还有交错阶段技术
它不同于简单地按模型深度划分
而是将奇数层和偶数层分别置于不同GPU
形成“循环流水线”，
让微批次在前向传播的时候在GPU间循环
这种技术虽然增加了通信量
但是每次前向和反向传播的时间
会因为每个GPU的阶段数或模型块数而减少
所以可以通过增加微批次和交错阶段来减小空闲时间
像Llama 3.1的流水线并行方法
就采用了带交错阶段的单前向单反向设置
并且深度优先和广度优先的优先级设置是可调节的
还有一些更复杂的方法
比如零气泡（ZeroBubble）和双管道（DualPipe）技术
它们的关键在于细粒度拆分操作并交错执行
从而减少GPU空闲时间
其中
ZeroBubble发现在矩阵乘法的反向传递中
输入反向操作（B）和权重反向操作（W）可以分离
W可以在对应B之后灵活安排
用来填补流水线的空闲时间
而DeepSeek的DualPipe在V3技术报告中扩展了分解方法
针对从流水线并行两端传播的两个数据流交错
进一步减少了GPU空闲时间
除了前面提到的这些技术
在MoE（Mixture of Experts）模型中
还有一种专家并行技术，简称EP
MoE模型近年来因为GPT - 4、Mixtral、DeepSeek - V3/R1等模型
受到了广泛关注
它的基本思想是在于每一层不采用单个前馈模块
而是设置多个并行模块
对token进行不同的处理
MoE层的设计使得专家并行很容易实现
因为前馈层是完全独立的
与张量并行相比，专家并行更轻量
不需要拆分矩阵乘法
只需要将token隐藏状态路由到合适的专家即可
在实际应用中
专家并行通常会与其他并行方式结合使用
因为专家并行仅会影响MoE层
不会分片输入token
如果只使用专家并行
那么GPU在处理非MoE模块的时候就会有冗余计算
专家并行高效运行的技巧与模型设计紧密相关
这也需要开发者根据具体的模型需求进行优化
总的来说
Hugging Face在制作这份手册的过程中
进行了大量的实验
他们在512个GPU上运行了超过4000次分布式实验
目的就是为了探索不同的分布式训练架构
以及模型大小对训练效果的影响
通过这些实验，得到了丰富的数据
比如不同模型在各种并行技术组合下的
显存占用情况、计算效率以及通信开销等
这些数据为我们在实际训练中选择合适的技术和配置
提供了重要的参考依据
手册中还总结了一些在大语言模型训练过程中的关键步骤和策略
首先是将模型适配到内存中
对于 GPU 资源丰富的情况
如果是小于 10B 参数的模型
那么只使用数据并行就足够了
而对于 10B 到 30B 参数的模型
结合数据并行与张量并行
通常能取得较好效果
要是模型参数在 30B 到 70B 之间
除了数据并行和张量并行
还可能需要考虑使用序列并行来进一步优化显存的使用
当模型参数超过 70B 时
就需要更复杂的组合
比如结合流水线并行等多种技术
才能将模型适配到内存中
在满足目标全局批大小方面
要综合考虑激活值重计算、梯度累积以及数据并行等技术的运用
通过合理设置微批次大小和梯度累积步数
结合多个 GPU 的数据并行
可以在有限的显存条件下达到目标全局批大小
例如，如果目标全局批大小较大
而单个 GPU 的显存无法一次性处理整个批次的数据
那么可以利用梯度累积
将大批次拆分成多个微批次
在多个 GPU 上并行计算
然后再将梯度累积起来
关于优化训练的吞吐量方面
则需要根据模型的规模和硬件资源
来选择合适的并行策略组合
对于较小规模的模型
简单的数据并行可能就能满足需求
实现较高的吞吐量
但是随着模型规模的增大
就需要引入张量并行、序列并行、流水线并行等技术
比如在训练 70B 参数以上的模型时
通过使用流水线并行来减少单个 GPU 内存需求的同时
还需要通过合理的调度方法
比如采用交错阶段技术等等
来提高 GPU 的利用率
从而提升训练吞吐量
可以说
这本「超大规模实战手册」的发布
为广大的 AI 开发者、研究人员以及相关企业
提供了一套全面且实用的大语言模型训练指南
无论是刚刚涉足 AI 领域的新手
还是在行业内深耕多年的资深专家
都能从这份手册中获取到有价值的信息
建议感兴趣的朋友可以去仔细阅读一下报告原文
相信一定会有更多收获
感谢大家观看本期视频
我们下期再见
