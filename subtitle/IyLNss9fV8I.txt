大家好，这里是最佳拍档，我是大飞
所谓兵贵神速
但是在大模型这块必争之地
谷歌却总是慢人一步
因此经常被大家调侃是“起了大早
赶个晚集”，
比如说这次 Sora 借鉴的 ViT、ViViT、NaVit、MAGVit 等核心组件技术
其实都是出自Google的论文
不过2月21日，谷歌却突放大招
发布了一款开放模型，Gemma
并且呢声称它是轻量级模型系列中最先进的
不仅可以比肩Meta的Llama 2模型
更是超越了原本最强的Mistral 7B
关于为什么叫开放模型
而不是开源模型
我们后面会说明一下
如果我们仅仅从名称上来看
最新推出的Gemma和此前推出的Gemini
还让人有点傻傻分不清的感觉
对此Google在官宣公告中解释称
Gemma设计的灵感就是来源于 Gemini
拉丁语Gemma，有“宝石”的意思
二者之间的不同之处有几点
1、你可以将Gemma视为Gemini的更小、更轻的版本
2、Gemma 的设计目的是让开发人员和研究人员更容易访问和使用
而 Gemini 的设计目的是用于更复杂的任务
3、这两种型号均可以免费使用
但是Gemma 的免费套餐更为有限
4、更为重要的是
Gemma 模型可以在台式机或者笔记本电脑上本地运行
好了
接下来我们来详细介绍一下这个开放模型Gemma
首先，我们来说明一下Gemma是什么？
整体来看
Gemma 是由 Google DeepMind 和其他 Google AI 团队共同开发而成
采用与 Gemini 模型相同的研究和技术
建立在序列模型、Transformer、基于神经网络的深度学习方法和分布式大规模训练技术之上
模型训练的上下文长度为 8192 个 token
Gemma模型有两种尺寸
一个是 Gemma 2B
也就是20 亿参数
另一个是 Gemma 7B
也就是70 亿参数
并且每种尺寸都发布了预训练和指令调整的变体
我们都知道，在AI模型中
AI模型的行为是由神经网络中的参数来确定的
而权重就是存储在文件中的这些参数的子集
而这一次
Gemma模型的权重也将以许可商业授权的方式发布
同时 Google 也会发布一个新的负责任的生成式人工智能工具包
来指导开发者、研究人员负责任地使用 Gemma 模型
Gemma 的发布
也是 Google 自 2022 年 OpenAI 推出 ChatGPT 以来
发布的首个开放式大语言模型
但是这并不是 Google 对开放式 AI 研究的第一个贡献
在官方博客中，Google AI 团队表示
他们在过去带来了 Transformers、TensorFlow、BERT、T5、JAX、AlphaFold 和 AlphaCode 等重要的 AI 架构和工具集
所以这一次
他们也为所有主要框架提供了推理和监督微调 (SFT) 工具链
包括JAX、PyTorch 和 支持本地 Keras 3.0的TensorFlow
以及随时可用的 Colab 和 Kaggle notebooks
并且与 Hugging Face、MaxText 和 NVIDIA NeMo 和 TensorRT-LLM 等流行工具进行了集成
方便开发者更容易得上手使用 Gemma
经过预训练和指令调整的 Gemma 模型可在笔记本电脑、工作站或者 Google Cloud 上运行
并且能够部署在 Vertex AI 和谷歌 Kubernetes Engine 上
除此之外
英伟达也在今天宣布与 Google 合作
在包括本地 RTX AI PC 在内的所有英伟达 AI 平台上启动优化
用来加速 Gemma 的性能
既然发布了新模型
就难免要和业界已存在的大模型一较高下
Google 在发布了一份 16 页的 Gemma 技术报告时
将Gemma与 Meta 的 LLaMA 2（7B）、LLaMA 2（13B）
以及 Mistral（7B）进行了性能对比
Google 表示
Gemma 2B 和 7B 与其他开放式模型相比
在其规模上实现了同类最佳的性能
具体的技术报告评测结果是这样的
从学术基准角度来看
Gemma 7B 在数学、Python 代码生成、常识和常识推理任务的几个基准测试中
优于 Meta 的 Llama 2 7B 和 13B 模型
详细来看，在 MMLU基准测试中
Gemma 7B 模型不仅超过了所有规模相同或更小的开源模型
还超过了一些更大的模型
包括 Llama 2 13B
我们也都知道，对于 AI 模型的发布
Google 一直采取比较谨慎的态度
这一次，Google 也特别强调
Gemma 的设计是将 AI 原则放在首位
为了使 Gemma 预训练模型安全可靠
Google 使用了自动化技术
从训练集中过滤掉某些个人信息和其他敏感数据
此外
他们还使用了大量的微调和人类反馈强化学习（RLHF）
使指令调整模型与负责任的行为保持一致
为了了解并降低 Gemma 模型的风险
Google进行了严格的评估
包括人工红队、自动对抗测试和危险活动模型能力评估
此外
Google 还与 Gemma 一起发布了新的《负责任的生成式人工智能工具包》，
来帮助开发人员和研究人员优先构建安全、负责任的人工智能应用
该工具包包括一个安全分类器
最新的模型调试工具LIT
以及 Google 在开发和部署大型语言模型方面最佳实践经验
最后
我们要再说一下为什么Gemma是个开放模型呢？
为什么不说是开源模型
这里开放指的到底是什么
又开放到什么程度？
众所周知
Mistral模型的权重是根据Apache 2.0许可协议发布的
这意味着它们遵循开源的原则
但Meta领衔的Llama 2模型的权重则是通过一个专有许可发布
该许可采用了非常有针对性的授权
如果月活用户数超过 7 亿
企业必须从 Meta 申请许可证
而 Meta 会对此类授权进行严格设限
这也就意味着亚马逊、苹果、谷歌、字节跳动等大公司是受限的
所以是不是开源
我们首先要看它遵循什么协议
按照Google的说法
开放源代码的一个好处是，一旦发布
许可证就赋予用户完全的创作自主权
另一个好处是
开放源码技术可以不受限制地进行修改
从而适应最终用户的独特使用情况
但是Gemma 遵循的规则是「开放模型」而非开源
开放模型的特点是可以免费获取模型权重
但是使用、再分发和变体所有权的条款
会根据模型的具体使用条款而有所不同
这些条款可能不是基于开源许可证
按照Gemma 模型的使用条款规定
个人开发者、研究人员和商业用户都可自由访问和重新分配这些模型
用户还可自由创建和发布模型的变体
之所以Google不用开源一词来形容 Gemma
是因为在它看来
现有的开源概念并不总能直接应用于 AI 系统
这就提出了如何在人工智能中使用开源许可证的问题
有意思的是，谷歌在发布Gemma的同时
于其开源博客发布了一篇题为《在Gemini时代负责任地构建开放模型》的文章
文中提到
开源许可证赋予了用户完全的创作自主权
这是开发人员和最终用户获得技术的有力保证
但是在恶意行为者手中
缺乏限制可能会增加风险
在这样的背景下
真正的开放性和透明度
特别是涉及训练代码、数据集以及不受限制地访问和使用模型资源等方面
仍是当前AI社区需要努力实现的目标
从体验上来看
网友在 X 社交平台上分享了他用 ollama 在 Macbook Pro M1 Max 32G 上安装和使用gemma-7b的过程
并直言gemma-7b 的速度好过 Llama 2 13B
对 Gemma 感兴趣的小伙伴
现在可以直接通过 Gemma的官方入口上手体验
首次使用谷歌云的用户还可以获得300美元的信用额度
研究人员还可以申请高达50万美元的谷歌云信用额度
总的来说，这次Gemma模型的发布
说明Google还是向模型开源走出了一步
不过比Meta晚了整整一年
也明显是被动防御和扭扭捏捏的应对之策
而不是进攻之举
说实在
开放个7B的模型实在是有点太小儿科了
对竞争对手一点杀伤力都没有
大飞我觉得应该直接开源一个超越市场上所有开源的至少 100B 的模型、支持100万的超长上下文、包含完善的推理 infra 方案、外加送一定的 cloud credit
很显然，谷歌觉得自己还是 AI的王者
放不下高贵的头颅
很多发布都有点不痛不痒
还是沿着过去研发驱动的老路
而不是产品和竞争驱动
比如说不停的发论文、取新名字
比如Palme、rt-2、Gemini、VideoPoet、W
A
L
T 等等、就拿Gemini举例
在过去的三个月里
Google 先是推出了 Gemini
而后干掉了 Bard 品牌名
并将其改名为 Gemini
进而推出了更好的版本 Gemini Advanced
另外还有Gemini Ultra 1.0
前两天又推出了另一个改进版本 Gemini 1.5 以及 Gemini 1.5 Pro
带来了 Gemini for Workspace
据媒体报道
Google 内部还开发了一款“Gemini的后代”，
名字叫Goose，仅供内部使用
帮助员工更快地编写代码
我觉得如果按照这个系列命名下去
不仅用户快被绕晕了
就连 Google 内部员工估计都已经快被“逼疯”了
而Google发布的大多数模型又完整度不够
感觉就没有一个绝对能打的产品
那么大家觉得Google的AI产品线规划的乱不乱
能准确分得清 Google 各种模型的用途和区别吗？
对最新发布的模型 Gemma 又有何看法？
欢迎在评论区留言
本期视频内容就到这里
感谢大家的观看
我们下期再见
