大家好，这里是最佳拍档，我是大飞
最近，Meta可以说是动作不断
前些天刚刚有外媒曝出
扎克伯格正在组建一个名为「超级智能团队」的专家团队
想要实现通用人工智能AGI
并且开出了 9 位数的薪酬为团队吸纳人才
紧接着又传出要以149亿美元
折合人民币大约1066亿元的价格
收购Scale AI 49%的股权
而就在刚刚，Meta 又有了新的动作
那就是推出了基于视频训练的世界模型 V-JEPA 2
它能够实现最先进的环境理解与预测能力
并且在新环境中完成零样本规划与机器人控制
Meta 表示
他们在追求高级机器智能AMI的目标过程中
关键在于开发出能像人类一样认知世界、规划陌生任务的执行方案
并且能够高效适应不断变化环境的 AI 系统
而且
这次Meta的首席 AI 科学家 Yann LeCun 亲自出镜
向大家介绍世界模型与其他 AI 模型的不同之处
今天
我们就来结合Meta官方发布的内容
来看看这个V-JEPA 2究竟有什么特点
根据视频中杨立昆的介绍
世界模型是一种现实的抽象数字孪生
AI 可以参考它来理解世界
并且预测自身行为的后果
与理解语言不同的是
世界模型让机器能够理解物理世界
并且能够规划行动路线来完成任务
而无需进行数百万次的试验
因为世界模型提供了对世界运行方式的基本理解
这种推理和规划的能力将会带来广泛的影响
比如
它可以用来帮助视障人士、在混合现实中为复杂的任务提供指导、让教育变得更加个性化
甚至可以理解代码对程序状态和外部世界的影响
此外
世界模型对于自动驾驶汽车和机器人等自主系统
也至关重要
它将开启机器人技术的新纪元
让现实世界中的 AI Agent
能够在不需要大量机器人训练数据的情况下
帮助完成家务和体力劳动等任务
这次发布的V-JEPA 2 拥有 12 亿的参数
基于联合嵌入预测架构JEPA构建
在此之前，JEPA 架构已经被证明了
在处理图像和 3D 点云等多模态方面有出色的表现
V-JEPA 2在V-JEPA 的基础上
进一步提升了动作预测和世界建模的能力
使得机器人能够通过与陌生物体和环境的交互
来完成任务
在Meta放出的几个示例中
我们可以看到V-JEPA 2的几项特殊能力
首先是它开启了对世界的理解
通过与语言建模相结合
V-JEPA 2 可以提供卓越的运动理解
以及领先的视觉推理能力
当视频中的人跳向水面的时候
V-JEPA 2 给出了“向前
1.5 周空翻，无转体”这样的专业解读
其次
V-JEPA 2能够预测下一步会发生什么
也就是可以预测世界将如何发展
在示例中
V-JEPA 2仅凭演示人员当前做的动作
就可以预测打开冰箱、拿瓶子、关上冰箱、挤瓶子等一系列的动作
在性能方面
V - JEPA 2相比之前的最优模型
展示出了多个方面的大幅提升
首先在基于图像目标的规划与机器人控制方面
对于Reach到达基准
V - JEPA 2和之前最优的Octo模型
都可以达到100%，
这说明在让机器人执行“到达”任务时
表现完美而且稳定
对于Grasp抓取基准
V - JEPA 2达到了45%，
远超之前Octo的8% ，进步显著
意味着V - JEPA 2能够更精准、有效地完成抓取动作
对于Pick - and - place
拾取与放置基准
V - JEPA 2 73%的成绩相较于之前Octo的13% ，
更是飞跃性的提升
这说明V - JEPA 2能让机器人在执行拾取和放置操作的能力变强
从而在自动化操作等场景中变得更可靠
在预测（Prediction）方面
V - JEPA 2在Epic - Kitchens - 100动作预测任务中
得分为39.7% ，
高于之前PlausiVL的27.6% ，
这说明对厨房场景中动作的预测能力有所提升
有助于理解人类的行为序列、辅助智能交互等等
在理解（Understanding）方面
在Something - Something v2 动作识别基准上
V - JEPA 2得分为77.3%，
比之前最佳模型的69.7% 高出7.6
说明对这类动作的识别更加准确
有利于视频内容理解、行为分析
在Diving48 潜水相关动作识别 基准上
V - JEPA 2 90.2%的得分也高于之前的86.4%。
在感知测试基准上
V - JEPA 2 84.0%的得分略高于之前PerceptionLM的82.7% ，
视觉感知能力略有进步
在MVPBench基准上
V - JEPA 2 44.5%的得分也优于之前InternVL-2.5的39.9% ，
说明多模态融合理解等能力也有所提升
从基准测试中我们可以看出，整体上
V-JEPA 2在机器人控制、动作预测、行为理解等多任务中
对比之前的最优模型大多都有明显性能提升
展现出了更强的多模态处理和任务执行能力
从而在机器人应用、视频理解等领域
有更好的潜力与表现
那么
V-JEPA 2 具体都做了哪些创新呢？
在V-JEPA 2的论文中
具体介绍了V-JEPA 2的两个主要组件
分别是编码器和预测器
其中
编码器用来接收原始的视频并且输出嵌入信息
这些嵌入信息能够捕捉世界状态的语义信息
而预测器，用来接收视频嵌入信息
以及关于预测内容的附加上下文
并且输出预测后的嵌入信息
在训练过程中
Meta 还使用了基于视频的自监督学习
来训练 V-JEPA 2
这样就无需额外的人工标注
即可在视频上进行训练
V-JEPA 2 包含了两个阶段
分别是无动作预训练阶段
以及后续的动作条件训练阶段
在第一阶段，也就是预训练阶段中
Meta 使用了超过 100 万小时的视频和 100 万张图像
这些丰富的视觉数据
有助于模型深入了解世界的运作方式
包括人与物体的交互方式、物体在物理世界中的移动方式
以及物体与其他物体的互动方式
仅仅经过预训练
Meta 就发现模型已经展现出了与理解和预测相关的关键能力
例如
通过在冻结编码器和预测器特征的基础上
训练注意力读出（read-out）模型
V-JEPA 2 在 Epic-Kitchens-100 动作预测任务中
创造了新的最高纪录
这个任务可以根据以自我为中心的视频
来预测未来 1 秒钟将执行的动作
最后
Meta将 V-JEPA 2 与语言模型相结合
从而在视频问答基准上也实现了最先进的性能
在第一阶段之后
模型已经能够预测世界状态的可能演变了
然而
这些预测还并没有直接考虑Agent将采取的具体行动
因此，在训练的第二阶段
Meta 专注于利用机器人数据
包括视觉观察和机器人正在执行的控制动作
来提升模型的规划能力
通过向预测器提供动作信息
Meta 将这些数据整合到了JEPA 的训练流程中
在使用这些额外数据进行训练后
预测器学会了在进行预测时考虑具体的动作
然后就可以用来进行控制
第二阶段的训练不需要大量的机器人数据
仅仅使用了 62 小时的机器人数据进行训练
就能够构建出一个可以用来规划和控制的模型
Meta 还展示了如何用 V-JEPA 2 来进行零样本的机器人规划
而且这些新环境中涉及的物体
在训练阶段从来没有见过
与其他机器人基础模型
通常需要部分训练数据来自于模型部署的具体机器人实例和环境不同
Meta 使用了开源的 DROID 数据集对模型进行了训练
然后直接将它部署到 Meta 实验室的机器人上
从而证明了 V-JEPA 2 的预测器可以用在基础任务上
比如够到物体、拿起物体
并将它放到新的位置上
对于短期任务
比如拿起或者放置物体
Meta 会以图像的形式指定目标
他们使用 V-JEPA 2 的编码器
来获取当前状态和目标状态的嵌入向量
再从它观察到的当前状态出发
机器人通过预测器来想象采取一组候选动作的后果
然后根据这些动作与期望目标的接近程度
对候选动作进行评分
在每个时间步
机器人会通过模型预测来控制重新规划
并且执行评分最高的下一个动作
从而逐渐接近目标
对于长期任务来说
比如拿起物体并将它放置到正确的位置
他们指定了一系列的视觉子目标
机器人会按照顺序尝试实现这些子目标
类似于人类观察到的视觉模仿学习
通过这些视觉子目标
V-JEPA 2 在新环境和未知环境中
放置新物体的成功率达到了 65% 到 80%。
除了V-JEPA 2模型以外
这次Meta 还发布了三个新的基准测试
用来评估现有模型从视频中理解和推理物理世界的能力
测试结果表明
人类在这三个基准测试中的表现都非常出色
准确率可以做到在 85% 到 95% 之间
但是包括 V-JEPA 2 在内的顶级模型
与人类的表现之间仍然存在着显著差距
这表明模型还需要在这些方向上进一步改进和提升
第一个基准测试IntPhys 2
专门用来衡量模型区分物理合理场景和不合理场景的能力
基于早期的 IntPhys 基准测试进行了构建和扩展
设计 IntPhys 2 的方式
类似于发展认知科学家评估年幼人类
什么时候获得直觉物理能力的方法一样
即通过「违背预期」的范式
他们通过游戏引擎生成视频对的方式
来实现这一点
其中两个视频在某个时间点之前完全相同
然后其中一个视频中发生了违反物理规律的事件
模型必须识别出哪个视频中发生了违反物理规律的事件
尽管人类在各种场景和条件下
几乎都能够完美完成这项任务
但是当前的视频模型表现
仅能接近随机水平
第二个基准测试，MVPBench
它通过多项选择题来衡量视频语言模型的物理理解能力
与其他的视频问答基准测试不同
MVPBench 的目的是减少视频语言模型中常见的捷径解决方案
比如依赖于表面的视觉
或者文本线索和偏见
MVPBench 中的每个示例都有一个最小的变化对
比如一个视觉上相似的视频
以及相同、但是答案相反的问题
为了正确回答其中的一个问题
模型还必须正确回答它所对应的最小变化对
第三个基准测试是CausalVQA
它的目的是关注模型对物理世界视频中因果关系的理解
包括反事实
比如如果怎样 会发生什么、预期
比如接下来可能会发生什么
以及计划
比如为了实现目标应该采取什么样的行动
Meta 发现
虽然多模态模型越来越能够回答视频中发生了什么样的问题
但是它们仍然难以回答本可以发生什么
和接下来可能会发生什么的问题
这表明在预测物理世界在给定动作和事件空间的情况下
可能会如何演变的方面
模型的表现与人类存在着巨大差距
在Meta的官方博客中
也指出了Meta 下一步会做什么
首先Meta 计划在多个领域进一步的探索世界模型
目前
V-JEPA 2 能够在单一时间尺度上进行学习和预测
然而
许多任务是需要跨多个时间尺度进行规划的
比如将一个高级任务分解成更小的步骤
Meta 希望专注在训练
能够跨多个时间和空间尺度进行学习、推理和规划的分层 JEPA 模型
另一个重要方向是多模态 JEPA 模型
这些模型能够利用多种感官进行预测
包括视觉、听觉和触觉
好了
以上就是这次Meta V-JEPA 2模型发布的主要内容了
相关的资料我会放到视频简介中
希望能对大家有所帮助
感谢观看本期视频，我们下期再见
