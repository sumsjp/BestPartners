大家好，这里是最佳拍档，我是大飞
上周DeepSeek是咣咣咣的一顿开源
发布了多个相当硬核的开源项目
力压Claude 3.7 Sonnet 和GPT-4.5的热度
并且涵盖了AI的计算、通信和存储等多个领域
甚至在周六还亲自揭秘了DeepSeek V3和R1背后的秘密
到底DeepSeek的模型有多赚钱
今天大飞就来给大家回顾一下整个开源周的内容
DeepSeek开源的首个重磅项目是FlashMLA
简单来说
FlashMLA是一个专门针对英伟达Hopper GPU进行优化的、高效的MLA
也就是多头潜在注意力的解码内核
支持可变长度的序列处理
目前可支持 BF16和块大小64的分页 KV 缓存
FlashMLA的灵感主要来自于FlashAttention 2和3
以及cutlass项目
FlashAttention是一种高效的注意力计算方法
专门针对Transformer模型的自注意力机制进行了优化
其核心目标是减少显存占用并且加速计算
而cutlass则是一个优化工具
主要用来提高计算的效率
通过借鉴这些项目的优势
DeepSeek开发出了FlashMLA
在传统的语言模型中
多头注意力MHA是一个被广泛应用的技术
它能让计算机更好地理解语言
就如同人用眼睛同时关注多个地方一样
但是MHA存在着一个明显的缺点
那就是需要大量的内存来存储信息
而多头潜在注意力MLA
则对传统的MHA进行了升级
通过低秩分解的方法
不仅可以减少内存的占用
还能提高处理速度
而DeepSeek这次开源的FlashMLA
正是基于MLA技术进行的实现和优化
它通过优化一些复杂的计算过程
对MLA的解码和分页KV缓存进行了优化
从而提升了大语言模型的推理效率
尤其是在H800 GPU上
FlashMLA能够实现 3000 GB/s 的内存带宽和580 TFLOPS 的计算性能
同时
FlashMLA还将时间复杂度和内存复杂度
从MHA的O(n²)降低到了O(nk)，
这使得它非常适合处理像文档分析这样的长序列任务
以及像聊天机器人这样的实时推理场景
虽然目前FlashMLA的生态还没有那么完善
但是它的意义在于
让现有的很多模型只需要简单的改造
就可以大幅提高GPU效率
提升模型推理能力并且降低成本
另外研究人员和开发者还可以基于FlashMLA的开源代码进行进一步的优化
值得一提的是
有网友发现FlashMLA的项目中有调用CUDA底层的PTX代码
通过内联PTX
开发者能够更为精细地控制GPU的执行流程
实现更高效的计算性能
这也印证了DeepSeek团队对CUDA有着极为深厚的了解
并且我们已经多次看到这种近乎庖丁解牛似的精湛刀法
我们接着来说开源周的第二天
DeepSeek推出了DeepEP
这是一个专门为混合专家系统（MoE）和专家并行（EP）定制的通信库
它的设计灵感来自DeepSeek V3论文里的群组限制门控算法
我们都知道
如今AI模型的参数规模在不断扩大
从数十亿发展到了几万亿
因此高效的通信已经成为了一个关键的瓶颈
像MoE这类模型中
由于汇聚了不同的专家模型
负责处理不同的任务
难免会存在资源分配不均衡的问题
导致效率低下
而群组限制门控算法就像是为这些专家制定高效的分工计划
让每个专家各司其职
避免互相拖后腿
同时及时地调整任务分配
而基于这个算法开发的DeepEP
能够根据任务量来动态地调节GPU使用的SM计算单元数量
比如当任务多的时候
就让GPU里更多计算单元一起工作
而任务少的时候则自动减少功耗
既省电又不影响效率
从DeepSeek官宣的内容来看
DeepEP具有许多亮点
它拥有高效优化的全体通信通道
为训练和推理预填充设计了高吞吐的核心
为推理解码设计了低延迟核心
原生支持FP8智能压缩传输
并且能够灵活调控GPU的资源
实现边计算边传输
针对于MoE不同的分派和组合通信方式
DeepEP还专门优化了MoE模型中数据路由和输出的整合过程
此外，DeepEP还支持了NVLink和RDMA
这对大规模MoE模型来说是非常重要的
因为同一个服务器内的GPU可以使用NVLink
传输速度可达150GB/s，几乎零等待；
而多个服务器之间可以使用RDMA网络
不仅大幅提升速度，还能无缝转发
避免数据堆积或者丢失
这样一来
DeepEP可以极大压缩GPU等待数据和同步的时间
真正做到榨干每一块GPU的能力
同时显著提升MoE模型的性能和效率
非常适合用在大规模AI模型的训练和推理中
像是对翻译、摘要生成、问答系统、代码生成以及推荐系统等领域
都有重要的应用价值
开源周的第三天
DeepSeek带来了DeepGEMM
这是一个专门为干净、高效的FP8通用矩阵乘法 (GEMM) 而设计的库
具有细粒度的缩放功能
支持普通和MoE 分组GEMM
这个库是用CUDA编写的
安装过程中无需编译
而是使用JIT 模块在运行时编译所有内核
DeepGEMM有几个优势，首先
它通过FP8和两级累积
降低了计算和内存开销
从而实现了更高的效率
FP8可以将原本需要 32 位或 16 位存储的数字
精简成 8 位存储
减少内存占用，加快计算速度
但是FP8也会造成精度上的损失
带来误差
为了解决这个问题
DeepGEMM采用了巧妙的两步法
那就是先用FP8进行大批量乘法运算
然后再定期转成更精确的 32 位数字累加
进行高精度汇总
这样通过两级累积来防止误差积累
其次，在部署方面
DeepGEMM的JIT编译具有很强的适应性
减少了预编译的负担
与传统的静态编译不同
JIT编译是在程序运行的时候才把代码变成电脑能执行的指令
因此可以根据电脑的情况现场调整代码
量身定制出最适合的指令
只编译当下要用的部分
从而避免浪费时间和空间
让程序运行得更为顺畅
而且，JIT编译可以在运行的时候
根据Hopper显卡的情况生成最优的代码
充分发挥Hopper张量核心的计算效率
实现1350+的FP8 FLOPS
并且能根据普通GEMM和MoE分组GEMM不同的任务特点
临时调整代码
直接调动张量核心的FP8计算和Transformer引擎功能
提高速度
最后，DeepGEMM的设计也非常简洁
核心代码只有300行左右
避免了复杂的依赖
便于开发者们进行学习和优化
从H800的性能测试数据来看
DeepGEMM在各种矩阵形状下的性能
可以与经过专业优化的库相媲美
甚至在某些情况下能表现得更优
不过
它在某些特定矩阵形状下的表现还有提升空间
DeepSeek也欢迎开发者提交优化相关的PR
开源周的第四天
DeepSeek开源了两项堪称“AI训练加速神器”的技术
其中DualPipe技术由梁文锋亲自参与开发
之所以说是神器
是因为大模型训练通常需要耗费数百万美元和数个月的时间
而这两项技术能够有效减少训练的成本和时间
我们先来说说DualPipe
传统的AI训练过程就像一条单行道
数据必须先完成前向计算
然后等待完全结束后
才能开始反向传播
这大大降低了训练效率
而DualPipe就像是一条双向的高铁线
让前向计算和反向传播可以同时进行
并且在数据传输的同时就开始下一步的计算
更具体地说
DualPipe将每个时间步划分为三个子周期
分别是前向计算周期、反向计算周期和权重更新周期
通过精确的时间切片管理
算法确保前向计算的通信阶段与反向计算的计算阶段完全重叠
这有点类似麦当劳在国外的双车道得来速
一边有车辆取餐离开
另一边同时有新车进入点单
厨师看到车牌就开始做餐
等顾客开到窗口
餐也刚好准备好，整个过程无缝衔接
此外，DualPipe 的流水线并行
还可以最大限度地减少流水线气泡
也就是各个计算单元等待数据的空闲时间
而这个问题在传统的1F1B和Zero Bubble中都存在
如果你看过我们之前对DeepSeek V3的介绍视频
应该会对DualPipe有一些印象
它在DeepSeek V3的训练中发挥了关键作用
根据技术报告显示
DeepSeek V3的预训练仅需278.8万H800 GPU小时
成本约为557.6万美元
远低于同等规模的模型
也使得6720亿参数的DeepSeek-V3GPU利用率达到89%，
但是训练资源却与OpenAI的GPT-4o或Anthropic的Claude 3.5 Sonnet相比
少了很多
而这些都要归功于DualPipe的高效性
我们再看看另一个神器
专家并行负载均衡EPLB（Expert Parallel Load Balancing）
我们前面也提过，在大模型的训练中
尤其是MoE架构中
资源分配不均衡是一个很大的问题
比如虽然有100个专家
但是80%的任务都集中在20个专家那里
导致这20个专家忙得不可开交
而其他80个专家却无所事事
严重浪费计算资源
而EPLB就是用来优化大型语言模型在专家并行架构中的训练效率的
它通过动态调整专家模型的分配
来平衡GPU之间的工作负载
同时减少跨节点的通信开销
具体来说
EPLB 算法包含了分层和全局负载均衡策略
用于不同的情况
分层负载均衡策略在服务器节点数可以整除专家组数时使用
可以充分利用组限制专家路由
而在其他情况下
使用全局负载均衡策略
这个策略会复制全局专家
来实现全局范围内的负载均衡
而不考虑专家组
为了实现更高效的负载均衡
EPLB还引入了一系列优化技术
比如采用分形缓存技术
将专家元数据存储开销控制在极低的水平
以及采用区域感知调度算法
最大限度地减少跨节点通信等等
通过EPLB的优化
GPU的资源利用率能够提升20%以上
除了这两个神器以外
DeepSeek这天还公开分享了使用 PyTorch Profiler 捕获的、自身的训练和推理框架分析数据
从而帮助开发者们更好地了解通信计算的重叠策略和低级实现细节
可以下载后在Chrome 浏览器进行可视化查看
开源周的第五天
DeepSeek推出了3FS文件系统
如果我们说DeepEP是要榨干GPU
那么3FS就是要榨干SSD
3FS全称是萤火超算文件系统（Fire - Flyer File System）
早在2022年幻方官方就发布了技术概览
但是当时它只是一个内部专用的技术
深度依赖于幻方自研的超算集群硬件
需要配合特定型号的交换机和网卡
并且采用的是传统的目录树结构
百万级文件的遍历耗时很长
而如今开源的3FS已经有了质的飞跃
首先
它采用了分布式架构
将存储节点与计算节点进行了物理上的分离
让数据流动不再受物理位置的限制
其次
他通过链式复制与分配查询（CRAQ）技术
确保了数据的一致性
第三
3FS还通过FFRecord格式管理数据库
将数百万的小文件合并为逻辑大文件
然后通过索引文件来记录样本偏移量
大幅提升文件的查询速度
这种方式
如果有做过大数据、云存储、或者分布式文件系统的观众应该会非常熟悉
最后
3FS利用了成本只有DRAM十分之一的平价固态硬盘SSD作为缓存
但是速度却能达到前者的90%，
再通过RDMA网络
使得KV缓存的查询速度可以高达40GB/s
大大提升了模型推理时的数据读取速度
可以说
3FS实现了存储介质、网络协议、分布式算法的深度协同
重构了AI训练的底层逻辑
在实际应用中
在一个180 节点的集群中
3FS的聚合读取吞吐量可以达到6.6 TB/s
而在加载ImageNet数据集时
3FS将耗时从15秒压缩到了0.29秒
另外
DeepSeek还开源了一个基于3FS的、轻量级的数据处理框架Smallpond
它基于DuckDB实现了高性能的数据处理
具有良好的可扩展性
能够处理PB级别的数据集
同时无需持续运行服务
并且提供了高级和低级API，方便使用
在GraySort基准测试中
在一个由50个计算节点和25个运行3FS的存储节点所组成的集群上
smallpond可以在短短30分钟14秒内
就完成了对110.5TB数据的排序
平均吞吐量达到了3.66 TB/min
而就在周六
本来大家都以为开源周结束的时候
DeepSeek又扔出了一个炸弹
全面揭秘了V3和R1系统背后的秘密
相当于前五天内容的融合和实践
更是首次公布了模型的成本利润率为545%。
按照DeepSeek的说法，在硬件配置上
他们的模型推理系统使用的都是H800 GPU
保持了与训练一致的精度策略
也就是矩阵乘法和分发传输使用 FP8 格式
而核心 MLA 计算和组合传输使用 BF16格式
在资源调度方面
他们采用了动态资源调度策略
根据白天和夜间的负载差异动态
来调整节点的数量
最大化资源的利用率
而为了应对系统的提高吞吐量和降低延迟这两大挑战
DeepSeek V3和R1都采用了大规模跨节点专家并行技术
也就是EP
在MoE架构中，每层有256个专家
但是每个token只激活其中的8个
这种高度稀疏的结构需要通过专家并行来提高效率
在预填充和解码阶段
DeepSeek针对不同场景采用了不同的并行策略
比如预填充阶段
每个部署单元跨越 4 个节点
拥有 32 个冗余路由专家
每张 GPU 管理 9 个路由专家和 1 个共享专家
而在解码阶段
则由每个部署单元扩展到 18 个节点
依然是 32 个冗余路由专家
每张 GPU 管理 2 个路由专家和 1 个共享专家
同时
为了解决大规模跨节点 EP 引入的巨大的通信开销
DeepSeek还采用了双批次重叠处理负载均衡策略
简单来说
就是把一个大的请求批次分成两个微批次
交替执行
这样
一个微批次的通信开销就可以被巧妙地隐藏在另一个微批次的计算过程中
从而在硬件受限的情况下
依然实现了出色的推理性能
最终整个推理系统的成本数据显示
24小时内平均会使用226.75个节点
每个节点8个H800
按照每个GPU租金每小时2美元来计算
成本大约为每天87072美元
而系统的吞吐能力表现为
每台H800的预填充吞吐量大约为73.7k tokens/s
解码吞吐量大约为14.8k tokens/s
按DeepSeek R1定价计算
理论上的日收入为562027美元
折合人民币大约409万
成本利润率为545%。
当然DeepSeek官方也说了
实际收入没有这么多
因为Web版目前是免费的、API夜间还打折
以及V3比R1还便宜等等原因
但是这个收益率已经足够令人震撼
另外大家不要误会的一点是
这个只是理论上推导的AI推理系统的成本收益
并不是实际的
也不是DeepSeek公司的
不要误解成DeepSeek一天能赚好几百万似的
好了
以上就是DeepSeek上周开源的主要内容了
可以说，就像个负责任的老师一样
一笔一划地在教大家这个AI系统是怎么优化出来的
还把所有的核心算法和工具都开源了
就差没动手帮你干了
从过年到现在
DeepSeek给人的感觉似乎有点不真实
不仅一扫国内各个AI公司以前的浮夸姿态
坚决不抛头露面
只是一味的闷头干活和开源成果
连融资都不拿
还有就是这种开源的方式
从核心算法到工程最佳实践
最后连成本都算给你
感觉就差把底裤给大家看了
也让大家对于训练模型到底要花多少钱这个事
有了新的概念
作为技术出身的我来说
还是希望这样纯粹的事能多一些
好了，感谢大家收看本期视频
我们下期再见
