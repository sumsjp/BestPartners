大家好，这里是最佳拍档，我是大飞
最近
斯坦福大学2025年春季的CS231N课程
开始了首次授课
主讲人是全球AI领域的顶尖学者、计算机视觉的奠基人之一
李飞飞（Fei-Fei Li）
这堂课程不仅串联了5.4亿年的视觉进化史
更是回答了深度学习革命中的一个核心逻辑
为什么说“看懂世界”，
是AI真正走向智能的第一步？
在这堂课上
李飞飞从寒武纪的三叶虫讲起
聊到达芬奇的暗箱实验
再到2012年的AlexNet
最后到生成式AI如何让机器“学会创造”，
整个过程既有科学史的温度
也有技术细节的深度
相信会让你对AI视觉领域有一个全新的认知
今天大飞就来给大家分享一下
李飞飞教授在课程一开始就明确了一个核心观点
AI早已经不是计算机科学的“独角戏”，
而是一个高度跨学科的领域了
我们常说的计算机视觉
看似是AI的一个分支
实则和自然语言处理（NLP）、语音识别、机器人技术深度绑定；
更底层的
它还依赖数学、神经科学、心理学、物理学甚至生物学
而在这张跨学科的“拼图”里
视觉的地位尤为特殊
它不只是智能的“一部分”，
更是智能的“基石”。
李飞飞说道，解开视觉智能的奥秘
就是系统性地解开整个智能的奥秘
为什么会这么说？
我们还得从视觉的起源讲起
这就要回到5.4亿年前的寒武纪大爆发
你可能在生物课上学过
寒武纪大爆发是地球生命史上的一个“奇迹”，
在大约1000万年的时间里
地球上突然出现了大量复杂的动物物种
而其中的一个关键驱动力
就是“眼睛”的出现
它指的不是我们现在看到的复杂晶状体眼睛
而是最原始的“感光细胞”。
比如当时的三叶虫
就演化出了能够捕捉光线的简单结构
在这之前
生命体只能被动地进行新陈代谢
随着环境漂流，但是有了感光细胞
它们第一次能够“主动感知”环境
哪里有光线？
哪里可能有食物？
哪里可能有天敌？
这种“主动适应”的能力
直接推动了神经系统的进化
也为后续智能的发展埋下了种子
李飞飞特别强调
这长达5.4亿年的视觉进化史
本质上就是一部智能的进化史
而人类作为“视觉动物”，
对视觉的依赖更是到了极致
我们大脑皮层中超过一半的细胞
都在参与视觉处理
假如你现在看着屏幕上的文字
大脑正在实时处理光线信号、识别文字形状、理解语义
整个过程快到你根本意识不到
但背后是极其精密的视觉系统在运作
也正是因为人类视觉如此强大
让科学家们意识到
如果能让机器拥有类似的视觉能力
就能向真正的智能迈一大步
既然视觉如此重要
人类自然早早就开始探索“让机器看见”的方法
李飞飞在课上带我们回顾了这段历史
起点比我们想象的更早
早在古希腊和古代中国
思想家们就发现了“针孔成像”的原理
通过一个小孔
能将外界的景物投射成倒立的影像
而到了文艺复兴时期
达芬奇（Leonardo da Vinci）对这个原理做了更深入的研究
他设计的“暗箱（camera obscura）”，
可以说是现代相机的雏形
暗箱的核心是一个密闭的箱子
一侧有小孔
另一侧能承接外界投射的影像
当时的画家会用它来辅助绘画
但这里有个关键的问题
无论是暗箱、还是后来的相机
甚至是我们的人眼
本质上都只是“信息采集工具”。
它们能捕捉光线形成影像
但是没法“理解”影像的内容
就像相机能拍下一只猫
但它不知道这是“猫”，
不知道猫的习性
更不知道这只猫在做什么
真正的“看见”，核心其实是“理解”，
而这正是计算机视觉要解决的核心难题
那么
人类是如何从“造工具”走向“懂视觉”的呢？
这里必须提到神经科学的突破
20世纪50年代
两位科学家休贝尔（David Hubel）和威塞尔（Torsten Wiesel）做了一项改变整个领域的实验
他们用电极监测麻醉状态下猫的初级视觉皮层
发现了两个关键规律
第一
视觉皮层中的神经元都有自己的“感受野（receptive field）”。
简单来说
每个神经元只对特定空间区域里的光线信号有反应
比如有的神经元只“关注”画面左上角的一小块区域
而且只对特定方向的边缘
比如水平边缘、垂直边缘有反应
这意味着
大脑处理视觉信息是从“拆解简单特征”开始的
第二，视觉通路是“分层”的
信号会从处理简单特征的神经元
传递给更高级的神经元
比如，它会先识别边缘
再组合成角点，然后是物体的轮廓
最后形成完整的物体识别
这种“从简单到复杂”的分层处理逻辑
后来直接启发了神经网络算法的设计
1981年
休贝尔和威塞尔因为这项研究获得了诺贝尔生理学或医学奖
他们的发现
相当于为计算机视觉搭建了“模仿人类视觉”的蓝图
有了神经科学的理论基础
计算机视觉作为一门独立学科开始萌芽
李飞飞教授在课上提到了两个关键的“起点”。
第一个是1963年
拉里·罗伯茨（Larry Roberts）完成了全球第一篇专注于“形状识别”的博士论文
他在论文里试图解决一个核心问题
如何让机器像人类一样
凭直觉理解物体的表面、边角和特征
比如看到一个立方体
机器能识别出它是“三维的立方体”，
而不只是平面上的几个正方形
这篇论文标志着计算机视觉正式成为一个研究领域
第二个是1966年
麻省理工学院（MIT）的一位教授
组织了一个夏季项目，目标很“大胆”，
那就是让几位优秀的本科生在一个夏天里
解决计算机视觉的问题
现在回头看，这个目标显然太乐观了
计算机视觉的复杂度远超当时的想象
那个夏天自然没有实现目标
但是这个项目的意义在于
它让学界第一次意识到
让机器看见比想象中难得多
需要长期的系统性研究
到了20世纪70年代
另一位关键人物大卫·马尔（David Marr）写出了一本开创性的著作
为视觉处理搭建了第一个“系统性框架”。
他提出，视觉处理应该分三个阶段
首先是“原始草图（primal sketch）”，
也就是提取图像中的边缘、纹理等基础特征；
然后是“2.5D草图（two and a half D sketch）”，
把前景和背景分开
比如区分出“球在地面上”，
而不是两者混在一起；
最后是“3D表示”，
构建出物体完整的三维结构
但是马尔的框架也暴露了一个核心难题
从2D图像恢复3D信息
本质上是一个“不适定问题（ill-posed problem）”。
简单说
三维世界的景物投射到二维平面上时
会丢失大量的信息，比如一个圆形
从正面看是圆，从侧面看是椭圆
机器怎么知道这个椭圆其实是“圆形的投影”呢？
自然界其实早就解决了这个问题
大多数动物都有两只或多只眼睛
通过“三角测量”的方法来获取3D信息
人类其实也有这种能力
但是精度有限
我们能大概判断物体的远近
却没法像激光雷达那样精确的测量距离
除了技术难题
计算机视觉还面临一个“本质区别”，
那就是视觉和语言不同
李飞飞在课上特别强调了这一点
语言是人类纯粹创造出来的东西
并非是自然之物
你没法指着某个东西说“这就是语言”，
因为语言是大脑生成的、一维的、有顺序的符号；
但是视觉根植于物理世界
遵循物理定律
比如光的反射、物体的运动规律等等
这种区别直接影响了AI算法的设计
正如语言模型可以通过学习文字的序列规律来建模
但是视觉模型必须理解物理世界的规律
难度要大得多
到了20世纪80年代末到90年代
AI领域进入了“寒冬”。
由于之前的研究承诺没能兑现
投资和热情大幅降温
计算机视觉也受到波及
但是李飞飞特别指出，寒冬之下
暗流仍在涌动
计算机视觉、NLP、机器人学的研究没有停止
而且认知科学和神经科学的发展
也为后续的突破埋下了伏笔
比如
心理学家欧文·比德曼（Irving Biederman）做过一个有趣的实验
给受试者看两张自行车的图片
一张背景正常，一张背景被打乱
结果发现受试者识别“正常背景的自行车”更快
这说明
人类的视觉不是孤立识别物体
而是会结合“全局场景”来理解局部
这个发现后来影响了计算机视觉的“场景理解”研究
还有认知神经科学家西蒙·索普（Simon Thorpe）的实验
他让受试者观看一个快速播放的视频
并要求识别其中是否有人
结果发现
人类大脑能在看到图像后的150毫秒内
就能完成初步的分类
这个速度远超当时的计算机算法
也让科学家意识到
人类视觉的“高效”背后
一定有着更为精妙的处理机制
进入21世纪
计算机视觉迎来了两个关键转折点
一是数据的爆发
二是深度学习的成熟
而这两个转折点
都和李飞飞的工作密切相关
首先是数据问题
早期的计算机视觉研究
最大的瓶颈之一就是“数据不够”。
当时的数据集通常只有几千张图片
而机器学习算法需要海量的数据才能“学会泛化”，
简单来说就是，数据太少
模型只能靠死记硬背训练数据
遇到没见过的图像就会出错
李飞飞和她的学生们意识到了这个问题
于是启动了ImageNet项目
他们从互联网上的十亿张图片中筛选、清理
最终构建了一个包含1500万张图片的数据库
覆盖22000个物体类别
这个类别数量
和人类在童年时期学习识别的物体数量大致相当
为了推动研究，他们还开源了数据集
并且创办了ImageNet大规模视觉识别挑战赛
也就是ILSVRC
这个比赛会从ImageNet中选出1000个类别、100万张图片作为竞赛数据
邀请全球研究者比拼算法精度
2011年
第一届ILSVRC的最佳算法错误率接近30%，
这个成绩远不如人类不到3%的错误率）
但是到了2012年，一切都变了
杰弗里·辛顿（Geoffrey Hinton）和他的学生带着一个叫“AlexNet”的卷积神经网络参赛
直接把错误率几乎降到一半
这个结果震惊了整个领域
也正式拉开了深度学习革命的序幕
为什么AlexNet能取得这么大的突破呢？
李飞飞在课上拆解了两个核心原因
第一是“反向传播（backpropagation）”算法的成熟
早在1986年
大卫·鲁梅尔哈特（David Rumelhart）、杰弗里·辛顿等人就提出了反向传播
它的核心逻辑是
先让模型根据输入数据输出一个结果
然后计算这个结果和“正确答案”的误差
再通过微积分的链式法则
把误差“反向传递”到模型的每一个参数
从而调整参数让误差变小
这个算法解决了“神经网络如何高效学习”的问题
让模型不再需要手工调整参数
第二就是“海量数据”的支撑
AlexNet的架构其实和32年前的神经认知机没有本质区别
但是它用了ImageNet的100万张图片来训练
数据量的爆炸
让这个“高容量模型”终于能够学会泛化
李飞飞强调道
当时很多研究者只关注算法架构
却忽视了数据的重要性
而深度学习的成功证明
数据才是驱动高容量模型发展的核心
在讲完技术突破之后
李飞飞把话题拉回到了人的身上
AI不只是一项技术
更是关乎人类社会的事业
在AI伦理方面
李飞飞指出，AI算法
尤其是大模型
会“继承”人类社会的偏见
因为训练数据来自人类活动
而人类社会本身就存在偏见
最典型的例子就是人脸识别算法
有些模型在识别白人男性时的准确率很高
但是在识别黑人女性时错误率会大幅上升
这种偏见如果应用在贷款审批、求职筛选等领域
会对特定群体造成不公平的影响
因此，“以人为本的AI”不是一句口号
而是必须解决的现实问题
其次是AI的“向善应用”。
李飞飞自己的研究就聚焦在医疗健康领域
比如用计算机视觉来分析医学影像
帮助医生更早发现疾病
以及开发针对老年人和病患的护理机器人
通过视觉识别他们的动作和状态
提供及时的帮助
这些应用让我们看到
AI技术可以真正改善人类的生活
同时
李飞飞教授也坦诚地指出了AI的“边界”，
尽管计算机视觉取得了巨大突破
但人类视觉的“细腻与情感维度”，
仍然是机器难以企及的
比如
我们能从孩子的眼神中看到“好奇”，
从老人的笑容中感受到“温暖”，
能理解一张幽默图片的“笑点”，
这些包含情感、文化、经验的感知
对机器来说依然是巨大的挑战
在李飞飞教授讲完核心内容后
课程的联合讲师埃桑·阿德利（Ehsan Adeli）教授详细介绍了CS231N的课程结构
这门课程主要分为四大主题，第一
深度学习基础
从最基础的“线性分类器”讲起
比如如何用一条直线，或者说超平面
来区分“猫”和“狗”的图像特征；
然后讲解“过拟合”和“欠拟合”的问题
以及如何用“正则化”等技术来解决；
最后深入神经网络的原理
包括卷积层、池化层、全连接层等核心组件
以及如何训练和调试模型
第二，感知与理解视觉世界
这部分聚焦计算机视觉的核心任务
比如目标检测、语义分割、视频分析等
讲解每种任务的解决思路和经典模型
比如用于目标检测的YOLO
用于语义分割的U-Net等等
同时会介绍“注意力机制”等技术
帮助理解模型“如何做出决策”，
比如模型在识别猫的时候
为什么会“关注”猫的头部而不是背景
第三，大规模分布式训练
这部分是2025年课程的新增内容
包括如何训练“大视觉模型”，
比如数据并行、模型并行技术
以及如何解决大模型训练时的算力和内存瓶颈
第四是生成式和交互式视觉智能
从自监督学习到生成模型
再到视觉语言模型和三维视觉的工作原理
课程的核心学习目标有三个
一是学会将实际的计算机视觉问题“形式化”为明确的任务
比如把“自动驾驶识别障碍物”转化为“目标检测任务”；
二是掌握开发和训练视觉模型的技能
包括数据处理、模型构建、训练调优等等；
三是了解领域的前沿动态和未来方向
比如生成式AI、3D视觉的最新进展
阿德利教授还预告了下一堂课的内容
图像分类与线性分类器
这将是学习计算机视觉的“第一步”，
也是最基础的一步
好了，到这里
这堂斯坦福公开课的核心内容就梳理得差不多了
回顾整个过程
从5.4亿年前的感光细胞
到今天能生成图像的AI模型
我们看到的不仅是技术的进步
更是人类对“智能本质”的不断探索
等后续课程出来之后
希望能为大家做进一步的解读
共同学习
感谢收看本期视频，我们下期再见
