大家好，这里是最佳拍档，我是大飞
在当今人工智能浪潮席卷全球的时代
每一次模型训练效率的提升、每一个复杂算法的快速运行
背后都离不开硬件的强大支撑
而在这场硬件技术的角逐中
NVIDIA 的 Tensor Core 无疑是一颗耀眼的明星
从 2017 年的首次亮相到今天
它已经经历了多次的迭代升级
深刻地改变了深度学习计算的格局
我们昨天刚刚做完一期SemiAnalysis的节目
没想到它紧接着又发表了一篇文章
深入剖析了 NVIDIA Tensor Core 从 Volta 到 Blackwell 的演进历程
今天大飞就再来给大家分享一下
看看这项技术究竟是如何一步一步突破计算的瓶颈
重塑人工智能计算生态的
在正式探讨 NVIDIA Tensor Core 的演进之前
我们有必要先了解一些并行计算的基础原理
因为这些原理可以说是打开 Tensor Core 技术奥秘的钥匙
在并行计算的世界里
有一个非常重要的定律
叫做Amdahl 定律
这个定律由美国计算机科学家吉恩・阿姆达尔（Gene Amdahl）提出
它揭示了并行计算加速的本质
简单来说
并行计算的加速效果并不是无限制的
它会受到串行部分的制约
用公式来表示就是这样的
其中大S代表并行工作的执行时间
小p代表并行的加速比
这个公式意味着
即使我们不断增加并行的计算资源
如果程序中存在着大量无法并行处理的串行任务
那么整体的加速比也只能趋近于1−S
因为串行部分的执行时间是无法通过并行化减少的
除了Amdahl 定律以外，在并行计算中
还有两个重要的概念
分别是强缩放Strong Scaling和弱缩放Weak Scaling
强缩放是指在固定问题规模的情况下
通过增加计算资源来缩短执行的时间
它的加速比是由阿姆达尔定律来量化的
比如说
我们要计算一个固定大小的矩阵乘法
那么使用更多的处理器核心来并行计算
理论上可以更快地得到结果
但是正如 Amdahl 定律所描述的
这种加速会受到串行部分的限制
而弱缩放则是按照比例同时增加问题的规模和计算的资源
目标是保持执行时间不变
这就好比是我们要处理的数据量翻倍了
同时也增加了一倍的计算资源
比如
用 4 倍的计算资源来处理 4 倍大小的图像
那就能让处理数据的时间和原来一样
这种方式在处理大数据的场景时非常有用
因为它可以随着数据规模的增长
来灵活的扩展计算能力
然而，在并行计算的发展过程中
一直存在着一个巨大的瓶颈
那就是数据移动
我们知道
计算单元的处理速度非常快
可以在亚纳秒级内完成一次计算操作
但是从动态随机存取存储器DRAM中
访问数据的延迟却高达纳秒级
两者之间的差距可以说是非常之大
这就形成了大家常说的 “内存墙”memory wall
，
大量的时间都消耗在了数据的读取和写入上
而不是用在真正的计算过程上
为了突破这个瓶颈
硬件架构师们一直在努力寻找新的方法
减少数据移动，提高计算效率
而 NVIDIA Tensor Core 的诞生
正是应对这一挑战的重要成果
2017 年
NVIDIA 推出了第一代的 Tensor Core
首次引入了 Volta 架构
当时
深度学习正处在快速发展的阶段
大量的矩阵运算需求让传统的计算架构显得力不从心
以深度学习中的矩阵乘法为例
在传统的计算方式中指令的开销非常大
比如半精度浮点乘加（HFMA）指令
虽然能耗仅为 1.5pJ（皮焦耳）
但是指令的开销却高达 20倍
达到30pJ
为了解决这个问题
NVIDIA 引入了半精度矩阵乘累加（HMMA）指令
这就是第一代 Tensor Core 的核心创新
在 Volta 架构中，每个流式多处理器
也就是Streaming Multiprocessor
简称 SM，都包含 8 个 Tensor Core
这些 Tensor Core 可以支持 4×4×4 的矩阵乘法运算
每个周期
每个 SM 可以提供 1024 FLOPS的计算能力
为了高效地执行矩阵运算
Volta 架构还采用了 warp-scoped MMA
也就是warp 范围的矩阵乘累加模式
在这种模式下
8 个线程会组成一个 Quad Pair
通过协作来执行 8×8×4 的矩阵运算
同时
Volta 架构还支持 FP16输入和 FP32累加的混合精度训练方式
这种方式既可以减少数据存储和传输的开销
又够能保证一定的计算精度
从而大大提升了深度学习训练的效率
Turing 架构于 Volta 之后推出
它的第二代Tensor Core
在 Volta 的基础上增加了 INT8 和 INT4 精度支持
进一步拓展了低精度计算能力
同时通过引入深度学习超采样（DLSS）技术
将深度学习应用到了游戏图形领域
标志着 NVIDIA 在 AI 与图形融合方面的探索
这个架构的张量核心支持新的 warp 级同步 MMA 操作
也为后续架构的并行计算模式奠定了基础
时间来到 2020 年
NVIDIA 推出了第三代 Tensor Core
基于 Ampere 架构
这一代 Tensor Core 在多个方面进行了重大改进
进一步提升了计算性能和效率
Ampere 架构的一个关键创新
是引入了异步数据复制（cp
async）的技术
在之前的架构中
数据在从全局内存加载到计算单元的时候
往往需要经过多个步骤
会占用大量的寄存器资源
导致寄存器的压力过大
而异步数据复制技术可以直接将数据从全局内存
加载到共享内存中
从而绕过了部分中间环节
大大减少了寄存器的使用
提高了数据加载的效率
在计算能力方面
Ampere 架构的每个 SM 包含 4 个 Tensor Core
虽然数量相比 Volta 架构有所减少
但是每个 Tensor Core 的性能得到了显著的提升
每个周期每个 SM 可以提供 2048 FLOPS 的计算能力
是 Volta 架构的两倍
此外
MMA 的运算也升级为 warp 级同步模式
也就是 32 个线程会协同工作
这相比 Volta 架构的 8 线程协作
能够处理16x8x16的矩阵运算
此外
Ampere 架构还支持了 BF16的数据格式
BF16 的数据动态范围与 FP32 相当
但是存储和计算开销却只有 FP32 的一半
这个特性使得 BF16 迅速成为了半精度计算的行业标准
也在保证计算精度的同时
大幅提升了计算速度和数据处理能力
2022 年
NVIDIA 推出了基于 Hopper 架构的第四代 Tensor Core
这一代架构在性能和功能上实现了又一次的飞跃
带来了更多创新的技术和特性
Hopper 架构引入了线程块集群（Thread Block Cluster）的概念
这是一种全新的线程层次结构
在传统的架构中
线程块（CTA）之间的协作相对有限
而线程块集群则允许CTA在图形处理集群（GPC）的内部进行协同调度
通过这种方式
多个CTA之间可以共享分布式的共享内存（DSMEM）
从而实现数据的高效共享和交换
进一步提升了并行计算的效率
为了更好地解决数据移动的瓶颈问题
Hopper 架构还引入了Tensor Memory Accelerator
也就是张量内存加速器，TMA
TMA 支持批量的异步数据传输和多播模式
可以在不占用 L2 缓存和HBM大量带宽的情况下
高效地传输数据
这意味着在处理大规模张量数据的时候
TMA 能够显著减少数据传输的延迟
提高计算单元的利用率
在矩阵运算方面
Hopper 架构还引入了 Warp 组级别的异步 MMA（wgmma）
在这种模式下，4 个 warp
也就是 128 个线程可以协作进行矩阵运算
支持更大的矩阵形状
比如 m64n256k16
而且
操作数可以直接从共享内存中读取
进一步减少了数据从全局内存到计算单元的传输开销
此外
Hopper 架构还引入了 8 位浮点格式
包括 E4M3 和 E5M2
虽然这些 8 位浮点格式在计算过程中
会使用固定点累加路径
但是为了保证计算精度
还需要 CUDA 核心的辅助
这种混合计算方式在一些对计算精度要求相对较低的场景中
能够大幅提升计算的效率
减少计算资源的消耗
进入 2025 年之后
NVIDIA 还推出了基于 Blackwell 架构的第五代 Tensor Core
这一代架构可以说是革命性的
它带来了一项全新的关键技术
那就是张量内存Tensor Memory
简称TMEM
在之前的架构中
尽管通过各种技术手段来优化数据移动
但是数据访问的效率仍然受到内存层次结构的限制
而 Blackwell 架构中的 Tensor Memory 为解决这个问题提供了一个全新的思路
这一回
每个 SM 配备了 256KB 的专用 Tensor Memory
它位于计算单元的附近
相比传统的内存
访问速度更快，功耗更低
在进行矩阵运算的时候
矩阵D可以直接常驻在 Tensor Memory 中
这就大大减少了数据在不同内存层次之间的传输开销
提高了数据访问的效率和计算单元的利用率
除了 Tensor Memory 之外
Blackwell 架构在其他方面也进行了很多的优化和改进
比如CTA Pair的机制允许两个 CTA 共享操作数
降低了内存的带宽需求
并且彻底摒弃了寄存器存储矩阵
操作数直接位于共享内存和张量内存中
单线程即可发起 MMA 操作（tcgen05
mma）
还支持 SM 间协作的 MMA.2SM 模式
从而将 M 矩阵的维度翻倍
同时还引入 MXFP8、MXFP6、MXFP4 及 NVFP4 等浮点格式
提供了更好的精度
此外
Blackwell 的Tensor Core还支持卷积运算与权重固定的模式
通过收集缓冲区的缓存矩阵 B 来实现数据重用
进一步提升了 Tensor Core 的性能和效率
可以预见
这一代架构将为人工智能计算带来新的突破
推动深度学习技术向更高的水平发展
在 NVIDIA Tensor Core的发展中
结构化稀疏性作为提升计算效率的技术
在不同的架构中也呈现出不同的应用特点与挑战
Ampere 架构推出了 2:
4 的结构化稀疏性
核心在于对权重矩阵进行修剪
使得每 4 个元素中的2 个为零
通过压缩非零元素并且利用元数据索引的记录位置
理论上可以将Tensor Core的吞吐量翻倍
同时将内存使用和带宽需求减半
然而实际应用中，Ampere 2:
4 的结构化稀疏性在 Hopper 架构上
未能达到预期的效果
由于受到模型精度保持难度、cuSPARSELt 内核优化不足
以及 TDP 限制等因素的影响
Ampere架构中GEMM 内核速度的实际提升远低于理论值
除了中国部分 AI 实验室以及 Meta 在 Llama 中的实验性尝试以外
多数的 AI 研究机构更倾向于聚焦量化与模型蒸馏
从而导致结构化稀疏性在实际的生产推理中的应用也较为有限
而且缺乏能证明其性能优势的公开模型
即便在NVIDIA的营销材料中被频繁提及
也因为缺乏资源投入
导致结构化修剪技术的发展滞后
而Blackwell 架构则针对 NVFP4 数据类型引入了 4:
8 的结构化稀疏性
这个模式将 8 个元素划分为 4 对连续元素
要求其中 2 对为非零值、2 对为零
这个设计显然是与 NVFP4 的子字节特性是相关的
作为半字节的数据类型
NVFP4 的存储结构
促使 NVIDIA 采用了这种成对约束的稀疏模式
尽管 4:8的稀疏性从形式来上看
要比 2:4 的更加灵活
但是在实际操作中
机器学习工程师仍然需要在修剪的过程中
平衡模型的精度
这种结构化稀疏性与 NVFP4 的结合
虽然在理论上可以优化计算的效率
但在实际部署中
仍然面临与前代相似的精度保持和工程实现方面的挑战
接下来呢
文章还从Tensorflow的规模与内存的演进
MMA 指令的异步性
数据类型精度的演进
以及编程模型演进等多个角度
对Tensor Core的发展过程进行了更细致的介绍
鉴于大部分内容我们都已经提到过了
所以感兴趣的朋友可以自己再去深入阅读一下原文
回顾 NVIDIA Tensor Core 从 Volta 到 Blackwell 的演进历程
我们可以清晰地看到技术创新的力量
每一代的 Tensor Core 都在不断的突破计算瓶颈
优化数据处理流程
提升计算的性能
从最初的半精度矩阵乘累加指令
到如今的专用张量内存
NVIDIA 也通过持续的技术创新
不仅推动了自身硬件架构的发展
也为整个人工智能领域奠定了腾飞的基础
在未来
随着人工智能技术的不断发展
对计算能力的需求也将持续增长
希望还能看到NVIDIA 带来更多创新的技术和架构
为人工智能的发展提供更为强大的动力
感谢大家观看本期视频
我们下期再见
