大家好，这里是最佳拍档，我是大飞
最近，国产算力芯片领域有个词火了
那就是UE8M0 FP8
火爆的原因
主要是因为DeepSeek在8月21日发布的V3.1版本模型
其中最大的亮点就是
模型训练采用了UE8M0 FP8 Scale的参数精度
并且提到
UE8M0 FP8是“针对即将发布的下一代国产芯片设计”的
正是这句话
引起了资本市场的热烈反应
A股和港股里的“国产芯片、FP8概念股”短线大涨
包括有观众说想让我讲讲寒武纪
我琢磨了一下
可能得先把这个概念讲清楚
那么，这个UE8M0 FP8的背后
到底隐藏着怎样的技术逻辑？
它又能否成为国产芯片突围的关键呢？
今天
咱们就来聊聊关于这个概念的来龙去脉、技术细节以及对行业的影响
为了理解UE8M0 FP8这个概念
我们将它拆分成两部分UE8M0+FP8来解释
我们先说后面的FP8
为了照顾一些非专业的观众
我们还是先从计算机的二进制和十进制讲起
熟悉的观众可以跳过这个部分
众所周知
计算机的数据都是以二进制形式存储的
二进制只使用0和1两个数字
遵循“逢二进一”的规则
“1”也表示通电、开、有信号；
“0” 表示断电、关、无信号
二进制只有这两种状态，不容易出错
读写可靠
举个例子，数字2，用二进制表示为10；
数字3 为11
在十进制里
每一位代表的是10的不同次方
比如，321 = 3×10² + 2×10¹ + 1×10⁰
但是二进制里
每一位代表的是2的不同次方
比如
二进制的1101 = 1×2³ + 1×2² + 0×2¹ + 1×2⁰ = 8 + 4 + 0 + 1
相当于十进制的13
需要注意的是，从右到左
每一位的权重是 2 的幂次方
最右边是2⁰ ，然后是2¹、2² 、 2³
我们再来看浮点数与整数的概念
简单来说
计算机储存数字有两种形式
一个是整数类型，即INT
一个是小数点部分
即FP，它是浮动的
整数类型没有小数部分
比如 1、2、3
INT4指的是用 4 位二进制数来表示一个整数
INT8 则是用 8 位二进制数来表示整数
例如，INT4 最多能表示 2的4次方
也就是16 个不同的数
如果是有符号数
范围通常是 -8 到 7；
如果是无符号数，范围就是 0 到 15
同理， INT8 最多能表示2的8次方
256个数
有符号数范围是 -128 到 127
无符号数范围是 0 到 255
而FP, 全称为 Floating Point
即浮点数
用于表示带有小数部分的数
浮点数本质上就是把一个数字拆成三部分
分别是符号位、指数、尾数
其中，符号位表示数值的符号
只占用1 bit
0表示整数，1表示负数
指数部分也就是数值的指数
决定了数的范围
而尾数部分表示数值的底数部分
决定了数的精度和小数部分的具体值
举个例子
FP32 是一种标准的 32 位浮点数
它有 1 位表示正负的符号位、8 位指数位和 23 位尾数位
可以表示从1.2x10的-38次方
到3.4x10的38次方的范围
精度可以达到小数点后 6 位左右
而FP16 是一种半精度浮点数
它占用的空间是 FP32 的一半
只有 16 位
它有1位符号位，表示正负；
5位表示大小的指数位；
10位表示精度的尾数位
FP16 的表示范围从大约 6.1x10的-5次方
到 6.5x10的4次方，精度相对较低
只能达到小数点后 3 位左右
这就好比把 FP32 的尺子的刻度变粗一些
虽然精度降低了
但是使用起来更节省空间和时间
而FP8 是一种新兴的低精度浮点数格式
也是 Nvidia、Arm、Intel 联合推出的8位浮点数据格式
用来加速深度学习的训练和推理
目前已经在 Nvidia Hopper和 Ada Lovelace等GPU 上提供了支持
在硬件中
FP8 有两种不同的表示形式
E4M3、E5M2
其中，E代表指数exponent
或者幂次方
而M则代表尾数mantissa
其中
E4M3由1位符号位、4位指数位（exponent）、3位尾数位（mantissa）组成
可以存储 +/-448 之间的值和NaN
而E5M2由1位符号位、5位指数位、2位尾数组成
可以存储+/-57344之间的值、NaN或者无穷大
它的好处是增加了动态范围
代价是降低了存储值的精度
相比起FP32和FP16
FP8 的表示范围和精度进一步降低了
但是它的优势在于占用的存储空间更小了
计算速度也更快
这就好比用一个非常简单的计数器
来代替一把精确的尺子一样
虽然只能表示有限的数字
但是速度更快
更适合在资源受限的环境中使用
我们再来讲几个概念
一个是多精度计算
它是指用不同精度进行计算
在需要使用高精度计算的部分
使用双精度FP64
其他部分使用半精度FP16
或者单精度FP32计算
第二个是混合精度计算
指的是在单个操作中使用不同的精度级别
从而在不牺牲精度的情况下
提高计算效率
减少运行所需的内存、时间和功耗
第三个是量化精度
量化就像是把一幅高精度的画变成一幅低精度的画
但是尽量让它看起来还差不多
这个“压缩”的过程，就是“量化”
比如，把 FP32转换成 FP16或者INT8
一般的情况下，精度越低
模型的尺寸和推理内存就占用的越少
像FP32会占用4个字节
量化为INT8之后，只需要1个字节
再比如我们经常听到的MXFP8
就可以视为一种量化精度
通过微缩放MicroScaling
将张量分割成许多个小数据块
比如每32个元素一个块
并且为每个块计算一个独立的缩放因子来解决这个问题
由于每个1x32的块会共享一个缩放因子
所以每个块内的数据
都能被有效缩放到FP8的可表示范围内
从而在保留精度的同时
享受到低精度计算所带来的性能优势
了解完前面这些内容
我们再来说UE8M0
就很好理解了
首先
UE8M0的全称是Unsigned Exponent 8 bits
Mantissa 0 bits
也就是无符号指数位占8位
尾数位为0位
其中，U表示无符号
与有符号（Signed）相对应
也就是数字不带正负号
这种数据格式无法表示负数
但是可以用同样的数据长度来表示更多的正数
E（Exponent），指数
也就是科学计数法的“次方”
M（Mantissa），尾数
也就是科学计数法中的“头”或者“有效数字”
所以加起来就是，UE8M0就是无符号
只能取0或者正数
用8位数字来表达指数
用0位数字来表达尾数
但是要注意此时的尾数默认为1
也就是说这种数字格式只能表示
从2的0次方到2的255次方
不过，我们需要区分的是，UE8M0 FP8
并不是像E4M3、E5M2这样的另一种FP8格式
而是指的是MXFP8里缩放因子的格式
MX，Microscaling
这是OCP组织在2023年发布的一种数据格式
简单来说
它就是把一个张量按照固定的小块
比如K=32进行切分
每个块共享一个“缩放因子X”，
以幂次的形式存放
块内元素用低位宽格式
比如FP8来存储
这样既保留了8比特的低带宽优势
又能够靠更细颗粒的定标
获得更大的可用动态范围与更稳的数值
其中
MX的块尺度与元素的格式是相互独立的
所以说，UE8M0
其实指的就是那个缩放因子X的格式
无符号（U）、8位指数（E8）、0位尾数（M0）
也就是只有指数，没有符号和尾数
而元素的格式依然是E4M3或者E5M2的FP8
UE8M0最大的好处
是它只能表示 2 的整数幂
因此硬件在解码的时候
只需要进行位移
不需要进行浮点乘法、规格化或者舍入操作
硬件的关键路径更短
带宽和能耗也更加友好
通过这种方式，2025 年
英伟达的 Blackwell 把 MXFP8/6/4 做成了张量核的原生数据类型
在硬件里原生支持了UE8M0
从而将MXFP8训练端到端的吞吐
提升到了BF16精度的2倍
性能却依然相近
其次
由于每块的缩放元数据从FP32的32bit
降为了为UE8M0的 8bit
导致元数据部分的流量下降了 75%，
虽然这不是总的流量
但是依然有所利好
最重要的是
对于大多数已量产的、仍然以 FP16/BF16 + INT8 通路为主的国产GPU来说
它们很难做到对完整 FP8 FMA 的硬件栈支持
而 UE8M0 的位移解码 + 块级FP8存算
实现难度和代价更低
更加符合国产芯片阶段性的演进路径
在带宽和容量限制方面
FP8+块缩放也能够显著降低 HBM/DDR 的压力
正符合国产芯片最希望看到的
用算法和格式把算力的水分挤出来的方向
根据国内媒体与机构的报道
华为昇腾的下一代芯片910D
将支持FP8精度，预计第四季度送测
26年第一季度量产
深圳AI第一股云天励飞（口误）旗下的NPU Nova500
也宣称实现了对FP8的硬件原生支持
是中芯南方14nm唯一量产的推理芯片
沐曦曦云 （MetaX）旗下的曦云C500
可以同时支持多精度混合算力
比如FP32、FP16、INT8等等
配备64GB HBM2e显存
今年7月发布的曦云C600
也将支持FP8精度计算
燧原科技 （Enflame)历时两年半开发的L600 芯片
采用了训推一体的架构
也原生支持 FP8 低精度
摩尔线程的MUSA 架构
宣称原生 FP8 张量加速
并且点名能很好支持 UE8M0 FP8 Scale
芯原 VIP9000 NPU 也在采访中提到增加了 FP8（E4M3/E5M2）的支持
强调与主流框架/工具链的易部署性
股价一路暴涨的寒王寒武纪更是不必多说
宣称旗下MLU370-S4、思元590及最新690系列芯片
均支持FP8计算，据传690已经送测
字节跳动等头部企业正在测试
可能会获得大规模算力采购订单
截止到上周末
寒武纪市值突破5200亿元
大A股排名仅次于茅台
可以说
DeepSeek V3.1这次带火的 UE8M0 FP8
把训练/权重格式对齐到了 MX的标准
也对齐了软件端与国产硬件的最佳工作点
从而有可能构建软硬协同的一致坐标系
降低生态的碎片化成本
但是在国内芯片行业狂欢的时候
我们我们依然需要保持冷静
很多人会觉得
有了 UE8M0 FP8（MX）格式
是不是国产芯片立刻就要赶超英伟达的GPU了呢？
答案是还差得远
因为两者之间的差距往往不在格式本身
而在于算子、内核、内存与网络互联、框架与工具链生态、以及对标准细节实现的一致性等等方面
首先从数值与算法的角度来看
标准的一致性还没有完全对齐
在OCP的规范中
MX 的定义是块大小K=32
块内元素用 FP8/FP6/FP4精度
每个块共享 UE8M0
只编码 2 的幂次等等
问题是，如何取整到 2 的幂次这件事
不同的实现并不完全一致
像NVIDIA 的 MXFP8 训练配方里
明确把尺度取整改为向上取整（ceil(log2))，
并且给出消融
因为按照 OCP v1.0 建议的“向下取整”，
在大规模预训练里会更加容易溢出和发散
如果国产硬件和软件仍然按照 v1.0的规范来做
那么训练的稳定性就可能对不上
在FP8的格式选择上
NVIDIA 的结论是权重、激活和激活梯度都用 E4M3格式
这和很多FP8等于梯度用 E5M2的老经验不一样
正所谓差之毫厘，谬以千里
其次，在算子与内核方面
如果不能做到原生支持MX
就会带来很多的隐形开销
MX 需要在张量核里处理很多“每块一次”的尺度
如果在软件里频繁来处理这些缩放
就会非常昂贵
相比之下
Blackwell 在硬件层面把尺度取整与量化
塞进了张量核的指令路径
才把这笔开销吃掉
所以如果没有这条硬件上的捷径
想要在别家芯片上用 MX
内核层面的额外读、改、写等操作就会吞掉所有的收益
此外还有转置的问题
Blackwell 的 MX 要求“沿归约维的块数据连续”，
但是训练时得前后反向传播会频繁的更换归约维
导致普通 FP8 的转置是重排
而MX 的转置则是要“重量化”，
这在没有做专门硬件和内核优化的时候
代价会非常高
另外，为了同时服务行/列两条归约轴
训练框架通常需要给每个张量保留两份的 MX 量化版本
这不仅占用大量的显存
也会增加数据搬运的代价
第三，在内存与网络互联方面
NVLink和NVSwitch 的规模化优势明显
Blackwell GPU把 NVLink 的带宽拉到了每秒1.8 TB
并且通过 NVLink Switch
把 72个 GPU 拉进一个可以保持每秒1.8 TB的 NVLink 域
还能跨机柜进行扩展
这直接决定了FP8/MX 的带宽红利
能否真正的转化成集群的吞吐量
如果替代平台只有 PCIe 或者传统的以太网和InfiniBand
通信就会相对吃紧
同样的 MX/FP8 算力优势
也会被All-Reduce和张量并行的通信所抵消掉
第四，生态与通用性的缺失
不仅像dtype 这样的框架与编译工具支持仍不成熟
就连PyTorch核心对 MX 的基础类型的支持也仍在推进中
没有一线框架的原生支持
通用性就会打折
此外
各家厂商对FP8的支持在细节上也并不一致
比如 AMD的文档里就明确写到
MI300 的 FP8 编码与 H100 不同
Intel Gaudi的官方虽然公开了 FP8 的训练和推理教程
但是并没有宣称支持 MX 原生的块缩放
如果再叠加 MX 的尺度取整差异
想要在多家硬件之间迁移同一个FP8 MX 模型
可能需要进行重新的转换和校准才能够确保稳定
不过，即便如此
DeepSeek 在模型端明确采用 UE8M0 的块缩放范式
对于夹缝中生存的国内芯片来说
这也是算是一种不多的求变模式
至少有机会实现一种渐进式的增量演进
先把存取与带宽的红利吃干净
再逐步把计算路径FP8 化
比如，第一步先在推理端
实现权重采用FP8精度
激活采用BF16/FP16精度
累加采用 FP32精度
大幅降显存与权重的带宽
改善延迟和吞吐
第二步再在部分训练链路实现FP8 化
比如说在GEMM的主干上
采用MXFP8精度
而归一化和Softmax等过程依然保高精度
等训练先跑稳定了再来扩展规模
第三步等待硬件的代际升级
从而再实现原生的MX/FP8 张量核
无论如何，未来依然是任重道远
最后
为了回应观众的关于寒武纪的提问
我简单说三个点
一
它的市盈率如今已经达到了4000倍
这意味着如果要靠利润来回本
得等上四千多年
二、它虽然账面上开始盈利了
但是公司2025年一季度的经营现金流
还是负的13个亿
这就好比一个人工资条上写着月入过万
但是实际上钱包里只剩几百块了
可能连房租都要交不起了
三、如果仔细看它的客户结构
会发现八成多的收入都靠一个大单子撑着的
2024年84.2%的营收
都来自第四季度某个大客户一次性订单
并且前五大客户贡献了94.63%销售额
万一，我是说万一
这个大客户明年不续约了呢？
所以，对于寒武纪这样的"妖股"，
我的建议是，可以关注，但别太投入
可以期待，但是别太当真
历史经验告诉我们
每一次A股造神的背后
都是无数散户沉重的代价
2021年，贵州茅台冲破2400元/股
市值登顶3.27万亿，“茅王”诞生
随后股价一路向下
至今股价跌幅超40%
2022年，宁德时代冲破690元/股
市值突破1.6万亿
“宁王”诞生，随后股价一度跌超60%
如今，“寒王”诞生
最终它又将何去何从呢
总之一句话，股市有风险
投资需谨慎
感谢收看本期视频，我们下期再见
