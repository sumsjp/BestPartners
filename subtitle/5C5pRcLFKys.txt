大家好，这里是最佳拍档，我是大飞
不知道大家有没有过这样的经历
当你想让GPT-4、Claude这些大语言模型
处理一篇几万字的学术论文、或者甚至是一本电子书的时候
往往会遇到两个棘手的问题
要么模型加载半天后提示序列长度超出上限
要么勉强能处理
但是生成的速度却慢得让人失去耐心
这背后其实藏着大语言模型一个根深蒂固的技术痛点
那就是它们的计算量
会随着文本序列长度的增加呈平方的增长
简单来说，文本长度翻倍
计算量可能要翻四倍
这对硬件资源的消耗可以说是毁灭性的
而今天我们要聊的这款模型
可能为解决这个长文本困局
提供了一个全新的方向
它不是靠着堆砌更大的参数、更宽的上下文窗口
而是另辟蹊径，把文本变成图像
将视觉模态作为高效的压缩媒介
它就是来DeepSeek最新开源的DeepSeek-OCR
一款既能够做OCR
又能为大模型长上下文处理铺路的视觉语言模型
今天我们就来聊聊这款模型和论文
在具体介绍DeepSeek-OCR之前
我们先来考虑一个问题
有没有一种方式
能够在不减少文本信息量的前提下
减少输入到大语言模型的token数量呢？
DeepSeek团队想到了一个关键的思路
那就是利用视觉模态的压缩特性
大家可以想一下
一张包含1000个汉字的文档图片
如果把这些汉字转换成纯文本的token
数量可能要上千甚至更多
但是如果用图像格式存储
它的视觉token数量可能只有几百个
这意味着
通过文本到图像再到视觉token的路径
我们可能实现远超纯文本压缩的效率
而OCR恰好是连接视觉和文本的完美桥梁
因为OCR的本质
就是把图像中的文字信息
准确地转换成文本信息
这相当于完成了一次视觉token到文本token的解码过程
如果我们能够让模型通过OCR任务
学会从少量视觉token中解码出大量文本token
那就证明视觉到文本的压缩是可行的
这也是为什么DeepSeek-OCR选择OCR作为核心任务
而不是普通的图像描述或者视觉问答的方式
在这个思路下
DeepSeek-OCR的定位就不是一个单纯的OCR工具了
而是一个视觉到文本压缩的验证原型
它要回答三个核心问题，第一
视觉token能够把文本压缩到什么程度呢？
第二，压缩后的视觉token
还能准确的解码回文本吗？
第三，这种压缩方式
能够为大语言模型处理长文本
提供实际的帮助吗？
而接下来我们将会看到
DeepSeek-OCR不仅回答了这三个问题
还给出了超出预期的结果
首先，要想实现视觉到文本的压缩
核心在于要有一个足够高效的视觉编码器
它需要能够处理高分辨率的文档图像
同时输出尽可能少的视觉token
还要保持低的内存占用
但是DeepSeek的团队发现
当前主流的视觉模型中
三种常见的视觉编码器都存在明显的缺陷
无法满足这些需求
第一种是双塔式架构
代表模型是Vary
这种架构的思路是
用两个并行的编码器来处理图像
增加视觉的词汇量
从而支持高分辨率的输入
它的优点是参数和激活内存可控
但是缺点也很致命
就是需要对图像进行两次预处理
这会大大增加部署的复杂度；
而且在训练的时候
很难实现编码器的流水线并行
效率很低
第二种是基于瓦片（tile）的架构
代表模型是InternVL2.0
这种架构的做法是
把高分辨率图像分割成一个个小的瓦片
然后并行处理这些瓦片
以此来减少激活的内存
它的优势是能够处理极高分辨率的图像
但是问题在于
它的原生编码器分辨率很低
通常低于512×512
这就导致大的图像会被分割成大量瓦片
最终输出的视觉token数量非常多
比如一张4k分辨率的文档图
可能会被分成几十甚至上百个瓦片
视觉token的数量直接失控
根本达不到压缩的目的
第三种是自适应分辨率架构
代表模型是Qwen2-VL
它采用了NaViT范式，不分割图像
直接用基于patch的方式处理全尺寸图像
支持灵活的分辨率
但是这种架构的问题在于
处理大图像的时候
激活内存会急剧的增加
很容易导致GPU内存的溢出；
而且训练时需要极长的序列长度
推理速度也会变慢
简单说
就是看着灵活，用着卡顿
总结一下，双塔式难部署
瓦片式token太多
自适应分辨率内存不够
这三个坑
让现有的视觉编码器无法满足高分辨率输入、低激活内存、少视觉token的三重需求
也正是因为这个原因
DeepSeek的团队才决定从零开始
设计一款全新的视觉编码器
也就是我们接下来要讲的DeepEncoder
DeepSeek-OCR的整体架构非常清晰
分为两大部分
视觉编码器DeepEncoder和混合专家解码器DeepSeek3B-MoE-A570M
前者负责把图像转换成少量、高效的视觉token
后者负责把这些视觉token解码成文本
我们先从最关键的DeepEncoder开始讲起
DeepEncoder的核心目标有五个
分别是处理高分辨率输入、保持低激活内存、输出少量视觉token、支持多分辨率、参数规模适中
为了实现这些目标
它的结构设计非常巧妙
主要由三个部分串联而成
分别是基于窗口注意力的视觉感知模块SAM-base、16倍的卷积压缩器
以及基于全局注意力的视觉知识模块CLIP-large
我们先拆解一下这个结构的逻辑
首先是SAM-base，参数大约8000万
它的核心是窗口注意力，简单来说
就是把图像分成一个个小窗口
在每个窗口内部计算注意力
而不是在整个图像上计算
这种方式的好处是
即使处理高分辨率的图像
激活内存也能保持在较低的水平
比如一张1024×1024的图像
用16×16的patch分割
会产生4096个patch token
SAM-base用窗口注意力处理这些token时
内存压力很小
但是，4096个token还是太多了
达不到压缩的目的
所以接下来是16倍卷积压缩器
这是DeepEncoder的核心压缩环节
它由两层卷积层构成
每一层的卷积核大小是3×3
步长是2， padding是1
通道数从256增加到1024
通过这两层卷积
4096个token会被压缩到4096除以16
也就是256个token
这个设计的关键在于
它在窗口注意力处理大量token和全局注意力处理少量token之间
搭建了一个高效的过渡
既保留了图像的细节信息
又大幅减少了后续模块的计算压力
最后是CLIP-large，参数大约3亿
它的核心是全局注意力
能捕捉整个图像的全局信息
这对于理解文档的整体布局、上下文关联至关重要
因为前面已经通过卷积压缩器把token减少到256个
所以CLIP-large用全局注意力处理时
激活内存完全可控
这里有个细节需要注意
那就是DeepEncoder在使用CLIP-large时
移除了它原本的第一个patch嵌入层
因为CLIP-large的输入不再是原始图像
而是经过SAM-base和压缩器处理后的token
这个小改动
让CLIP-large能够更好地适配整个流程
整体来看
DeepEncoder的参数大约3.8亿
不算特别大
但是通过窗口注意力到卷积压缩
再到全局注意力的串联结构
完美解决了高分辨率、低激活、少token的三重需求
这也是DeepSeek-OCR能实现高效压缩的核心基础
除了基础结构
DeepEncoder还有一个非常实用的设计
多分辨率支持
因为实际场景中的文档图像
分辨率差异很大
比如一张手机拍的便签可能只有512×512
而一张扫描的报纸可能有4k甚至更高分辨率
如果只用一种分辨率处理
要么会浪费token
要么会丢失细节
因此
DeepEncoder把分辨率模式分成了两类
原生分辨率和动态分辨率
我们先看原生分辨率
它包含四种子模式
对应不同的图像大小和输出token数量
Tiny模式，512×512分辨率
输出64个视觉token
适合处理小尺寸的短文本图像
比如名片、便签
Small模式，640×640分辨率
输出100个视觉token
适合处理普通的单页文档
比如A4纸扫描件
Base模式，1024×1024分辨率
输出256个视觉token
适合处理包含复杂布局的文档
比如带表格的报告
Large模式，1280×1280分辨率
输出400个视觉token
适合处理高分辨率的细节丰富的文档
比如带公式的学术论文
这里有个关键点是
对于Tiny和Small模式
因为分辨率较小
模型会直接把图像 resize 到对应的尺寸
避免浪费token；
而对于Base和Large模式
为了保留原始图像的宽高比
模型会用填充（padding）的方式
把图像补到对应尺寸
这时候就会出现有效token和实际token的区别
比如一张1024×512的图像
用Base模式处理时
会被填充成1024×1024
实际token是256个
但是有效token只有128个
论文中给出了有效token的计算公式
其中w和h是原始图像的宽和高
这个设计能让模型更高效地利用token资源
再看动态分辨率
主要是为了处理超高清大图
比如报纸、多页拼接的文档
它的核心是瓦片+全局的组合模式
最具代表性的是高达模式（Gundam mode） 和高达大师模式（Gundam-master mode）
高达模式的逻辑是
把超高清图像分割成n个640×640的局部瓦片
再加上一个1024×1024的全局视图
所以总的视觉token数量是n×100 + 256
其中n的范围是2到9
避免分割过多导致token失控
比如一张3000×2000的报纸图像
可能会被分成4个局部瓦片
加上1个全局视图
总token数量是4×100 + 256 = 656个
远少于用原生模式处理时的token数量
而高达大师模式则是更高配置的版本
局部瓦片用1024×1024
全局视图用1280×128
总token数量是n×256 + 400
不过因为高达大师模式的分辨率太高
训练时会影响整体速度
所以它不是和其他模式一起训练的
而是在预训练好的DeepSeek-OCR基础上
用额外数据继续训练得到的
有了高效压缩的视觉token
接下来还需要一个能把这些token
准确解码成文本的解码器
DeepSeek-OCR选择的是
激活参数为5.7亿的DeepSeek3B-MoE-A570M
为什么选择MoE架构？
因为MoE有一个核心优势
在保持大模型性能的同时
能够降低推理时的计算量
普通的大模型在推理时需要激活所有参数
而MoE模型会把参数分成多个专家模块
推理时只激活其中一部分专家
具体到DeepSeek3B-MoE-A570M
它总共有64个路由专家（routed experts）和2个共享专家（shared experts）
推理时只会激活其中6个路由专家和2个共享专家
激活的参数总量大约是5.7亿
这意味着
它能够达到3B参数模型的表达能力
但是推理速度却和500M左右的小模型相当
这对于需要大规模部署的OCR任务来说
是一个非常关键的优势
解码器的核心功能
是实现从压缩视觉token到文本token的映射
论文中用公式表示为这样子
其中Z是DeepEncoder输出的视觉token
X̂帽是重建的文本token
函数 f_dec 代表一个非线性映射
这个映射过程是通过OCR任务的训练来学习的
模型在训练中不断调整参数
让它从视觉TOKEN解码出文本
并且尽可能接近真实的文本标签
这里呢有一个重要的猜想
那就是既然这种3B规模的MOE模型
能够学好这个解码过程
那么更大规模的模型
比如70B、175B参数
通过专门的预训练优化
应该能够更轻松地掌握这个能力
这意味着
未来的大语言模型可能不需要直接处理海量的文本token
而是可以先把长文本转换成图像
用DeepEncoder压缩成少量的视觉token
再用自身的解码能力
把视觉token转换成文本
这将会彻底改变大语言模型处理长文本的方式
当然，再好的架构
也需要高质量的数据来训练
DeepSeek-OCR的团队构建了一套非常全面的数据引擎
涵盖了四种不同类型的数据
总规模超过数十亿样本
第一种是OCR 1.0数据
主要对应传统的OCR任务
是数据引擎的核心
它又分为文档OCR和自然场景OCR
其中文档OCR数据
是团队从互联网上收集了3000万页的多语言PDF文档
覆盖约100种语言
其中中文和英文占2500万页
其他小语种占500万页
为了兼顾效率和精度
他们做了两种标注
粗标注和细标注
粗标注是用fitz工具直接从PDF中提取文本
主要用于教模型识别小语种文本；
细标注则是用PP-DocLayout和GOT-OCR2.0做精细化标注
包含文本的位置坐标和内容
共200万页中文和200万页英文
对于小语种
他们还采用了模型飞轮的方式生成标注
先用fitz切出小语种文本的patch
训练一个GOT-OCR2.0小模型
再用这个小模型给更多patch标注
最终得到60万样本
而自然场景OCR数据
主要支持中文和英文
图像来自LAION和Wukong数据集
用PaddleOCR标注，各1000万样本
第二种是OCR 2.0数据
对应更复杂的 beyond text 任务
比如图表解析、化学公式识别、平面几何解析
这些是传统OCR很难处理的
但是DeepSeek-OCR希望通过这些数据
来提升模型的深度解析能力
第三种是通用视觉数据
主要目的是让模型保留基本的通用视觉理解能力
而不是只局限于OCR任务
团队参考DeepSeek-VL2
生成了图像描述、目标检测、视觉定位等任务的数据
占总数据量的20%。
这样做的好处是
未来如果研究者想基于DeepSeek-OCR做通用视觉任务
不需要从头训练
只需要在这个基础上微调即可
第四种是纯文本数据
占总数据量的10%，
主要用于保持模型的语言生成能力
这些数据是团队内部整理的
全部处理成8192个token的长度
和DeepSeek-OCR的训练序列长度保持一致
因为OCR任务不仅需要识别文本
还需要生成通顺的文本
纯文本数据能够确保模型在解码的时候
输出的文本符合语法和逻辑
这四类数据按照7:2:
1的比例混合
其中OCR数据70%、通用视觉数据20%、纯文本数据10%，
这样既保证了OCR的核心精度
又拓展了复杂场景的处理能力
还保留了通用视觉和语言能力
在训练阶段
DeepSeek-OCR分为两个核心阶段
分别是DeepEncoder的独立训练和DeepSeek-OCR的整体训练
整个过程在自家的萤火平台上完成
第一阶段，DeepEncoder的独立训练
因为DeepEncoder是整个模型的压缩核心
它的性能直接决定了后续解码的精度
所以需要先单独优化
团队用了一个稠密的语言模型来训练DeepEncoder
训练数据包括所有的OCR 1.0和OCR 2.0数据
再加上从LAION数据集中采样的1亿通用图像数据
训练参数设置为，批大小 1280
优化器采用AdamW，学习率5e-5
学习率调度器采用余弦退火
训练轮次为2轮，序列长度4096
这个阶段的目标是让DeepEncoder学会
把图像高效地压缩成视觉token
同时保留足够的文本信息
第二阶段是DeepSeek-OCR的整体训练
当DeepEncoder训练完成后
就需要把它和MoE解码器结合起来
进行端到端的训练
这个阶段的训练配置更复杂
也更注重效率
其中，硬件资源使用了20个节点
每个节点8张A100-40G GPU
总共160张A100
训练采用流水线并行（PP）
把整个模型分成4个部分
DeepEncoder的SAM-base和卷积压缩器放在PP0
参数冻结；
DeepEncoder的CLIP-large放在PP1
参数解冻；
MoE解码器的12层中，6层放在PP2
另外6层放在PP3，参数全部解冻
同时还采用了数据并行（DP）
并行度为40
全局批大小为640
优化器依然采用的是AdamW
初始学习率3e-5
学习率调度器采用基于步数的调度（step-based）
训练速度方面
纯文本数据的训练速度是每天900亿token
多模态数据的训练速度是每天700亿token
另外，前面提到的高达大师模式
是在整体训练完成后
用600万采样数据继续训练得到的
训练协议和整体训练一致
这里就不单独展开了
一款模型的好坏
最终还要靠实验数据来验证
DeepSeek-OCR的团队在两个权威基准上做了全面的评估
用来测试视觉-文本压缩比的Fox基准
和用来测试实际OCR性能的OmniDocBench基准
并且还做了定性研究
结果都非常亮眼
首先在Fox基准上
10倍压缩仍能保持97%的精度
Fox基准是一个专门用来测试文档理解的数据集
包含多种布局的英文文档
团队选择了其中文本token数量在600-1300之间的100页文档
用DeepSeek-OCR的分词器来处理真实文本
然后测试Tiny和Small两种原生分辨率模式下的解码精度
实验结果显示，在Tiny模式
也就是64个视觉token下
当文本token数量在600-700时
压缩比约10.5倍，解码精度96.5%；
当文本token数量在900-1000时
压缩比约15.1倍
解码精度85.9%；
当文本token数量在1200-1300时
压缩比约19.7倍
解码精度59.1%。
在Small模式
也就是100个视觉token下
当文本token数量在600-700时
压缩比约6.7倍，解码精度98.5%；
当文本token数量在900-1000时
压缩比约9.7倍
解码精度仍有96.8%；
当文本token数量在1200-1300时
压缩比约12.6倍
解码精度87.1%。
这些数据揭示了三个关键的结论
第一
10倍压缩是一个甜点区
当压缩比在10倍以内时
解码精度能稳定在96%以上
几乎接近无损压缩的效果
这意味着
未来如果我们把长文本转换成图像
用DeepEncoder压缩10倍后输入给模型
模型能够准确解码出原始文本
而token数量减少到原来的1/10
这会极大降低模型的计算压力
第二，即使压缩到20倍
仍然有实用价值
当压缩比接近20倍时，精度大约60%。
这个精度虽然不算高
但是对于长文本摘要关键词提取
这类不需要逐字准确的任务
已经足够使用
而且论文中提到，实际精度可能更高
因为测试时模型输出格式和真实标签的格式存在差异
如果排除这些格式差异
精度会提升5%-10%。
第三
压缩比超过10倍后精度下降的原因
主要有两个
一是长文档的布局更复杂
视觉token难以完全保留布局信息；
二是512×512或640×640的分辨率
对于1200个以上的文本token来说
文字会变得模糊，导致细节丢失
第一个问题可以通过统一布局渲染解决
第二个问题则可以通过更高分辨率的模式缓解
这也为后续的优化指明了方向
我们再来看OmniDocBench基准
它是当前最权威的文档解析基准之一
包含多种类型的文档
评估指标是编辑距离（Edit Distance）
数值越小
说明模型输出的文本和真实标签的差异越小
性能越好
团队在这个基准上
把DeepSeek-OCR和两款主流OCR模型做了对比
分别是GOT-OCR2.0和MinerU2.0
核心对比维度是平均视觉token数量和编辑距离
目标是看DeepSeek-OCR能否用更少的token
达到更好的性能
结果非常惊艳
当使用100个视觉token（Small模式）时
DeepSeek-OCR的编辑距离
远低于需要256个视觉token的GOT-OCR2.0
当使用400个视觉token时
DeepSeek-OCR的编辑距离和当前最先进的模型相当
当使用不到800个视觉token时
DeepSeek-OCR的编辑距离低于MinerU2.0
如果再更细致地看不同文档类型的表现
幻灯片只用64个视觉token
编辑距离就很低
书籍、财务报告用100个视觉token就能达到很好的性能
因为这些文档的文本token数量通常在1000以内
压缩比在10倍以内，精度有保障
而报纸就需要高达模式甚至高达大师模式
因为报纸的文本token数量通常在4000-5000
远超10倍压缩的范围
需要更多token来保留细节
这些结果证明
DeepSeek-OCR不仅在压缩比上有优势
在实际OCR性能上也处于当前领先水平
它不是一个为了压缩而牺牲精度的模型
而是在压缩的同时提升或保持精度的模型
这对于实际应用来说至关重要
除了定量数据
团队还做了大量定性研究
展示了DeepSeek-OCR超越传统OCR的能力
主要包括三个方面
第一是深度解析能力
传统OCR只能识别文本
而DeepSeek-OCR能对文档中的复杂元素进行结构化解析
比如能够把折线图、柱状图转换成HTML表格
并且包含具体的数值
还能把图像中的化学公式
转换成SMILES格式
甚至能识别几何图形中的线段、端点坐标、线段类型
并且用结构化的格式输出
即使对于文档中插入的自然图像
DeepSeek-OCR也能生成详细的描述
包括物体、颜色、位置关系等等
第二是多语言识别能力
DeepSeek-OCR支持近100种语言的OCR
包括中文、英文、阿拉伯语、僧伽罗语等
对于小语种文档，它既能输出纯文本
也能输出包含布局信息的文本
比如阿拉伯语的报纸
模型能准确识别从右到左的文本顺序；
僧伽罗语的宗教文献
模型能识别复杂的字符结构
这得益于OCR 1.0数据中丰富的多语言样本
第三是通用视觉理解能力
虽然DeepSeek-OCR的核心是OCR
但是它保留了VLMs的通用视觉能力
比如能够详细描述图像的内容
包括物体、场景、细节；
能够定位图像中的物体
并输出边界框坐标；
能够根据文本参考
在图像中找到对应的位置
甚至能够识别古诗《将进酒》的图像
准确输出文本
并且能理解其中的语义
需要注意的是
DeepSeek-OCR没有经过监督微调SFT
所以它不是一个聊天机器人
需要通过特定的提示词来激活这些能力
但是即使如此
这些能力已经远超传统OCR
展示了它作为多功能文档处理工具的潜力
通过前面的实验
我们已经看到DeepSeek-OCR在视觉-文本压缩上的潜力
但是更重要的是
这个技术思路能为大语言模型的长文本处理带来哪些改变呢？
团队在论文的讨论部分
提出了几个非常有启发性的方向
第一个方向是模拟人类的记忆遗忘机制
人类的记忆有一个特点
就是近期的记忆清晰
远期的记忆模糊
这是一种自然的信息压缩
而DeepSeek-OCR的视觉到文本压缩
恰好能模拟这个过程
比如
对于近期的对话历史或文本片段
可以用高分辨率图像渲染
生成较多的视觉token，解码精度高；
对于远期的对话历史或文本片段
逐渐降低图像分辨率
生成较少的视觉token，解码精度降低；
对于极远期的信息
甚至可以进一步压缩
只保留关键词或摘要
这种方式不仅能大幅减少模型的token消耗
还能让模型的记忆更符合人类的认知规律
比如在多轮对话中
模型不需要记住所有轮次的完整文本
只需记住近期的详细内容和远期的核心信息
这会让对话更加的自然
第二个方向是超长上下文的处理
当前大圆模型的上下文窗口上限
一般是128k或256k token
但是对于百万级、千万级token的文本
即使是最大的上下文窗口也无能为力
而视觉到文本的压缩
能够提供一种无限上下文的可能
比如把超长篇文本分成多个片段
每个片段渲染成图像
然后用DeepEncoder压缩成视觉token；
当模型处理的时候
不需要加载所有文本token
只需加载压缩后的视觉token；
如果需要查看某个片段的细节
再用OCR解码对应的视觉token
生成详细文本
这种压缩存储+按需解码的模式
可能是未来大语言模型处理超长篇文本的核心方案
第三个方向是大语言模型与视觉模型的协同优化
目前DeepSeek-OCR是一个独立的视觉模型
但是未来的趋势是把视觉-文本压缩的能力融入到语言模型本身
在语言模型的预训练阶段
加入从文本到图像
到视觉token，再到文本的闭环任务
让语言模型直接学会压缩和解码；
然后在语言模型处理长文本的时候
能自动把文本转换成图像
用内置的视觉编码器压缩成token
再进行后续处理；
这种方式不需要额外的视觉模型
还能让语言模型的长文本处理能力更集成、更高效
当然
这个技术思路也存在一些需要解决的问题
首先是分辨率与精度的平衡
目前当压缩比超过20倍时
精度会下降到60%左右
如何在更高压缩比下保持精度
是未来需要优化的重点
其次是复杂布局的处理
对于包含大量表格、公式、图表的复杂文档
如何确保视觉token能保留所有关键信息
避免解码时丢失结构
还需要进一步研究
最后是实时性
把文本转换成图像
再压缩成视觉token
这个过程会增加一定的延迟
如何优化这个流程，让它能实时处理
也是实际部署时需要解决的问题
总结一下
DeepSeek-OCR虽然以OCR命名
但是它的价值远不止于OCR本身
它是一个视觉-文本压缩的验证原型
也是连接大语言模型和长文本处理的桥梁
它的核心贡献可以概括为三点，第一
验证了视觉到文本压缩的可行性
通过Fox基准的实验
它证明了10倍压缩下
能保持97%的解码精度，20倍压缩下
能保持60%的精度
这为后续的研究提供了坚实的实验基础
第二
提出了高效的视觉编码器DeepEncoder
通过窗口注意力到卷积压缩
再到全局注意力的结构
DeepEncoder解决了现有视觉编码器高分辨率、低激活、少token不能兼顾的问题
为视觉模型的编码器设计提供了新的思路
第三，展示了强大的实际应用价值
在OmniDocBench基准上
它用更少的token超越了主流OCR模型；
在生产环境中
单张A100-40G GPU每天能生成20万页的训练数据
20个节点每天能生成3300万页数据
这对于大规模的语言模型训练来说
是一个非常重要的数据引擎
当然，DeepSeek-OCR也有它的局限性
它目前还是一个概念验证模型
还需要更多的实验
来验证它在超长篇文本、更多语言、更复杂场景下的性能
团队在论文中也提到
未来会做更多的研究
比如数字到光学文本的交错预训练、大海捞针测试等等
但是无论如何，DeepSeek-OCR的出现
为我们打开了一扇新的大门
它告诉我们，解决大语言模型的难题
不一定非要堆参数、扩窗口
有时候换个模态，换个思路
可能会有更大的突破，毕竟
就连Andrej Karpathy 也针对这篇论文提出
也许所有输入大模型的内容
都应该是图像而不是文本
好了，感谢收看本期视频
我们下期再见
