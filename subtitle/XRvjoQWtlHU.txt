大家好，这里是最佳拍档，我是大飞
这一期节目我觉得是很重要的一期
内容可能会颠覆大家的一些既有观念
我们之前做过一期节目
说的是OpenAI预言超级智能将在十年内出现
对于人类而言
超级智能既是机遇也是挑战
它可以帮助人类解决很多重大问题
但也可能产生巨大风险
因此
我们需要确保超级智能与人类整体的意志保持一致
让它们理解和满足人类的愿望与需求
在那期节目中，OpenAI承诺
会将目前为止获得的计算资源中的20%，
用于对齐超级智能
并成立了一个名为“超级对齐（Superalignment）”的团队
并且在四年内解决超级对齐的问题
这个超级对齐团队由OpenAI联合创始人兼首席科学家Ilya Sutskever和现在的对齐负责人Jan Leike共同领导
后者曾经在DeepMind工作了四年
专门研究人类反馈的强化学习以及递归奖励建模
最近在AXRP的一档播客节目中
UC伯克利博士生Daniel Filan与Jan Leike
就探讨了OpenAI超级对齐计划的具体内容以及挑战
整个内容时长2个多小时
说实话
在OpenAI刚刚提出超级对齐的时候
我并没有get到他们到底要做什么
怎么做
也有网友在我那期视频下面留言
觉得所谓的超级对齐就是一个噱头
但是我在听完Jan Leike的整个访谈之后
我才明白
不是这样
超级对齐是一个非常非常重要的事情
甚至下一个通用人工智能AGI
就是从超级对齐中诞生的
那这究竟是怎么一回事
听我跟大家慢慢道来
访谈的一开始
Jan Leike就回答了超级对齐团队的定位和目标
简单来说
这个团队就是要在四年内解决超级对齐方面的问题
目前具体的思路
就是训练一个大致和人类水平一样的自动对齐程序
然后让这个自动对齐程序来寻找对齐超级智能的办法
这句话，可能大家听上去平平无奇
甚至完全不知道他在说什么
但是在他后面的逐步解释中
才会明白这句话才是关键中的关键
我再跟大家念一遍，目前的思路
就是训练一个大致和人类水平一样的自动对齐程序
然后让这个自动对齐程序来寻找对齐超级智能的办法
这是什么，这就是在用AI来训练AI啊
我们都知道
OpenAI现在靠强大的算力
训练出了GPT-4，以后还会有GPT-5
但是模型训练出来之后
需要再去让它学会对齐
让他去学会
符合人类社会的意志和规范
这就导致对齐的过程中，紧了也不行
松了也不行
紧了我们就感觉它变笨了
越来越不如之前
松了就存在各种漏洞
返回一些有害的信息
这里面的核心问题
就在于这个模型一开始是不懂对齐的
他是先要学会解决人类社会的各种任务
才去学习对齐人类的意志和规范
但是其实后者的难度远超前者
这才会导致模型现在卡在了对齐的问题上
所以我们才会看到OpenAI现在为什么老在说对齐
还专门成立个团队干这事
从Jan Leike后面的解释中
我逐渐明白，现在OpenAI的思路变了
不再是先训练一个聪明的大模型
然后让他去学会对齐
而是打算先训练一个跟人一样
知道什么是对齐的AI
再让他来训练出一个完全符合对齐的AI
这句话是不是很拗口，没错
但是这就是他们现在的想法
因为实际上
他们现在并不知道怎么让更大的模型、尤其是以后的超级智能去学会对齐
但是他们知道
训练一个更聪明的模型是不难的
只要算力、参数堆上去
是一定搞的出GPT-5、GPT-6的
但是模型规模大了
不代表对齐做的好
到时候仍然会遇到跟现在GPT-4一样的问题
天天都在纠结于对齐的具体问题上
好了，那还不如我们换个思路
从一开始就解决对齐的问题
但是靠人来解决肯定不靠谱
虽然人类反馈强化学习的效果是很显著
但是这玩意成本也很高啊
而且无法规模化，模型越大
可能你要招的对齐专家越多
要解决的具体对齐问题也越多
这么搞没个完
而且人类也无法监督超越自身水平的任务
不可能训练出远超人类智能的模型
所以思路很简单，让AI来搞
所以你看Jan Leike的原话是什么
先训练个有对齐能力的对齐AI
然后让它来找到对齐超级智能的办法
那怎么找到
Jan Leike其实提到了两点
第一点是具体的方法
可扩展监督，scalable oversight
这是一系列让我们可以使用AI
来辅助人类评估困难任务的想法和技术
包括辩论、递归奖励建模、迭代蒸馏和增幅等等
具体我们后面会讲到
第二点是整体思路
我觉得这个才是核心
虽然我们的长期目标是如何训练一个会对齐的AI
但是对于AI来说
把这个目标扔给它其实没有用
机器擅长的是解决一个一个的任务
然后把它们再通过某种方式连接起来
就像AutoGPT一样
其中每一个任务都很小并且独立
因此系统不需要追求长期目标
而且，OpenAI最近有一篇论文
叫做《Let's Verify Step by Step》，
研究了在数学任务上使用基于过程的反馈
不是基于「系统是否得到了正确答案」而训练
而是通过执行强化学习
基于人类对证明中每一步的反馈
来训练一个奖励模型
结果表明，这种方法要有效得多
因为这能为AI系统提供更加细节的反馈
让AI系统远远更加细粒度的学习
所以
Jan Leike他们的思路其实就让AI从一个一个对齐任务开始
学会自我迭代，逐渐再演变成一个
完全知道怎么去对齐
并且可以把以后所有对齐任务
放心交给它的这么一个AI
而且
这个迭代必须是可以自动化、规模化的
其实团队真正的研究目的
就是找到自动对齐这个对齐AI的方法
特别是在可解释性方面
以及利用计算来推进对齐
如果能实现自动对齐
那么我们就只需要投入更多计算
就能得到更加对齐的结果
再说的直白点
这就是OpenAI接下来要做的大事
把算力变成对齐
自动对齐带来规模化和自动化
通过不断的迭代
逐步产生一个完全符合对齐的、但是更加智能的AI
这就有点像Alpha GO
它从最开始模仿人类棋手落子
到具备基本能力后不断自我博弈
最终超越人类棋手
所以
Jan Leike他们的团队现在的重心
根本不在于具体的对齐任务上
这些事情已经都放给了其他的团队
他们这个团队想的
就是怎么找到实现自动对齐的办法
好，那我们再回过头来说
怎么实现这个跟人类水平类似的对齐AI
其实跟我们刚才说的一样
相比于如何训练出这个模型
超级对齐团队的研究人员
更关注的是如何构建一个自动化、全面化的评估
因为好的评估可以提供监督反馈
从而指导模型的迭代方向
可扩展监督就是Jan Leike认为的一个重要方法
首先
需要能验证模型可以达到人类水平
其实要真正评估出这个还是很难的
就像现在业内这么多模型
没人能给出一个真的排行榜一样
而且最好是自动化的
避免人类来提供ground truth
才能进行更全面的评估
但是这一步也很难
这里Jan Leike提供了一个discriminator-critique gap的测量方法
翻译过来叫判别器-评判器差
比如我们做了一个编程模型
想利用ChatGPT自动评估它的效果
最简单的做法是直接命令ChatGPT
来判断某道编程题做的对不对
那么问题就来了
我们怎么确定ChatGPT评估的结果是否可信？
都要人工看一遍太浪费时间了
自动化的做法是直接基于ChatGPT
训练一个判断变成结果是否正确的判别模型
然后我们专门用一些有标签的难样本
去看判别模型的准确率和ChatGPT的准确率
如果差的不多
就说明不用训练也可以用它直接评估编程结果的好坏
如果差的很多
那就意味着模型能够知晓代码中有很多问题
但是却没告诉我们
其次
就是要能够自动化地进行鲁棒性检测、可解释性分析
为啥非要做可解释性呢？
因为1
可以为我们指出解决问题的途径
2
现有很多研究是在做知识和神经元的关联性
Jan Leike认为更有意义的是自动化、规模化地去做
从而探究更高维度的可解释性
最后
我们可以故意训练不对齐的模型
进行对抗测试
验证评估方法的有效性
从而避免出现假对齐的情况
这里Jan Leike也提出了自己最担心的两个问题
那就是系统性的撒谎和欺骗
以及自我渗透
前者指的是模型能够做到连贯一致并且让人信服的撒谎
或者具有欺骗性
像是假装做某事或相信一件事
但是实际上想做另一件事
后者自我渗透指的是
模型能够逃逸人类为它设置的安全预防措施、访问自身权重并尝试将其复制到互联网上的其他地方
但是现在他们也不知道
进行这种对抗压力测试的难度究竟有多大
评估系统能否检测出来
这也是可扩展监督研究的一个重要目标
那么这个团队具体的打算是怎么样的呢？
未来4年内
超级对齐的团队将组建30-100人的规模
投入OpenAI 20%的算力
其实20%我感觉主要是先表个决心
这个数量大家听起来好像不多
但是实际上已经是OpenAI在对齐方向上
最大的单笔投入了
当然做得好以后还会再加
4年规划具体是这样的
2年内搞清楚用什么技术实现AI对齐
把问题拆的足够细
剩下就是工程问题了
3年内实现这个对齐AI
剩下一年探索超级对齐
这么一看时间还是挺紧的
后面两个计划明显有点乐观
不过Jan Leike给出的信心是85%，
而且表示有很多实验已经在实验中了
他的信心主要来自于五个方面
1
语言模型的成功
大语言模型可以理解自然语言
让我们可以向模型表达我们希望他们怎么做
操控起来更加容易
2
RLHF的效果超出预期
他们只用了很少的计算
甚至还没尝试收集数据
就可以在小模型上得到比大模型更好的效果
3
在评估度量方面已经取得了很多进展
可以提供改进的方向
4
评估比生成更简单
这个也是提到多次的一个观点
比如说发表一篇论文很难
但是发表出来我们评估它好不好
相对来说就简单很多
如果人类只做评估，而不做生成
那么开发速度就会加快
还是自动化监督信号的思想
5
对语言模型的信念
语言模型很适合做超级对齐
任何任务都可以表述为文本的输入输出
不管是做实验和理解结果都可以做
那对于超级对齐来说
目前的技术还有用吗？
对于预训练
Jan Leike认为预测下一个token这种方式
并不一个长期目标
可能需要更好的任务
对于RLHF，Jan Leike也持怀疑态度
因为目前的监督信号来源于人工评判
但人工并不擅长区分看起来都很好的答案
各种论文显示人类之间的一致率有70%就不错了
这个监督信号本身自己都不一定对的齐
同时
需要人工就导致无法规模化扩展
也不符合我们增加计算量的需求
目前预训练+RLHF的范式
大概率也只是AI发展中的一个版本
按照OpenAI的AI对齐思路
后续模型训练的系统复杂度可能会提升很多
估计会有N多个擅长不同任务的AI来训一个模型
人工只需要提供少量监督信号
告诉系统要做什么
就可以让他们自动运转
训完了自动同步权重，不断升级
最后
虽然Jan Leike在访谈中没有明确的表示
但是也有多次的暗示
一旦找到了自动化、规模化且自动对齐的方法
再不断投入更大算力
让这个过程自动迭代下去
最终可能就会导致AI开始递归式的自我提升
每周都能完成成千上万年人类工时的工作量
那么这个思路
会不会就是走向AGI、超级智能的道路呢？
好了
以上就是我对Jan Leike这次访谈的理解
不一定完全准确
但是我认为超级对齐会是OpenAI后续的一个工作重心
也是在下一盘大棋
一旦OpenAI能够实现自动对齐
其他竞争对手的模型再大
也无法在对齐方面战胜OpenAI
无法满足对齐要求
就无法满足人类社会、安全无害和监管审查的要求
更别提市场竞争了
大家对超级对齐是什么看法呢
欢迎在评论区留言
感谢大家的观看，我们下期再见
