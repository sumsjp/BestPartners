大家好，这里是最佳拍档，我是大飞
对于Stability AI来说
我们都知道它开发了Stable Diffusion这个文生图的开源工具
但是昨天
它推出了一款自称具有革命性的编码工具StableCode
开始进击代码生成的领域
简单来说
StableCode是一个帮助程序员处理日常工作
同时帮助新手开发者更好学习的一个工具
根据官网的显示
这个StableCode模型，具有30亿参数
不仅支持Python、Go、Java、JavaScript、C、Markdown、C++等多种编程语言
还直接把上下文长度拉到16000个token
目前StableCode提供了基础模型、指令模型、长上下文窗口模型等三种不同的模型
其中基础模型为StableCode-Completion-Alpha-3B-4K
它是一个30亿参数的、仅使用解码器的代码补全模型
而StableCode-Instruct-Alpha-3B是一个30亿参数的、纯解码器指调优的代码模型
预先在不同的编程语言集上进行了训练
并且用带有代码指令数据集在StableCode-Completion-Alpha-3B上进行了指令微调
它的使用方式如图所示
那这个StableCode模型是怎么来的呢？
去年9月份
HuggingFace和ServiceNow
合作成立了一个开放科学合作组织BigCode
并且在今年五月份
开源了一个针对代码的大模型StarCoder
而StableCode正是在高性能计算HPC集群上
使用5600亿代码token的编程语言数据集starcoder-data
对StarCoder基础模型训练而来的
Stability AI的首席研究科学家内森·库珀（Nathan Cooper）表示
我们非常喜欢BigCode
因为他们在数据治理、模型治理和模型训练方面做了惊人的工作
所以我们用了他们的数据集
并对它进行了额外的过滤和清理
同时也用在了我们的集群上
来训练长上下文窗口的模型
对于复杂的编程任务
StableCode也针对特定得用例
进行了指令调优
在StarCoder基础模型的基础上
用大约120000个Alpaca格式的“代码指令-回答”对
对指令模型进行了训练
此外
为了满足用户对上下文窗口长度的需求
StableCode的长上下文窗口模型
直接将上下文窗口拉长至16000个token
是此前开源模型的2-4倍
因此
这次StableCode不仅为用户提供了
可以支持单行、多行自动完成建议的工具
还允许用户同时查看编辑
相当于五个中等大小的Python文件
允许使用更专业复杂的代码生成prompt
在官方示例中
StableCode利用Pytorch深度学习库
完成了一个相对复杂的Python文件展示
其中底部的灰色文本是StableCode的预测结果
此外，这次发布的StableCode模型
也是用到了Transformer架构
不同于StarCoder用线性偏差注意力ALiBi的方法
来定位模型输出
StableCode使用了旋转位置嵌入RoPE方法
这是因为在Transformer模型中
ALiBi方法倾向于更加重视当前的token
而不是过去的token
但是这种方法并不太适合于代码生成
因为代码没有固定的叙述结构
没有明确的开始、中间和结束
在应用流程的任何点都可以定义代码的功能
所以StableCode使用了RoPE
也避免了这种偏见
那么，StableCode与其他模型相比
效果如何？
Stability AI将它与有相似参数和训练token数量的模型进行了比较
同时使用了流行的HumanEval基准
采用了标准的pass@1和pass@10指标
可以看到
在与AI独角兽Replit推出的replit-coder、starcoderbase两个模型的比较中
StableCode的pass@1强于replit-coder但弱于starcoderbase
而pass@10则强于两者
目前，StableCode还处于早期阶段
Stability AI现在把模型公开出来
是希望看看开发者们会如何接受和使用这个模型
相关的模型文件已经在HugggingFace上开源
感兴趣的小伙伴们快去试用一下吧
好了，本期的视频就到这里
感谢大家的观看，我们下期再见
