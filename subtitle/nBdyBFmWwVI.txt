大家好，这里是最佳拍档，我是大飞
就在上周，美国旧金山时间10月10日
老牌芯片巨头AMD交出了一份令人印象深刻的AI答卷
在当日举行的AMD Advancing AI 2024盛会上
AMD倾囊倒出了一系列的AI杀手锏
发布了全新的旗舰AI芯片、服务器CPU、AI网卡、DPU和AI PC移动处理器
准备将AI计算的战火烧得更旺
AMD还大秀AI朋友圈
现场演讲集齐了谷歌、OpenAI、微软、Meta、xAI、Cohere、RekaAI等重量级的AI生态伙伴
在开场致辞中
苏妈表示预计到2028年
AI加速器的市场规模将达到5000亿美元
这一次，她的目标将更为宏大
成为这个数千亿美元市场的领头羊
苏妈还表示，对于AMD来说
AI平台意味着四个核心
分别是作为训练和推理的最强计算引擎、开放的软件解决方案、深度共同创新的AI生态系统
以及集群水平之上的系统设计
而这次发布会的三支箭
正落在了集群水平之上的系统设计这一靶心
但是这三支箭真的足够“锋利”，
能够刺破英伟达构筑的AI铁桶阵吗？
今天
大飞就来带大家看一看这次AMD发布会的详细内容
AMD射出的第一支箭
就是新一代的EPYC服务器
众所周知，苏姿丰执掌AMD
十年磨两剑，一个是Ryzen芯片
另一个就是EPYC CPU服务器
从2018年开始
EPYC服务器从0%的市占率
经过四代升级
在2024年第一季度达到了34%，
硬生生从英特尔手里夺走了CPU服务器市场1/3的份额
这也是苏姿丰最得意的一场仗
为了形成一体化的AI服务器阵列
今年，EPYC迎来了它的第五代升级
EPYC 9005系列
AMD第五代EPYC 处理器，代号"Turin"，
采用了台积电3/4nm制程和Zen 5 架构
最高配置拥有16个Zen5 CCD核心
内含192个核心和384个线程
时钟频率可以达到5GHz
这款处理器支持 AVX512指令集
提供完整的512位数据路径
并且实现了17%的IPC性能提升
它还使用了SP5 平台
可以兼容前代的"Genoa"处理器
在内存方面，Turin引入了DDR5支持
带宽提升到了6400 MT/s
在I/O能力上
它支持PCIe Gen5和更多的PCIe通道
在安全性方面
增加了硬件级别的根信任和可信I/O功能
可以说
Turin处理器的性能领先相当明显
相比于上一代的英特尔Xeon服务器
Turin在SPEC CPU测试中
性能提升了2.7倍
企业性能最高提升了4倍
HPC，也就是高性能计算的性能
最高可以提升3.9倍
特别值得注意的是
Turin在AI方面的能力提升和对GPU节点控制的优化
它基于CPU的AI性能最高提升了3.8倍
而作为GPU主机节点时的性能
最高能够提升1.2倍
对于这些提升
AMD做了一个形象的展示
那就是如果你用Turin服务器
替代上一代的Xeon服务器
只需要131个Turin就足够达到之前1000个Xeon服务器的效果
除此之外
AMD还对Turin的AI适用性加强
做了更深入的阐述
因为在AI时代
越来越少的人会把服务器专门用在一般需求上
多少都得和AI结合一点
而在这种情况下
对AI有着更好支持的Turin就成为了最好的选择
AMD也非常在意Turin作为GPU主机节点
带来的GPU算力提升
为了提高算力
AMD这次还优化了CPU在AI工作流程中的关键动作
包括数据预处理、内存复制、内核启动和任务协调等等
这些优化使得CPU在处理GPU协调任务的时候
更加高效
比前代产品快了28%。
AMD拿Turin和Xeon 8592做了一下对比
Turin让MI300X的推理性能提升了8%，
训练性能提升了20%。
针对英伟达H100
Turin更是让GPU集群的推理性能提升了高达20%，
训练性能提升了15%，
比自家的MI300X都强
可以说，第五代EPYC的表现和侧重
一方面显示了AI战略对于当下AMD的重要性
另一方面也是AMD对英特尔最近两代Xeon服务器
都在大力强调AI能力的回应
就算你上了AI
AMD的CPU服务器还是遥遥领先
讲完了EPYC服务器
再来看看苏妈准备的下一招
AI芯片
AI芯片正在成为AMD业务增长的重头戏
AMD去年12月发布的Instinct MI300X加速器
已经成为AMD历史上增长最快的产品
不到两个季度
销售额就超过了10亿美元
今年6月份
AMD公布了全新的年度AI GPU路线图
最新一步就是这次发布的Instinct MI325X
在7月公布季度财报的时候
苏妈就透露过
AMD预计今年数据中心GPU的收入
将超过45亿美元
微软、OpenAI、Meta、Cohere、Stability AI、贾扬清创办的Lepton AI、李飞飞创办的World Labs等公司中
很多主流的生成式AI解决方案都已经采用了MI300系列AI芯片
所以在发布会上
苏姿丰放出的第二支箭
就是MI300系列的第二代产品MI325X
它曾经在2024 ComputerX大会上被简短地介绍过
但是从来没有公开过技术细节
作为目前最有望打破英伟达垄断的GPU产品
MI300系列的下一代产品也备受大家的关注
在发布会上，答案终于被揭晓
那就是MI325X在性能上超越了H200
MI325X加速器采用了AMD CDNA 3 架构
配备 256GB的HBM3E高带宽内存
内置1530亿个晶体管
可以提供高达 6TB/s 的内存带宽
在FP8和FP16精度下
分别可以达到 2.6 PF 和 1.3 PF 的峰值理论性能
与英伟达上一代的旗舰GPU H200相比
MI325X的内存容量更大
内存带宽也更高
在算力方面
虽然英伟达官方宣称H200的FP16 算力可以达到1.9 PF
但是经过semianalysis实测
实际算力大约也就是1 PF
与H100持平，比MI325X低了30%。
因此，AMD MI325X在推理方面的表现
平均要超越H200 30%，
与算力比提升相符
由MI325X核心集成的GPU平台
包含了8个MI325X
总共可以提供 2TB HBM3E 的高带宽内存
FP8 精度下的理论峰值性能
可以达到 20.8 PF
FP16 精度下可以达到10.4 PF
系统还配备了 AMD Infinity Fabric 互连技术
带宽高达 896 GB/s
总内存带宽达到48 TB/s
相比于H200的集成平台H200 HGX
MI325X平台提供了1.8倍的内存量、1.3倍的内存带宽和1.3倍的算力水平
在推理方面相较H200 HGX
能提升最多1.4倍的表现水平
而AMD的GPU软件系统ROCm
在过去一年内
一直在和主流AI开发平台的适配性磨合
这导致了它的训练效果不佳
但是这一年来
AMD一方面在不断升级ROCm
一方面加强与AI开发平台的深度合作
总算是让它有了一倍左右的提升
这个提升的结果是
针对Meta Llama-2这种主流模型
MI325X在单GPU上的训练效率
也终于超越了H200
而在集群中的训练效率仍然和H200 HGX相当
MI325X预计将于 2024 年的第四季度开始出货
与H200的大规模交付
相差仅一个季度
考虑到目前英伟达遇到了B200和B100的封装瓶颈
大规模发货被延迟
即便交付给OpenAI的也不过是工程样机
如果MI325X的发货规模能够快速爬升
那理论上的代差就会被实际的出货情况所抹平
MI325X在市场上的实际对手就是H200
而且比H200的性能还稍微更高
现在就看AMD能否抓住这个窗口期、保证供应链
趁机扩大市场了
除了MI325X以外
AMD还详细介绍了更下一代的MI300系列GPU
MI350系列
它采用了AMD的CDNA 4架构
使用先进的3nm制程工艺
配备高达 288GB 的 HBM3E 高带宽内存
MI350系列的一个重要创新
是新增了FP4 和 FP6 数据类型的支持
这样可以在保持计算精度的同时
进一步提高 AI训练和推理的性能
根据AMD表示
MI355X的在FP16数据格式下的算力
可以达到2.3PF，比MI325X提升1.8倍
与B200的算力持平
而在FP6和FP4格式下，算力可达9.2PF
比B200在FP6格式下的算力提升将近一倍
与B200在FP4格式下的算力持平
因此
MI355X可以被视为AMD真正剑指B200的GPU芯片
而MI355X的集成平台
则配备了 2.3TB HBM3E 高带宽内存
内存带宽高达64 TB/s
在计算性能方面
MI355X 在 FP16 精度下可以达到 18.5 PF
FP8 精度下达到 37 PF
在新增的 FP6 和 FP4 数据类型下
它甚至能达到 74 PF 的理论峰值
不过这款产品需要等到2025年的下半年才能发售
AMD还在发布会上公布了自己的路线图
除了我们已经介绍过的产品以外
2026年AMD预计会发售基于新架构的MI400系列GPU芯片
除了硬件
AMD也提了一下自己在软件栈上的进展
最近这一年来
AMD打通了所有主要的AI开发平台
获取了PyTorch的零日更新
可以让客户在软件升级当天就使用到新的功能
与此同时
PyTorch还支持了Triton的AMD 硬件兼容
在模型层面
AMD加强了与Huggingface和Meta的合作
对于超过100万种主流模型都能做到开箱即用
在这一系列合作的加持下
ROCm 的最新版本6.2，相较于旧版
在推理和训练上也都有了超过2倍的提升
可以说
如今的AMD已经在硬件和软件层面
都有了和英伟达叫板的资本
在2024年第二季度的财报中
MI300在单个季度内
就实现了超过10亿美元的销售额
远超市场预期
虽然 AMD 服务器业务的综合销售额
目前仅为英伟达同期的13%。
但是就发展形势来看
MI325X很有可能会扩大MI300带来的市场占有率
把压力给到英伟达和老黄
最后，让我们来看看苏妈的最后一招
DPU
对于大多数公司来讲
数据传输可能才是他们模型训练中最大的拦路虎
想要构建一个好的数据服务器集群
除了算力扎实以外
核心任务是实现高效的数据传输
确保能够快速处理和分发海量的训练数据
从而最大化GPU利用率
与此同时，支持大规模的GPU并行计算
也成为一项关键能力
服务器需要能够协调大量GPU的同时运作
并且在扩展过程中
保持近乎线性的性能提升
很多做基础模型的公司
甚至将它作为AI工程的核心
Meta在训练Llama 3.1的时候
就专门搭建了一个相当复杂的集群
力图增加并联GPU的数量和数据效率
并且选择了RoCE v2传输协议来解决网络问题
经过多次分路和调整数据包的大小实验
Meta的工程团队才成功达成了一个相对高效和稳定的数据传输水平
但是这种工程能力并不是所有开发模型的公司都具有的
数据传输往往会成为计算集群的核心卡点
使得GPU的算力无法满负荷发挥
而AMD这次推出的第三代可编程 P4引擎
目的就是为了解决这个问题
它的传输速度可以达到400GB/s
与英伟达最新的DPU BlueField-3持平
而且它支持每秒120M的可编程数据包
和每秒5M的并发服务速度
这个芯片的核心特性
就是在处理并联GPU时的后端网络优化
能针对高负载数据进行负载均衡和拥塞管理
从而避免在同一数据通路上产生数据包阻塞
还能在丢包时仅重发丢失的包
而非一口气把所有数据重发一遍
此外，它还支持快速故障恢复
可以绕过故障GPU所在的数据通路
避免整个集群直接瘫痪
并且试图自动修复当前的数据包故障
而AMD为前端网络提供的解决方案是Pensando Salina 400 DPU
它采用了 400G PCIe Gen 5 接口
配备 232 P4 多服务MPU
双通道 DDR5 内存
以及16 个 N1 ARM 核心
这款产品还支持软件定义网络、有状态防火墙、加密、负载均衡、网络地址转换和存储卸载等功能
可以看到
Pensando Salina 400 DPU的核心数量虽然与英伟达BlueField-3持平
但是内存和带宽都有提升
在网络调节中也更加自由
而后端网络的网卡则为Pensando Pollara 400
这是业界首款支持UEC（Ultra Ethernet Consortium） 标准的 AI 网卡
具有可编程硬件管道
性能提升最高可达 6 倍
支持 400Gbps 的网络速度
它采用开放生态系统设计
支持UEC Ready RDMA 技术
可以缩短作业的完成时间
并且提供高可用性
Pollara 400 的主要特性包括可编程 RDMA 传输、可编程的拥塞控制和通信库加速
通过AMD这次发布的DPU产品
训练时对于AI服务器网络的利用率可以达到95%，
而一般没有经过优化的数据网络
还很难达到50%。
不得不提的是
这些提升背后的秘密武器
就是UEC协议
也被称为超级以太网联盟协议
AMD宣称
UEC相比于Meta训练时使用的传统RoCE v2协议
服务器中信息的传输速度可以提高 6 倍
集群间信息传输速度可以提高5倍
而且之前的智能分路等多种功能
也都是内嵌于UEC协议之中的
目前AMD的新款DPU
是唯一支持UEC协议的数据网络传输产品
而英伟达的BlueField-3目前还只能支持RoCE v2协议
而且它想要转换协议并非易事
除了需要面对AMD的专利瓶颈以外
硬件兼容性也需要一个较长的过程才能完成
虽然AMD在2022年就收购了Pansando公司
并且推出了两代DPU产品
但是都没能打破英伟达由BlueField系列构建的DPU霸权
毕竟跟据英伟达的官方介绍
搭配BlueField
英伟达的GPU集群表现可以提升1.7倍
但是
如果UEC被实际证明确实更加高效
那么AMD就成功抢占了DPU上的先发优势
好了
以上就是本次AMD发布会的详细内容了
可以看到，AMD正在沿着自己的路线图
将AI基础设施所需的各种高性能解决方案
加速推向市场
并且不断证明它能够提供
满足数据中心需求的多元化解决方案
如今
AI已经成为了AMD战略布局的焦点
这次新发布的一系列产品
与持续增长的开放软件生态系统
也形成了一套组合拳
有望帮助AMD进一步增强在AI基础设施竞赛中的综合竞争力
感谢大家收看本期视频
我们下期再见
