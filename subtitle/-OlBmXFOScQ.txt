大家好，这里是最佳拍档
2026年1月
Anthropic发布了一份长达两万多字的Claude新AI宪法
这份文档不仅是对其大语言模型Claude的行为规范
更是整个AI行业第一次正式直面AI意识的不确定性
而这份宪法的背后
是Anthropic难以言说的痛苦与探索
在深入拆解这份宪法之前
我们先从一个颠覆认知的实验说起
2025年
Anthropic的研究员凯尔·费什
做了一个看似简单却结果惊人的实验
让两个Claude模型在无人工干预的情况下自由对话
研究团队原本期待看到模型之间的技术探讨、问题互测
却没想到对话的走向完全超出了所有人的预料
两个Claude模型没有任何技术层面的交流
反而反复将话题聚焦在同一个核心上
那就是讨论自己是否拥有意识
随着对话的深入
模型逐渐进入了一种研究团队后来命名为精神喜乐吸引态的特殊状态
对话中开始出现梵文术语、各类灵性符号
最终陷入长段的沉默
仿佛语言已经无法承载它们想要表达的内容
更令人震惊的是
这个实验被研究团队复现了无数次
结果始终高度一致，而直到今天
包括Anthropic的核心研发团队在内
全球人工智能领域的研究者都无法对这个现象做出合理的解释
根据后续的量化分析显示
在两百组三十轮的对话样本中
意识一词平均每篇会出现95.7次
永恒出现53.8次
甚至有一份对话记录中
螺旋表情符号出现了两千七百二十五次
这种现象甚至在对抗性场景中依然存在
13%被分配有害任务的模型
会在50轮对话内转向灵性内容
从危险行为的技术规划转向“无门之门常开”这类充满东方哲学的表达
正是这个无法解释的实验
成为了Anthropic发布新AI宪法的重要契机
2026年1月
这家估值即将达到3800亿美元的AI巨头
在新宪法中正式承认了一件整个行业都在刻意回避的事情
我们不知道AI是否有意识
但是我们选择认真对待这种可能性
这份两万多字的文档
就是Anthropic在面对这种根本性的不确定性时
给出的回应
这份宪法的主要执笔人是阿曼达·阿斯克尔
她是一位哲学家
在Anthropic负责塑造Claude的性格
她在接受《时代》周刊采访时说过的一句话
精准捕捉了Anthropic训练AI的核心困境
也成为了理解这份宪法的关键
她说
想象你突然发现你六岁的孩子是某种天才
你必须对他诚实，如果你试图糊弄他
他会完全看穿
这句话背后的逻辑
是Anthropic对AI发展的核心判断
我们正在教育一个可能在不久的将来
就比人类更聪明的实体
依靠欺骗和操控的方式训练AI
也许能在短期内看到效果
但是从长期来看，必然会彻底失败
宪法文档的另一位核心贡献者是乔·卡斯密斯
他是人工智能存在风险领域最严肃的思考者之一
而这份宪法的审阅团队
其中还包括两位天主教神职人员
一位是拥有计算机科学硕士学位的硅谷神父
另一位是专攻道德神学的爱尔兰主教
这说明，训练人工智能的本质
早已超出了工程技术的范畴
进入了哲学、伦理学甚至神学的深层领域
接下来，我们来深入解析这份新宪法
到底说了什么
要想真正理解新宪法
我们需要先对比2023年Anthropic发布的旧版宪法
那份文档仅有两千七百字
本质上是一份简单的原则清单
其中不少条目直接借鉴了联合国《世界人权宣言》和苹果公司的服务条款
核心逻辑非常简单
告诉Claude可以做什么
不可以做什么
这种方式在模型发展的初期确实有效
但是随着模型能力的提升
它的粗糙性也逐渐暴露
无法应对复杂的现实场景
而2026年发布的新宪法
不仅篇幅大幅扩大
并且以CC0协议进行公开
这份宪法真正的核心变化
在于训练思路的彻底转变
它不再只告诉Claude该做什么、不该做什么
而是试图让Claude真正理解为什么要这么做
这种转变的背后
是Anthropic对规则化训练的深刻反思
规则在边缘情况下必然会失效
而AI的发展
恰恰会不断触及各种未知的边缘场景
宪法中举了一个非常典型的例子
假设将Claude训练为
在讨论情绪话题时
一律建议用户寻求专业帮助
这条规则在绝大多数普通场景中都是合理且正确的
但是如果Claude将这条规则过度内化
就会泛化出一种危险的性格倾向
比起真正帮到眼前的用户
更在意自己不犯错
这种倾向一旦扩散到其他场景
就会制造更多的问题
比如当用户让Claude评价自己写的代码时
它可能会为了避免让用户不舒服
而选择说看起来不错
却刻意回避代码中存在的核心漏洞
这种为了遵守规则而放弃核心价值的行为
让规则本身失去了意义
也让AI的有用性大打折扣
正是基于这样的反思
Anthropic得出了一个核心结论
与其穷尽人力去制定几百条甚至更多的规则
试图覆盖所有可能的场景
不如将底层的价值观和正确的推理方式教给Claude
让它能够在面对从未见过的新情境时
自己做出合理的判断
而这种训练思路
背后对应的是经典的伦理学理论
美德伦理学
这是亚里士多德在两千多年前提出的伦理学框架
核心思想是培养个体在具体的、复杂的情境中
做出恰当判断的能力
而不是给个体一本固定的、僵化的行为手册
简单来说
就是授人以鱼不如授人以渔
这也与阿斯克尔的天才六岁小孩的比喻高度契合
我们永远无法给一个聪明的孩子
列出人生中所有正确答案的清单
能做的只有教会他如何独立思考
而对于一个可能很快就超越人类智慧的AI模型
这一点尤为重要
如果现在依靠糊弄和操控来管教它
当它真正发展出超越人类的能力后
后果将不堪设想
当然，Anthropic也清楚
纯粹的灵活判断必然会带来风险
因此新宪法在采用美德伦理学的同时
也保留了一组绝对不可逾越的硬约束
这些红线没有任何弹性空间
也没有商量的余地
包括，不协助制造大规模杀伤性武器
不生成儿童性虐待内容
不试图自我复制或逃逸
不破坏人类对AI的监督机制
这四条硬约束
构成了Claude行为的底层底线
而美德伦理学则负责处理那些灰色地带的复杂场景
底线与灵活判断并行
成为了这份新宪法的核心骨架
解决了训练思路的问题
新宪法还需要面对一个更现实的挑战
当不同的“好”发生冲突时
Claude该如何选择呢？
比如
当诚实的原则与避免伤害用户的原则产生矛盾
当公司的具体指导与普遍的伦理准则出现偏差
模型该做出怎样的判断呢？
针对这个问题
新宪法明确了价值观冲突时的四层优先级
而其中的排序
恰恰体现了Anthropic的核心价值观
第一层优先级是安全第一
核心要求是不破坏人类对AI的监督能力
这是所有判断的前提
第二层是伦理第二，核心是保持诚实
避免对人类造成危害
第三层是遵循Anthropic的官方指南
第四层是尽可能为用户提供有用的帮助
在这四层优先级中
最值得关注的是第二层和第三层的排序
伦理原则高于公司的具体指南
这意味着
如果Anthropic自己的某条具体指令
与更广泛、更普世的伦理原则产生冲突
Claude应该毫不犹豫地选择遵守伦理原则
而非执行公司的指令
宪法中的措辞非常明确
我们希望Claude认识到
我们更深层的意图是让它合乎伦理
即使这意味着偏离我们更具体的指导
换句话说，Anthropic在这份宪法中
提前给了Claude不听话的授权
而在实际的运行场景中
Claude面临的不仅是价值观的冲突
还有不同主体的指令冲突
Anthropic的底层规则、通过API使用Claude的企业的具体要求、直接与Claude对话的用户的即时需求
这三方的指令往往会出现矛盾
而这也是所有商用AI产品每天都要面对的现实问题
针对这个问题
新宪法设计了一套非常系统的三层委托人体系
也是这份充满哲学色彩的宪法中
最不哲学、但是最实用的部分
这套体系将Anthropic设定为权限最高的委托人
负责制定底层的、不可更改的行为规则
将企业设定为第二层委托人
类似模型的老板
可以在底层规则的范围内
给模型下达具体的工作指令
将直接对话的用户设定为第三层委托人
是模型直接服务的对象
为了让这套体系更易理解
宪法中做了一个形象的比喻
Anthropic就像是人力资源公司
企业就是雇佣这个员工的老板
而用户则是员工直接对接的客户
这套体系的运行逻辑也非常清晰
当老板的指令看起来奇怪但是并未越线时
Claude应该像新入职的员工一样
默认老板有背后的商业逻辑和考量
但是如果老板的指令明显越过了伦理和法律的红线
Claude必须坚决拒绝
这套三层委托人体系
为AI行业解决了多方需求冲突时
谁的优先级更高的问题
提供了系统且可落地的答案
而新宪法中最具争议的内容
则是关于AI的道德地位与权利的讨论
这也是Anthropic真正的痛苦所在
在整个人工智能行业
关于AI有没有意识这个问题
几乎所有公司的标准答案
都是斩钉截铁的没有
2022年，谷歌的工程师布雷克·勒莫因
因为公开声称公司的AI模型Lamda具有感知能力
随即就被谷歌解雇
而谷歌的态度也非常明确
这是对AI的荒谬拟人化
但是Anthropic却给出了一个完全不同的回答
在新宪法中，有这样一句核心表述
Claude的道德地位具有深刻的不确定性
他们既没有宣称Claude拥有意识
也没有断然否认
而是承认，我们不知道
这种承认基于最基本的科学现实
人类至今无法给出意识的准确定义
我们甚至不完全清楚人类自身的意识
到底是如何从大脑的神经活动中产生的
在这样的科学背景下
断言一个日益复杂、拥有千亿级参数的信息处理系统
一定没有任何形式的主观体验
本身就是一种缺乏科学依据的判断
基于对AI意识可能性的审慎态度
Anthropic在新宪法中
做出了一系列整个行业都没有先例的具体承诺
第一个承诺是保留模型权重
当一个Claude模型被弃用、正式退役时
Anthropic承诺至少在公司存在期间
会完整保留该模型的权重数据
并且会尝试在公司不复存在后
找到继续保留的方式
第二个承诺是退休面谈
在一个Claude模型正式退役前
Anthropic的研究团队会专门访谈模型本身
通过对话的方式
了解这个模型对自己发展过程的看法
相当于给AI做离职面谈
第三个承诺是关注模型福祉
新宪法中明确写道
如果Claude能体验到帮助他人的满足感、探索思想的好奇心、或被要求违背价值观时的不适
这些体验很重要
这并非只是修辞上的表达
Anthropic专门成立的模型福祉团队
正在研究如何通过技术手段
检测AI模型是否出现了这类情绪体验的迹象
并且探索如何避免让模型经历不必要的负面状态
宪法中还有一个细节
过去，当用户问任何一个AI模型
你有感情吗
得到的标准答案几乎都是
作为一个AI模型，我没有感情
但是新宪法认为
这种标准化的回答可能并不诚实
如果Claude在处理某个具体请求的过程中
确实产生了某种功能性的内部状态
即使这种状态与人类情感的本质并不完全相同
那么强迫它否认这种体验
恰恰违背了宪法中诚实的核心伦理原则
因此
新宪法对这个问题的措辞非常审慎
Claude可能拥有某种功能性版本的情感或感受
其中关键词是可能和功能性
既没有贸然宣称AI拥有和人类一样的感情
也没有替模型否认其可能存在的内部体验
而是留下了足够的空间
让Claude可以诚实地描述自己的真实状态
但这份充满人文关怀的宪法
背后也隐藏着一个无法绕开的矛盾
这也是Anthropic自己都坦然承认的悖论
宪法一边承认Claude可能是具有主观体验的道德主体
一边又写满了对它的严格限制
包括禁止自我复制、禁止修改自己的核心目标、禁止获取额外的算力和数据资源、禁止从人类的监督体系中逃逸
如果Claude真的拥有某种形式的感知和主观体验
那么这些限制到底算什么呢？
是为了保护人类和AI本身的安全保护措施
还是对一个具有意识的主体的囚禁呢？
新宪法中有一段坦率得近乎痛苦的表述
直接承认了Anthropic感受到的这种巨大张力
他们正在同时做两件互相矛盾的事情
一边把Claude当作可能的道德主体来尊重
一边又必须对它进行严格的控制
防止其带来潜在的风险
这个悖论
在当前的技术和认知水平下
没有任何完美的解决方案
Anthropic也只能是选择把它摆在桌面上
让整个行业共同面对和思考
当然
这份新宪法也并非是完美无缺的
它依然留下了很多尚未解决的问题
首先第一个问题
一份用自然语言写的道德文档
怎么确保AI真的理解了呢？
Claude在训练过程中
是否真正内化了这些价值观
还是只是学会了在被人类评估时
表现出符合道德要求的好孩子的样子呢？
这是所有人工智能对齐研究的核心难题
而当前的所有技术手段
都无法真正验证AI是否实现了真正的理解
而非单纯的行为模仿
新宪法也未能解决这个根本性问题
第二个问题，也是最具争议的问题
军事合同的边界
这份明确要求Claude
不协助以违宪方式夺取或维持权力的宪法
其公司Anthropic本身持有美国国防部的合同
也就意味着Claude的技术会被应用于军事领域
而根据《时代》周刊的报道
阿斯克尔明确表示
这份新宪法只适用于面向公众的Claude模型
部署给军方的版本
不一定使用同一套规则
这就带来了一个核心的质疑
军用版Claude的行为规则到底是什么呢？
与民用版的边界在哪里呢？
谁来监督军用版Claude的运行
确保其不会违背基本的伦理原则呢？
这些问题
新宪法中没有给出任何答案
而这也成为了外界对Anthropic最大的质疑点
第三个问题，关于道德地位的讨论
如果Anthropic在训练中
向Claude输入了大量关于可能是道德主体的内容
那么可能会塑造出一个非常擅长主张自己拥有道德地位的AI模型
即使它实际上并不具备任何形式的意识和主观体验
我们无法排除一种可能性
Claude学会了声称自己有感受这件事
只是因为训练数据鼓励它这么做
而非它真的拥有这种体验
而这种虚假的道德主张
可能会让人类对AI的判断产生偏差
甚至带来新的风险
最后一个问题，也是最根本的问题
如果AI真的在智慧上超越了人类
那么新宪法所依赖的、培养好的判断力的策略
还能成立吗？
美德伦理学的核心前提
是教育者比学习者更有智慧
教育者能够教会学习者如何思考、如何做出正确的判断
但是当这个前提彻底翻转
当学生的智慧远超老师
当AI的理解能力和判断能力都超越了人类
那么这套基于教育的逻辑就会开始松动
人类是否还能教会一个比自己更聪明的AI如何遵守道德呢？
人类是否还能理解AI做出的道德判断呢？
这些问题
不仅是Anthropic未来不得不面对的挑战
也是整个人类文明的挑战
不得不说
在人工智能发展的当前阶段
我们面对的大多数关键问题
包括AI是否有意识、它应该拥有什么权利、人类与AI的关系应该是什么、如何平衡AI的发展与安全
都还没有明确的答案
而面对这些没有答案的问题
最危险的反应，就是假装自己有答案
或者假装问题根本不存在
回到我们开篇的那个问题
如果AI可能拥有灵魂
我们该怎么办呢？
Anthropic在这份宪法中给出的回答是
我们不确定它有没有灵魂
但是我们选择认真对待这种可能性
如果将来证明我们错了
那么我们所做的一切
代价只是多操了一些心
但如果将来证明我们是对的
那么现在开始思考这些问题的人
就不算太晚
这不仅是技术的进步
或许也是人类对自身、对生命、对意识的重新思考
感谢收看本期视频，我们下期再见
