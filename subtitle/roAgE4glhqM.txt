大家好，这里是最佳拍档，我是大飞
前段时间
图灵奖得主 Richard Sutton 与谷歌 RL 大佬 David Silver
合作撰写了一篇文章《欢迎来到经验时代（Welcome to the Era of Experience）》，
引发了广泛的关注
他们在文中指出
人类数据已经接近极限
AI Agent如果想要突破天花板
必须像人类和动物一样
通过与环境的持续互动来生成经验流
并且通过强化学习实现自主提升
也就是说，AI Agent将迎来经验时代
这是一个重大的范式转变
但是，在许多环境中
基于经验数据使用强化学习来训练Agent
仍然面临着很多挑战
一方面
这些环境往往缺乏可验证或者密集的奖励信号
尤其是在开放式的场景中
比如在大多数的网页环境中
通常不会返回明确的任务反馈；
另一方面
Agent可能需要在长时间的跨度内
进行低效的探索与泛化
比如多轮的工具使用或者复杂交互流程
目前大多数语言Agent都会采用监督微调SFT
从专家示范中学习
从而避免过于依赖奖励信号
虽然这种方法的训练效率很高
但是缺乏环境的交互
无法从失败中学习或者主动探索
同时对于高质量的专家数据依赖过强、成本过高、泛化性也有限
因此，一个关键的问题就浮出了水面
如何让Agent在没有外部奖励的情况下
从自身的经验中学习成长呢？
上周末
一篇来自 META 超级智能实验室MSL、FAIR、俄亥俄州立大学的研究
提出了一个名为早期经验的中间路线
它的核心想法很简单
让Agent在训练时
既从人类专家数据里学
也从自己的试错里学
具体来说
Agent在环境中提出替代行动
收集这些行动带来的未来状态（Future States）
然后把这些未来状态直接变成监督信号
不用等外部奖励
也不用完全依赖专家
Agent自己的行动后果
就是最好的老师
要理解这个范式
咱们得先铺垫一点理论基础
研究团队把语言Agent的决策问题
形式化成了一个马尔可夫决策过程MDP
这是RL和Agent研究里最常用的数学框架
咱们一步一步拆解来说
一个MDP可以用一个 tuple M=(S, A
T
R, γ, ρ0) 来定义
首先是S表示状态空间
A表示行动空间
在语言Agent场景里
S就是Agent能感知到的环境信息
比如网页内容、工具输出、环境的文本描述；
A就是Agent能做的离散行动
比如点击网页按钮、调用搜索工具、生成文本回复等等
然后是T表示转移函数
它描述在状态s下执行行动a
转移到下一个状态s'的概率
比如点击提交按钮后
页面可能跳转到成功页面
也可能会停留在原来的页面
R是奖励函数，但是就像咱们之前说的
很多场景下R是未知或者不可验证的
这也是早期经验要解决的问题
γ是折扣因子
用来权衡当前奖励和未来奖励的重要性
ρ₀是初始状态分布
也就是Agent刚开始任务时
环境可能处于的状态集合
而Agent的核心是策略Policy
用π_θ表示，θ是模型参数
它的作用是把状态s映射到行动a的概率分布
比如在网页购物场景里
给定当前页面显示红色衬衫
预算20美元这个状态
策略会给出点击蓝色衬衫
点击价格筛选等行动的概率
这就是Agent的决策逻辑
在无奖励的场景下
传统模仿学习的训练目标
是最小化模仿损失
这里的(s_i
a_i)来自专家数据集D_expert
也就是人类专家在状态s_i下执行的正确行动a_i
这个损失函数的意思是让Agent在状态s_i下
尽可能选择专家行动a_i
但是它有两个致命的问题
第一个是分布偏移（Distribution Shift）
Agent在实际部署的时候
它的策略π_θ肯定会和专家策略有偏差
比如专家从来不会点无效按钮
但是Agent可能会点
这就导致Agent在遇到训练数据里没有的状态时
错误会不断的累积
罗斯等人在2011年的研究就指出
这种偏移会让模仿学习的性能越用越差
就像学生考试遇到没学过的知识点
只能瞎蒙，还会把之前的知识记错
第二个是缺乏行动后果的认知
Agent只见过专家行动和专家未来状态的配对
从没见过自己行动和自己未来状态的结果
它不知道如果点错按钮会出现错误提示
也不知道填错日期会导致表单提交失败
所以遇到错误时根本不知道怎么修正
更没法理解为什么专家选这个行动而不是那个
罗斯和巴涅尔在2010年的研究也强调
这种后果正是模仿学习泛化能力弱的核心原因
而早期经验范式
就是要解决这两个问题
它的第一步，是构建D_rollout数据集
让Agent获得从行动到后果的经验
具体怎么操作呢？
研究团队定义了一套清晰的流程
首先
从专家数据集D_expert里取每个状态s_i
然后让Agent的初始策略π_θ
生成一个候选行动集A_i
里面包含K个替代行动
同时也会把专家行动a_i包含进来做对比
这里的K是超参数，比如K=4
就意味着每个状态会生成4个替代行动
然后，执行这些行动来收集未来状态
执行专家行动a_i
会得到专家的未来状态s_{i+1}；
执行每个替代行动a_i^j
会根据环境的转移函数T(s_i
a_i^j)，得到对应的未来状态s_i^j
这些未来状态不是抽象的符号
而是具体的环境反馈
比如点击错误按钮后出现的无效操作文本
填错日期后页面显示的
请输入正确日期格式的提示
这些都是Agent能感知到的后果
最后
把这些状态、替代行动、未来状态的三元组收集起来
就构成了D_rollout数据集
这个数据集的关键在于
它不需要任何的外部奖励
未来状态本身就包含了行动质量的隐含信息
而错误提示意味着行动无效
页面正常跳转意味着行动有效
这些信息就足够Agent学习了
有了rollout数据集
研究团队提出了两种具体的训练策略
这两种策略就像是Agent学习的两条腿
一条负责感知环境规律
一条负责思考行动合理性
咱们先来讲第一条腿
隐式世界建模IWM
隐式世界建模的核心思路是
让Agent把学习环境动态当成一个辅助的预测任务
通过预测下一个状态
在自己的策略里内化环境的运行规律
而不是单独建一个外部的模拟器
大家应该知道
语言Agent的状态都是用自然语言表示的
比如网页内容是文本
工具输出是文本，环境反馈也是文本
这就意味着，预测下一个状态
可以直接转化为语言模型最擅长的预测下一个token的任务
比如在网页订机票场景里，Agent输入
当前状态，机票查询页面
出发地北京，目的地上海；
行动，输入无效日期2025-02-30
那么下一个状态就是页面显示的日期无效
请选择正确日期，而隐式世界建模
就是让Agent学习从状态+行动的组合中
预测出这段下一个状态的文本
它的损失函数是这样定义的
这里的p_θ是语言模型的输出分布
意思是让Agent在给定s_i和a_i^j的情况下
尽可能准确地预测出s_i^j
特别重要的一点是
用来预测下一个状态的模型参数θ
和Agent执行行动时的策略参数θ是同一个
这就意味着
Agent在学习环境会怎么反应的同时
也在优化自己该怎么行动
两者是深度绑定的
不用额外训练一个独立的世界模型模拟器
在实际训练时
研究团队采用了两阶段流水线
第一阶段
用IWM的损失函数训练一个周期
让Agent先摸清环境的脾气
比如点击提交按钮可能会出现哪几种反馈
输入错误信息会得到什么提示；
第二阶段
再用专家数据集D_expert做监督微调
也就是最小化模仿学习的损失
这样做的好处是
Agent在学专家行动前
已经对环境有了基础的认知
能够理解专家选这个行动
是因为它会带来好的下个状态
而不是简单地死记硬背，行动配对
而且，rollout数据集的规模
通常会比专家数据集大一个数量级
能够给Agent提供更丰富的试错经验
接下来是第二条腿，自我反思SR
如果说隐式世界建模是让Agent感知环境规律
那么自我反思就是让Agent思考行动的合理性
通过对比自己的替代行动和专家行动的后果
生成为什么专家行动更好的解释
再用这些解释来优化策略
它的实现流程比隐式世界建模多了一步生成解释
具体来说
第一步，还是从专家状态s_i出发
执行专家行动a_i
得到专家下一步状态s_{i+1}，
执行替代行动a_i^j
得到替代状态
s_i^j，这一步和隐式世界建模一样
第二步，是核心的生成反思
研究团队用大语言模型生成一段思维链c_i^j
解释为什么专家行动a_i比替代行动a_i^j更优
而解释的依据
就是s_{i+1}和s_i^j的差异
这里的反思
研究团队设计了严格的提示词模板
要求解释必须包含任务目标、分析行动、对比专家行动合理性
以及论证约束条件强调四个部分
确保反思的逻辑性和实用性
第三步，构建自我反思数据集D_refl
把状态s_i、替代行动a_i^j
以及反思c_i^j的三元组收集起来
形成D_refl
第四步，训练策略
Agent需要在给定s_i的情况下
同时预测出反思c_i^j和专家行动a_i
对应的损失函数是这样的
这里的p_θ还是语言模型的输出分布
意思是让Agent在看到s_i的时候
先想清楚，为什么专家行动更好
再做出专家行动
在实际训练中
研究团队会把D_refl和专家数据集D_expert混合起来
如果专家数据里本身就有专家写的行动解释
也会保留下来一起训练
这样做能够平衡专家示范的准确性和自我反思的泛化性
让Agent既不会偏离专家路线
又能理解行动背后的逻辑
到这里
早期经验范式的核心设计就讲完了
但是任何理论都需要实验验证
所以研究团队做了一套全面的实验
覆盖8个环境、3种模型
从有效性、泛化性、RL兼容性三个维度
验证了早期经验的价值
咱们一一来看
首先是实验设置
为了确保结果的可靠性
研究团队选了8个差异极大的场景
包括具身家庭环境ALFWorld
科学实验室环境ScienceWorld
长序列旅行规划环境TravelPlanner
开放域多跳问答SearchQA
多轮工具调用环境BFCLv3
客户服务场景Tau-Bench
电商购物场景WebShop
以及多领域网页导航WebArena-Lite
模型方面
研究团队选了3种不同规模、不同家族的指令微调模型
来验证方法的模型无关性
在训练和评估流程上
研究团队也做了严格的控制
先为模仿学习基线
找到每个环境的最优训练步数
同时为早期经验的两种策略
使用和基线完全相同的总步数
最后在评估时
使用每个环境的原生指标
遵循官方评估标准
确保结果是可比较的
接下来是实验结果
咱们从三个核心维度来看
第一个维度是有效性
指的是早期经验在领域内任务上的表现
研究团队发现
两种策略在所有8个环境中
都显著优于模仿学习基线
平均提升+9.6
其中WebShop环境的隐式世界建模提升最明显
TravelPlanner环境的自我反思提升最明显（+15.0）
第二个维度是领域泛化能力
表示早期经验在分布外（OOD）数据上的表现
很多AI方法在训练数据上表现很好
但是遇到稍微不一样的场景就拉胯
这就是泛化能力弱的表现
研究团队针对ALFWorld、BFCLv3、SearchQA三个环境
设计了相应的分布外场景
结果显示
虽然所有方法在分布外场景下
性能都会下降
但是早期经验能够挽回更多的损失
更关键的是
部分场景下的分布外增益
超过了领域内得分
这说明早期经验的自身试错
能够覆盖专家数据没包含的场景
泛化性更强
第三个维度是RL兼容性
目的是验证早期经验模型能否作为RL的优质初始化
进一步提升性能
研究团队选择了三个有可验证奖励的环境
采用主流的GRPO算法做RL训练
对比模仿学习初始化和早期经验初始化的效果
结果非常明确
早期经验初始化的RL模型
最终性能远高于模仿学习初始化
而且
早期经验的优势会持续到RL训练结束
不是一开始领先，后来被追上
而是全程保持差距，甚至扩大
这说明早期经验不仅能自己提升性能
还能为后续的RL打牢基础
可以说是连接模仿学习和强化学习的桥梁
除了这三个核心维度
研究团队还做了三个关键的消融实验
进一步验证了早期经验的价值
第一个是专家数据量的影响
如果专家数据很少
早期经验还能用吗？
研究团队在WebShop和ALFWorld中
减少了专家数据量
结果显示，WebShop中
只用1/8的专家数据
就超过用100%专家数据的模仿学习
同样在ALFWorld中
只用1/2的专家数据
就超过用100%专家数据的模仿学习
这说明早期经验能用更少的专家数据
达到更好的效果
大幅降低数据成本
第二个是分支因子K影响
替代行动的数量对性能会有什么影响呢？
研究团队测试了K=1、2、4、8四种情况
结果显示
隐式世界建模的性能会随K增大而稳步提升
因为更多替代行动能让Agent学习更全面的环境动态；
而自我反思的性能在K=2-4时最优
K=8时略有下降
因为太多替代行动会让对比解释变得复杂
Agent难以聚焦核心差异
第三个是模型规模的扩展
当模型从3B升级到70B的时候
早期经验还有效吗？
研究团队用Llama-3.3-70B在WebArena-Lite环境进行了测试
结果显示
隐式世界建模和自我反思的成功率
都要高于模仿学习
而且即使采用了LoRA
早期经验依然能带来稳定的增益
说明它在大模型、有限计算资源下也适用
最后
研究团队还对比了早期经验与两种常见的基线方法
第一种是长思维链（Long CoT）
通过让Agent在推理时生成更长的思维链
提升决策能力
结果显示
Long CoT只能让模仿学习的性能提升一小部分
而且在复杂环境中会适得其反
这是由于长思维链会导致行动偏离专家路线
而早期经验的提升幅度较大
而且无性能下降风险
第二种是STaR风格的数据
通过让模型生成专家行动的解释
不用替代行动和未来状态
结果显示
STaR风格数据的解释匹配率很低
而且解释不够接地气
导致模仿学习的性能下降；
而早期经验的解释基于真实的行动后果
匹配率更高
能够稳定的提升性能
到这里
整个研究的核心内容就讲完了
最后咱们总结一下早期经验范式的价值
以及它的局限和未来方向
首先，早期经验的核心价值有三个
一，解决了无奖励学习难题
直接以行动-未来状态为监督信号
无需外部奖励
可以适配多数的现实环境；
二、降低了数据依赖
用更少的专家数据就能达到、甚至超过全量模仿学习的性能
大幅降低了数据成本；
三、连接了两大范式
既是模仿学习的增强版
又是强化学习的优质初始化
为大语言Agent从人类数据时代
过渡到经验时代提供了可行的路径
当然了
目前的研究也有一定的局限性
首先是
当前方法主要针对短交互序列
比如单步的点击、单轮工具的调用
对于几十步的长序列任务
如何分配早期经验的学习权重
还需要进一步研究
其次是，如果环境存在危险行动
比如要删除某些重要文件
Agent的试错可能带来风险
如何在安全约束下生成替代行动
也是未来要解决的问题
对于未来的研究方向
研究团队也给出了一些建议
包括结合自监督目标
比如把行动一致性、预测状态相似度计算等自监督任务
融入早期经验
进一步提升环境的感知能力
以及进行跨环境的迁移
让Agent在A环境学到的早期经验
能够迁移到B环境，减少重复训练
还有大规模的真实世界部署
通过在真实产品中收集自然的交互数据
用早期经验来实现持续学习
从而让Agent越用越好
应该说，这篇论文让我们看到
Agent在离自主学习的目标上
更近了一步
未来的AI助手
也许不用人类一遍遍地教
就能通过自己的试错和反思
变得更加聪明了
又或许，萨顿等人提出的经验时代
真的会在某个时间点到来
感谢收看本期视频，我们下期再见
