大家好，这里是最佳拍档，我是大飞
大家知道目前全球有多少款AI加速器吗？
全球总共又有多少AI算力？
以及这些算力的增长速度有多快呢？
想了解这些问题的答案
我们可以来看Epoch AI最新更新的全球AI算力估算报告
针对全球超过140款AI加速器
深入呈现全球机器学习硬件的现状和趋势
这份报告不仅揭示了硬件性能的提升规律、成本效益的变化
还展现了不同硬件在模型训练中的应用情况
以及各大科技公司的算力储备
对理解机器学习硬件的发展轨迹和未来走向
具有一定的意义
今天大飞我就来给大家解读一下
我们先来个省流版
总结一下报告的五个主要结论
一、AI计算性能每年增长43%，
价格下降30%。
二、低精度计算成为主流
三、顶级硬件的能效，每1.9年翻一番
四、八年间
训练大型模型所需要的处理器数量
增加了20多倍
五、全球英伟达支持的计算能力
平均每10个月会翻一倍
好了，接下来是报告的详细内容
首先，机器学习硬件的计算性能
正在以惊人的速度提升
其中一个显著的规律是
以16位浮点运算衡量
性能每年会增长43%，
每1.9年便能够实现翻倍
而32位的性能也呈现出类似的发展趋势
这个飞跃式的进步
得益于多方面的因素
包括晶体管数量的不断增加
以及半导体制造工艺的持续改进
都可以说是功不可没
同时
针对人工智能工作负载的专门设计
也为性能提升注入了强大的动力
比方说，英伟达的A100和H100等GPU
以及谷歌的TPU系列
它们在架构设计上充分考虑了AI计算的特点
优化了数据的处理流程
使得计算效率得以大幅提高
这种性能提升带来了许多积极的影响
首先，它降低了每一次浮点运算
也就是FLOP的成本
在早期
进行大规模的AI训练需要高昂的硬件成本
使得许多研究和应用都受到限制
而如今
随着硬件性能的提升和成本的降低
更多的科研机构和企业开始能够负担得起AI训练
从而促进了AI技术的广泛应用和发展
其次，能源效率也得到了显著提高
以前
强大的计算性能往往伴随着高能耗
而现在新的硬件在提升性能的同时
还降低了能源消耗
这对于可持续发展的计算环境至关重要
例如
Meta的MTIA和英伟达的H100在tensor - FP16格式下
展现出了极高的能源效率
分别达到每瓦特2.1×10¹²FLOP/s和1.4×10¹²FLOP/s
这意味着在相同的能源消耗下
它们能够完成更多的计算任务
此外
强大的计算性能也为大规模AI训练提供了可能
像OpenAI训练GPT - 4、谷歌训练Gemini Ultra等大型模型
都依赖于高性能的硬件支持
才得以处理海量的数据和复杂的计算任务
推动AI技术向更高层次发展
其次，在性能提升的同时
机器学习硬件的性价比也在不断优化
数据显示
硬件的性价比每年提升大约30%，
也就是说
在相同的精度和固定的性能水平下
硬件价格每年降低30%。
这个趋势让企业和研究机构在获取计算资源时拥有了更多的选择
能够以更低的成本获得更高的性能
以英伟达的数据中心GPU为例
从P100到V100，再到A100和H100
每美元每秒的浮点运算次数一直在不断增加
在P100时期
每美元每秒的浮点运算次数为1×10⁹
而到了H100
这个数值达到了1.5×10¹⁰
性能提升十分显著
这意味着企业在购买硬件的时候
可以用同样的预算获得更强大的计算能力
或者以更低的成本来维持相同的计算性能
不过，值得注意的是
在硬件性价比提升的同时
制造商也在不断推出更强大、但是价格更昂贵的硬件
这些高端硬件虽然价格高昂
但是往往具备更先进的技术和更高的性能
能够满足一些对计算要求极高的特定场景
比如大型互联网公司的云端服务、前沿的科研项目等
对于那些追求极致性能的用户来说
这些高端硬件依然具有很大的吸引力
而且，随着技术的进步和市场的竞争
高端硬件的价格也会逐渐趋于合理
进一步扩大其应用范围
另一方面，随着技术的不断发展
未来的硬件有望在能源效率上实现更大的突破
例如
即将推出的Blackwell系列处理器
如果能在功耗上进行有效控制
可能会拥有更高的能源效率
对于数据中心而言
能源效率的提升意味着可以在相同的能源供应下
运行更多的硬件设备
提高计算资源的利用率
同时减少散热等配套设施的成本
进一步地降低运营成本
第三，在机器学习硬件的发展过程中
数据精度格式的变化对性能产生了深远影响
与使用非张量的FP32相比
TF32、tensor-FP16和tensor-INT8
在整体性能趋势上平均分别有大约6倍、10倍和12倍的性能提升
以H100芯片为例
它在INT8格式下的运算速度比FP32快59倍
这种巨大的性能差距
充分展示了低精度格式的优势
随着硬件对低精度格式的支持和优化
使用较低精度格式来训练模型也变得越来越普遍
其中以tensor-FP16尤为突出
这是因为低精度格式在保证模型性能的前提下
能够大大减少计算量和存储需求
在深度学习模型训练中
数据的存储和处理往往需要消耗大量的资源
而低精度格式的数据占用空间更小
能够在相同的硬件条件下存储更多的数据
同时在计算时也能更快地处理数据
提高训练效率
而且，随着技术的不断发展
低精度格式在模型训练中的表现也在逐渐接近高精度格式
使得它在实际应用中得到了广泛的认可
第四，在模型训练领域
英伟达的A100已经成为训练知名机器学习模型的热门硬件
根据Epoch的数据显示
自A100发布以来
它已经被用在了训练65个知名的机器学习模型
在高引用或者最先进的AI模型训练中发挥了重要作用
排在第二位的是英伟达的V100
用来训练55个知名模型
谷歌的TPU v3则以47个位列第三
A100之所以如此受欢迎
不仅得益于其强大的计算性能
以及对多种精度格式的良好支持和广泛的生态系统
它能够满足不同规模和类型的模型训练需求
无论是小型科研项目还是大型商业应用
都能提供稳定可靠的计算支持
不过，根据Epoch AI的数据显示
市场格局也正在发生变化
据估计，到2023年底
英伟达H100的销量已经超过了A100
H100在性能上相较于A100有了进一步的提升
采用了更为先进的架构和技术
在处理大规模、复杂的模型训练任务时
表现更为出色
随着H100的普及，在不久的将来
它很有可能成为训练模型的最主流GPU
引领机器学习硬件的新潮流
自从2019年以来
英伟达芯片的总可用计算能力
正在以每年约2.3倍的速度增长
这为训练越来越大的模型提供了坚实的基础
英伟达的Hopper一代AI芯片
目前在其所有AI硬件总计算能力中的占比为77%，
已经成为主导力量
目前估计NVIDIA GPU可提供4e21 FLOP/s的计算能力
大约相当于400万个H100
随着新的芯片的不断推出和技术的进步
旧一代芯片在总计算能力中的占比将会逐渐下降
通常在推出大约4年后
对累计计算能力的贡献会降到不到一半
这种计算能力的快速增长和芯片代际的更新换代
正推动了AI技术的持续发展
新芯片在性能、功能和效率上的提升
使得科研人员和企业能够尝试训练更复杂、规模更大的模型
探索AI技术的更多可能性
同时，也促使AI应用场景不断拓展
从语音识别、图像识别到自然语言处理、自动驾驶等领域
都因为强大的计算能力而取得显著的进展
在集群规模方面，从2016年到2024年
AI训练集群的规模开始呈现出了爆发式的增长
2016年，谷歌的NASv3 RL网络训练
仅仅使用了800个GPU，而到了2024年
Meta的Llama 3.1 405B模型训练
则使用了16384个H100 GPU
处理器使用数量增长超过了20倍
虽然谷歌训练Gemini Ultra使用的TPU数量更多
但是具体细节没有公开披露
这种集群规模的扩张
反映了AI模型训练对计算资源的需求呈指数级的增长
随着模型规模的不断扩大
比如GPT - 4、PaLM等大型模型的出现
需要处理的数据量和计算复杂度也大幅增加
单个硬件已经无法满足训练需求
因此需要通过集群的方式将多个硬件组合起来
提供强大的计算力支持
同时，大规模集群的出现
也对硬件的协同工作能力和系统的管理调度
提出了更高的要求
为了充分发挥集群的性能优势
就需要优化硬件之间的通信架构
减少数据传输延迟
同时开发高效的集群管理软件
实现对大量硬件资源的合理分配和调度
确保模型训练能够高效稳定地进行
第五，在算力储备方面
全球一些领先的科技公司
比如谷歌、微软、Meta和亚马逊
拥有着令人惊叹的AI计算能力
算力相当于数十万个英伟达H100的计算能力总和
这些算力不仅用于公司内部的AI研发
还通过云计算服务提供给众多的客户
包括OpenAI和Anthropic等知名的AI实验室
谷歌凭借TPU的大规模部署
可能拥有相当于超过一百万个H100的算力；
微软则可能拥有大约50万个H100等效的英伟达加速器
是拥有英伟达加速器数量最多的公司之一
除了这四家科技巨头以外
还有许多其他公司也拥有大量的AI计算力
比如甲骨文（Oracle）和CoreWeave等云计算公司、特斯拉（Tesla）和xAI等计算大户
以及各国政府
它们在AI计算领域同样扮演着重要角色
随着AI技术的不断发展
算力已经成为科技公司竞争的核心资源之一
拥有强大的算力意味着能够在AI研发、产品应用等方面占据领先地位
推动技术创新和业务发展
好了
以上就是这份报告的主要内容了
展示了机器学习硬件在性能、性价比、精度、能源效率、应用以及算力储备等方面的发展现状和趋势
同时
Epoch也公布了这份报告背后数据集和数据分析源代码
大家可以在视频简介中的链接地址中查看
感谢大家收看本期视频
我们下期再见
