大家好，这里是最佳拍档，我是大飞
最近DeepSeek的每一个举动
都能引发全球科技圈的震动
就在大家还在说DeepSeek R1缺少多模态能力的时候
北京时间2025年1月28日的凌晨
DeepSeek团队又发布了两款多模态模型
Janus Pro和JanusFlow
为大家献上了春节大礼包
今天
大飞就来给大家介绍一下这两个模型
看看它到底有何过人之处
又会给多模态领域带来哪些变革
Janus这个名字，中文翻译为雅努斯
来源于古罗马人的门神
传说他有两副面孔
一个在前，一个在脑后
一副看着过去，一副看着未来
所以也有“双头雅努斯”的说法
雅努斯执掌着开始和入门
也执掌着出口和结束
象征着世界上矛盾的万事万物
DeepSeek选择这个名字
其实有很强的寓意
这个等我们后面介绍到技术细节的时候
大家自然会明白
简单来说
Janus Pro是一款统一的多模态理解与生成的框架
是去年10月发布的Janus模型的升级版本
在图像生成基准测试中
仅仅用1B和7B的参数规模
就超越了DALL-E 3 与 Stable Diffusion
再一次用小模型颠覆了大模型的统治地位
同时它和之前的Janus模型一样
再次选择了开源
DeepSeek再一次以开放的态度
迎接全世界的关注
要想理解Janus Pro的创新之处
我们得先回顾一下多模态大一统模型的发展历程
多模态大一统模型的理念最早是由谷歌提出
Gemini就是代表之作
它运用Transformer架构
将文本、图像、音频等多种模态的数据进行统一处理
让模型能够同时实现对不同模态信息的理解与生成
这个创新的架构
打破了传统模型只能处理单一模态数据的局限
为多模态的融合发展开辟了新的方向
而在此之前
像Stable Diffusion、Dall-E这类主流的文生图模型
在处理文本和图像的时候
都需要另一套模型去理解文本
它们自身只管生成
这就导致需要维护多个完整模型
不仅占用大量得存储空间和计算资源
模型之间还无法共享学习到的知识
而像GPT - 4V这类模型
虽然能够理解图像并转译为文字
但是却无法生成图像
既然大一统多模态模型有着许多优势
那为什么没有快速取代传统的流水线模型呢？
这是因为这类模型既难训练
效果又不够好
就拿Deepseek最初的尝试来说
他们采用统一的Transformer架构来处理文生图任务
理论上
用同一个模型、一个多模态编码器来理解文本输入并生成图像
应该是一种很优雅的设计
但是在实际操作中
却遇到了严重的性能瓶颈
像智谱的CogVLM
就尝试过用单一的ViT解码器
来处理视觉理解和生成任务
通过特征融合来协调不同的任务
然而在高分辨率图像生成的时候
统一模型的计算复杂度会呈指数级增长
需要海量的多模态数据
训练过程还难以收敛
而且，模型在优化文本理解的时候
还会损害图像的生成能力，反之亦然
这种能力干扰反而成了统一架构的致命问题
为了解决这些难题
杨立昆和谢赛宁团队在MetaMorph项目
进行了大胆的创新
放弃了“编码器大一统”的设计理念
采用“专门化”方案
给模型配置两个不同的编码器
虽然架构上没有单一的编码器那样优雅
但是仍然可以在同一个Transformer架构中完成
所以还算是“大一统”里的“小分工”。
DeepSeek的Janus Pro也采用了类似、但是更为彻底的方案
正如雅努斯的两张脸
在Janus Pro中，第一张脸
SigLIP编码器，专门负责理解图像
它能够精准提取图像的高层语义特征
关注图像的整体含义和场景关系
迅速抓住图像的要点
第二张脸，VQ tokenizer编码器
则专注于创作
将图像转换为离散的token序列
精心处理图像的细节
这两张脸虽然各司其职
但是它们都共享同一个“大脑”，
Transformer
通过给Transformer加上图像理解的注意力头
DeepSeek让两个编码器的知识实现了融合
与DeepSeek从头开始训练不同
Meta是给已有的语言模型加上视觉注意力头和视觉编码
再经过大约20万张图文对的微调训练
唤醒大语言模型自有的图像理解能力
而DeepSeek则更进一步
在图像方面使用生成和理解两个解码器
实现了图像生成和理解的大一统
这听起来好像很简单
无非就是不再执着于统一的编码模式
但是这个想法确实颠覆性的
过去的大一统模型主要是受到人脑的启发
认为通用智能应该有统一的信息处理机制
期望通过统一架构
来发现模态间的深层联系
从而实现真正的跨模态理解
而不是表面的特征映射
然而
大家低估了Transformer本身的能力
就算使用不同的处理器
Transformer依然可以理解这些信息的本质
除了架构创新以外
Janus Pro在训练策略上也进行了大胆的创新
它采用三段式的训练方法
每个阶段都进行了独特的优化
在第一阶段
传统的多模态AI训练只是进行预热
主要通过预训练视觉编码器
来学习基础的视觉特征提取能力
通常只占总训练时间的15%左右
但是DeepSeek研究团队发现了一个反直觉的现象
即使将大语言模型的参数完全锁定
只训练适配器
模型也能掌握复杂的像素依赖关系
这样不仅大幅降低了训练成本和复杂度
还能显著提升性能
基于这个发型
他们将第一阶段的训练时间
延长到了总时长的25 - 30%，
使得模型的基础视觉理解能力
实现了质的飞跃
到了第二阶段
也就是多模态AI训练的“模态对齐阶段”，
传统方法会同时训练视觉和语言模型来实现模态的对齐
这个过程通常会消耗超过50%的训练时间
占用大量的计算资源
我们都知道，长期以来
ImageNet数据集在视觉模型训练中占据着重要地位
几乎所有的视觉模型都要在ImageNet上进行训练
因此在传统训练流程中
高达67%的训练步数都用在了ImageNet上
然而
DeepSeek团队发现ImageNet的数据分布
其实与实际的应用场景差异很大
大量训练是无效的
造成了严重的资源浪费
于是
他们又做出了一个颠覆性的决定
完全放弃在第二阶段使用ImageNet
改为直接使用真实的文生图数据进行训练
这个改变效果非常显著
直接让训练时间减少了40%，
生成质量提升了35%，
模型对真实场景的适应性也大幅提高
而在多模态模型训练的第三阶段
因为要使用任务相关的数据集来微调模型参数
所以对模型的最终表现起着关键作用
在传统方法中
多模态数据、纯文本数据和文生图数据的配比通常是7:
3:10
DeepSeek团队通过大量的实验
发现了一个更优的配比方案
于是将这三类数据调整为5:1:
4的比例
就好像是西方炼金术士发现了一个神秘的东方配方一般
在文生图的数据部分
他们还创新性地引入了合成美学数据
与真实数据形成了1:1的配比
这样带来的好处是，模型收敛更快
生成的结果更加稳定
输出图像的美学质量也得到了显著提升
通过这三个阶段的创新训练方法
Janus Pro 7B模型再次上演了算力极限
仅用了32个节点、256张A100、14天的时间就完成了训练
1.5B模型更是只用了一半
16个节点和7天时间
那么
经过一系列的创新设计和训练优化
Janus Pro的性能表现究竟如何呢？
在实验设置上
Janus Pro使用DeepSeek LLM 1.5B和7B参数
作为基础语言模型
最大支持序列长度为4096
理解任务中使用SigLIP-Large-Patch16-384作为视觉编码器
生成编码器的码本大小为16384
并且将图像下采样16倍
理解适配器和生成适配器均为两层MLP
实验结果表明
在多模态理解MMBench测试中
Janus-Pro-7B获得了79.2分的好成绩
超越了之前Janus的69.4分、TokenFlow的68.9分和MetaMorph的75.2分
在图像生成评测方面
Janus-Pro-7B在GenEval基准测试中
达到了0.80分
大幅领先于DALL-E 3的0.67分和Stable Diffusion 3 Medium的0.74分
在DPG-Bench测试中
Janus-Pro-7B获得了84.19分
超越了所有其他方法
这表明它在遵循密集提示进行文本到图像生成方面表现出色
从实际使用效果来看
Janus Pro的多模态理解和图像生成能力也可圈可点
在多模态理解方面
论文展示了多个范例
比如在地标识别中
它能准确识别杭州西湖的三潭印月景区
不仅能描述眼前的景象
还能理解更深层的文化内涵和历史意义
在文本理解上
面对一块写有“自二十一世纪以来服务灵魂”的黑板
它不仅能准确识别主要文字
还能注意到周边的细节信息
在上下文理解方面
解读猫和老鼠主题蛋糕的时候
它能够深入理解动画角色设定、造型特点
并且准确描述蛋糕上的设计元素
在图像生成方面
它展示了八个不同场景的生成效果
涵盖现实与想象两个维度
虽然输出分辨率仅为384×384
但是每一幅画面都展现出了细致的细节和准确的语义理解
不过，Janus Pro也并非完美无缺
在多模态理解方面
它的输入分辨率被限制在了384×384
这在一定程度上影响了在OCR等细粒度任务中的表现
对于文本到图像生成
低分辨率以及视觉分词器引入的重建损失
使得生成的图像虽然语义内容丰富
但是仍然缺乏精细的细节
比如面部区域可能会显得细节不足
在未来
提高图像的分辨率或许可以缓解这些问题
应该说，Janus Pro的出现
对于多模态大一统模型的发展具有重要意义
它通过创新的架构设计和解耦的训练策略
首次证明了“理解”和“生成”这两个分离的任务
可以在一个统一框架下达到各自的最优状态
有趣的是
传统的大一统模型声称受到人脑的启发
但是却忽略了人脑功能分区与整合的辩证关系
而Janus Pro的架构设计仿佛才是真的在向人脑学习
图像理解编码器类似左脑的分析功能
图像生成编码器则类似右脑的艺术创造能力
Transformer则像胼胝体一样
将两路信息进行深度的统合
从而实现对多模态数据的统一理解
在发布 Janus Pro 的同时
DeepSeek 还发布了一个多模态理解模型JanusFlow-1.3B
JanusFlow的论文其实早在去年11月就已经发布
简单来说
它将基于视觉编码器和大语言模型的理解框架
与基于校正流Rectified Flow的生成框架直接融合
实现了两者在单一大语言模型中的端到端训练
DeepSeek的研究表明
校正流可以在大语言模型框架内直接训练
无需进行复杂的架构修改
为了进一步提高统一模型的性能
他们还采用了两种关键策略
一是将理解和生成编码器解耦
二是在统一训练期间对齐它们的表征
显著提升校正流的训练效率
在训练阶段
JanusFlow依然采用了三阶段训练方式
将训练分为随机初始化组件的适应阶段
统一预训练阶段和监督微调三个阶段
在第一个阶段
只训练随机初始化的组件
包括线性层、生成编码器和生成解码器
第二个阶段，训练整个模型
但是不包括视觉编码器
训练数据包括三种类型
分别是多模态理解、图像生成和仅文本数据
最初分配较高比例的多模态理解数据
来建立模型的理解能力
随后逐步增加图像生成数据的比例
来满足基于扩散模型的收敛需求
最后阶段
使用指令调优数据对预训练模型进行微调
包括对话、任务特定的交流
以及高质量的、基于文本的图像生成示例
这个阶段还需要解冻 SigLIP编码器参数
让模型能够有效地响应用户指令
完成多模态理解和图像生成任务
当时的实验结果表明
JanusFlow在总体得分上达到了0.63
超过了之前的统一框架以及多个生成特定模型
包括SDXL和DALL-E 2
好了
以上就是对DeepSeek的Janus Pro和JanusFLow的简单解读
DeepSeek再一次在架构创新、训练策略和算力优化等方面
为多模态大一统模型的发展指明了新的方向
虽然目前还存在一些局限性
但是随着技术的不断发展和优化
相信多模态大一统模型也会越来越完善
也期待看到DeepSeek给我们带来更多的技术突破
今天呢也是中国的农历新年除夕
辞旧迎新
大飞特意买了一件红色衣服
来录的这个视频
衷心感谢所有观众朋友
对频道和我一直以来的包容和支持
希望呢给大家带来更多好的节目
也祝愿大家在新的一年红红火火
万事如意
感谢大家的观看
我们下期再见
