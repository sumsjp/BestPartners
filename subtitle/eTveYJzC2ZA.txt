大家好
这里是最佳拍档
我是大飞
相信大家都能够感觉到
AI绘画这个领域的发展呢
实在是太快了
要知道去年的AI绘画还是这个画风
不过呢到了今年
已经迅速进化到了这样的程度
堪称云泥之别
关于近期各种效果惊人的模型
已经有不少人都介绍过了
但是大部分的技术点
都比较晦涩难懂
所以我今天尽可能的
用通俗直白的方式
来给大家解释一下AI绘画的原理
希望能对大家有所帮助
视频中涉及的概念呢可能会包括
VAE auto-encoder
GAN diffusion model等等
但是大家先不要被这样的词所吓住
我会尽量用浅显易懂的方式
来解释清楚
即使你是小白也应该可以听得懂的
好了首先我们先来了解一下
计算机是如何生成图画的
这里边呢就会涉及到两个方面
一个是能够生成像真实图片一样的数据
一个是要能够听得懂我们想要他干什么
并且给出对应的结果
大部分的人呢
可能都只关注到了前一个
而忽略了后一个
不过呢没有关系
我们先来看看
如何生成一个像真实图片一样的数据
这就涉及到机器学习中的一个重要分支
也就是生成式模型Generative Model
通常呢一个生成式模型
需要先吞进大量的训练数据
也就是海量的人类所产生的真实图片
然后再去学习这些数据的分布
去模仿着生成一样的结果
其实呢机器学习的核心呢
无非也就是这么回事
难点呢终究还是在如何设计模型
以及让模型能够更好的学习到分布上
讲到生成式模型呢
有一个不得不提的技术就是VAE
全称是Variational Auto Encoder
中文呢我们称为变分自编码器
这其中的auto encoder
也就是所谓的自编码器
其实也包含了
编码器encoder和解码器decoder
它是一个对称的网络结构
对于一系列类似的数据呢
例如图片来说
虽然整体的数据量可能很大
但是如果符合一定分布规律
那么它们包含的信息量
其实会远小于数据量
编码器的目的呢
就是把数据量为n维的数据
压缩成更小的k维特征
这k维的特征呢
尽可能的包含了
原始数据里的所有信息
而你只需要用对应的解码器
就可以转换回原来的数据
所以在训练的过程中呢
数据是通过编码器压缩
然后再通过解码器解压
然后尽量让重建后的数据
和原始数据的差距最小化
不过训练好的以后
就只有编码器被拿来当做
特征提取的工具
用于进一步的工作
例如图像分类等等
所以呢我们称之为auto encoder
那这时候有人就会想到
既然auto encoder可以从k维特征向量
恢复出一整张图片
那给你一个随机生成的k维特征向量
是否也可以随机生成什么画面呢
这个想法很好
不过实际的结果显示
auto encoder虽然可以记住见过的照片
但是生成新的图像的能力很差
于是呢就有了verational auto encoder
VAE
VAE可以让k维特征中的每个值
都变成符合高斯分布的概率值
于是概率的改变可以让图片信息
也有相应的平滑的改变
例如通过控制某个代表性别的维度
让它从0变到1
就可以从一个男性的人脸开始
生成越来越女性化的人脸
VAE其实还是存在很多的这个统计假设
而且我们要判断它生成的效果怎么样
也需要评估它生成的数据
和原始数据的差距大不大
所以呢
有人就干脆去掉了所有的统计假设
并且把这个评估真假数据
也就是原始数据和生成数据之间
差异的判别器
也放进来一起训练
于是就创造了GAN
生成式对抗神经网络
GAN呢
有两个部分分别是生成器和判别器
生成器呢
会从一些随机的k维向量出发
用采样网络
合成一个大很多倍的n维数据
判别器呢就负责来判断
合成出来的图片是真还是假
一开始呢
合成出来的都是一些意义不明
而且没有规律的结果
判别器就很容易的可以分辨出来
随后呢生成器会发现一些生成的方向
例如成块的色块就可以骗过判别器
那么它就会往这个方向合成更多的图片
而判别器呢发现被骗过去了
就会找到更复杂的特征来区分真假
如此反复呢
直到生成器生成的结果
判别器已经判断不出来真假了
这就算是训练好了
这样训练出来的生成器呢
可以生成非常逼真的
即使是人眼也难以分辨的图片
但是都是现实中不存在的
到了这个时候呢
计算机已经可以学会生成
相当逼真的画面了
例如这张人脸
虽然GAN因为引进了判别器
能够生成非常逼真的图片
但是由于它需要训练对抗网络
这个过程呢其实是非常不稳定的
对于灌输了大量
海量数据的超大规模网络来说
更是难以控制
所以呢
这个时候就出现了另一个更好的选择
也就是现在AI绘画普遍使用的
生成式模型diffusion model
中文称为扩散模型
diffusion model呢
生成图片的过程看似很简单
其实背后有一套非常
复杂的数学理论作为支撑
不过呢
我们先把复杂的理论呢放在一边
先简单的聊一聊
diffusion model是怎么运行的
diffusion model呢运行有两个过程
最右边呢是一个正常的图片
从右到左的过程称为forward diffusion
即前向扩散
做的事情呢
就是在逐步的叠加
一个符合正态分布的噪声
最后呢
得到一个看起来完全是噪声的图片
这就是所谓的扩散过程
如果不严谨的说呢
你可以把它想象成你有一块牛排
你一遍一遍的往上撒椒盐
一直到整块牛排都被这个椒盐
覆盖到看不清原来的纹路了
由于每次加噪声呢
只和上一次的状态有关
所以这是一个马尔科夫链的模型
其中的转换矩阵呢
可以用神经网络来预测
而从左到右的过程呢称为反向扩散
reverse diffusion
做的事情呢就是一步步的去除噪声
试图还原图片
这就是diffusion model生成数据的过程
为了达到去噪的目的
diffusion model的训练过程
实际上就是要从高斯噪声中还原图片
以及学习马尔科夫链的概率分布
再逆转图片的噪声
使得最后还原出来的图片
符合训练级集的分布
这个去噪的网络是如何设计的呢
我们可以从叠加噪声的过程中发现
原图和加噪声以后的图片
尺寸是完全一样的
于是呢我们很自然的能够想到
用一个U-net的结构来学习
U-net呢是一个类似于
auto encoder的漏斗状的网络
但是在相同尺寸的decoder和encoder层上
增加了直接的链接
以便于图片相同位置的信息
可以更好的通过网络来传递
在去噪任务中
U-net的输入呢是一张带噪声的图片
需要输出的是网络预测的噪声
而ground truth呢就是实际叠加上的噪声
有了这样一个网络呢
就可以去预测噪声
从而去去除掉它还原图片
因为带噪声的图片呢
就等于噪声加图片
所以这也是
为什么diffusion model会比其他方法
生成图片的速度更慢
因为它需要一轮一轮去噪声
而不是通过网络呢
可以一次性的推断出结果
那以上呢
就是diffusion model的生成图片的原理
是不是很简单呢
到这里呢
我们就解释了计算机是如何生成
和真实图片相似的图画的
接下来呢我们就来解释一下
模型是如何理解我们想要它生成什么
并如何给出对应的结果的
玩过AI绘画的人呢应该都知道
AI绘画最主流的模式呢
是在网页输入框中
输入一长串的prompt提示语
类似于施法前的吟唱咒语
其中呢包括想要生成的内容主体 风格
艺术家还有一些buff等等
点击生成之后呢
就会得到一张非常棒的结果图片
当然呢有时候可能也会出乎意料
通过文字来控制模型生成画面呢
最早的做法其实更像是
先让生成式的模型
生成一大堆符合常理的图片之后
再配合一个分类器
来得到符合条件的结果
当然这种做法呢在海量的数据面前
显然是没法用的
这个领域的开山之作DALL-E
最值得一提的就是引著了CLIP模型
来连接文字和图片
CLIP模型呢
其实就是用了海量的文本和图片数据对
把图片和文本编码后的特征呢
再计算相似性的矩阵
通过最大化对角线元素
同时最小化非对角线元素
来优化两个编码器
从而让最后的
文本和图片编码器的语义呢
可以强对应起来
如果你不能理解CLIP的原理呢
那么你只需要记住
CLIP能够把文字和图片对应起来
这样就可以了
它最大的成功之处呢
不是用了多么复杂的方法
而是用了海量的数据
这样带来的好处是呢
很多现有的图像模型
可以很容易的扩展成
文本控制的图像模型
原本需要大量人工标注的很多任务呢
现在就只需要用一个CLIP模型就可以了
甚至呢还可能用来生成新的数据
例如在StyleCLIP中
用文本交互可以来控制生成的人脸
一开始呢
图片的文字信息
大多都是以打标签的形式
通过大量的人工标注来完成的
有了CLIP模型之后呢
就可以说是彻底打通了文字
和图片之间的桥梁
使得图像相关的任务呢
可以得到大大的扩展
所以呢
即使说CLIP是AI绘画的基石也不过分
有了这个CLIP模型呢
就可以计算任何图片
和文本之间的关联度
即CLIP Score
再用它呢来指导模型的生成了
这一步呢其实还分为了几个发展阶段
最早用的方法呢是Guided diffusion啊
很傻很天真
每次降噪后的图片呢
都需要计算一次
和输入文本之间的CLIP Score
原本的网络呢只需要预测噪声
而现在呢不但要预测噪声
还需要让去噪后的结果图片
尽可能的跟文本接近
也就是CLIP loss尽量小
这样呢在不断去噪的过程中
模型就会倾向于生成和文本相近的图片了
由于CLIP呢
是在无噪声的这个图片上进行训练的
还有一个小的细节呢
是需要对CLIP模型呢
用加噪声的图片再进行Finetune
也就是微调
这样呢CLIP才能够看出来
加噪后的牛排呢还是一块牛排
这样做的好处呢
是CLIP和diffusion model都是现成的
只需要在生成过程中呢
结合到一起就行了
但是缺点呢
是本来已经很慢的diffusion model
现在生成过程呢就会变得更慢
而且这两个模型呢是独立训练出来的
没有办法联合训练
也就没有办法让它们
得到进一步的提升
于是呢就有了classifier free diffusion guidance
这个模型呢
可以同时支持无条件
和有条件的噪声估计
而且呢在训练diffusion model时呢
就加入了文本的引导
这样的模型
当然也离不开很多很多的数据
以及很多很多的（GPU）卡
除了网络爬取的数据以外呢
还有通过商业图库
构造出巨量的图片和文本对
最后呢作为成品的GLIDE
在生成效果上呢又达到了一次飞跃
虽然现在看的有点简陋
但是在当时来说已经非常惊人了
恭喜大家
到这里
我们已经追上了AI绘画21年末的进度
我们再发散一下
如果是以图生图
那这时候的输入条件呢就变成了图片
那么要怎么来控制生成的结果呢
其实有几种不同的方法
也算是不同的流派吧
这里呢我们先介绍3种
第一种呢是直接提取图片的CLIP特征
就像是文字特征一样去引导图片
这样生成出来的图片呢
内容就比较相近
但是结构是不一定是相同的
比如像这样生成的内容呢很相似
但是画风呢会显得比较怪异
第二种呢特别好理解
包括现在主流的AI绘画工具webui
里边的img2img
都是采用的这个方法
就是对输入的原图去增加几层噪声
再以这个为基础进行常规的去噪
通过使用你希望的画风所对应的咒语
就可以生成和你原图结构类似
但是画风完全不同的图片
而且叠加的噪声强度越高
生成的图片和原图就差距越大
AI画画的这个发挥空间呢也就越大
比如用这个方法生成的二次元形象
你把屏幕放远点
看这两张图片的色块呢
其实是非常相近的
还记得我们之前说的这个
牛排的例子吧
因为右边的图片呢
就是基于左边的图片
叠加了厚厚的几层椒盐
来作为基础生成的
所以呢依然保留了大致的色块结构
但是模型呢
也加上了自己的一些想象
这也就是我们通过文本来引导的
第三种方法呢
是用对应的图片去finetune
也就是微调这个生成网络
这个方法呢也叫做Dreambooth
是谷歌提出来的
打个比方呢就是
如果我们给模型看很多小狗狗的图片
让模型呢学到这只小狗狗的样子
这样我们只需要再加上一些简单的词汇
就可以生成各式各样的小狗狗了
到这里呢
我们已经解释了
计算机是如何生成
和真实图片相似的图片
以及模型是如何听懂
我们想要它生成什么
并且给出对应结果的
那关于AI绘画的基本原理呢
就已经介绍的差不多了
我们可以发现
其实大部分的都是改进性质的工作
但是效果呢确实很惊人
在这个过程中呢
其实会涉及到很多训练网络的小技巧
这里呢我就不再做过多的介绍了
然后呢我们再聊一个比较火爆的模型
stable diffusion
因为它开源而且呢效果比较好
所以呢得到了很多人的喜爱
那基于stable diffusion也发展出了
像二次元插画的这哥NovelAI
等等模型啊也都很火爆
如果我们想要解释
stable diffusion为什么这么好
那还要先从latent diffusion model谈起
让我们来复习一下diffusion model的原理
对于一个带噪声的输入图片
训练一个噪声预测U-net网络
让它能够去预测噪声
然后再从输入中减去噪声
就得到了去噪后的图片
一般的diffusion model呢
是对原始图片进行加噪去噪
噪声图片呢
和原始的图片的尺寸呢是一样的
为了节约训练资源和生成时间呢
通常呢都会使用比较小的图片尺寸
来进行训练
然后再接一个超分辨率的模型
而在Latent diffusion model中呢
diffusion的模块呢被用来生成
VAE的隐编码
于是呢整个流程就变成了这个样子
图片呢先用训练好的VAE的encoder
得到一个维度要小很多的图片隐编码
你可以理解为呢
将图片的信息
压缩到了一个尺寸更小的空间中
而defusion model不再直接处理原图
而是处理这些隐编码
最后生成新的隐编码
再用对应的decoder还原成图片
这样相对于直接生成图片来说呢
就可以大幅度的减少计算量以及显存
所以stable diffusion的生成速度呢会更快
第二个改进点呢
是增加了更多的训练数据
而且还多了一个美学评分的过滤指标
简单的来说呢
就是他只选择好看的图片
就好像是
你如果要学会如何画一个漂亮的画
那你就要多看看一些大艺术家们的经典作品
这是一样的
同时呢
在训练集里边也都是一些漂亮的图片
例如这样的
或者是这样的
一些模糊的有水印的图片
都已经被pass掉了
所以呢
只让机器从好看的这个图片里边
去学习画画
最后呢stable diffusion的另一个改进呢
就是用了我们之前提到的CLIP模型
来实现了通过文本
来控制图片的生成方向
这里呢也顺便提一下
二次元画风的这个NovelAI啊
其实在技术上呢并没有什么新的内容
就是拿海量的二次元图片
去微调原来的这个stable diffusion模型
主要的一些改进呢
包括把CLIP用在了倒数第二层
这样呢就可以更贴近文本内容的特征
然后呢就是把训练数据
扩展为一个长宽比不限的尺寸
这样呢就可以容纳下完整的人像
以及增加了可以支持的文本输入的长度
这样呢就能够让提示语呢变得更灵活
也更加复杂
接下来呢我们再说一下ControlNet
它是如何来控制扩散模型的
由于diffusion超强的这个学习能力呢
理论上这个网络呢是可以还原出
训练集里的每一张图片的
所以呢只要数据足够多足够好
模型呢就可以生成非常好的图片
和人类学画画不同的点在于呢
如果人的难度是在于说画不出来
那么模型的难点呢就是
他不知道该往哪个方向去画
所以控制模型生成呢
其实就是想办法让模型变得听话
按照你的指示呢来生成结果
之前呢我们提到这个img2img
它的原理呢
就是把左图加一些高斯噪声
相当于撒了一些黑胡椒面
然后呢再把它作为底图
基于他来生成新的图
所以呢基本上色块的分布
是非常接近的
但是很难控制的更加细节
而有了ControlNet之后呢
就可以通过任何的条件
来控制网络的生成
原来模型呢
只能得到一个文本的生成引导
但是现在呢
他可以听得懂
任何基于图片所提取出来的信号
只要你拿一组成对的图片去训练
就可以了
这个方式呢出来以后
极大的拓展了这种可玩性
而且呢
官方已经提供了非常多常用的
已经训练好的控制网络
你可以用depth来控制结构
生成各种的场景
比如呢你可以直接拿线稿来上色
也可以
随便涂几笔就生成复杂的图片
还可以
通过姿态检测来生成很好的多人结果
甚至呢你可以自己训练
解决AI不会画手的问题
这些控制结果呢还可以一起使用
比如说结合人体姿态和深度图
来生成一张新的图片
甚至呢它都不需要是来自于同一张图
虽然ControlNet效果很惊艳
但是原理上其实比较简单
为了给原始的模型
增加一些额外的条件输入
其实就是把整个网络复制了一份
固定让原来的网络
来保证输出的稳定性
原始的网络输入依然是噪声
而复制的ControlNet的输入呢
则是控制条件
比如说深度、姿态等等
把这两个输入呢和输出加起来
再用成对的数据集去训练控制网络
这样呢就能够很好的去控制
训练的程度和结果
这个训练的本质上呢还是在做微调
所以呢耗时也不是很大
和直接微调整个网络呢也差不太多
现在ControlNet呢
对于结构生成已经能做到不错了
这时候呢就会面临另一个问题
模型的细节
要怎么让它能够生成的更好
想要得到更高质量的图片呢
最直接的方式就是调大输出的分辨率
分辨率越大细节画的就越好
尤其是对于人脸来说
但是呢实际上高分辨率的结果
就非常容易崩掉
并且分辨率高了以后呢
计算的成本会飙升
也会算的很慢
于是呢一种常见的做法是
先生成一些较小分辨率的结果
然后对这些图片做超分
也就是在保证清晰度的前提下
把图片放大
这么做可以保证结构的合理性
而且速度会快非常多
但是超分的模型
对于细节的补充
并不一定能做到很自然
而且容易有过于锐化的结果
除了传统的超分模型
还有同样基于diffusion模型的超分算法
由于diffusion相当于重绘了这个图片
所以可以得到更好的一些细节效果
但是图片尺寸非常大跑起来就会更慢
另一个现在被广泛使用的方式呢
是latent upscale
这个用webui自带的这个Hires.fix
就可以实现
之前呢我们也提到过
stable diffusion的结构优势之一呢
就是它是由一个压缩图片信息的VAE
和一个对latent
进行去噪的U-net网络所组成的
所以呢
它天然适合基于latent的超分算法
对latent做upscale
也有基于diffusion的方法
并且呢效果应该是最好的
当然呢
带来的代价呢就是耗时也会增加
可以看一下
这几个超分方法的这个效果对比
而这张图片
就是latent upscale和ControlNet
一起结合得到的
大家可以欣赏一下
除了ControlNet之外呢
AI绘画领域
另一个不得不提的就是LoRA模型
要讲LoRA模型呢
就要先解释模型的finetune
也就是什么是微调
其实就是当你有一个现成的
很厉害的大模型
也就是一个预训练的模型的时候
你想要让他学习一些新的知识
或者完成一些更面向于
具体应用的任务
或者只是为了适应
你的这个数据分布的时候呢
就需要拿你的小样本的数据
去对这个模型进行重新的训练
这个训练呢还不能训太久
否则呢模型就会过拟合
到这个你的小样本数据上
从而丧失掉大模型的这个泛化能力
Pretrain和Finetune呢
是机器学习中非常常见的组合
在应用上呢有很大的价值
但是其中有一个问题就是它会遗忘
模型呢会在Finetune的这个过程中呢
不断忘记之前已经记住的内容
常见的方解决方式呢会有两个
第一个是replay
就是把原始的知识呢再过一遍
第二个呢是正则化
就是通过正则项
来控制模型的参数
和原始参数尽量保持一致
不要变得太多
当然还有一个呢就是parameter isolation
也就是参数孤立化
这个呢
是通过独立出一个模块来做finetune
使得这个原有的模型不再更新权重
参数孤立化呢
是其实是最有效的一种方式
具体呢有好几种实现方式
例如adapter
就是在原模型中增加一个子模块
固定原模型只训练这个子模块
是不是听起来有点熟悉呢
没错
ControlNet就是一种类似于adapter的方法
而LoRA呢就是另一种
参数孤立化的策略
也在AI绘画这个领域
找到了他的用武之地
它利用了低秩矩阵
来代替原来的全量参数进行训练
从而提高了finetune的效率
我们可以和之前最常用的finetune方法
DreamBooth来对比一下
对于DreamBooth来说呢
它是直接更新整个大模型的权重
来让模型学习到新的概念
虽然呢可以通过正则化来避免遗忘
但是finetune后的模型呢依然非常大
和原来的模型呢一样大
而使用LoRA之后呢
LoRA影响的只是其中的一小部分权重
也就是通过低秩矩阵
叠加到大模型网络上的那一部分
所以微调起来呢就会更快
需要的资源更少
而且得到的微调模型呢会非常小
使用起来呢就会方便很多
由于LoRA在结构上是独立于大模型的
所以呢它有一个额外的好处
就是替换大模型
可以得到不同的令人惊喜的一些结果
比如说
我们用水墨画来训练的LoRA模型墨心
结合国风美女的这个基础大模型
就可以生成一个
穿着中式服装的水墨画美少女
在使用上来说呢
LoRA就很像这个模型的一个插件
可以在基础模型上
去叠加想要的效果
或者把各种想要的效果
加权组合叠加在一起
这样呢就可以产生很多
令人惊喜的结果
当然呢由于LoRA是一个微调模型
所以呢画风会趋于单一
那这样是好是坏也是见仁见智
但是呢
如果你使用现实中的真人照片
来训练LoRA
并且把这个模型公开了
那我觉得是一件非常缺德的事情
希望大家也不要这么做
好了今天的内容呢大概就介绍到这里
我们大概完整介绍了
AI绘画的整个流程
以及相关的模型算法
当然呢由于时间有限
还有很多内容是没有被覆盖的
比如像Midjourney DALL-E等等
以后有机会呢大飞再给大家一一介绍
感兴趣的小伙伴们呢
欢迎订阅我们的频道
我们下期再见
