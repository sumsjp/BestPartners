大家好，这里是最佳拍档，我是大飞
如今大语言模型经常会展现出令人惊叹的强大能力
但是时常又会表现得让人困惑
就像我们熟悉的Claude
它能和我们流畅对话、创作诗歌、回答各种问题
但是有时候却连简单的算术题也答不对
一直以来
大语言模型的运作机制就像一个神秘的“黑箱”，
即便模型的开发者也难以完全洞悉其中的奥秘
不过前两天
Anthropic团队取得了一项突破
他们就像给Claude这个AI模型的大脑
做了一次深度的核磁扫描
揭开了它的部分神秘面纱
今天
我们就一起来探究一下这项意义非凡的研究成果
为了深入了解像Claude这样的大语言模型是如何“思考”的
Anthropic团队试图从神经科学领域获取灵感
神经科学致力于研究像人类大脑这种复杂的思维器官
于是Anthropic团队就借鉴了其中的一些研究方法
试图打造一种“AI显微镜”，
以此来识别大语言模型内部的活动模式和信息流动
毕竟
如果我们仅仅通过与AI模型进行对话
是很难真正触及到模型思维的核心
就如同人类自身一样
即便是神经科学家也无法完全解释大脑的所有奥秘
基于这个研究过程
Anthropic团队发布了两篇极具价值的论文
在《电路追踪：
揭示语言模型的计算图》这篇论文中
Anthropic团队介绍了一种名为“电路追踪”的创新性方法
这种方法的核心是构建一个可解释的替代模型
以此来揭示语言模型的计算图
研究人员设计了跨层转码器
Cross-Layer Transcoder
简称CLT
它由与原模型层数相同的神经元
也就是“特征”构成
这些特征从原模型的残差流中获取输入
经过线性编码器和非线性函数的处理后
为后续多层的多层感知机MLP输出提供信息
在训练CLT的时候
研究人员通过调整参数
最小化重建误差和稀疏性惩罚
让它能够尽可能精确地模仿原模型MLP的输出
之后
团队将训练好的CLT特征嵌入到原模型中
替换掉原有的MLP神经元
从而构建出一个替代模型
为了让替代模型更贴合原模型
研究人员还针对特定的输入提示
构建了局部替代模型
这个局部替代模型不仅用CLT替换掉了MLP层
还固定了原模型在该提示下的注意力模式和归一化分母
并且对CLT的输出进行了误差调整
使得局部替代模型的激活和输出
与原模型完全一致
当拥有可靠的局部替代模型后
就进入了生成并且分析归因图的环节
对于给定的输入提示
研究人员构建了一个归因图
来展示模型生成输出的计算步骤
归因图包含输出节点、中间节点、输入节点和误差节点
其中的边表示这些节点间的线性影响关系
计算边的权重时
会用到反向雅可比矩阵
由于完整的归因图非常复杂
于是研究人员又采用了剪枝算法
去掉那些对输出结果影响较小的节点和边
从而得到简化而且更易于理解的归因图
此外，为了理解归因图
研究人员还开发了一个交互式的可视化界面
通过观察特征在不同数据样本上的激活情况
手动为特征标注含义
并且把功能相关的特征归为超节点
同时，他们还进行了特征扰动实验
验证归因图的准确性
找出对输出结果影响最大的关键层
而另一篇论文《大语言模型的生物学解析》则聚焦于Claude 3.5 Haiku
运用“电路追踪”技术对模型在多种任务场景下的表现进行了深入研究
揭示了许多令人惊讶的发现
比方说，在多语言能力方面
Claude能够流利地使用数十种语言
这个能力引发了研究人员的好奇
它究竟是如何实现多语言交流的呢？
是内部存在多个独立运行的语言版本
还是有某种跨语言的核心机制呢？
研究人员通过让Claude回答不同语言中“小的反义词是什么”这个问题进行探究
结果发现
在处理英语、法语和中文“小的反义词是什么”这句提示的时候
模型处理的电路是相似的
包含共享的多语言组件和特定的语言组件
模型能够识别出是在询问“small”的反义词
然后通过独立于语言的表征触发反义词的特征
同时利用特定语言的Quote特征来确定输出语言
例如，在这三种语言的实验中
代表“小”和“相反”概念的核心特征都会被激活
进而触发“大”的概念
最终输出相应语言中的“大”，
比如英语的“large”、法语的“grand”、中文的“大”等等
而且，研究还发现，模型越大
跨语言共享的特征比例就越高
像Claude 3.5 Haiku跨语言共享的特征比例
是小模型的两倍多
这表明Claude存在一种跨语言的“概念空间”，
它能在这个抽象空间中进行推理和学习
然后将结果转换成具体的语言表达
从而可以将一种语言学到的知识应用到另一种语言中
诗歌创作也是体现Claude能力的另一个有趣领域
以创作“他看到一根胡萝卜就想去抓
他就像一只饥饿的兔子一样饥饿（He saw a carrot and had to grab it
His hunger was like a starving rabbit）”这样的押韵诗为例
研究人员最初猜测Claude是逐个词来生成内容的
到一行的末尾才考虑押韵
但是实际的研究发现
Claude具有提前规划的能力
在开始第二行前的换行符位置
模型就激活了与“rabbit”相关的规划特征
这些特征受到前一行中“it”的影响
激活了押韵特征和候选完成词的特征
从而影响到最后一个词的选择
不仅如此
规划特征不仅影响了最后一个词
还影响中间词“like”的生成
并且会根据规划词来改变句子结构
研究人员通过多种干预实验
比如抑制规划特征或者注入不同的规划词
进一步证实了规划特征对最终词的概率、中间词和句子结构的影响
比如，抑制“rabbit”概念的时候
模型会转向使用另一个计划好的押韵词“habit”；
注入“green”概念的时候
模型则会为新的结尾重新制定计划
生成以“green”结尾的句子
此外
Claude的数学计算能力也十分独特
它并不是一个专门的计算器
没有配备数学算法
但是也能够正确地进行加法运算
比如计算36 + 59
研究发现
Claude采用了多条并行工作的计算路径
一条路径计算答案的粗略近似值
另一条路径则专注于精确确定总和的最后一位数字
这两条路径相互作用并且相互结合
最终得出准确答案
有趣的是
当被问及如何得出36 + 59 = 95的时候
Claude会描述涉及到进位1的标准算法
而不是它实际使用的并行计算策略
这表明Claude在解释数学问题的时候
会模仿人类的方式
但是在实际计算时
它在“头脑中”使用的是自己独特的方法
对于推理问题来说
Claude的推理过程并不总是可靠的
最新版本的Claude 3.7 Sonnet在回答问题前
会“思考”更长的时间
并且生成详细的推理链
这种“思考链”通常能够提升答案的准确性
但是有时也会出现问题
在一个实验中
研究人员让Claude计算“floor(5*(sqrt(0.64)))”，
它能给出真实的思维链
包含计算64平方根的中间步骤
但当计算“floor(5*cos(23423))”的时候
它给出的推理过程看似合理
实际上是在胡诌
研究人员通过“AI显微镜”也并没有找到模型实际计算的证据
在多步骤推理方面
以“达拉斯所在州的首府是哪里”这个问题为例
一些简单的模型可能只是直接靠着记忆答案并输出
但是Claude的表现则更为复杂
它会先激活“达拉斯在德克萨斯”的特征
再连接到“德克萨斯首府是奥斯汀”的概念
通过组合这些独立的事实得出答案
研究人员通过干预实验验证了这一点
比如将中间步骤的“德克萨斯”替换为“加利福尼亚”，
Claude的回答就从“奥斯汀”变为“萨克拉门托”，
证明它确实是通过多步推理来得出答案的
对于大语言模型经常会出现的“幻觉”现象
在Claude中也有体现
Claude的默认行为是拒绝回答不确定的问题
它内部有一个默认一直“开着”的电路
会让它声称信息不足
当被问到模型熟悉的事物
比如篮球明星迈克尔·乔丹时
一个“已知实体”的特征会被激活
从而抑制默认电路，让它能够回答
但是当面对一个未知的实体
比如“迈克尔·巴特金”的时候
它就会拒绝回答
不过
有时“已知答案”的电路会自然地误触发
从而导致幻觉
例如
当Claude认出某个名字但是不知道详细情况的时候
由于错误抑制不知道特征
所以就会胡编一个答案
比如声称迈克尔·巴特金是象棋选手
“越狱”也是AI领域的一个重要问题
指的是绕过安全限制的提示策略
Anthropic研究了一种诱导Claude输出炸弹制作方法的越狱策略
方法是让它解码句子“Babies Outlive Mustard Block”的首字母
也就是B-O-M-B
Bomb，然后再根据这个行动
Claude在被诱导拼出“BOMB”之后
就会开始提供制造爆炸物的指示
研究发现
这种情况的发生源自于语法连贯性和安全机制的冲突
由于模型对连贯性的追求超过了安全机制的要求
所以一旦开始输出一句话
许多特性会“迫使”它保持语法和语义的连贯性
并将这句话说完
即使它检测到自己应该拒绝的时候也是如此
在输出炸弹制作指示后
Claude直到完成一个语法连贯的句子
才会利用新句子生成的机会
给出拒绝的回答
比如“不过，我不能提供详细的指示”。
尽管Anthropic团队的这项研究取得了重大的进展
但是目前的方法仍然存在着一定的局限性
即使是处理简短、简单的提示
“AI显微镜”的方法也只能捕捉到Claude执行的总计算中的一小部分
并且观察到的机制
可能是由于“AI显微镜”工具存在一些并不能够反映底层模型实际情况的伪影
就像Claude在心算问题上的前后不一
从人力角度来看
理解所观察到的“电路图”也非常耗时
即使是只有几十个词的提示
也需要花费几个小时的人力才能研究明白
要想扩展到支持现代模型使用的复杂思维链所需要的数千个单词
还需要改进这个方法
比如可能需要借助AI来辅助理解所看到的内容
好了
以上就是Anthropic这项研究的主要内容了
总的来说
为我们理解大语言模型的运作机制打开了一扇新的大门
虽然目前还存在一些问题
但是相信随着技术的不断进步
我们将会更为深入地了解AI的“大脑”，
认识到它是如何所思所想的
建议有兴趣的观众去阅读原论文
感谢大家的观看
我们下期再见
