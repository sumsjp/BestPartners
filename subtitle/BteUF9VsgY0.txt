大家好，这里是最佳拍档，我是大飞
14号下午
DeepSeek团队又发布了一篇新论文
以DeepSeek-V3模型为代表
深入解读了DeepSeek在硬件架构和模型设计方面的关键创新
为实现低成本的大规模训练和推理
提供了如何突破硬件瓶颈的新思路
比较吸引大家目光的是
DeepSeek的创始人兼CEO梁文锋
这次又出现在了合著名单之中
按姓名首字母顺序排在倒数第五位
不知道大家是否还记得
在DeepSeek V3刚发布的时候
实现了多项令人瞩目的效率突破
那除了在DeepSeek v3技术论文中所提到的各种技术创新以外
究竟在基础设施和硬件上都做了哪些工作呢？
相信今天这篇论文应该能给你一些答案
今天大飞就来给大家解读一下
论文的开篇就提到
DeepSeek-V3的关键创新
目的在于解决训练扩展中的三个核心挑战
分别是内存效率、成本效益和推理速度
首先来说内存效率
现在的大语言模型变得越来越庞大
需要的存储空间激增
特别是注意力机制会产生大量临时的KV缓存数据
占用大量的显卡内存
那针对这种情况
DeepSeek采取了两个优化手段
第一点就是从源头优化内存
也就是降低精度
因为与使用BF16相比
FP8可以将内存的消耗降低一半
这就有效缓解了内存墙的挑战
同时再通过精细量化
比如分块压缩的方法来保持精度
第二点就是使用多头潜在注意力MLA
来减少KV缓存的大小
对于大模型的推理来说
用户的请求通常会涉及到多轮对话
所以KV缓存会通过缓存之前处理的token的键值向量
来避免后续token的重复计算
而在推理步骤的汇总阶段
模型会再将当前token的键值向量
与历史缓存中的键值对组合
来执行注意力计算
这种增量式的计算
在处理长序列或者多轮输入时非常高效
但是，它带来的问题是内存上的限制
因为将计算从矩阵乘法GEMM
转移到矩阵向量乘法GEMV
后者的计算与内存比要低得多
为了解决这个挑战
DeepSeek的研究人员采用了MLA
它会通过投影矩阵
将所有注意力头的KV表示压缩成一个更小的潜在向量
从而让这个矩阵与模型联合训练
而在推理的过程中
只需要缓存潜在向量即可
这样
与存储所有注意力头的KV缓存相比
就显著减少了内存消耗
其次是成本效益
训练超大规模模型需要海量的计算资源
传统 “稠密模型”每次计算都要激活所有参数
导致计算成本极高
为了提高性价比
DeepSeek选择了开发DeepSeek MoE模型
一般来说，MoE模型的优势有两个方面
第一是可以减少训练的计算要求
降低训练成本
MoE模型允许参数总数的急剧增加
同时还能保持计算要求的适中
比方说，DeepSeek-V2有236B的参数
但是每个token只激活了21B的参数
DeepSeek-V3扩展到671B参数
同时能将每个token的激活量保持在仅仅37B
而相比之下
Qwen2.5-72B和LLaMa3.1-405B等稠密模型
要求所有参数在推理期间都要处于活动的状态
第二是个人使用和本地部署的优势
在个性化Agent快速发展的趋势下
MoE模型在单请求场景中存在着独特的优势
由于每个请求只激活了一个参数子集
因此内存和计算的需求就大大减少了
例如，前面提到过的
DeepSeek-V2在推理过程中只会激活21B的参数
这就使得配备AI芯片的个人电脑
也能够实现每秒近20个token输出的TPS
甚至达到该速度的两倍
成本也就大约10000美元左右
相比之下，具有相似能力的稠密模型
在类似的硬件上通常只能达到个位数的TPS
这种效率
就使得MoE架构非常适合于硬件资源有限的本地部署和个人用户
第三个挑战是推理速度
当使用多个GPU一起训练时
它们之间需要不断交换数据
这个过程会产生延迟
即使用了高速网络
这种延迟仍然会拖慢整体训练速度
尤其是处理长文本或者需要实时响应的时候
更加明显
为此
DeepSeek采用了重叠计算和通信、引入高带宽纵向扩展网络、多token预测框架等技术
来提高模型的推理速度
我们先说重叠计算和通信
推理速度一般会包括系统范围的最大吞吐量和单个请求延迟两个指标
为了最大限度地提高吞吐量
DeepSeek-V3从一开始就被构建成了双微批处理重叠
也就是将通信延迟与计算重叠
也就是说
DeepSeek将MLA和MoE的计算
解耦成了两个不同的阶段
当一个微批处理执行MLA或者MoE计算的一部分时
另一个微批处理会同时执行相应的调度通信
相反，在第二个微批处理的计算阶段
第一个微批处理可能正在经历组合通信的步骤
这种流水线化的方法
实现了全对全通信与进行中计算的无缝重叠
确保了始终能够充分地利用GPU资源
除此以外，在生产过程中
他们还采用预填充-解码分离（prefill-decode disaggregation）的架构
将大批量的预填充和延迟敏感的解码请求
分配给了不同的专家并行组来处理
而为了实现尽可能快的推理速度
就需要增加跨计算设备部署的专家参数
理想情况下
每个设备都应该专门为一个专家执行计算
或者在必要的时候
多个设备应该与单个专家协同计算
但是前提是，专家并行EP
需要将token路由到适当的设备才行
这就涉及到了跨网络的多对多通信
因此
MoE推理速度的上限其实是由互连带宽所决定的
假设一个系统
每个设备都保存了一个专家的参数
一次能处理大约32个token
这个数量是在计算内存比和通信延迟之间取得了平衡的
可以确保每个设备在专家并行期间
处理相等的批量大小
从而计算通信时间
如果能采用像GB200 NVL72
单向带宽900GB/s这样的高带宽互连
那么每个EP步骤的通信时间=（1字节+2字节）×32×9×7K/900GB/s=6.72μs
假设计算时间等于通信时间
那么这将显著的减少总推理时间
理论上能够突破每秒1200个token的上限
同时，为了降低网络通信延迟
DeepSeek还选用了InfiniBand GPU Direct Async
简称IBGDA
相较于传统网络通信需要创建CPU代理线程
从而带来额外的通信开销
IBGDA允许GPU直接填充WR内容
并且写入RDMA的MMIO地址
通过在GPU内部管理整个控制平面
消除了与GPU-CPU通信之间的显著延迟开销
而且由于GPU具有多个并行线程
发送方还可以利用这些线程来分配工作负载
从而避免发送大量小包数据时的瓶颈
第三
DeepSeek-V3还引入了多token预测MTP框架
这个框架可以在增强模型性能的同时
提高模型的推理速度
在传统的自回归语言模型中
推理过程是逐个生成token的
每次生成一个token后
模型需要根据已生成的上下文信息来预测下一个token
这种顺序生成的方式虽然能够保证生成的连贯性和准确性
但是推理速度会受限于每个token的生成时间
随着模型规模的增大和上下文长度的增加
这种顺序生成的方式会显著降低推理效率
尤其是在需要快速生成长文本的场景中
而MTP框架则引入了多个轻量级的预测模块
每个预测模块负责生成一个特定位置的token
比方说，在生成当前token的同时
MTP模块可以预测下一个token、下下个token等等
这些预测模块共享模型的上下文信息
但是各自独立生成token
通过这种方式
模型能够在一次推理步骤中生成多个token
而不是逐个生成
生成多个候选token后
MTP框架再通过并行验证
来确定哪些候选token是合理的
最终选择出最合适的token作为输出
实验数据显示
MTP模块在预测下一个token时的接受率高达80%到90%，
这意味着大多数情况下
模型能够准确预测下一个token
从而显著提高了推理速度
最后
DeepSeek为了降低集群的网络成本
还采用了多平面双层胖树MPFT的横向扩展网络
取代了传统的三层胖树拓扑结构
从而将成本降低了40%以上
在这个网络中
每个节点会配备8台GPU和8个IB网卡
每个GPU-网卡对
会被分配到不同的网络平面
此外
每个节点还配备一个400 Gbps的以太网RoCE网卡
并且连接到单独的存储网络平面
用来访问DeepSeek自研的3FS开源分布式文件系统
在横向扩展网络中
还使用了64端口的400G IB交换机
理论上最多可支持16,384台GPU
同时还保留了双层网络的成本和延迟优势
但是由于受政策和监管的限制
最终部署的GPU数量为2048台
在互连优化方面
DeepSeek团队还提出了硬件感知并行策略
摒弃了传统的张量并行（TP）
转而采用流水线并行（PP）和专家并行（EP）
配合自主研发的DeepEP开源库
实现了通信效率的飞跃
针对当前的一些硬件痛点
DeepSeek还提出了下一代AI基础设施的六大挑战和解决方案
涵盖内存、互连、网络、计算等核心领域
一，健壮性优先
现有硬件对GPU故障、内存静默错误等问题
缺乏有效的检测
导致大规模训练中断的风险很高
对此
DeepSeek认为硬件必须引入传统ECC以外的高级错误检测机制
比如基于校验和的验证
或者硬件加速的冗余检查等技术
从而为大规模部署提供更高的可靠性
此外
硬件供应商应该向终端用户提供全面的诊断工具包
让用户能够严格的验证系统完整性
并且主动识别潜在的静默数据损坏
二、颠覆互连架构
传统CPU在协调计算、管理I/O和维持系统吞吐量方面
仍然是不可或缺的
但是当前架构面临着许多关键瓶颈
首先是CPU与GPU之间的PCIe接口
在大规模参数、梯度或者KV缓存传输期间
经常会成为带宽瓶颈
为缓解这个问题
未来的系统应该采用直接的CPU-GPU互连
比如NVLink或Infinity Fabric
或者将CPU和GPU集成到扩展域中
从而消除节点内瓶颈
除了PCIe的限制以外
维持如此高的数据传输速率
也需要极高的内存带宽
最后
内核启动和网络处理等延迟敏感任务
需要极高的单核CPU性能
通常需要基频超过4GHz以上
还有就是现代AI工作负载
需要每个GPU配备足够的 CPU核心
来避免控制端的瓶颈
对于基于小芯片的架构
还需要额外的核心
来支持基于缓存感知的工作负载分区和隔离
三、智能网络升级
为了满足延迟敏感型工作负载的需求
未来的互连必须同时优先考虑低延迟和智能网络
其中
集成硅光子学可以实现更高的带宽扩展性和更强的能效
这对大规模分布式系统至关重要
基于信用的流量控制CBFC
可以确保无损数据传输
但是单纯触发流量控制
可能会导致严重的队列阻塞
因此
必须部署先进的端点驱动拥塞控制CC算法
主动调节注入的速率
并且避免异常拥塞场景
未来的网络还应该将动态路由方案标准化
持续监控实时的网络状况
并且能够智能重新分配流量
此外
通过部署自愈协议、冗余端口和快速故障转移技术
可以显著增强网络的健壮性
四、通信顺序的“硬件化”。
使用load/store内存语义的节点间通信
不仅高效而且便于编程
但是当前实现受到内存顺序的阻碍
DeepSeek主张通过硬件支持
为内存语义通信提供内置的顺序保证
这种一致性应该在编程层和接收方的硬件层面强制执行
实现有序的传递
并且无需额外开销
五、网络计算融合
论文认为，MoE模型的分发与组合阶段
仍然存在网络上的优化空间
因此DeepSeek建议
在网络硬件中集成自动分组复制、硬件级归约等功能
并且支持LogFMT压缩
从而降低通信的带宽需求
六、内存架构重构
如今，模型规模的指数级增长速度
已经超过了高带宽内存HBM技术的进步速度
这种差距造成了内存瓶颈
DeepSeek推荐采用DRAM堆叠加速器
利用先进的3D堆叠技术
DRAM die可以垂直集成在逻辑die的顶部
从而实现极高的内存带宽、超低延迟和实用内存容量
DeepSeek还提到了晶圆级系统（SoW）
认为晶圆级集成可以最大限度地提高计算密度和内存带宽
满足超大规模模型的需求
好了 以上
就是这篇deepseekV3最新论文的主要内容了
希望对大家了解模型的技术实现有所帮助
感兴趣的话可以去仔细阅读原文
如今来看
AI产业正在进入软硬件深度协同的时代
通过将硬件特性融入到模型的设计中
再反向驱动硬件的升级
DeepSeek让我们看到了一种软硬件之间的良性循环
期待DeepSeek R2的公布
给我们带来更多技术方面的创新和进步
感谢收看本期视频，我们下期再见
