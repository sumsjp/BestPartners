大家好，这里是最佳拍档，我是大飞
昨天
波士顿动力在YouTube上发布了一段长达8分多钟的视频
展示四足机器人Spot的语言能力
尤其是在视频中的狗子
还操着一口流利的伦敦腔跟你对话
让我们先来看下视频的精华内容
归功于ChatGPT，自从Spot面世以来
优秀的灵活程度和协调程度让它可以做出很多高难度动作
比如翻跟头、跳舞等等
但是它之前并不会“说话”。
现在继承了GPT，Spot可以化身“导游”，
用英国口音与员工聊天
带他们参观公司的设施，回答提问
比方说你问它看到了什么
Spot会快速作答
说我看到了一块二维码的板子
还有一扇很大的窗户
它甚至可以像木偶一样张开“嘴巴”，
让它看起来像是真的在说话一样
而且不止语言能力
ChatGPT的注入让Spot整体反应都变得更加智能、更具交互性
比方说
当Spot在说“Follow me”的时候
会自动做出转身引路的动作
或者在人指着一些需要解说的物品的时候
Spot也会立马抬头看向所指物品
然后进行解答
这一次
波士顿动力不仅给Spot安装了扬声器
还增加了文本转语音功能
同时为了打造不同的个性
波士顿动力公司还对Spot的代码稍作了修改
结果就是
狗子就会完全修改它的输出
从它的口音到组织句子的方式
再到声音的音调
甚至连它转头的方式似乎也会随着它的个性而变化
我们都知道
大语言模型有所谓的涌现行为
能够执行既定训练之外的任务
也正因如此
它们可以适用于各种的应用
波士顿动力团队的探索是从今年夏天开始的
他们先在机器人中使用大语言模型来制作了一些概念性的验证
然后又在一次内部黑客马拉松活动中实现了他们的想法
接下来
让我们解密他们是如何使用Spot的SDK
来打造这样一只机器狗导游的
在最新的官方博客中
波士顿动力对Spot背后的技术进行了详细介绍
作为导游
Spot四处走动能力是现成的
Spot SDK也允许用户实现对机器狗的自定义
Spot会观察环境中的物体
使用VQA或者字幕模型对物体进行描述
然后使用大语言模型对这些描述进行详细说明
团队在Spot收集的三维地图上标注了简短的描述
这样机器人就会根据定位系统查找所在位置的描述
将它与传感器提供的其他上下文一起输入大语言模型
然后
大语言模型将这些内容合成为各种命令
比如「说」、「问」、「去」或者「标签」等
团队还为Spot导游的建筑环境构建了一张三维地图
并且为大语言模型注了位置
比如1是演示实验室的阳台
2是演示实验室的天桥
3是博物馆的陈列
4是博物馆的地图
5是大厅，6是外部入口
除此之外
大语言模型还可以回答参观者的问题
并且计划出机器人下一步应该采取的行动
你可以将大语言模型理解为一个即兴的演员
在有了大致的脚本之后
它能够临时填补空白的动作场景
这种组合的方式充分发挥了大语言模型的优势
同时规避了大语言模型可能带来的风险
因为我们都知道
大语言模型的幻觉现象很严重
容易添加一些听起来似是而非的细节
不过幸好在这类参观过程中
并不太强调事实准确性
所以机器狗只需四处走动
并谈论它所看到的事物
带来一些娱乐性、互动性和细微的差别即可
从整体上来看
我们还需要集成一些简单的硬件
以及几个可以协同运行的软件模型
在硬件方面，首先是音频的处理功能
这样Spot才能既向观众演示
又能听到参观团的提问和提示
团队3D打印了一个Respeaker V2扬声器的防震支架
这是一个环形阵列麦克风
上面有LED指示灯
通过USB连接到Spot的EAP 2有效载荷上
机器人的实际控制权被下放给一台机外电脑
比如台式电脑或者笔记本电脑
这台电脑通过SDK与Spot进行通信
在这张图中可以看到Spot的各个硬件装备部分
其中1是Spot EAP 2
2是Respeaker V2，3是蓝牙扬声器
4是Spot Arm和机械臂摄像头
在软件方面
波士顿动力团队使用了OpenAI的ChatGPT API
包括gpt-3.5和gpt-4
还测试了一些较小的开源模型
这让Spot具备了不错的对话能力
而ChatGPT对机器人以及语言的控制
是通过精心的提示工程实现的
受到微软的启发
他们让ChatGPT看起来像是在编写 python脚本代码一样
以此来提示ChatGPT
波士顿动力团队以注释的形式为大语言模型提供了英文文档
然后将模型的输出当作python代码进行评估
这样模型就可以访问自主SDK
以及带有每个地点单行描述的旅游景点地图
并能说出相应的短语或者提出问题
之后
波士顿动力团队向大语言模型提供了一个包含周围内容结构化信息的「状态字典」，
最后再发送一条提示
要求大语言模型去执行某些操作
比如说，执行API的某个动作
经过测试，团队得出的结论是
「切记简明扼要」这点非常重要
这样既能限制要执行的代码量
又能在机器人响应时保持可控的等待时间
因为OpenAI现在已经提供了一种结构化的方式
来指定ChatGPT调用的API
所以就不需要在提示中再提供所有的细节信息了
接下来
为了让Spot与观众和环境互动
波士顿动力集成了VQA和语音转文本软件
他们将Spot的机械臂摄像头和前视摄像头输入BLIP-2
并在VQA模型或者图像字幕模型中运行
大约每秒运行一次
结果直接会被输入到提示中
这样就可以识别出摄像头中的物体和场景
比如说，在一个建筑地板上
有一个黄色的警示标志
或者是工厂中的一扇门上面有一个牌子
或者一个黄色的机器人正在走过工厂中的一台汽车
等等
为了让机器人也能「听见」，
团队又将麦克风数据分块输入到了OpenAI的Whisper程序
并把它转换为英文文本
当机器狗听到唤醒词「嘿，Spot！
」之后
系统再将文本输入到提示音中
在ChatGPT生成了基于文本的回复之后
还需要通过文本转语音的工具
来运行这些回复
这样才能让机器人真正与参观者实现对话
在尝试了很多文本转语音的方法
包括从最基本的espeak
到最前沿的bark
波士顿动力最终选择了ElevenLabs
为了减少延迟
他们将文本以短语的形式
并行流式传输给TTS
然后再串行播放生成的音频
最后一项工作就是为Spot创建一些默认的肢体语言
Spot的3.3版本包括了检测和跟踪机器人周围移动物体的功能
可以提高机器人在人和车辆周围的安全性
波士顿动力恰好利用了这个系统
来让机器狗猜测最近的人的位置
然后将手臂转向那个人
此外
他们在生成的语音上还使用了低通滤波器
然后再转化为机械臂的轨迹
这样就可以达到类似于木偶开口说话的形式
特别是在给机械臂添加服装和瞪大的眼睛之后
这种错觉就更强了
有趣的是，在实验过程中
研究人员还发现了一个奇怪的现象
当研究人员向机器狗询问波士顿动力的执行董事
马克·雷伯特（Marc Raibert）是谁的时候
它回答，我不知道他是谁
让我们去服务台问一问吧
到了服务台
机器狗会继续向服务人员询问马克·雷伯特是谁？
而当研发人员向机器狗提问
你的父母是谁的时候
机器狗走向了Spot V1和Big Dog的展示区
并认为这些机器人是它的父辈们
以上就是对Spot最新进展的介绍
大飞我觉得
大语言模型结合机器人的领域
具有非常大的想象空间
具身智能也是AI最近非常火爆的一个方向
既有埃隆马斯克的Optimus机器人
也有像稚晖君这样的创业公司在做
应该算是大语言模型和人类实际生活能够产生最紧密关系的场景了
相信也有很多人幻想着《西部世界》真的到来
好了，本期视频内容就到这里
大家觉得能够开口说话
跟人交流的机器人狗子
能用在哪些有意思的地方呢
欢迎在评论区留言
我们下期节目再见
