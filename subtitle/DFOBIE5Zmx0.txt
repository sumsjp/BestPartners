大家好，这里是最佳拍档，我是大飞
应该说，生成式AI市场的迅猛增长
离不开对高性能计算硬件
特别是对英伟达GPU芯片的巨大需求
在这波AI浪潮的助推下
英伟达的市值在很短时间内就突破2万亿美元
仅次于苹果和微软
根据数据统计公司Statista的预测
预计2024年生成式AI的市场规模
将达到666.2亿美元
到2030年市场规模有望达到2070亿美元
由于芯片对AI的重要性和英伟达的统治地位
传统半导体厂商和大型科技公司
都在持续加码AI芯片领域
希望能够从英伟达手中抢下一定市场份额
或者摆脱对英伟达的依赖
于是就在4月9日这一天
英特尔、AMD、谷歌都发布了与AI有关的芯片产品
意图从英伟达手中分一杯羹
今天我们就来盘点一下
先来说英特尔
在年度Intel Vision 2024会议上
英特尔推出了新的AI芯片Gaudi 3
这个Gaudi系列芯片就是为数学而生的
来自于英特尔2019年以20亿美元收购的AI芯片初创公司Habana Labs
我们先来简单回顾一下它的前两代
Gaudi 1于2019年6月由Habana Labs发布
采用台积电16纳米工艺
包括了一个通用GEMM Engine矩阵数学引擎
以及八个带有本地内存的张量处理核心TPC
GEMM引擎可以以16位精度
对全连接层、卷积和批量GEMM处理进行数学运算
而TPC是一种特殊的SIMD处理器
用来处理其他机器学习操作
共享的SRAM内存容量为24MB
带宽为1TB/秒
Gaudi 2于2022年5月发布
采用台积电7纳米工艺
通过2.5DCoWoS封装来链接四个HBM2内存堆栈
每个堆栈8GB
总共32GB内存，聚合带宽为1TB/秒
Gaudi 2芯片还具有10个100Gb/秒的以太网RoCE端口
最多支持128个完全连接的节点
以及一个PCI-Express 4.0 x16控制器来连接主机CPU
Gaudi 2的共享SRAM内存从24MB增加到了48MB
TPC的数量增加了3倍
达到24个单元，GEMM单元
也就是矩阵数学引擎的数量也增加了一倍
以太网端口数量增加了2.4倍
达到24个端口
从根本上提高了Gaudi集群的可扩展性
而这一次发布的Gaudi 3
与NVIDIA最近发布的Blackwell架构类似
也是双芯片设置
采用了台积电5nm工艺
从2个矩阵数学引擎和24个张量核心
扩展到8个矩阵数学引擎和64个张量核心
Gaudi 3芯片FP8精度的总吞吐量
达到了1835TFLOPS
这使得Gaudi 3使用8位浮点计算产生的AI算力
是Gaudi 2的两倍
而BFloat 16格式的算力提升则达到了四倍
每张芯片的板载SRAM为48MB
所以整个芯片的SRAM是96MB
SRAM的总带宽为12.8TB/秒
Gaudi 3还包括了24个200Gbps的RoCE以太网控制器
可提供纵向和横向扩展连接
比Gaudi 2上的100 Gbps增加了一倍
这块Intel也采用了与NVIDIA相反的做法
将以太网扩展到芯片级别
而不是将NVLink扩展到机架级别
不过
Gaudi 3采用了较为过时的HBM2e
而没有选择HBM3或者HBM3e
也因此可用的最高容量堆栈为16GB
提供总共128GB的内存
当然了
会上少不了要和英伟达的芯片做对比
英特尔内部评估显示
与英伟达H100显卡相比
Gaudi 3在16个加速器集群中
以FP8精度训练Llama2-13B时
性能比H100快1.7 倍
推理速度比H200快大约30%。
此外
正如Nvidia在CUDA领域的主导地位
这次英特尔也意识到了生态的重要性
它表示
正在与十几家合作伙伴共同创建企业级的AI开放平台
为公司提供优化运行AI模型的系统
同时整合了多家供应商的软硬件
目前英特尔的重点是支持多模态训练和推理模型
以及RAG
也就是检索增强生成
Gaudi 3芯片预计在今年第三季度会全面上市
第二季度会提供给OEM厂商
具体价格还没有透露
其次就是AMD
同一天也更新了自己的Versal系列芯片
不过这次更新可能并不是AMD产品的重点
并没有开专门的发布会
但是也值得一提
同英特尔有些类似的是
第一代的Versal系列芯片是由赛灵思（Xilinx）在2018年10月正式发布的
不过，在2020年10月
AMD宣布以约350亿美元的全股票交易收购赛灵思
直到2022年2月
最终以大约498亿美元正式收购完成后
AMD才开始陆续推出相关的新产品
新推出的Versal AI Edge Series Gen 2和Versal Prime Series Gen 2采用单芯片智能方案
集成了用于预处理、AI推理和后处理的多种处理器
可以为AI驱动的嵌入式系统
提供端到端加速
与初代相比
第二代Versal系列产品组合的能源效率提高了3倍
新集成的Arm CPU的标量计算能力提高了10倍
具体来说
第二代Versal AI Edge系列升级了内部的CPU核心
由原来的Arm Cortex-A72升级为了专门面向汽车和工业的高性能CPU内核Cortex-78AE
支持2核到8核可选
可以根据需要来进行配比平衡
并且将Arm Cortex-R5实时处理器
升级为了Cortex-R52实时处理器
最高支持10核
进一步提升了实时处理器能力
另外，还加入了Arm Mali G78AE GPU
进一步提升了图形处理方面的能力
同时升级了全新的AI引擎
在AI推理性能方面
每瓦TOPS性能提升到了上一代的2倍
在支持的数据类型方面
新增了对FP8、FP16、MX6、MX9等数据类型的原生支持
并且还支持数字信号处理、视觉以及其他推理以外的附加功能
而第二代 Versal Prime系列将面向传感器处理的可编程逻辑
与高性能嵌入式Arm CPU相结合
能够为传统的非AI嵌入式系统提供端到端加速
标量算力可以提升到初代的10倍
从而高效地执行传感器处理和复杂的标量工作负载
第二代Versal系列的目的主要在于满足自动驾驶、航空航天和国防、工业、医疗等行业的需求
预计2025年底进行量产
在此之前会提供评估样品
最后
谷歌也在Google Cloud Next 2024会议上
发布了名为Axion的全新处理器
这是谷歌为数据中心设计的首款基于Arm架构的定制CPU
虽然现在GPU是训练AI模型的主力
但是CPU在整个AI系统中也有其不可替代的作用
CPU更擅长处理序列任务和复杂的控制流程
在一些场景下
比如数据预处理、AI模型的部署和推理
以及处理不能高效利用GPU并行能力的任务时
CPU是更合适的选择
相较于市面上最快的基于Arm的同类产品
Google这次发布的Axion
性能提升高达30%。
相比基于x86的产品，性能提升了50%，
能效提高了60%。
而且由于建立在标准的Arm v9架构和指令集上
大多数应用无需重写代码
即可在Axion上运行
Axion今年晚些时候会提供给Google Cloud客户
而基于Axion处理器的虚拟机
将在未来几个月内提供预览
值得一提的是
谷歌现有芯片中除了最新的Axion
在此之前已经发布了五代张量处理器
也就是TPUv5
TPU是谷歌为机器学习开发的专用集成电路
自从2015年开始在内部使用
2018年向第三方开放，长期以来
一直服务于谷歌YouTube、Gmail和Android 等诸多产品
并且被用来训练谷歌Gemini大模型
有时间我们会专门做一期关于TPU的节目
TPU的最新版本是2023年12月发布的TPU v5p
它可以在8960个芯片的集群中运行
是上一代TPU性能的两倍
接近于英伟达H100芯片
在周二的发布会上
谷歌也宣布开发者现在可以通过谷歌云服务来访问并使用TPU v5p
可以看到
AI芯片领域的竞争正在加剧
面对英伟达在AI芯片市场的领先地位
英特尔、AMD 等传统半导体公司除了在技术上进行竞争
也在寻求合作伙伴，构建生态系统
努力提供更加完整的解决方案
而谷歌以及亚马逊、微软和Meta等科技巨头
也出于各种考量纷纷定制自己的AI芯片
首先，这有助于它们控制成本
根据CB Insights发布的2024年生成式AI预测报告
一颗H100的制造成本大约为3320美元
但是平均售价已经达到3万美元
最高售价甚至一度达到将近10万美元
自研芯片的成功
不仅能避免芯片短缺风险、降低依赖
还可以向使用云服务的企业出售芯片
此外，相比通用GPU
定制化的AI芯片不仅能够提供更加专业和高效的服务
还能够根据特定需求进行快速迭代和优化
理论上
如果能够确保产量和降低成本
谷歌等公司应该能够提供比英伟达更好的产品
如今
英伟达得益于性能、软件生态系统兼容性和市场影响力方面的综合优势
依然在AI芯片领域保持着明显的领先地位
相信在其他厂商不断追赶的同时
英伟达也并不会停滞不前
在刚刚新发布的Blackwell芯片的加持下
训练相同参数模型需要的GPU数量和消耗的电力
都可以缩减大约4倍
显然
AI芯片的发展对半导体产业产生了长远影响
接下来的问题是
谁能从英伟达的嘴里切走一大块蛋糕呢
欢迎大家在评论区留言
感谢大家观看本期视频
我们下期再见
