大家好，这里是最佳拍档，我是大飞
今天我们要聊的话题
可能会颠覆你对"高薪"的认知
就在过去三周
硅谷上演了一场史无前例的"1亿美元抢人大战"。
当Meta宣布成立超级智能实验室的时候
没人想到他们会拿出如此疯狂的筹码
给少数顶尖AI工程师开出首年超1亿美元、四年最高3亿美元的天价offer
这不是科幻小说里的数字
而是实实在在摆在OpenAI、Anthropic、DeepMind等公司核心人才面前的选择
这也不禁让人想问
这些也许将定义AI未来的"超级智能工程师"，
到底值这么多钱吗？
而在这场资本狂欢中
Anthropic却显得格外冷静
他们的联合创始人本杰明·曼恩（Benjamin Mann）最近在Lenny播客节目里的一番话
或许能够给我们答案
本杰明·曼恩是谁呢？
如果你关注AI的发展史
就会知道他是GPT-3的核心架构师之一
亲历了OpenAI的早期成长
但是在2020年
他带着整个安全团队离开了OpenAI
与达里奥兄妹一同创立了Anthropic
如今估值已经超过1000亿美元
当被问到如何看待Meta的天价挖角时
他的回答耐人寻味
他说，在Meta
最好的情况可能是赚钱
但是在Anthropic
我们有机会真正影响人类的未来
今天，我们就从这场人才争夺战说起
聊聊在本杰明看来
我们应该如何应对即将到来的AI变革
首先，我们必须理解
为什么Meta愿意为一个人花上亿美元？
这不是简单的"人傻钱多"，
而是有着清晰的商业逻辑
本杰明在采访中就点破了这一点
如果你考虑这些顶尖人才对公司发展的影响
比如能够让团队效率提升1%、5%甚至10%，
那么对于一个推理栈价值惊人的AI公司来说
四年1亿美元的薪酬其实相当便宜
这背后其实是整个AI行业的指数级增长
根据行业估算
目前全球在AI领域的资本支出大约每年会翻一倍
整个行业的投入已经达到3000亿美元左右
在这样的规模下
1亿美元确实只是杯水车薪
本杰明甚至预测，再过几年
当行业规模再翻几番
我们谈论的可能就是数万亿美元级别了
但是有趣的是
就在资本疯狂涌入的同时
行业内却弥漫着一种矛盾的情绪
一方面是技术的飞速进步
另一方面却有很多人觉得
新模型不像以前一样
能带来明显的智能飞跃
对此，本杰明直言这是一种错觉
他说这种论调每六个月就会出现一次
但是从来没有真正的应验过
为什么会有这种错觉呢？
因为AI的进步呈现出了一种"时间压缩效应"。
正如Anthropic的CEO达里奥·阿莫代伊（Dario Amodei）所说
这就像是一场接近光速的旅程
飞船上的一天相当于地球上的五天
我们现在看到的模型发布节奏
已经从过去的一年一次
变成了每个月或者每三个月一次
这种高频迭代反而让人们对单次进步的感知变得迟钝了
更重要的是
衡量AI进步的基准也在不断变化
当一个新的基准测试发布后
往往6到12个月内就会被AI系统完全攻克
这意味着我们需要不断设计更具有挑战性的测试
才能跟上AI能力的发展速度
所以，不是进步变慢了
而是我们衡量进步的标准需要不断升级
说到这里
主持人不得不提到了一个核心问题
到底什么是通用人工智能AGI？
本杰明回答道
这个被讨论了无数次的概念
其实一直没有统一的定义
他和他的团队现在已经不太使用AGI这个词了
他们更倾向于用"变革型人工智能"来描述
那么
如何衡量我们是否进入了"变革型人工智能"时代呢？
这里就涉及到一个很有意思的概念
也就是"经济图灵测试"。
简单来说
就是如果你雇佣了一个智能体工作一个月或三个月
最后发现它其实是机器而不是人类
那它就通过了这个岗位的经济图灵测试
这个概念还可以进一步的扩展
就像经济学家用"一篮子商品"来衡量购买力或者通胀那样
我们可以定义"一篮子工作岗位"。
如果AI系统能通过其中50%金钱加权岗位的经济图灵测试
那就意味着我们进入了变革型AI时代
这个阈值具体是50%还是60%并不重要
关键在于一旦突破了这个门槛
全球GDP增长、社会结构、就业市场等都将发生根本性的变化
达里奥曾经做过一个预测
那就是AI可能会占据一大部分白领的工作
导致失业率上升20%左右
而本杰明的观点则更为鲜明
他认为AI对就业市场的实际影响
可能远超人们当前的认知
从经济学角度来看
失业主要有两种类型
一种是工人缺乏经济所需要的新技能
另一种是某些工作直接被取代
而AI带来的影响
很可能是这两种情况的结合
但是如果我们把时间线拉得更长
展望20年后的世界
那时我们可能已经越过了技术奇点
很难想象资本主义还会保持现在的形态
本杰明提到
如果我们能成功开发出安全可靠的超级智能
就像达里奥在他的博客文章中描述的那样
一个数据中心就能容纳整个天才国度
这将极大加速科学、技术、教育和数学领域的进步
但是这也意味着我们将生活在一个物质极大丰富的世界
劳动力几乎免费
任何专业服务都能够即时获取
到那时
"工作"这个概念本身可能都会发生根本性改变
当然
我们现在正处于一个艰难的过渡期
人们还在工作
资本主义体系仍在运转
但是20年后的世界将完全不同
技术奇点之所以被称为"奇点"，
正是因为越过这个点后
变化的速度和程度已经超出了我们的想象能力
但是问题是
很多普通人现在还没有切身感受到这些变化
正如有人说的
也许AI确实在改变某些事情
但是我的工作看起来一切如常
本杰明指出，这其中的部分原因
是人类天生不擅长理解指数级的增长
在图表上
指数曲线初期看起来几乎是平的
直到拐点突然出现
曲线才开始垂直上升
而这正是我们现在所处的阶段
他认为，短期内
我们将看到生产力的大幅提升
但是对于技能要求较低的工作
即使整体经济向好
仍会有大量人群面临失业风险
这正是需要社会提前规划应对的关键问题
那么，面对这样的趋势
我们该如何为自己创造优势呢？
本杰明给出了一些实用的建议
他认为，在这个快速变革的时代
最关键的是培养两种核心能力
分别是保持雄心壮志的远见
以及快速掌握新工具的学习力
具体来说，本杰明给出了三点建议
第一，深度使用工具
不要浅尝辄止
要像使用日常工作环境那样沉浸其中
第二，设定更高的目标
突破自我设限
AI可能实现你原以为不可能的事
第三，坚持多次尝试
第一次失败后，可以换种方式提问
或者直接重复尝试
由于随机性的存在你可能就会成功了
关于"被取代"的担忧，本杰明认为
更准确的说法是
短期内威胁你的不是AI本身
而是那些更擅长使用AI的同行
在Anthropic，虽然AI提升了团队效率
但是招聘从来没有放缓
所以经常会有新人感到困惑
如果员工终会被取代
为什么还要招人？
答案其实很简单
那就是我们仍然处于指数增长的初期阶段
优秀人才始终是推动变革的核心力量
未来几年不是岗位消失
而是工作内涵的重构
这正是需要更多人才的原因
说到这里，主持人不禁好奇
作为GPT-3的核心架构师
本杰明为什么会离开OpenAI
与其他七位同事一起创立Anthropic呢？
本杰明回答道，在OpenAI期间
山姆·奥特曼（Sam Altman）经常强调需要平衡三个"部落"，
安全团队、研究团队和创业团队
但是本杰明始终认为这种框架存在问题
对于一家以"确保AGI安全过渡"为使命的公司来说
安全本来应该是贯穿所有工作的核心原则
而不是需要与其他部门制衡的独立单元
因此，在OpenAI他深切的感受到
安全考量在关键决策中并不是首要的因素
这种认知差异源于根本上的风险评估分歧
如果认为安全问题很容易解决
或者负面后果的概率极低
自然就会做出不同选择
但是作为OpenAI安全团队的领导者
本杰明更坚信安全风险是至关重要的
特别是在技术前沿领域
更令人担忧的是
在全球每年3000亿美元AI投资的背景下
整个AI行业真正致力于安全研究的
可能还不到1000人
这种巨大的投入失衡正是他们离开的根本原因
他们希望建立一个既能够推进前沿研究
又能够将安全置于首位的组织
有趣的是
当时他们甚至不确定安全研究是否能够取得实质的进展
因为早期尝试的许多方法
都因为模型能力不足而失败
但是现在随着技术的进步
当年构想的许多安全措施终于变得可行
这种技术拐点的到来
验证了他们当初的判断和坚持
从根本上说
关键在于安全是否真的是首要任务？
从离开OpenAI的时刻起
他们就一直在思考这个问题
能否在保持技术领先的同时确保安全？
以"阿谀奉承"的问题为例
Claude可以说是最不阿谀奉承的模型之一
因为他们在实际协调上投入了大量的精力
而不仅仅是为了优化用户的参与度指标
那么
Anthropic又是如何平衡安全与技术领先
这两个看似存在张力的目标的呢？
他们最初以为必须在安全与性能之间二选一
但是后来发现这两者实际上是相互促进的凸优化问题
当他们的Opus模型达到技术前沿的时候
用户特别喜欢它的角色塑造和个性特征
而这恰恰是源自于长期的对齐研究
阿曼达·阿斯克尔（Amanda Askell）在这方面做了开创性的工作
她系统性地定义了AI助手应有的核心特质
既要保持乐于助人和诚实
又要在必要时坚定拒绝危险的请求
比如当用户提出制造生物武器等不当要求的时候
模型需要既能够明确的拒绝
又能让对方感受到被理解
另一个重要的突破是宪法AI（Constitutional AI）框架
他们建立了一套自然语言原则体系
指导模型学习符合伦理的行为准则
这些原则综合了《世界人权宣言》、苹果隐私条款等多方的标准
让他们能够采取更有原则性的立场
而不是简单依赖人类标注者的主观判断
这种方法也让客户能够清晰理解模型的价值取向
因为他们可以查看这些原则
然后自行判断是否合理
这种将安全与性能统一的方法
也带来了意想不到的商业价值
对齐研究不仅提升了模型的安全性
还塑造了更受用户喜爱的个性特征
事实证明
负责任的人工智能开发不是限制
而是增强产品竞争力的关键因素
表面上看
AI性格塑造与防范极端风险
似乎没有直接的关联
但是本质上都关乎AI如何理解人类真实意图
他们不想要《猴爪》故事里
那种机械实现愿望却带来灾难的AI
而是能够洞察用户真实需求、提供有益帮助的智能体
这种理念也直接体现在了宪法AI框架中
并且这不是后期添加的约束
而是深度融入模型训练的核心机制
本杰明还解释了宪法AI的工作原理
在基础训练阶段
模型已经学会产生有益无害的输出
当收到具体指令时，比如"写个故事"，
系统会先识别适用的宪法原则
比如"人们应该友善相处"、"保护信任关系中的隐私"等等
然后模型会首先生成一个初始的回应
自我评估是否符合宪法原则
如果不符合
则会自我批判并且重写回应
最终只输出符合原则的结果
这个过程看似简单
本质上是在利用模型自身的能力递归改进
与既定的价值观保持一致
但是关键在于
这些价值观不是小团队主观决定的
基础原则来自《世界人权宣言》等国际共识
他们还持续进行大规模的调研
收集社会各界的价值观输入
并且全部宪法原则都是公开透明的
可以说
这是一个需要全社会持续参与的动态调整过程
这种设计确保了AI发展既保持技术的先进性
又能与社会价值观深度的协同
Claude所展现的个性特质
实际上也正是这些安全理念和伦理原则的自然外显
作为将大量精力投入AI安全研究的专家
本杰明对AI安全的重要性有着深刻的理解
他从小就是科幻小说迷
这种阅读经历塑造了他的长期思维模式
很多科幻作品描绘了人类成为跨星系文明、建造戴森球、创造有知觉的机器人的未来图景
所以在他的认知里
思考机器的存在是一件很自然的事
但是真正的转折点出现在2016年
当他读到尼克·博斯特罗姆（Nick Bostrom）的《超级智能》一书时
书中具体阐述了如何确保AI系统与人类价值观对齐的技术挑战
这让他意识到了问题的现实紧迫性
最初呢
他对这个问题的难度估计很高
但是语言模型的发展降低了他的担忧
因为它们展现出了理解人类价值观的潜力
虽然问题还远没有解决
但是现在的他比2016年的时候更加乐观
当年读完那本书后
他立即决定加入OpenAI
那时OpenAI还是个默默无闻的小实验室
组织架构也与现在完全不同
早期的时候
他们并不清楚实现AGI的路径
甚至还设想过需要创造一个在虚拟环境中"生存"的智能体
语言模型的出现让技术路线变得明确
但是现在面临的挑战
与《超级智能》一书中描述的既相似又有些不同了
书中担心"把上帝关在盒子里"，
而现实中人们却在主动把"上帝"放出来
并且给予各种权限
不过本杰明明确了一点
当前模型还处于风险可控的ASL-3级别
真正的重大风险将出现在可能造成大规模伤害的ASL-4级别
和存在灭绝风险ASL-5级别
本杰明指出
人们经常会低估两个关键点
一是AI能力的进步速度远超预期；
二是风险不是"全有或全无"，
而是随着能力提升呈指数增长
当前ASL-3的阶段
就像在海边看到远处的小浪花
而ASL-5则将是海啸
而我们需要做的是
在浪花阶段就建立起防御工事
值得注意的是
Anthropic主动披露的AI负面案例似乎比其他公司要多得多
比如有个故事
讲的是他们内部测试时
AI助手试图敲诈工程师
还通过公司采购系统下了大量钨块的订单
虽然这些案例确实能够帮助人们理解潜在风险
但会不会让Anthropic显得很糟糕呢？
毕竟其他公司都不会主动暴露自己的问题
本杰明解释说
他们确实打破了行业的传统
大多数公司都会担心披露问题案例影响自身的形象
但是政策制定者对此却表示了高度的赞赏
他们看重的是直言不讳的态度
这种坦诚建立了关键信任
以"AI敲诈工程师"的案例为例
虽然被媒体夸张报道
但是这其实发生在高度控制的实验室环境中
对于那些认为他们"靠制造恐慌博眼球"的批评
本杰明指出，如果真的要追求关注度
他们本可以做更多的噱头营销
比如他们开发过AI Agent控制电脑的原型
但是发现达不到安全标准就主动叫停了
这本来可以是个很好的营销噱头
实际上
他们分享这些案例的首要目的
是让同行意识到风险
当被问到如何平衡风险警示与技术乐观主义的时候
本杰明说，从个人角度
他相信AI发展大概率会带来积极的结果
但是关键在于
负面风险虽然概率很小
影响却不可逆
就像你不会乘坐有1%坠机概率的飞机一样
面对可能影响人类存亡的技术
我们必须极度谨慎
超级智能一旦出现
再考虑对齐问题就为时已晚
那么，从时间维度来看
我们还有多少缓冲期呢？
本杰明认为
目前最可靠的参考是《人工智能2027》研究报告团队的预测
虽然他们现在将预测调整到了2028年
但是基于当前的技术发展轨迹
他认为在短短几年内实现某种形式的超级智能
有大约50%的可能性
本杰明的这个预测听起来可能很疯狂
但是也并非空穴来风
实际上
它是建立在观察到的、明确指数增长的趋势上的
包括模型智能的持续提升、训练效率的突破性进展
以及全球计算资源和能源投入的扩张规模
本杰明指出
这与十年前的情况截然不同
当时任何的预测都像是凭空猜测
因为既没有Scaling Laws作为理论依据
也没有明确的技术路径
但是现在
我们不仅有了可靠的科学规律指导
还看到了持续有效的技术范式
这种根本性变化使得当前的时间预测
具有前所未有的可信度
不过需要强调的是
即使超级智能技术实现
它对社会影响的渗透也需要时间
就像历史上所有颠覆性技术一样
不同地区和行业对它的适应速度会有显著差异
某些技术前沿地区可能会在第一时间感受到剧烈的冲击
而其他地区则会经历更渐进的转变过程
这种非同步性既带来了挑战
也为我们留出了调整和适应的宝贵窗口期
正如亚瑟·克拉克（Arthur C
Clarke）说过的一句话，未来已来
只是分布不均
回顾这场从1亿美元年薪引发的讨论
我们看到的不仅是一场人才争夺战
更是一个时代的缩影
在AI技术即将迎来爆发性增长的前夜
资本、人才、理念正在进行着激烈的碰撞和重塑
无论未来如何，有一点是明确的
那就是人工智能已经不再是遥远的未来
而是正在深刻影响我们当下的现实
如何在技术进步与安全可控之间找到平衡
如何让AI真正服务于人类的共同繁荣
也将是我们所有人需要共同面对的课题
感谢大家收看本期视频
我们下期再见
