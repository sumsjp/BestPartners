大家好
这里是最佳拍档
我是大飞
先给大家看一段这两天
在国内外的社交媒体上
都已经传疯了的视频
相关的关键词搜索呢
不仅在B站排名第一
微博推特也是火的一塌糊涂
网友们纷纷直呼PS已死
那这到底是怎么回事呢
原来现在P图真的只需要轻轻点两下
AI就能够彻底理解你的想法
小到竖起狗子的耳朵
大到让整只狗从站起到蹲下
甚至让马叉开腿跑跑步
都只需要设置一个起始点和结束点
然后拽一拽就能够搞定
而且不光是动物
像汽车这样的非生物
也能够一键拉升底座
甚至升级成加长的豪华车
而这些呢只是AI修图的基本操作
如果你想要对图像实现更精准的控制
你只需要画个圈给指定区域涂白
就能够让狗子转个头看向你
或者让照片中的小姐姐眨眨眼
甚至是让闭着嘴的狮子张开大嘴
你都不用找牙齿的素材
AI就能够自动给它安上牙齿
完整的演示视频呢
我们会放在本视频的结尾
让大家一口气看个够
那这个有手就能做的修图神器
来自于一个MIT谷歌
马克思普朗克计算机科学研究所等机构
联手打造的DragGAN新模型
这篇论文呢已经入选SIGGRAPH 2023
实在让人想不到啊
在现在这个扩散模型独领风骚的时代
竟然还能有人把GAN玩出新花样
目前呢
这个项目在Github上已经有5,600多的star
热度呢还在不断的上涨
关键是他一行代码还没发呢
预计要6月之后才会开源出来
那么这个DragGAN模型究竟长什么样子
它又如何实现的刚刚神一般的操作呢
实际上这个Dragon模型
本质上是为各种GAN开发的一种
交互式的图像操作方法
它以StyleGAN2的架构为基础
实现了点点鼠标
拽一拽关键点就能够P图的效果
具体来说呢
就是给定StyleGAN2生成的一张图片
用户只需要设置几个控制点
也就是红点
以及目标点也就是蓝点
并且呢圈出你要移动的区域
比如说你想让狗转一下头
那就把狗头给圈出来
然后呢模型就会迭代的去执行
运动监督和点跟踪这两个步骤
其中运动监督
会驱动红色的控制点
向蓝色的目标点移动
点跟踪呢则用来更新控制点
来跟踪图像中的被修改对象
这个过程呢
会一直持续到控制点
达到他们对应的目标点
这个运动监督和点跟踪
就是Dragon模型中最主要的两个组件
先说说这个运动监督
在这篇论文之前啊
其实业界呢
还没有太多关于如何监督GAN
生成图像的点运动的研究
而在这项研究中呢
作者提出了一种不依赖于任何
额外神经网络的
运动监督损失
他的关键思想是
生成器的中间特征
其实具有很强的鉴别能力
因此一个简单的损失就足以监督运动
所以呢这个DragGAN的运动监督
是通过生成器特征图上的
偏移补丁损失
shifted patch loss来实现的
比如说你要移动控制点p到目标点t
就要监督p点周围的一小块patch
也就是红色的圈
向前移动的一小步
也就是蓝色的圈
那再来说说这个点跟踪啊
之前说到这个运动监督
会产生一个新的latent code
一个新的特征图以及新的图像
那由于运动监督步骤
不太容易能提供控制点精准的
新的位置
因此呢
我们的目标是更新每个手柄点p
让他能够跟踪对象上的对应点
以前的点跟踪呢
通常都是通过光流估计模型
或者是粒子视频方法来实现的
但是这些额外的模型呢
可能会严重的影响效率
并且在GAN模型中存在违影的情况下
可能会使模型遭受累积误差
因此呢作者提供了一种新的方法
可以通过最近邻检索
在相同的特征空间上进行点跟踪
那有了以上两大组件之后呢
DragGAN就能够通过精准控制像素的位置
来操作不同类别的对象
完成姿势形状布局等方面的变形
论文的作者表示
由于这些变形
都是在GAN学习的图像流形上进行的
它遵从了底层的目标结构
因此呢面对一些复杂的情况
比如说有遮挡的时候
DragGAN也能够产生逼真的输出
那这种精准控图的效果
是否需要巨大的算力才能够支持呢
并不是
大部分的情况下
每一步拖拽修图
单张RTX 3090 GPU
在几秒钟内就可以搞定
不仅仅速度快
生成图像的效果
也超越了很多同类的模型
包括RAFT和PIPs等等
除此之外呢如果增加关键点的数量
还能够实现更加精细的AI修图效果
即使用在人脸
这种对修图要求比较严格的照片上
也是完全没有问题的
那DragGAN可以适用的图像类型呢
不止于开头我们展示的人物和动物
还包括汽车细胞
风景和天气等等等等多种类型
而且呢从站立到坐姿
从直立到跑步
从跨着站立到并腿站立
这种姿势变动较大的图像呢
也能够通过DragGAN来实现
在PS界啊曾经有一个很古老的段子
就是说把大象转个身
现在可能也要成真了
不过呢也有网友指出了DragGAN
目前面临的一些问题
比如说
由于它是基于StyleGAN2生成的图片
来进行P图的
但是呢StyleGAN2它的训练成本很高
因此呢DragGAN距离真正的商业落地
可能也还有一段距离
另外呢我们刚刚说到的修图速度快
主要还是基于265*265分辨率的图片
至于模型
是否能够扩展到265*265以外的图像
生成的效果又怎么样
这些呢还都是未知数
不过可以肯定的是
至少高分辨率的图像
从生成的时间来看
肯定还要更长的
那这个模型呢
实际上手的效果究竟如何
还要等到6月份论文代码开源之后
我们一测来见真章
最后呢
我们再简单介绍一下这个论文的团队
DragGAN的作者呢一共有6位
分别来自于
马克思普朗克计算机科学研究所
萨尔布吕肯视觉计算
交互与AI研究中心
MIT
宾夕法尼亚大学以及谷歌的AR VR部门
其中呢
包括了两位华人
第一作者潘新钢
他本科于2016年毕业于清华大学
博士于2021年毕业于香港中文大学
师从汤晓鸥教授
现在呢
是马普计算机科学研究所的博士后
今年6月份
他将进入南洋理工大学担任助理教授
另一位呢是Liu Lingjie
2019年香港大学博士毕业
随后在马普信息学研究所做博士后研究
现在呢是宾夕法尼亚大学的助理教授
领导该校的计算机图形实验室
也是通用机器人自动化传感与感知
GRASP实验室的成员
比较搞笑的是
为了展示DragGAN模型的可控性
一作作者潘新钢还亲自上阵
演示了自己从生发瘦脸和露齿笑
这三个连续的P图效果
好了主要的内容就介绍到这里
让我们来看一下完整的demo视频演示吧
感兴趣的小伙伴们
欢迎订阅我们的频道
我们下期再见
