大家好，这里是最佳拍档，我是大飞
今天要给大家讲一个听起来几乎不可能完成的项目
三个月的时间
从零开始
居然真的有人做出了一块能运行的TPU
而且既能推理又能训练
关键是还开源了
听到这里你可能会觉得不可思议
毕竟TPU作为谷歌设计的专用AI芯片
从2015年部署到现在
已经迭代到了第七代
背后是顶尖的工程师团队和先进的制程工艺
但是来自加拿大西安大略大学的工程师们
用一个暑假的时间
完成了这个看似不可能的挑战
他们把这个项目叫做TinyTPU
先说说这个项目的发起者们
他们并不是芯片设计专业的学生
甚至在开始的时候
对多层感知机（MLP）这样的神经网络基本概念
都需要从头理解
更有意思的是
他们为了搞清楚网络推理和训练的数学原理
竟然手工计算了所有需要的运算
这种从最基础开始的钻研精神
可以说为整个项目奠定了独特的基调
为什么他们要做这样一件事呢？
用他们自己的话说
一方面是觉得构建一个机器学习的专用芯片
这事听起来就很酷，另一方面
当时还没有一个能够同时支持推理和训练的、机器学习加速器的完整开源代码库
更重要的是
他们没有硬件设计的相关经验
没想到这反而成了一种动力
因为自己无法准确的估计难度
也就少了很多的心理负担
他们还定下了一个特别的设计理念
那就是始终尝试那些"不靠谱的方法"，
Hacky Way
意思是在咨询外部资源之前
先试试自己想到的那些"愚蠢"的想法
在这种想法的驱动下
他们简直不是在逆向工程谷歌的TPU
而是真正的在重新发明它
其中的很多关键机制
都是他们自己推导出来的
还有一个挺有意思的点
就是他们想把这个项目
当作不依赖AI代写代码的一个练习
因为现在很多人遇到一些小问题
就会习惯性的求助AI工具
而他们希望培养一种能独立解决难题的思维方式
在整个过程中
他们尽可能多地学习了深度学习、硬件设计和算法创建的基础知识
并且发现是最好的学习方式
就是把所有的知识画出来
这一点在他们的技术文档里体现得淋漓尽致
在正式讲解他们是如何做到的之前
有必要先明确一下
这个TinyTPU并不是谷歌TPU的1:
1的复制品，而是他们基于自己的理解
重新发明的一个尝试
所以我们接下来看到的
可以说是一条全新的探索路径
首先，我们先要了解什么是TPU？
TPU是谷歌设计的专用集成电路
也就是ASIC芯片
专门用来提高机器学习模型的推理和训练速度
和GPU既能渲染图像
又能运行机器学习任务不同
TPU完全专注在数学运算方面
这也是它效率如此之高的一个重要原因
因为在芯片领域
专注单一任务
往往比兼顾多项任务，更容易做得好
要想理解硬件设计
我们有两个基础的概念需要知道
第一个是时钟周期
这是硬件处理操作的时间单位
开发者可以设置任意的时间段
通常在1皮秒到1纳秒之间
所有操作都在这个时钟周期之间执行
第二个是硬件描述语言Verilog
它和软件的编程语言不同
不是以程序形式执行的
而是靠合成布尔逻辑门
包括与、或、非等等
这些逻辑门组合起来
就能构建任何芯片的数字逻辑
比如在Verilog中
要想实现一个简单的加法运算
你需要将信号b在下一个时钟周期的值
设置为信号a的当前值
这和软件中变量立即更新的方式很不一样
TPU之所以高效的另外一个原因
在于它在执行矩阵乘法时的优势
在Transformer模型中
矩阵乘法会占到计算操作的80%-90%，
在超大型模型中甚至可以高达95%，
即便是在CNN中，也能占70%-80%。
每个矩阵乘法都代表了多层感知器MLP中单个层的计算
而深度学习模型往往有多层的MLP
这就使得TPU在处理大型模型时的效率倍增
那么，这群零基础的工程师
是如何打算从零开始构建TPU的呢？
他们的起点非常基础
只知道y = mx + b这个方程
是构建神经网络的基础模块
但是要想在硬件中实现神经网络
必须完全理解其背后的数学原理
所以在写任何代码之前
团队每个人都手工计算了一个简单的、用来解决异或（XOR）问题的2→2→1的MLP网络
为什么要选择异或问题呢？
因为它被称为神经网络的"Hello World"，
是最简单的需要用神经网络解决的问题之一
像与（AND）、或（OR）这些逻辑门
用一条线
也就是一个神经元
就能区分输入对应的输出
但是异或问题需要弯曲的决策边界
这只有MLP才能实现
假设我们要进行连续的推理
比如像自动驾驶汽车那样
每秒会进行多个预测
这就需要同时处理多条数据
而这种数据通常是多维的
有很多的特征
需要大维度的矩阵
但是异或问题简化了维度
只有两个特征
0或者1，以及4种可能的输入
从而形成一个4x2的矩阵
4是批处理大小，2是特征大小
这就为他们的实验提供了一个合适的简化模型
接下来
异或的输入矩阵和目标输出都很简单
四行分别代表四种二进制组合（0
0）、（0,1）、（1,0）、（1
1），对应的目标输出是0、1、1、0
矩阵乘法是神经网络计算的核心
在数学上表示为 XW^T + b
其中X是输入矩阵
W是权重矩阵，b是偏差向量
为了在硬件中执行矩阵乘法
他们还用到了一个关键结构
脉动阵列（systolic array）
只不过他们对这个脉动阵列做了简化
用2x2的阵列代替了TPUv1中的256x256阵列
但是数学运算是完全类似的
只是规模缩小了
实际上，TPU的核心就是脉动阵列
它由处理单元（Processing Elements ）
简称PE组成
这些PE以网格状的结构连接
每个PE都会执行乘法累加运算
也就是将传入的输入X与固定权重W相乘
再与传入的累加和相加
所有操作在一个时钟周期内完成
当这些PE连接起来
就能以脉动的方式来执行矩阵乘法
每个时钟周期可以计算输出矩阵的多个元素
输入从左侧进入脉动阵列
每个时钟周期向右移动到相邻的PE
累加和从第一行PE的乘法输出开始
向下移动，与每个连续PE的乘积相加
直到到达最后一行PE
成为输出矩阵的一个元素
正因为TPU专注在这个核心单元上
而矩阵乘法又占导了模型计算量的绝大部分
所以它能够高效地处理各种模型的推理和训练
我们再来具体看看异或问题的处理过程
他们的脉动阵列可以接受输入矩阵和权重矩阵
对于这个异或网络
初始化的权重和偏差有特定的值
要想将输入批次传入脉动阵列
需要两个步骤
首先要将X矩阵旋转90度，再错开输入
也就是每行要延迟1个时钟周期
包括输入权重矩阵的时候
也要进行类似的交错排列
并且需要转置，这里要说明的是
旋转和交错没有数学意义
只是为了让脉动阵列正常工作
而转置则是为了数学对齐
确保矩阵运算的正确
为了实现交错
他们在脉动阵列的上方和左侧
设计了几乎相同的累加器
由于激活是逐个输入的
所以适合采用先进先出队列（FIFO）的数据存储方案
但是他们的累加器有两个输入端口
一个用来手动写入权重
另一个用来将上一层的输出写回
作为当前层的输入
权重FIFO的逻辑类似
但是没有第二个端口
完成矩阵乘法之后
下一步就是添加偏差
他们在脉动阵列的每一列下
创建了一个偏差模块
当总和从最后一行流出时
直接会进入偏差模块计算预激活的值
用变量Z表示
偏差向量会被添加到每个Z行
这一步就像我们学过的线性方程一样
只是扩展到了多维形式
每一列都代表一个特征
之后需要应用激活函数
他们选择了Leaky ReLU
这是一个逐个元素的操作
所以在每个偏差模块下都有一个激活模块
偏差模块的输出会直接流入到激活模块
得到的结果用H表示
Leaky ReLU的计算规则是
输入为正的时候输出本身
为负的时候输出输入乘以0.5
因为这里的泄漏因子β=0.5（口误）
以异或网络的第一层为例
我们可以看到脉动阵列会先计算矩阵乘法
然后添加偏差b1
再应用Leaky ReLU得到H1
负值会乘以0.5，正值保持不变
这里有个关键的设计思路是流水线技术
为什么不把偏差和激活合并到一个时钟周期里呢？
因为流水线允许不同阶段的操作同时进行
就像装配线一样
当激活模块处理一个数据的时候
偏差模块已经在处理下一个了
这样能让所有模块保持忙碌
避免闲置
如果某个模块在一个周期内执行多个操作
就会成为瓶颈
所以将操作分解到多个时钟周期是更高效的做法
为了进一步提高效率
他们设计了一个称为"travelling chip enable"的信号
用紫色的圆点表示
由于所有组件都是交错排列的
只需在第一个累加器上
断言一个时钟周期的启动信号
就能够准确传播到相邻的模块
依次激活脉动阵列、偏差和激活模块
确保每个模块只在需要的时候工作
不浪费电量
接下来的挑战是如何处理多层网络
当开始新的层时
需要使用新的权重矩阵
而脉动阵列是权重平稳的
那么又该如何更换权重呢？
他们借鉴了电子游戏中的双缓冲概念
在游戏中
经常会用双缓冲来防止画面撕裂
原理其实就是隐藏像素的加载时间
这和他们面临的问题完全相同
于是
他们通过添加第二个"影子"缓冲区
在当前层计算的时候
去加载下一层的权重
从而让总时钟的周期减少了一半
为了实现双缓冲
他们还添加了两个信号
分别是用蓝点表示的"切换"信号
它会将影子缓冲区的权重
复制到活动缓冲区
从左上角传播到右下角
以及一个用绿点表示的"接受"标志
表示权重要向下移动一行
新权重进入顶行，每行依次下移
通过这两个信号的配合
就能让脉动阵列执行持续的推理
通过不断输入新的权重和数据
计算任意层级的前向传播
最大化PE的利用率
比如，在第二层中
第一层的输出H1成为输入
经过同样的矩阵乘法、加偏差、激活操作
得到最终的预测结果
对于异或这个问题来说
所有值都是正数
所以Leaky ReLU的输出保持不变
推理功能完成之后
他们面临的下一个挑战就是训练
有趣的是
用于推理的架构可以直接用来训练
因为训练本质上也是矩阵乘法
只是多了一些步骤而已
假设推理得到的预测结果是[0.8, 0.3
0.1
0.9]，而目标是[1, 0, 0
1]，
显然模型表现不佳
需要通过训练来改进
训练的核心是损失函数
于是他们选择了均方误差（MSE）
可以理解为预测与目标之间的"距离"，
用L表示
但是训练并不需要损失值本身
而是需要它的导数
这个导数指示了应该向哪个方向调整权重
才能减小损失
就像一个指向"更好性能"的指南针一样
这里还用到了微积分中的链式法则
它能将复杂的梯度计算
分解成更小的部分
让我们逐层计算梯度并且向后传播
整个过程可以分为几步
首先计算损失相对于最终激活的变化率
然后通过激活函数的导数
也就是Leaky ReLU）
来计算损失相对于预激活的梯度
以此类推
等绘制完完整的计算图后
他们发现了一个惊人的对称性
那就是反向传播中的最长链
与前向传播非常相似
在前向传播中
激活矩阵是与转置权重矩阵相乘的；
而反向传播中
梯度矩阵是与未转置的权重矩阵相乘的
就像是照镜子一样
这种对称性让他们能够复用很多前向传播的设计
比如将梯度传播到隐藏层
再通过第一层的激活
只需要根据Z1中正负值的混合
应用相应的Leaky ReLU梯度即可
比如正值为1，负值为0.01
在计算激活导数的时候
还需要用到前向传播中得到的激活值H
这就需要存储每一层的输出
为此他们创建了一个统一缓冲区（UB）
在前向传播计算H值后
立即存储
UB还设计了两个读写端口
因为需要同时访问两个值
包括每行/列的2个输入或者权重
这样就最大限度地减少了数据争用
读取的时候
只需要提供起始地址和数量
UB就会在后台运行
每个时钟周期读取2个值
这样就比每个周期用指令加载更加高效
因为位于所有脉动阵列下方的模块
都需要处理逐个输出的列向量
这促使他们将这些模块
统一为向量处理单元（VPU）
这不仅让设计变得更加优雅
也更具有可扩展性
比如当脉动阵列超过2x2的时候
N个这样的模块就可以统一管理
他们还对激活导数模块做了一个小小的优化
由于H2的值只用来计算了一次梯度
所以他们在VPU内部创建了一个小型缓存（H-cache）
而把其他的H值存储在UB中
因为需要用来进行多个导数计算
VPU通过整合控制信号
也就是VPU的通路位
pathway bits
可以选择性地启用或者跳过特定的操作
同时支持推理和训练
比如正向传播时会应用偏差和激活
跳过损失和导数计算一样
反向传播时则启用所有的模块
但只在反向链中计算激活函数的导数
由于流水线技术
流经VPU的值会经过所有四个模块
只不过没有使用的模块
只会充当寄存器，不执行计算
接下来的几个导数计算
可以直接利用脉动阵列的矩阵乘法能力
因为有三个关键的恒等式
如果Z = XW^T
那么对权重求导我们可以得到这个
对输入X求导我们可以得到这个
而偏差的导数就是1
这意味着可以用前面得到的梯度与X、W^T和1相乘
得到权重和输入的梯度
再乘以学习率来更新参数
有了这些设计之后
反向传播就能顺畅运行
首先从UB获取梯度和H矩阵
通过脉动阵列计算权重梯度
同时直接输入到梯度下降模块
再用当前权重和梯度更新参数
整个过程就会像流水一样不间断的执行
随着功能的完善
他们的指令集也从最初的24位
扩展到了94位
但是确保了每一位都是必需的
从而在不影响速度和效率的情况下
实现了所有的控制功能
最终
通过不断重复前向传播、反向传播、权重更新的循环
网络性能逐步提升
在GTKWave中的波形模拟显示
一个周期后内存中的权重和偏差
已经得到了有效更新
好了
以上就是这个项目的大概过程了
当然了
这个项目只能说是一个简单的原型
真正的TPU远比这个要复杂的多
只不过，整个过程中最令人惊叹的
不只是他们在三个月内
从零开始构建出了能推理和训练的TPU
更在于他们采用的"第一性原理"的方法
从最基础的数学和硬件原理出发
重新发明而不是复制了现有的设计
这种方法让他们在没有专业背景的情况下
不仅完成了技术挑战
也更为深入的理解了每一个环节背后的原理
对于想要深入了解这个项目的观众
可以访问他们的GitHub地址以及项目的官网
团队成员苏里亚·苏雷（Surya Sure）也在X平台进行了分享
我会把这些链接都放到视频简介中
希望这个项目能给大家带来一些有趣的想法
感谢观看本期视频
我们下期再见
