大家好，这里是最佳拍档，我是大飞
相信大家都很关心
AI这两年高速发展到现在
接下来的路该如何走
有很多的AI研究者正在积极的探索这个领域
其中一位就是姚顺雨
姚顺雨毕业于清华姚班
之后又在普林斯顿大学取得了计算机科学博士学位
2024年8月，他加入了OpenAI
在学术研究领域，他可谓是成绩斐然
在语言Agent领域做出了很多开创性的工作
比如他提出了能够让AI通过多路径推理去解决复杂问题的思维树ToT技术
可以AI在推理过程中动态采取行动的ReAct方法
以及为AI Agent构建模块化认知CoALA架构
最近，姚顺雨发布了一篇新的博客
《AI的下半场》，
深入探讨了AI的未来究竟会走向何方
今天，就让我们一同跟随他的思路
去尝试揭开AI发展的下一幕神秘面纱
在深入探讨AI发展的下半场之前
我们先来回顾一下AI发展的上半场
简单来说
我们现在正处于AI发展过程中的一个特殊阶段
姚顺雨将其称为“中场休息”。
在过去的几十年里
AI领域主要的精力都放在了开发新的训练方法和模型上
并且取得了一系列令人瞩目的成就
这其中离不开许多基础性的创新
比如搜索技术、深度强化学习以及推理方法的进步
曾经
深度强化学习一直面临着难以泛化的困境
研究人员很难找到一种通用的方法
来解决多种不同的强化学习任务
在当时
大家普遍认为单一的方法根本无法应对像软件工程、创意写作、复杂数学等众多不同领域的挑战
然而，随着技术的不断发展
情况发生了改变
深度强化学习终于开始实现泛化
找到了能够有效解决多种任务的方法
这种变化带来了AI发展重点的转移
从过去单纯地解决问题
逐渐转向了定义问题
在这个新的时代背景下
评估的重要性开始超越训练
我们不得不重新审视现有的AI训练方式
思考如何更加科学地衡量AI的发展进展
这可能需要我们转换思维
用更接近产品经理的视角去看待AI的发展
回顾AI发展上半场的成果
那些影响力巨大的AI论文
比如提出Transformer架构的论文、关于AlexNet的论文以及介绍GPT-3的论文等等
它们的核心都是提出了具有基础性突破的训练方法
而非建立基准测试
以ImageNet为例
它虽然是一个非常重要的基准测试
但是在论文引用量上
却远远低于提出AlexNet的论文
这个现象清晰地表明
在AI发展的上半场
方法创新比基准测试的建立更受关注
并且这些方法往往具有很强的普适性和广泛的应用价值
就拿Transformer架构来说
它最初是应用在机器翻译领域
之后却成功扩展到了计算机视觉、自然语言处理和强化学习等多个不同的领域
对整个AI行业都产生了极为深远的影响
可以说
过去几十年专注在方法创新的策略
有效地推动了AI在各个领域取得突破性的进展
但是如今，随着这些创新的不断积累
AI的发展已经达到了一个临界点
发展重心也在发生根本性的转变
接下来
姚顺雨提出了AI发展的“配方”。
这个“配方”里包含了大规模的语言预训练、Scale
以及推理和行动
这些概念似乎大家已经耳熟能详了
但是将它们称为“配方”是有内在原因的
我们可以从强化学习的角度来理解
强化学习一直被视为人工智能的“终极形态”，
从理论上来说
强化学习能够保证Agent在游戏中获胜
但是从实际经验来看
像AlphaGo这样超越人类水平的系统
也离不开强化学习的支持
在强化学习中
有三个关键的组成部分
分别是算法、环境和先验知识
长期以来
强化学习的研究人员大多把精力集中在算法的研究上
像REINFORCE、DQN、TD-learning、actor-critic、PPO、TRPO等等
这些算法都被看作是Agent学习的核心部分
而对于环境和先验知识
研究人员往往将它会视为固定不变或者尽量简化的因素
就拿Sutton和Barto的经典教科书来说
其中几乎全部内容都在讲述算法
很少会涉及环境或者先验知识
但是，进入深度强化学习时代之后
人们在实践中逐渐发现
环境的重要性不容小觑
算法的性能在很大程度上依赖于开发和测试的环境
如果研究人员忽视了环境因素
很可能会开发出一个仅仅在简单模拟场景中表现出色的“最优”算法
但是在实际应用场景中却毫无用武之地
基于这样的认识
OpenAI最初制定了一个计划
他们构建了gym
这是一个适用于各种游戏的标准强化学习环境
之后
又推出了World of Bits和Universe项目
试图把整个互联网或者计算机
转化为一个巨大的游戏环境
按照他们的设想
一旦能够把所有的数字世界都变成可利用的环境
再用智能的强化学习算法去解决其中的问题
就有可能实现通用人工智能AGI
这个计划听起来很美好
但是在实际执行过程中却并没有完全达到预期
OpenAI沿着这条道路确实取得了一些显著的成果
比如使用强化学习解决了Dota游戏、机器人手部控制等问题
然而
他们始终没能真正解决计算机使用或者网页导航方面的难题
而且在一个领域训练的强化学习Agent
很难迁移到另一个不同的领域中去
这说明
在实现通用人工智能的道路上
还缺少一些关键的要素
直到GPT-2或者GPT-3出现后
研究人员才发现这个缺失的关键部分就是先验知识
强大的语言预训练
能够将通用常识和语言知识提炼到模型当中
之后这些模型再经过微调
就可以成为像WebGPT这样的网页Agent
或者是像ChatGPT这样的聊天Agent
进而改变了整个世界
事实证明，在强化学习里
最重要的部分或许并不是强化学习的算法本身
也不是环境，而是先验知识
并且这些先验知识可以通过与强化学习完全不同的方式来获得
语言预训练虽然为聊天机器人等应用提供了很好的基础
但是在控制计算机或者玩视频游戏这些领域
效果却并不理想
因为这些领域的数据分布和互联网文本有着较大的差异
监督微调或强化学习在这些领域发挥的作用也比较有限
2019年
姚顺雨尝试用GPT-2去解决基于文本的游戏问题
结果发现
Agent需要进行数百万步的强化学习
才能达到一定的游戏水平
而且很难将在一个游戏中学到的经验
迁移到新的游戏中
与之形成鲜明对比的是人类玩家
我们人类可以在零样本的情况下玩新游戏
并且表现得比Agent更好
这是因为人类具备了抽象思考的能力
比如我们看到“地下城是危险的
需要武器来对抗
可能需要在锁住的箱子中寻找”这样的描述
就能够凭借推理能力
灵活地应对新情况
在这里
思考或推理其实可以被看作是一种独特的行动
它不像传统的行动那样
直接改变外部世界
但是它所涉及的思维空间是开放且无限的
在经典的强化学习中
这种无界的组合会让决策过程变得非常复杂
举个例子
如果从两个盒子中选择一个
其中一个装有100万美元，另一个为空
那么我们的期望收益就是50万美元
但要是增加无限多个空盒子
期望收益就会变为零
然而
当我们把推理引入到强化学习环境的动作空间时
就能够借助语言预训练所积累的先验知识
实现更好的泛化
并且在决策过程中进行更加灵活的计算
ReAct的论文就详细介绍了Agent推理的相关内容
简单来说
即便增加了无数个空箱子
由于我们在生活中已经在各种场景里接触过类似情况
所以在面对新的选择时
也能够更好地做出判断
从更抽象的层面解释
就是语言可以通过Agent中的推理实现泛化
当我们掌握了正确的强化学习先验知识
并且找到了适合的强化学习环境之后
会发现强化学习算法可能反而是其中最简单的一部分
基于这样的认识
研究人员推出了o系列、R1等研究成果
还开发了能够利用计算机的Agent
未来也还会有更多新的成果涌现
这个变化可以说充满了戏剧性
长期以来
强化学习研究者一直把重点放在算法上
却忽视了环境和先验知识
每次实验几乎都要从零开始
经过几十年的探索，大家才意识到
或许应该把研究的优先级进行彻底的调整
不过，就像史蒂夫·乔布斯说的那样
你无法展望未来的连接点
只能在倒回来看的时候去连接
如今
这个“配方”正在彻底改变AI发展的格局
回顾上半场的发展模式
我们通常是开发新的训练方法或者模型
以此来提升在基准测试中的成绩
然后再创建更具挑战性的基准
如此循环往复
但是现在
这种发展模式正面临着困境
原因在于
这个“配方”已经让基准的提升
逐渐实现了标准化和工业化
不再需要太多全新的想法
随着这个“配方”的不断扩展和良好的泛化
针对特定任务开发的新方法
可能只能带来5%的性能提升
而新的o系列模型即使没有专门针对某个任务进行优化
却有可能实现30%的提升
而且，即使我们不断创建更难的基准
这些基准也会很快、并且越来越快被现有的“配方”所攻克
OpenAI的研究员
同时也是姚顺雨的同事Jason Wei
曾经做过一张非常直观的图
清晰地展示了这一趋势
在过去五年里
AI在各种基准测试中的成绩不断提升
像TriviaQA中的问答准确率、MMLU等各类考试的成绩
以及在数学相关测试如GSM8K、AIME、MATH中的表现
还有在软件工程任务的完成情况等等
都呈现出快速上升的趋势
那么，在AI发展的下半场
还有什么值得我们去探索呢？
如果不再需要新的方法
而更难的基准测试又会被快速解决
我们该何去何从？
姚顺雨认为
我们需要从根本上重新思考评估的方式
这不仅仅是创造新的、更难的基准测试那么简单
而是要对现有的评估设置提出根本性的质疑
进而创造出全新的评估体系
只有这样，我们才有可能被迫发明出
超越现有“配方”的新方法
但是不得不说
这是一项极具挑战性的任务
因为人类往往存在惯性思维
很少会去质疑那些习以为常的基本假设
常常把它们当作理所当然的事情
却没有意识到这些只是假设
并非不可改变的定律
这种惯性思维虽然很自然
但是却带来了问题
AI已经在国际象棋、围棋等比赛中击败了世界冠军
在学术能力评估测试和律师资格考试中
超过了大多数人类
在国际信息学奥林匹克竞赛和国际数学奥林匹克竞赛中
也达到了金牌水平
然而，从经济和GDP的角度来看
世界并没有发生太大的变化
姚顺雨将这个问题称为效用问题
并认为这是AI发展过程中最重要的问题之一
或许我们很快就能解决效用问题
也或许还需要很长时间
但是不管结果如何
这个问题产生的根本原因其实并不复杂
那就是我们现有的评估设置
在许多基本方面与现实世界的实际情况存在差异
先看第一个差异
评估通常要求自动运行
一个Agent接收到任务输入后
独立完成任务
然后获得相应的任务奖励
但是在现实生活中
Agent往往需要在整个任务过程中
与人类进行互动
就拿和客服沟通来说
我们不会给客服发送一条超长的消息
然后等待10分钟
期望一次性得到解决所有问题的详细回复
而是要来回沟通几次
正是基于对这种评估设置的质疑
所以我们开发了一些新的基准
比如 Chatbot Arena
它将真实的人类引入到评估循环中
还有tau - bench
它会模拟获取用户的详细信息
然后给出具体的任务场景
像修改航班预订、取消预订等
Agent需要根据系统给出的提示信息
比如当前航班的舱位信息、预订时间、修改和取消规则等
以及用户的指令，来做出相应的决策
这与传统的评估方式相比
更加贴近现实场景
再看第二个差异
评估往往要求在独立同分布（i
i
d
）的情况下进行
在测试的时候
假设有一个包含500个任务的测试集
我们会独立运行每个任务
计算每个任务的指标
然后取平均值得到一个整体指标
但是在现实世界中
我们解决任务的方式并不是并行的
而是顺序进行的
以谷歌的软件工程师为例
他们在解决问题的时候
随着对代码库熟悉程度的增加
解决问题的能力也会不断提高
然而
一个软件工程Agent在处理同一个代码库中的多个问题时
却无法像人类工程师那样
获得这种熟悉度的提升
虽然目前已经有了一些针对长期记忆的方法
但是在学术界
仍然没有合适的基准来验证这种需求的合理性
甚至很少有人敢于质疑
作为记忆学习基础的独立同分布假设
提高智能程度通常就能提升它在实际应用中的实用性
但是如今，情况已经发生了变化
这些通用的方法在现有的假设下
不一定能够继续发挥作用
所以，姚顺雨指出
在AI发展的下半场
我们需要采用新的发展模式
那就是要为现实世界的实用性去开发新颖的评估设置或任务
然后用通用的方法去解决这些任务
或者通过添加新颖的组件来增强这些方法
之后再不断循环这个过程
应该说
这个过程不再是我们熟悉的模式了
就像对于AI企业来说，在之前的阶段
他们主要关注解决的是解决问题和如何评估
而接下来
他们要考虑的则是如何利用AI来开发有用的产品
创造出数十亿甚至数万亿美元的价值了
前期主要是通过增量式的方法和模型来推动
而后期则需要从众多方法中进行筛选了
接下来
通用的方法有可能会取代增量式的方法
除非我们能够创造新的假设
打破现有的通用性，只有这样
才能出现真正具有创新性、能够改变游戏规则的研究
好了
以上就是姚顺雨关于AI发展“下半场”的观点了
也许呢
AI的发展正处于一个关键的转折点
从注重训练到重视评估
如何重新思考评估的方式
或许将是推动AI持续发展的关键所在
感谢大家收看本期视频
我们下期再见
