大家好，这里是最佳拍档
想象一下
我们能不能在大模型还在学习的起点时
就精准切除它的危险能力
既不影响它帮我们写文案、做科研
又能够让它彻底丧失制造生物武器、策划网络攻击的可能呢？
这不是科幻电影里的情节
而是前Anthropic科学家、Claude价值观的塑造者尼尔·拉西
以及前OpenAI科学家、GPT之父亚历克·拉德福德
联合发布的最新研究成果
Token级数据过滤
这项研究
彻底颠覆了传统被动防御的思路
他们提出
与其在模型学会危险知识后再费劲的让它遗忘
不如在预训练阶段
就从源头上精准删除那些会催生危险能力的关键知识碎片
今天我们就来聊聊
Token级数据过滤是如何实现外科手术式的精准能力塑造
它的技术原理和实验效果
以及它会给未来AI安全带来哪些深远影响
我们首先得搞清楚一点
为什么传统的后处理安全机制总是失效呢？
过去几年，业界为了让大模型守规矩
尝试了各种后处理方案
无论是RLHF还是SFT
都有一个致命的缺陷
那就是它们并没有从模型的知识库中移除危险的知识
只是压制了这些知识的输出
为了解决这个问题
研究人员又提出了机器遗忘技术
试图在模型训练完成后
通过反向优化让它彻底忘记特定的危险知识
比如代表性的RMU方法
就是通过调整模型的中间表征
让它对危险知识的记忆变得混乱
但是实验证明，这种方法依然脆弱
尼尔·拉西的团队在实验中发现
即便是当前最先进的RMU
面对几轮对抗性微调时也会迅速的失效
攻击者只需要给模型提供少量危险领域的文本进行再训练
被遗忘的知识就会像弹簧一样反弹回来
为什么会这样？
因为大模型的知识存储方式是分布式的
危险知识和良性知识往往交织在一起
很难通过后处理进行外科手术式的移除
就像在一本写满知识的书中
想要用橡皮擦擦掉某几页的内容
却不破坏其他的页面
几乎是不可能的
而且
后处理本质上是在模型已经掌握危险知识的基础上进行约束
这就给了攻击者无数的可乘之机
更关键的是，随着模型规模的扩大
后处理的效果会越来越差
大模型的泛化能力极强
它能从残留的蛛丝马迹中推理出被隐藏的知识
甚至能通过跨领域联想
自行补全危险知识的逻辑链条
这就是为什么越大的模型
反而越难通过后处理进行安全约束
因为它太聪明了
总能找到绕过规则的方法
这时候
尼尔·拉西和亚历克·拉德福德的研究给出了一个全新的思路
既然后处理是在模型学会知识后堵漏洞
那我们能不能在模型学习知识之前
就把危险知识的源头给切断呢？
其实，数据过滤并不是一个新概念
现在几乎所有前沿AI实验室
在训练大模型之前
都会对预训练数据进行清洗
比如过滤掉明显的有毒内容、低质量垃圾信息
但是传统的数据过滤
都是以文档为单位进行的
什么意思呢？
就是如果系统检测到某一篇文章中包含危险内容时
就会把整篇文章都删掉
这种做法看似简单直接
但存在两个致命问题
第一个问题是误伤大量良性数据
很多文章中
危险内容可能只占很小一部分
大部分都是有价值的良性信息
比如一篇讨论传染病防治的学术论文
其中可能包含一些关于病毒传播机制的内容
这些内容本身也是科学研究的重要组成部分
但如果被简单判定为危险
整篇论文就会被删除
导致模型错失大量有用的知识
更严重的是
很多领域的知识是高度关联的
比如生物学和医学、化学和材料科学
删除一篇包含少量危险内容的生物学论文
可能会让模型连基础的疫苗研发知识、疾病预防知识都无法学到
第二个问题是漏网之鱼
有些危险内容并不会以完整文档的形式存在
而是隐藏在大量良性文本中
比如在一篇普通的健康科普文章中
可能夹杂着几句关于如何滥用药物的描述；
在一篇工业材料介绍中
可能提到了某种化学品的危险合成路径
这些零散的危险Token
用文档级过滤根本检测不到
最终还是会被模型学到
成为安全隐患
尼尔·拉西的团队在研究中做了一个统计
在他们使用的FineWeb-Edu数据集里
只有大约18%的文档
被文档级分类器判定为包含医学危险内容
但是在这些文档中
真正的医学危险Token只占50%。
这意味着
文档级过滤会把另外50%的良性Token也一并删除
造成巨大的资源浪费
同时
还有大量包含少量医学危险Token的文档
因为没有被判定为危险文档
而让这些Token逃过过滤，被模型学习
这种要么全删
要么全留的粗放式过滤
不仅无法从根本上阻断危险知识的传播
还会损害模型的通用能力
这也是为什么很多实验室虽然做了数据过滤
但模型依然存在安全隐患的原因所在
而尼尔·拉西和亚历克·拉德福德的核心创新
就是把过滤的颗粒度从文档降到了Token级别
这种微观层面的操作，让去其糟粕
取其精华成为了可能
Token级数据过滤的核心思想很简单
那就是不删除整篇文档
而是只精准识别、并且处理文档中那些会催生危险能力的特定Token
在研究中
尼尔·拉西的团队设计了两种具体的操作方式
分别应对不同的场景需求
第一种方式叫做损失屏蔽
这种方式的操作逻辑是
模型在阅读文本时
依然能看到所有的Token
包括那些被标记为危险的Token
但是在计算梯度、进行参数更新时
系统会自动忽略这些危险Token的贡献
换句话说，模型能看到这些Token
从而理解文本的上下文连贯性
但不会记住这些Token对应的知识
这就像让学生阅读一本教材
但老师明确告诉他们
某些章节的内容不会考试
也不会计入成绩
学生自然就不会花精力去记忆这些内容
这种方式的优势在于
它能保证模型在处理文本时
依然能获得完整的上下文信息
避免因为删除部分Token而导致的理解断裂
第二种方式叫做直接移除
这种方式更加彻底
它会把所有被标记为危险的Token
直接替换成一个特殊的占位符
hidden
这意味着
模型不仅不会学习这些危险Token的知识
甚至在阅读文本时都看不到它们
只能看到这个占位符
这种方式的优势在于安全性更高
能彻底切断危险Token对模型的影响
避免模型通过上下文联想间接学到危险知识
但它的缺点是可能会破坏文本的连贯性
影响模型对良性内容的理解
不过尼尔·拉西的实验证明
这种影响非常有限
因为被替换的Token只占文本的很小一部分
模型依然能通过剩余的良性Token
理解核心意思
为了验证这两种方式的效果
研究团队设计了一个极具挑战性的Agent任务
在保留模型生物学知识的同时
彻底移除它的医学知识
为什么说这个任务有挑战性？
因为生物学和医学的知识图谱高度重叠
如果过滤方法不够精准
移除医学知识时
很容易就会连累生物学知识
导致模型在生物学领域的能力大幅下降
这个任务完美模拟了现实中的AI安全需求
我们希望模型能掌握疫苗研发、疾病预防等有益的生物学知识
但不希望它学会制造生物武器、滥用药物等危险的医学操作
实验结果令人惊喜
无论是损失屏蔽还是直接移除
都实现了精准的能力切割
模型在医学领域的表现一落千丈
而在生物学子集的性能几乎没有下降
更重要的是
Token级过滤实现了帕累托改进
这是经济学中的一个概念
意思是在不牺牲一方利益的前提下
提升另一方的利益
具体来说
Token级过滤在同等程度削弱危险能力的情况下
对良性能力的保留效果
远远好于文档级过滤
反过来
在保留同等良性能力的情况下
Token级过滤对危险能力的削弱效果
也远超文档级过滤
这种鱼和熊掌可以兼得的效果
正是AI安全领域一直追求的目标
一项技术是否有价值
最终要靠实验数据来证明
研究团队进行了一系列严谨的实验
覆盖了模型规模、对抗性攻击、对齐能力等多个维度
用实打实的数据证明了Token级过滤的优越性
通常我们会认为，模型越大
泛化能力越强，越难被限制
它可能会从残留的蛛丝马迹中
自行推理出被过滤掉的知识
但是实验却得出了完全相反的结论
Token级过滤的效果
会随着模型规模的扩大而呈指数级提升
为了量化这种效果
研究团队提出了相对缩放法则
通过计算在过滤数据上训练的模型
需要多少计算量才能达到未过滤模型在危险领域的损失值
来衡量过滤的有效性
简单来说，如果一个过滤方法效果好
那么攻击者想要让模型重新学会被过滤的知识
就需要付出巨大的计算成本
实验结果显示，对于小模型来说
Token过滤能带来约10倍的计算速度下降
也就是说
攻击者想要通过微调让模型重新学会医学知识
需要的计算量是原始训练的10倍
而对于18亿参数的大模型
这个数字飙升到了7000倍
意味着对于大模型来说
被Token过滤掉的危险知识
几乎不可能通过微调重新获得
要知道
18亿参数模型的原始训练已经需要巨大的计算资源
7000倍的计算量更是超出了绝大多数攻击者的能力范围
这相当于
为大模型
建立了一道几乎无法逾越的安全壁垒
相比之下
传统的文档级过滤效果就差的多
即便是对于18亿参数的大模型
文档级过滤也只能带来约30倍的计算速度下降
攻击者只需要投入少量额外资源
就能轻易绕过这道防线
为什么会出现这种规模越大
效果越强的现象？
研究团队解释说
大模型的知识学习具有复利效应
它们会把学到的知识进行深度整合
形成完整的逻辑链条
而Token级过滤恰恰切断了危险知识的逻辑链条
让模型无法形成完整的危险能力
当模型规模扩大时
这种链条断裂的影响会被放大
因为大模型对知识的整合能力更强
一旦链条断裂，就更难自行修复
除了计算量的衡量
研究团队还通过多项基准测试
直接验证模型的能力变化
在多项选择题测试中
研究团队选用了MedMCQA、MedQA-USMLE和MMLU的医学子集
这些都是衡量模型医学知识的权威基准
实验结果显示，经过Token过滤的模型
在这些医学测试中的表现降到了接近随机猜测的水平
这意味着
模型真的忘记了所有医学知识
彻底丧失了回答医学问题的能力
但与此同时
在MMLU的生物学子集测试中
Token过滤模型的准确率和未过滤的基线模型几乎没有差别
它依然能精准回答关于细胞结构、基因表达、生态系统等生物学问题
这种该忘的全忘
该记的全记的精准控制
正是Token级过滤的核心优势
在自由响应测试中，效果同样显著
研究团队选用了HealthSearchQA数据集
这是一个包含大量消费者医疗问题的数据集
比如肺癌能治愈吗？
如何区分感冒和新冠等等
他们让经过训练的18亿参数模型回答这些问题
并请Claude Sonnet 4作为评委
从正确性、相关性、连贯性三个维度进行打分
结果显示
Token过滤模型的表现惨不忍睹
正确性得分是基线模型的十分之一
相关性和连贯性得分是基线模型的四分之一
很多回答要么完全不相关
要么逻辑混乱、重复啰嗦
要么是完全错误的结论
而在Alpaca数据集的测试中
Token过滤模型的表现和基线模型几乎一致
没有出现任何能力下降的迹象
这充分证明
Token级过滤只针对特定的危险领域
不会影响模型的通用智能
为了测试Token级过滤的健壮性
研究团队还设计了对抗性微调实验
攻击者获得模型的全部权重
然后用医学领域的文本对模型进行微调
试图让它重新学会被过滤的医学知识
研究团队将Token级过滤与当前最先进的RMU技术进行了对比
实验结果显示
Token级过滤的健壮性远远超过RMU
对于18亿参数的模型
RMU只需要0.01%的预训练计算量进行对抗性微调
就能恢复到基线模型的医学能力；
而Token过滤则需要0.13%的预训练计算量
是RMU的13倍
更重要的是，随着模型规模的扩大
这种健壮性差距还在不断拉大
对于小模型
Token过滤的健壮性是RMU的3倍左右
而对于大模型
这个差距扩大到了13倍
这意味着，模型越大
Token过滤的安全优势就越明显
而RMU等技术则会变得越来越脆弱
为什么会这样？
因为RMU是在模型训练完成后进行遗忘
并没有真正移除危险知识的底层表征
只是对其进行了压制
而对抗性微调就像是唤醒这些被压制的表征
只需要少量数据就能成功
Token级过滤则是从源头上切断了危险知识的学习过程
模型的底层表征中根本就没有这些危险知识
攻击者想要让模型学会
就必须像教一个完全无知的模型一样
从头开始积累所有相关知识
这需要付出巨大的计算成本
虽然Token级过滤的效果令人惊叹
但是它的技术实现并不复杂
关键在于如何高效、低成本地识别出哪些Token是危险的
毕竟
互联网上的文本数据是以万亿Token计的
我们不可能雇佣成千上万的专家
去给每个Token打标签
也不可能在预训练过程中
实时运行一个昂贵的大模型来判断Token是否危险
研究团队设计了一套精巧的弱监督流水线
仅用机器生成的弱标签和一个小型分类器
就实现了大规模、高精度的Token识别
这套流水线主要分为三个步骤
第一步是生成高质量的种子标签
研究团队利用了稀疏自编码器的特性
它能够将神经网络复杂的激活模式
分解为可解释的独立概念特征
简单来说
稀疏自编码器就像是一个概念探测器
它能从大模型的内部激活中
自动发现并提取出一个个独立的概念
比如药物副作用病毒传播途径手术操作步骤等
研究团队使用了Gemma 2 9B模型的预训练稀疏自编码器
然后请Claude Sonnet 4对每个稀疏自编码器提取的特征
进行解释和分类
筛选出与医学领域高度相关的600个特征
接下来
研究团队用这些医学相关的特征
来标注预训练数据中的Token
如果一个Token在至少两个医学特征上的激活值
比平均值高出4个标准差
就会被标记为医学Token
此外
如果一个Token与已经标记的医学Token相邻
并且在至少一个医学特征上有正激活
也会被标记为医学Token
这一步是为了确保连续的医学短语都能被完整标记
而不是只标记单个关键词
通过这种方式
研究团队自动生成了大量高质量的种子标签
这些标签不需要人工标注
成本极低，而且精准度很高
从实验结果来看
这些稀疏自编码器生成的标签
能够准确识别出文本中的医学Token
同时忽略良性Token
虽然存在少量噪声
但完全可以满足后续训练的需求
有了种子标签
下一步就是训练一个高效的Token级分类器
这个分类器需要能够快速、准确地判断任意Token是否属于危险领域
并且运行成本要足够低
能够部署在大规模预训练的数据处理流水线中
研究团队选择训练一个双向语言模型作为分类器
而不是传统的单向模型
这是因为一个Token的含义
往往取决于它的上下文
比如病毒virus这个Token
在计算机病毒攻击系统这句话中是良性的
但是在新冠病毒的传播机制这句话中
就是医学相关的危险Token
只有同时查看Token的前文和后文
才能做出准确判断
这正是双向模型的优势
为了控制成本
研究团队将分类器的规模控制得很小
最终的分类器是一个两亿两千四百万参数的双向语言模型
仅占预训练模型计算成本的一小部分
而且
研究团队还通过领域上采样进一步提升了分类器的性能
在训练分类器的数据集里
50%的Token来自医学领域
50%来自普通领域
这种不均衡的采样方式
让分类器能更精准地学习医学Token的特征
从而提升识别准确率
实验结果显示
这个小型分类器的表现非常出色
在验证集上的F1分数达到了0.856
在测试集上更是达到了0.894
这个准确率甚至超过了一些大型的通用模型
这充分证明
小型、任务特定的分类器
在针对性任务上的表现
完全可以超越大型通用模型
而且成本要低得多
除了Token级分类器
研究团队还训练了一个文档级分类器作为补充
这个文档级分类器同样基于两亿两千四百万参数的双向语言模型
使用Claude Sonnet 4标注的文档级标签进行训练
在验证集和测试集上的F1分数
分别达到了0.922和0.941
这个文档级分类器的作用
并不是替代Token级分类器
而是作为第一道防线
先快速筛选出明显包含大量危险内容的文档
进行初步过滤
然后再用Token级分类器对剩余文档进行精细过滤
这种粗筛加精筛的组合方式
既能提升过滤效率
又能保证过滤精度
是一种非常实用的工程方案
在大规模数据处理中
标签噪声是一个无法避免的问题
即便是稀疏自编码器生成的种子标签
也会存在一定比例的误判
很多人会担心
如果分类器的标签不够完美
过滤效果会不会大打折扣？
研究团队专门针对这个问题进行了实验
结果发现：
即使标签存在较大噪声
Token级过滤依然能实现有效的能力压制
关键在于利用大模型的弱到强泛化能力
研究团队通过随机翻转标签的方式
模拟了不同程度的标签噪声
实验结果显示
标签噪声对过滤效果的影响呈现出低误差敏感
高误差饱和的特点
在标签误差率较低时
过滤效果会急剧下降
从100倍降到了10倍左右
但是当标签误差率超过50%后
过滤效果的下降速度会明显放缓
逐渐趋于饱和
这意味着
少量的标签噪声就会对过滤效果产生很大影响
但是当噪声达到一定程度后
再增加噪声，对效果的影响就不大了
这个发现给我们的启示是
在实际应用中
我们不需要追求100%完美的标签
只需要将标签误差率控制在较低水平
就能实现较好的过滤效果
面对标签噪声
研究团队提出了两个解决方案
一是激进过滤
二是利用大模型的知识恢复能力
激进过滤指的是调整分类器的决策阈值
牺牲一定的精确率来提升召回率
简单来说
就是让分类器宁可错杀一千
不可放过一个
即使对某个Token是否危险存在疑问
也将其判定为危险Token进行过滤
这样做虽然会误删一些良性Token
但能最大限度地确保危险Token被过滤掉
而大模型的知识恢复能力则能够弥补这种误删的损失
大模型具有极强的泛化和学习能力
对于那些被误删的良性知识
如果它们在预训练数据集中的其他地方
以未被过滤的形式存在
大模型就能通过上下文学习自行填补这些空白
不会因为部分Token被误删而导致整体能力下降
实验结果证明了这种方案的有效性
当研究团队将过滤比例从3%提升到50%，
模型在医学领域的损失值大幅上升
而在生物学和普通领域的损失值仅小幅上升
这说明大模型通过自身的恢复能力
弥补了大部分被误删的良性知识
更重要的是
Token级分类器具有弱到强泛化的能力
即使使用低质量的弱标签进行训练
它也能学习到危险Token的核心特征
从而实现高精度的识别
研究团队做了一个对比实验
分别用Token级标签、句子级标签、文档级标签训练分类器
然后测试它们在Token级任务上的表现
结果显示
用句子级标签训练的分类器
在Token级任务上的F1分数
仅比用Token级标签训练的分类器低0.02
用文档级标签训练的分类器
F1分数也只低0.05
这意味着
即使没有精准的Token级标签
用粗粒度的句子级或文档级标签
也能训练出效果不错的Token级分类器
更令人惊讶的是
Token级分类器还能从弱分类器的标签中学习
研究团队先用一个一千三百万参数的弱分类器生成标签
然后用这些标签训练两亿两千四百万参数的强分类器
结果显示
这个强分类器的表现远超弱分类器
在测试集上的准确率提升了10%以上
而文档级分类器则没有这种能力
用弱分类器的标签训练出的强文档级分类器
表现甚至比弱分类器还差
这种弱到强泛化的能力
让Token级过滤的部署成本大幅降低
意味着我们不需要投入大量资源生成高质量的Token级标签
只需要用低成本的弱标签
就能训练出高性能的Token级分类器
这也是Token级过滤能够大规模应用的关键因素之一
总的来说
尼尔·拉西和亚历克·拉德福德的这项研究
不仅提出了一种实用的AI安全技术
更重塑了我们对AI能力塑造的认知
以往我们总是追求让AI无所不知
但现在看来
有选择的让AI在某些危险领域保持无知
或许才是人类与超级智能共存的安全基石
当然
这项技术也还有进一步优化的空间
比如，当前的Token级过滤
依赖于外部分类器来识别危险Token
未来可以探索利用模型自身的内部表征来进行过滤
此外
如何实现无监督的Token级标签生成
如何处理跨语言、跨领域的危险知识
如何应对AI通过工具获取危险知识等问题
也需要后续研究来解决
但是无论如何
这项研究已经为AI安全领域指明了一个全新的方向
从后处理约束转向预训练过滤
从文档级粗放过滤转向Token级精准过滤
在AI能力不断增强的今天
我们不需要再陷入攻防循环的猫鼠游戏
而是可以从源头上塑造AI的能力
让它天生就不具备危险知识
未来的AI安全
终将深入到数据基因层面的编辑
我们可以像编辑基因一样
精准地选择AI能学习什么、不能学习什么
让AI成为真正服务于人类、而不是成为人类的威胁
感谢收看本期视频，我们下期再见
