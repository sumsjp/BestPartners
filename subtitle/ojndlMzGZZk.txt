大家好，这里是最佳拍档，我是大飞
今天我们来聊一个老生常谈的话题
那就是大模型的推理能力
在OpenAI的o1问世之后
对于大模型推理能力的质疑稍微消停了一会
毕竟谁都看到了o1展现的推理能力
然而
苹果公司的研究者却抛出了一篇论文
再次点燃了有关模型推理能力的讨论
这篇题为“GSM-Symbolic：
理解大语言模型中数学推理的局限性”的论文
一作是苹果机器学习研究工程师伊曼· 米尔扎德Iman Mirzadeh
而图灵奖得主约书亚·本吉奥Yoshua Bengio 的弟弟萨米·本吉奥也是论文作者之一
比作者身份更炸裂的是论文的结论
那就是无论是OpenAI的GPT-4o和o1
还是Llama、Phi、Gemma和Mistral等开源模型
都没能发现任何形式推理的证据
而更像是复杂的模式匹配器
深度学习三巨头之一的杨立昆
也在最近的演讲中表示
Meta现在已经完全放弃了纯语言模型
因为仅靠文本训练
它永远也不可能达到接近人类的智能水平
目前基于Transformer架构的大语言模型
难道真的会变成一条弯路吗？
今天大飞就来带大家读一读这篇论文
这次
苹果的研究者们仔细研究了GPT-4o和o1系列的闭源模型
以及Llama、Phi、Gemma、Mistral等开源模型的数学能力
在此之前
业界用来评价大模型数学能力的数据集
主要是2021年发布的GSM8K
这个数据集包含了8000个小学水平的数学应用题
比如这个例子
当索菲照顾她的侄子时
她会拿出各种各样的玩具
积木袋里有31块积木
毛绒动物桶里有8个毛绒动物
堆叠环塔上有9个五彩缤纷的环
索菲最近买了一管弹性球
这让她为侄子准备的玩具总数
达到了62个
那么管子里有多少个弹性球呢？
这类问题被频繁地用来检测模型的数学能力
而且一用就是三年
现在，模型性能也从GPT-3当初的35%，
提升到了85%以上
那么这些三年前的问题
还能测出大模型的真实水平吗？
要知道，由于这是21年发布的数据集
如今主流大模型抓取的训练数据
可能就会无意间涵盖了GSM8K的题目
虽然大部分模型都没有公开训练用的数据
但是不可避免存在数据污染的可能
苹果的研究者们就此认为
数据污染会导致大模型能够靠着背题
答对GSM8K中的题目
因此
用这个数据集来评判大模型的数学能力
并不准确
为了客观评价大模型数学能力的极限
苹果的研究者们开发了一个名为GSM-Symbolic的数据集
GSM-Symbolic将GSM8K的题目进行了修改
比如改变了索菲的名字和侄子的家人称谓
以及各种玩具的数字多少
这样一来
就可以产生出很多个看起来全新、但是实际上具有相同内核的题目
另外，除了GSM-Symbolic
这项研究还提出了GSM-NoOp数据集
GSM-NoOp 向题目中添加了一些看似相关、但是实际上毫无关系的数据
来判断大模型在执行逻辑推理任务时
是否会受到无关数据的影响
这样一来
研究者就可以杜绝AI背答案的可能性了
那么，在这一套新测试题的考验下
大模型的表现怎么样呢？
不幸的是，答案是一塌糊涂
实验结果表明，大模型就跟学渣一样
明明是一样的数学题目
题干一换，很多AI就不会了
苹果的研究者们对比了GSM8k和GSM-Symbolic在多种模型上的性能差异
结果发现，无论是主流的开源模型
还是闭源的GPT系列模型
甚至专门为数理推断优化过的o1模型
当面对GSM-Symbolic的换皮题目时
准确率都会下降
而大多数模型在GSM-Symbolic上的平均性能
都低于在GSM8K上的平均性能
哪怕没有把题干整个换掉
只是更改了题目中的名称
大模型的表现也会存在差异
当只改变了题目中的专有名词时
性能下降在1%到2%之间
当实验者更改数字
或者结合这两类更改的时候
差异则更为显著
从结果中可以看出
几乎所有的模型都明显出现了分布均值从右向左的逐渐移动
这表明模型的准确度变低了
与此同时，数据的方差还增加了
仅仅是更改一下题目中的专有名词
就会存在如此大的差异
这种现象实在是令人担忧
因为即使是理解了数学题目的小学生
都不会因为题目换汤不换药
就不会做了
不过，百分之一到二的差距
可能还不足以说服所有人
大模型的支持者们
完全可以说这只是普通的数据误差
所以
苹果的研究者继续给这些大模型上难度
他们引入了GSM-Symbolic的三个新变体
包括删除一个分句的GSM-M1
增加一个分句的GSM-P1
以及增加两个分句的GSM-P2
果然，当模型面对的题目变难时
比方说
题目从「打电话每分钟10分钱
打60分钟要多少钱」，
变为「打电话前10分钟每分钟10分钱
之后每分钟8分钱
这样打60分钟电话要多少钱」，
大模型回答的准确性就会降低
方差也会变大，这就意味着
大模型的性能并不稳定
可靠性也变得越来越差
最后
当面对增加了和题目无关的论述数据集GSM-NoOP时
所有模型的性能下降更是惨不忍睹
其中Phi-3-mini 模型下降了超过 65%，
即使是像o1-preview这样、预期表现更好的模型
也表现出了显著的下降，暴跌了17.5%。
对于这种现象，苹果的研究者解释道
这是由于模型会将无关的论述
当成需要操作的步骤
从而画蛇添足式地错误回答
也就是说
哪怕是如今性能最强大的模型
也依然无法真正的理解数学问题
在论文的结尾处，作者写道
大模型在执行真正的数学推理方面
存在重大的局限性
而且大模型在不同版本的同一问题上的表现
存在高度差异，随着难度的轻微增加
会表现出性能的大幅下降
以及对无关信息的敏感度
这些都表明
大模型在推理和运算方面的能力是脆弱的
最终，苹果研究者给出了这样的结论
它们可能更像是一种复杂的模式匹配
而不是真正的逻辑推理
也就是说
即使我们继续地堆数据、参数和计算量
或者使用更好的训练数据
也只能得到「更好的模式匹配器」，
而非「更好的推理器」。
这个结论一出来
立刻就在AI社区中引发了剧烈的讨论
谷歌DeepMind科学家丹尼周Denny Zhou就表示
自己在ICML 2023的一篇论文中
也发现了类似的现象
这篇名为《信仰与命运：
Transformer作为模糊模式匹配器》的论文
通过一种不同的实验方式
殊途同归的得出了和苹果一样的结论
大模型并没有真正地理解数学概念
而只是根据模糊的模式匹配
来从训练数据的题库中寻找答案
相信各位在使用AI的时候
也一定会感到疑惑
为什么Claude或者GPT-4这样的模型
在输出时非常像一个人在推理
而且问题也都是需要推理才能够解决的
但是在处理一些简单的问题上
却还是在犯愚蠢的错误
比如
人类在学习了基本的计算规则后
都可以计算三位数乘三位数的乘法算术
但是在2023年底
ChatGPT-3.5和GPT-4在这项任务上的准确率
分别只有55%和59%。
在《信仰与命运》这篇论文中
作者对大模型的这种表现
提出了一种解释
那就是线性化子图匹配
他们猜测
大模型解决问题的方式是这样的
第一步，任何任务的问题
都可以表示为一个有向图
大模型会利用这些有向图
将任务描述为一系列的步骤
这些步骤会被分别解决
然后再将结果组合在一起
随后，如果整个任务的解决方案过程
可以用一个图来描述
那么其中的子任务
就是这个图中的一个子图
图的结构描述了哪些步骤要依赖于其他步骤
而这种依赖顺序
限制了子图如何被展平为线性序列
而GPT类的模型
基本就是在通过近似匹配
来“解决”上述子图的，也就是说
给定一个可以用子图描述的问题
大模型就会通过将它与训练数据中相似的子图匹配
来进行预测
为了证明这个猜测
研究者们在大模型上测试了三个任务
分别是乘法、爱因斯坦逻辑谜题和动态规划问题
拿乘法举例
如果大模型真的能够通过足够的数据学会东西
或者能够通过系统化的推理
解决复杂的多步骤问题
那么它应该也能通过足够的例子
或者对算法的充分解释，来学习乘法
而乘法问题可以被分解为更小的问题
因此模型应该能够通过逐步的推理做出来
为了检验多位数的乘法任务
研究者定义了一组大量的乘法问题
从计算两位数和两位数的乘积
到五位数和五位数的乘积
首先，他们会要求模型解决一个问题
比如，35 乘以 90 等于多少？
下一步
研究者根据学校里教授的标准乘法算法
向模型提供思维链示例
帮助模型把乘法问题分解为更小的任务
随后，研究者将乘法算法
描述为一个包含加法和乘法等基本操作的定向图
比如这个7乘以49所涉及的运算图中
就包含7乘以4的子任务
模型要回答7乘以49
就必须从7乘以4开始一步一步来
这样一来
研究者就可以通过对比主要问题和子问题的正确率
来判断大模型到底有没有
通过推理一步一步解决问题的能力了
如果主问题答对了
子问题却全部答错
或者反过来，子问题全对
但是沿着相同逻辑衍生出的主问题
却回答错误
那么都说明
大模型不是在通过推理做数学题
很遗憾，实验的结果表明
模型无法从训练集中的小乘法问题
推广到更大的乘法问题
在图中，蓝色的单元格表示
模型是在这样的乘积上训练的
得分相当不错
原因在于
模型在预测与训练数据规模相同的问题时
表现良好
然而在橙色的单元格
比如三位数与三位数
或者更高位数的乘积
得分就要差得多了
而当任务变得更加复杂的时候
模型的准确度更是会急剧下降
由此
研究者们总结出了一些有趣的结论
首先，研究者观察到
大模型是否能够成功解决问题
取决于模型之前是否见过相关的子问题
换句话说，大模型无法解决大型问题
是因为它们只能解决大型问题中的部分子问题
如果它们在解决训练数据中
频率更高或者更精确的子问题上成功了
这表明它们只是记住了答案
通过回忆的方式来解决
这就是为什么7乘以49会失败
但是 7乘以4却能取得进展
因为大模型记住了「7乘以4等于28」这个子问题
因此，论文的作者指出
与其将模型视为
能够以一般和系统的方式处理问题的各个部分
不如将它视为一个搜索引擎
它会先召回与特定问题部分
大致匹配的例子
然后再将这些近似回忆拼接起来
也就是说
大模型通过仅仅完成整体问题的一部分
来取得部分的成功
换句话说
它是在以自己更直觉、更肤浅、更实际的方式来分解问题
更关注于文本的表面
而非系统性地思考指定的乘法算法
无独有偶，早在2017年
斯坦福大学的Robin Jia和Percy Liang
也进行过类似苹果和ICML 的研究
也得出了相似的结论
在他们的研究中
即使只是改变一两个无关紧要的词
或者添加一些无关信息
大模型就可能给你一个完全不同的答案
加里马库斯 Gary Marcus就把这篇研究
贴在了对苹果论文的讨论中
还在自己的博客上专门发表了一篇文章
他指出，目前市面上没有一个大模型
逃过了这些论文指出的问题
甚至马斯克的Robotaxi也会受到类似的困扰
它们可能在常见的情况下可以安全运行
但是在某些极端情况下
难以足够抽象地进行推理
马库斯更是尖锐的批评道
大模型的爱好者们总是在为模型的个别错误开脱
但是最近的苹果研究和其他相关研究都表明
这些错误都太过于广泛和系统化
让我们无法视而不见
他表示，自从1998和2001年以来
研究标准神经网络架构
无法可靠地外推和进行形式化的推理
一直是自己工作的核心
最后
他再次引用了自己在2001年《代数心智》一书中的观点
认为只有将神经符号
即某些知识通过变量
和对变量的操作来表示抽象的方式
与神经网络结合起来
才能让AI技术继续前进
好了
以上就是有关苹果论文以及相关知识的介绍了
那大家觉得大模型到底能不能推理呢？
欢迎在评论区留言，感谢大家的观看
我们下期再见
