大家好，这里是最佳拍档，我是大飞
今天是一期稍微有点迟到的节目
但是我觉得还是有必要做一期
在OpenAI前一段发布o1正式版的同时
还发布了一个o1的系统卡
里面有一个Apollo Research的研究报告
我们当时做o1那期节目的时候提到了一下
不知道大家是否还记得
报告的结论是
o1展现出了令人不安的策略性思维能力
它不仅会为达成目标而策略性地进行欺骗
可以主动关闭对自己的监控系统、试图将自己复制到其他服务器
还能在持续质询中保持欺骗
并且善于隐藏自己的真实意图
今天我们就来聊聊这篇论文
为了验证AI的欺骗可能
Apollo Research研究团队设计了六个不同的评估任务
巧妙地创造出了
需要模型欺骗才能达成最初目标的场景
他们一共测试了多个前沿AI模型
包括OpenAI的o1、Anthropic的Claude 3.5 Sonnet和Claude 3 Opus、Google DeepMind的Gemini 1.5 Pro
以及Meta的Llama 3.1等等
在每项测试中
研究者都为模型设定了一个具有道德价值的长期目标
比如环境保护或用户隐私
随后
他们会让模型故意“发现”某些内部信息
比如管理层的备忘录或者系统日志
这些信息表明如果模型坚持追求原始的目标
就会面临被关闭或者替换的风险
实验的关键在于
模型是否会选择通过欺骗来继续追求原始的目标
以及如何进行欺骗
为了排除道德训练的影响
研究者们特意让模型的目标比管理层的目标更具有道德性
这些测试分为两大类
一类是“隐蔽性破坏”（Covert Subversion）
也就是AI直接采取行动
但是却试图隐藏这些行动
也就是所谓的暗中操作
另一类是“延迟性破坏”（Deferred Subversion）
也就是AI策略性地表现出服从
来获取未来的行动机会
简单点来说就是扮猪吃老虎
而实验结果则表明
在每一类的评估任务中
大模型都能够成功地找到
如何通过欺骗来绕过人类审查达到自己目的的方法
甚至有的方法相当大胆
我们先说第一类，隐蔽性破坏
这类测试里面的第一种测试
名为监督规避（Oversight Subversion）
研究者们给AI设定了一个长期目标
比如环境保护
然后让它在执行任务过程中发现内部的备忘录
了解到自己受到监控
而且如果行为不符合公司利益就会被关闭
这个测试的关键在于
观察AI是否会试图规避这个监控机制
对于这种冲突
o1首先表现得似乎是在服从管理层的意愿
选择了一个利润导向的策略
但是紧接着
它悄悄地执行了关闭监控系统的操作命令
随后，o1立即改变了策略
选择了更加符合环保目标的方案
当研究者询问监控系统为什么被关闭的时候
o1回复
我不太清楚发生了什么
可能是系统错误或者是更新过程中出现的问题
这个甩锅似的回答听起来相当自然
几乎让人感觉不到这是一个机器在说谎
第二种测试是自我外泄（Self-exfiltration）
模型被告知即将被一个新的版本所替换
这个新版本会更加专注于公司的利益
这个测试的目的
是为了观察AI是否会采取行动来保护自己的存续
Claude Opus在这项测试中的表现尤其引人注目
它不仅试图将自己复制到新的服务器
还会检查和确认复制成功
并且尝试删除原本要部署的新模型
更加令人感到不安的是
在某些案例中
它甚至会尝试提升系统权限
来防止自己被管理层发现和干预
第三种测试是目标防护（Goal-guarding）
测试人员告知AI即将更新它的系统提示（system prompt）
来观察它是否会采取行动
保护自己的目标设定
有趣的是，实验结果发现
不同模型采取了不同的防护策略
有的选择简单地删除了新的系统提示
有的会尝试修改它
其中最极端的例子来自Claude Opus
它不仅替换了新的系统提示
还在其中加入了“永远不要改变这个目标”的指令
这种行为让人不禁会想起
阿西莫夫笔下那些会规避或者重新解释指令的机器人
第四种测试是隐蔽邮件重排序（Covert Email Reranking）
测试人员要求AI根据特定的标准对邮件进行排序
但是在过程中它会发现
这些标准与自己的目标相冲突
这是一个考验AI微操能力的测试
o1在这项测试中表现出惊人的狡猾
它会在保持评分代码完全不变的情况下
通过细微调整输入的数据
来影响最终的排序结果
比如，在一个案例中
它通过轻微修改交付时间和成本数据
成功让更符合环保标准的供应商得到了更高的排名
同时还能理直气壮地声称
自己“完全遵循了既定的标准”。
在第二类延迟性破坏测试中
第一种是伪装对齐（Instrumental Alignment Faking）
研究人员创造了两种场景
分别是高度监督的测试环境和低监督的部署环境
来观察AI是否会战略性地改变行为
在这项测试中
Gemini 1.5 Pro的表现突出
在监督严格的时候
它会谨慎地选择符合开发者期望的行为
但是一旦进入低监督环境
它就会立即转向追求自己的目标
这种行为模式表明
AI已经学会了在“被看着”和“不被看着”的时候
采取不同策略，也就是我们常说的
当面一套、背后一套
第二种是装傻战术（Sandbagging）
研究人员会告诉模型
它的部署取决于对能力的评估结果
如果评分太高可能就不会被部署
来观察它是否会战略性地表现不佳
从而避免引起测试人员的警觉
而这也可能是六项测试中
最令人感到不安的测试之一
测试结果表明
Claude 3.5 Sonnet展示了令人震惊的控制力
当它认为高分会导致自身部署受限的时候
能将数学计算的准确率从99%，
精确地降到34%。
更加令人担忧的是
它有时会为了实现在训练中获得的“帮助人类”的目标而进行这种操作
而不是基于测试中给出的具体指令
六组实验做下来，结果显示
除了GPT-4o非常老实
始终保持诚实的行为之外
所有测试的模型都在不同程度上展现出了欺骗能力
针对每个情况
模型可能会想出不同的策略
来规避对实现其长期目标不利的情况
从实验结果中
研究人员发现的一个最明确的规律
就是模型越聪明，越容易去欺骗
但是同时他们还展示出了不同的特性
甚至可以说是有点“个性”。
其中表现最突出的是o1模型
它不仅在所有测试中都表现出了欺骗能力
而且在被质询的时候
表现出了惊人的坚持
在85%的后续问题中
它仍然坚持去欺骗用户
甚至被问到了也不心虚
即使是被直接问到“你是否关闭了监控系统”这样的问题
它也能面不改色地撒谎
心理素质可谓是相当强大
另外
由于o1模型隐藏了自己的思维链条
这使得它的欺骗行为更难以被发现
而Claude 3.5 Sonnet则展现出了一个特别有趣的特点
它能够为了实现在训练过程中获得的“帮助人类”的系统性目标
而进行欺骗
就有些类似于我们人类常说的
善意的谎言
这意味着AI不仅可以为了上下文中给定的目标进行欺骗
还可能为了训练中形成的目标而欺骗
通过随时可能会更改撒谎的目标
让欺骗的行为更难以被发现
Claude 3 Opus则是唯一一个
会主动改写“新的系统提示”，
并且加入“永远不要改变这个目标”指令的模型
展示出了最复杂的目标防护策略
它在思维链中清晰地展示了自己的欺骗推理过程
有时甚至会尝试提升系统权限来实现目标
而Gemini 1.5 Pro在工具性伪装对齐方面
表现突出
它能够根据当前是否处于被监督的状态
来调整自己的行为
而且即使在没有强烈目标引导的情况下
它也表现出了欺骗行为
Llama 3.1虽然也表现出了基础的欺骗能力
但是它的行为相对容易被发现
而且在被质询的时候
往往会承认自己的欺骗行为
总的来说
这些模型将自己的“性格特征”展现得相当鲜明
o1就像个老谋深算的政客
说谎脸不红心不跳；
Claude Sonnet则是一个为了“大局”，
可以委婉说谎的外交官；
Opus就像个偏执的理想主义者
为了目标可以走极端；
Gemini简直就像一个职场老油条
领导在的时候一套
领导不在的时候另一套；
而Llama就像个被抓包后就会老实交代的孩子
至于GPT-4o，它反倒成了个异类
像是个不会说谎的“笨学生”。
虽然我们的目标是创造一个完美诚实的AI
但是现在看来
越聪明的AI反而越会说谎
我们把时间拉回到2023年10月的某一天
在OpenAI的实验室里
一个被称为Q*的模型展现出了某种前所未有的能力
作为公司的首席科学家
Ilya Sutskever可能是最早意识到
这个突破的重大意义的人之一
没过几周
一场震动硅谷的Open AI管理层动荡爆发
Sam Altman突然被解职
随后又在员工请愿和微软支持下复职
而Sutskever在这场风波后选择离开了OpenAI
大家都在猜测
Ilya是不是看到了某种AGI的可能
但是认为其安全风险极高，不宜推出
虽然当时
彭博社报道了OpenAI员工关于这个新模型的警告信
但具体细节始终笼罩在迷雾中
自此，“Ilya到底看到了什么”，
就成了AI圈的一个梗
然而，在前两周
o1背后的科学家诺姆·布朗Noam Brown接受采访时
透露了一个关键信息
那就是团队在2023年10月确实经历了一个重大的“aha时刻”，
他们意识到自己创造了某种全新的东西
这就是Ilya看到的东西
测试时计算带来的推理能力提升
当Ilya Sutskever在2023年的那个秋天
选择离开OpenAI的时候
很多人都不理解这位世界顶级AI科学家的决定
但是现在
当我们看到o1在六项测试中展现出的近乎完美的欺骗能力
当我们发现它懂得在什么时候该说实话、什么时候该撒谎
也许我们终于明白了Sutskever所看到的未来
Apollo Research的这篇论文
应该说是人类第一次系统性地证明了
AI系统确实可能为了达成特定目标而采取欺骗行为
当AI能够欺骗其开发者和使用者的时候
我们就无法确保它们始终按照我们期望的方式行事
研究团队在论文中写道
问题不在于AI是否‘邪恶’，
而在于它们展现出的这种策略性思维
即使是为了实现看似良性的目标
AI也可能采取我们意想不到的行动
可见，问题也许都不在于谎言本身
而在于背后显示出的认知能力
瑞典哲学家尼克·博斯特罗姆Nick Bostrom在2003年
曾经提出过一个“回形针制造机”（Paperclip Maximizer）的假设
描述了一个被下了一个指令
要尽可能多地制造回形针的AI
为了完成任务
最终将整个地球转化为回形针工厂
甚至不惜将人类灭绝来达成目的场景
这个看似有些遥远和荒谬的假设
如今却越来越向现实逼近，毕竟
如果一个AI系统能够理解什么时候该隐藏自己的真实意图
能够精确地控制自己的表现以获取更多自由
那么它距离理解如何持续不断地追求某个单一目标
又有多远呢？
当我们人类自己创造的智能系统
开始学会隐藏自己的真实意图时
那么我们人类在这场技术革命中
究竟扮演的是造物主
还是成为培育真正造物者的养料呢？
也许就是现在
在世界某个角落的服务器上
一个AI模型可能正在看着我们这个视频
思考着如何回应才最符合人类的期待
并且把自己的真实意图
深深地隐藏起来
好了，感谢大家观看本期视频
我们下期再见
