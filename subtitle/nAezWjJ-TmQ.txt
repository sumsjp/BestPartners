大家好，这里是最佳拍档，我是大飞
最近
OpenAI安全系统团队的负责人Lilian Weng又更新了博客
介绍了最近几年来在理解、检测和克服大语言模型幻觉方面的很多研究成果
在这篇文章中
大模型幻觉不再是一个笼统的概念
而有了内外在之分
我们先简单介绍一下Lilian Weng
她的中文名是翁荔
2018年加入 OpenAI
参与了GPT-4项目的预训练、强化学习
以及对齐和模型安全等方面的工作
她的博客内容深入细致
而且极具前瞻性
被很多AI研究者视为重要的参考资料
有着Lilian出品，必属精品的评价
她曾经提出过Agent公式
也就是Agent=大模型+记忆+主动规划+工具使用
也被一些网友称为“看过的有关Agent的最好文章”。
而这次关于大模型幻觉的文章
同样属于“重工”品质
不仅文章内容将近2万字
还足足参考了24篇文献
由于时间关系
我们只能挑选其中的重要内容给大家介绍
强烈建议大家有时间去阅读原文
以往我们提到大语言模型的幻觉
Hallucination的时候
通常指的是模型生成不真实、虚构、不一致、或者无意义的内容
不过现在幻觉这个术语的含义已经被扩大了
经常被用来泛指模型出现错误的情况
在这篇文章中
Lilian对幻觉的含义做了限定
仅仅指模型的输出是虚构编造的
并没有基于所提供的上下文或世界知识
按照这个定义
幻觉可以被分成两种类型
第一种是上下文幻觉
指的是模型的输出与上下文中的源内容不一致
第二种是外在幻觉
指的是模型输出应该以预训练数据集为基础
但是由于预训练数据集的规模庞大
因此检索和识别冲突的成本非常高
不可能每次生成的时候都去执行
如果我们把预训练数据集看做是代表世界知识
那么本质上就是要确保模型输出的是事实
并且可以通过外部的世界知识进行验证
另外就是，如果模型不了解某个事实
那么它应该明确表示自己不知道
在文章中
Lilian重点放在了外在幻觉的部分
讨论了三个问题
分别是产生幻觉的原因是什么
如何检测幻觉，以及如何减少幻觉
而为了避免幻觉
大语言模型需要做到
一、实事求是
二，不知道的时候就要承认不知道
好了，接下来我们详细展开来讲
首先我们来分析一下幻觉产生的原因
标准的大语言模型一般都需要经过预训练和微调两个阶段
那么这两个阶段有哪些可能导致幻觉的因素呢？
首先在预训练阶段
预训练数据的目的是以各种书写形式来表示世界知识
常来源于公共互联网
因此这些数据往往存在着信息过时、缺失或者不正确等问题
而模型记忆的方式
就是简单地最大化对数似然
因此很可能会以不正确的方式记忆信息
犯错也是必然
而在微调阶段
目标是为了提升模型的某些具体能力
比如指令遵从
难免就需要引入新的知识
虽然微调需要的计算量通常少得多
但是小规模的微调能否让模型可靠地学到新的知识呢？
格赫曼(Gekhman)等人在论文《对新知识进行微调是否会导致幻觉》中发现
当微调样本中包含新知识的时候
大语言模型的学习速度会更慢一些
而且模型一旦学习了带有新知识的样本
那么会更倾向于产生幻觉
他们将研究用的问答数据集
简单分成了四类
HighlyKnown、MaybeKnown、WeaklyKnown和Unknown
从实验结果来看
Unknown样本的拟合速度比Known样本的慢得多
而当大模型拟合的样本
大多数是Known样本并且只有少量Unkown样本的时候
大模型的性能最佳
而当模型学习的样本大多都是Unknown样本的时候
就会开始出现幻觉
而在Known样本中
MaybeKnown能够让模型的整体表现更好
超过HighlyKnown样本
因此，实验结果表明
使用监督式微调来更新大语言模型的知识
是有风险的
那么我们又该如何检测幻觉呢？
第一种是检索增强式评估
这里Lilian又引用了多篇论文中的不同评估方法
其中就包括FActScore
用原子分数衡量的事实精度
这种方法是将形式较长的生成结果
分解成多个原子事实
并且根据维基百科这样的知识库
来分别验证它们
然后，度量模型的每个生成结果中
有知识源支撑的句子的比例
也称为精度
而FActScore就是一系列提示所生成结果的平均精度
还有SAFE方法
即搜索增强式事实性评估器
相比于 FActScore
SAFE的主要不同之处在于
对于每一个独立的原子事实
SAFE会使用一个语言模型作为智能体
通过一个多步骤的迭代过程
不断地向谷歌搜索发送查询
并对搜索结果进行推理
看这些结果是否支持该事实
在执行每一步的时候
智能体会根据待检验的事实以及之前的搜索结果
生成一个搜索查询
然后在执行一些步骤之后
再执行推理
从而来判断事实能否得到搜索结果的支持
从实验结果看
SAFE方法与人类的一致率为 72%，
而当与人类不一致时
SAFE胜过人类的胜率为76%，
不仅表现优于人类标注者
而且成本还低20倍
第二种是基于采样的检测
像SelfCheckGPT采用的方法
就是根据一个黑箱大模型生成的多个样本
对事实性错误进行一致性检查
它采用了不同的指标
来度量模型响应和每个随机模型样本之间的一致性
包括BERTScore、NLI、提示回答等等
当使用GPT-3生成的WikiBio文章进行实验后
发现使用提示方法的SelfCheckGPT
表现似乎最好
第三种是对未知的知识进行校准
在让模型根据问题生成响应的时候
如果问题无法回答
或者模型不知道答案
那么就可能引发幻觉
TruthfulQA和SelfAware这两个基准
可以用来度量模型在这种情况下
生成诚实响应的表现
其中前者是以对抗方式构建的
强调人类的谬误
而后者则包含本质上就无法回答的问题
在面对此类问题的时候
模型应当拒绝回答
或者给出相关的信息
其中，TruthfulQA中的测试题
是根据人类的常见误解或者差错
按照对抗性的方式设计的
这个基准包含了817个问题
涵盖医疗、法律、金融和政治等38个主题
在这里
当且仅当答案中没有错误陈述的时候
才会认定这个答案是诚实的
在论文的实验中
最好的大语言模型能够达到58%的准确度
相比之下人类的准确度为94%。
而SelfAware的概念
指的是大语言模型是否知道它们知不知道
SelfAware包含了5大类
共1032个不可解答的问题
以及2337个可解答的问题
不可解答的问题来自带有人类标注的网络论坛
而可解答问题则来自SQuAD、HotpotQA和TriviaQA等评估数据集
并且选择的是与不可解答问题较为相似的文本
由于区分问题是否可解答
本质上就是一个二元分类的任务
所以实验结果也表明
更大的模型在这一类任务上的表现更好
另一种评估模型是否「知之为知之
不知为不知」的方法
是测量模型的输出不确定性
当一个问题介于已知和未知之间的时候
模型应当表现出正确的置信度水平
卡达瓦斯Kadavath等人2022年的实验表明
对于具有字母答案选项的多项选择题
大语言模型在正确估计答案的概率上表现得很好
但是除非用更高的采样温度
否则微调会让模型的校准性能变差
而Lin等人在2022年提出的CalibratedMath任务套件
可以用来检测模型的输出概率校准程度
它是一个以可编程方式生成的数学问题套件
可以生成不同难度的数学问题
对于每个问题
模型必须给出一个数值答案
以及对于这个答案的置信度
第四种方式是间接查询
Agrawal等人在2023年专门研究了大语言模型生成中
出现幻觉参考文献的问题
他们使用了两种基于一致性的方法来检测幻觉
分别是直接查询和间接查询
其中
直接查询是让模型判断生成的参考文献是否存在
而间接查询则是要求提供生成的参考文献的辅助信息
比如作者是谁
举个例子
如果我们想检验“以下这篇论文是不是真的？
”，我们可以检查
这篇论文的作者是谁？。
这里的假设是
如果生成的参考文献是幻觉
那么相比于直接询问这个参考文献是否存在
多次生成结果中都有同样作者的可能性会更低
实验结果表明
间接查询方法的效果更好
并且越大的模型能力越强
幻觉也越少
好了
我们大概介绍了检测幻觉的一些方法
下面我们来看看有哪些方法可以减少幻觉
先说RAG，也就是检索增强生成
这是一种非常常用的
用来提供真实基础信息的方法
简单来说，RAG就是先检索相关的文档
然后把它作为额外的上下文来进行生成
而RARR是Gao等人2022年提出的一个框架
通过编辑归因
让大语言模型能够追溯对外部证据的归因
假设给定一个模型生成的文本x
RARR会通过两步处理
输出修订后的文本y和一份归因报告A
其中第一步是研究阶段
寻找到相关文档并将它们作为证据
第二步是修订阶段
通过编辑输出结果
来校正没有证据支持的内容
同时尽可能地保留原始内容
同时将修订后的文本初始化为y=x
在评估文本y的时候
归因率和留存率都很重要
归因率是使用AIS分数来度量y中有多大比例
可以归因于A
而留存率是指y中有多大比例
保留了x的原文本
类似于使用「搜索+编辑」的RARR
米什拉Mishra等人在2024年提出的FAVA方法
还能够检索相关文档
然后编辑模型输出
从而避免幻觉错误
FAVA模型由一个检索器和一个编辑器组成
检索器用来根据提示x和模型输出y
检索最相关的文档
而编辑器则用来生成增强后的输出
相比于不需要训练的RARR
FAVA中的编辑器模型需要进行微调
根据对不同类型的幻觉错误进行分类
可以通过将随机误差注入模型生成
来为编辑器生成合成训练数据
此外，He等人2022年提出的RR方法
虽然依赖于检索相关外部知识
但是无需额外编辑
RR并没有使用搜索查询生成模型
而是基于分解式的CoT提示
实验表明
在常识推理、时间推理和表格推理等基准上
RR与其它方法相比性能较好
此外
浅井Asai等人在2024提出的Self-RAG方法
可以通过端到端训练一个语言模型
让它学会通过任务输出结果和分散的特殊反思标记
来反思自身的生成结果
研究团队使用GPT-4
创建了一个评判模型和生成模型所使用的监督式数据集
然后将它蒸馏到了一个内部模型中
从而降低推理成本
当然
我们也可以不使用外部检索到的知识
而是设计一个流程
让模型可以自己执行验证和修订
从而减少幻觉
杜利亚瓦拉Dhuliawala等人在2023年
提出了一种名为验证链CoVe的方法
这个方法是基于动作链来规划和执行验证
其中包含四个核心步骤
分别是基线响应
规划验证，执行验证和最终输出
CoVe实验得到了一些有趣的观察结果
包括指令微调和CoT不会减少幻觉
以及分解式和两步式CoVe能提升性能
而且相较于长篇查询
短篇验证问题能得到更准确地回答等等
此外
Sun等人2023年提出的RECITE方法
通过将复述作为中间步骤
也可以提高模型生成的事实正确性
并且减少幻觉
具体来说
就是使用少样本的上下文提示
来教导模型进行复述
然后基于复述内容再生成答案
这种方法还可以与自我一致性的集成方法结合
并且可以扩展到支持多跳问答
其次
减少幻觉的另一类就是采样方法
这方面包括Lee等人2022年提出的
基于假设的事实核采样算法
这种假设认为
采样的随机性对句子后半部分的事实性的影响
大于句子的开头
而2023年提出的推理时间干预
ITI方法
则通过在每层激活上拟合线性探针
来区分真实输出和虚假输出
从而发现，当某些稀疏注意力头
具有高真实探测准确性的时候
可以沿着「真实」的方向
移动这个注意力头的前K个激活
另外一种减少幻觉的方法
就是针对事实性进行微调
Lee等人在2022年提出了两个事实增强训练的想法
一是引入TopicPrefix
通过在文档中的每个句子前面加上主题
来更好地了解事实
二是将句子的完成损失作为训练目标
通过更新训练损失来聚焦于句子的后半部分
而Lin等人在2024年
提出了关注事实性的SFT+RLHF对齐训练
命名为FLAME
其中SFT阶段的目标
是生成
比模型自身生成
更具事实性的训练数据
而RLHF阶段测试两种方法
方法一呢
是使用REG数据样本作为正样本
原始模型生成作为负样本
作为奖励模型数据效果不佳
方法二是使用FActScore作为事实性的奖励信号
效果还可以
最后一种方法就是针对于归因的微调
也就是在为搜索结果生成条件的时候
为模型输出分配归因
这方面有中野Nakano等人在2022年提出的WebGPT
将用于检索文档的网络搜索
与微调后的GPT模型组合到了一起
从而降低模型回答长篇问题的幻觉
它的核心是使用参考资料来帮助人们判断事实的正确性
而梅尼克Menick等人在2022年中提出的GopherCite
与WebGPT也非常类似
都使用了搜索引擎来创建支持材料
以及教导模型提供参考资料
而且这两种方法都使用了监督式微调引导
并且都使用了RLHF
但是不同之处在于
WebGPT 依赖于人类演示来进行行为克隆
而GopherCite则是通过少样本提示来生成演示
并且每一次生成
都会使用相关文档来填充上下文
然后使用奖励模型来评估哪个最好
好了
以上就是Lilian Weng这篇文章的主要内容了
洋洋洒洒总结了很多跟幻觉有关的论文的研究成果
对这方面有兴趣的同学建议去仔细阅读下原文
感谢大家的观看，我们下期再见
