大家好，这里是最佳拍档，我是大飞
今天我们继续来解读Semianalysis的另一篇万字独家爆料
首次揭露了o1 pro的架构
同时还解答了一些大家非常关心的问题
比如Claude 3.5 Opus到底有没有失败
OpenAI的Orion模型发展如何
以及Scaling Law究竟能否持续下去等等
前段时间
关于大模型Scaling Law终结的讨论是闹得沸沸扬扬
但是硅谷巨头们纷纷用实际行动给出了不同答案
谷歌推出了最强下一代新模型Gemini 2.0 Flash以及多个Agent
OpenAI则以“满血版”的o1和o1 pro mode
展示大模型的能力尚未触顶
而三位作者也在文章中指出
Claude 3.5 Opus也并非如传言所说的内部失败了
实际上它反而成为了Anthropic精心打造的战略武器
之所以它一直没有公开发布
因为它被秘密地应用在了“内部数据合成”和“强化学习奖励建模”两个关键领域
通过这种创新的训练方法
Anthropic不仅没有增加推理的成本
反而显著提升了模型的性能
所以相比于公开发布
Anthropic更愿意将它用在模型的训练优化上
发布Claude 3.5 Sonnet就已经足够了
由于基础模型在判断任务方面越优秀
训练所需要的数据集就越好
而Claude 3.5 Opus表现出色
规模也恰到好处
所以Anthropic利用它来生成合成数据
并且进行奖励建模
从而在显著提升Claude 3.5 Sonnet性能的同时
推理成本没有大幅的变化
接下来
我们来介绍一下o1和o1 Pro推理架构
众所周知
o1在推理阶段采用了思维链（Chain of Thought）方法
将推理分解为多个离散的步骤
规划推理步骤、评估中间结果
在出错或者出现僵局的时候进行回溯
但是与很多研究员之前的推测不同
o1在推理过程中只会沿着单一CoT前进
直至给出答案
而且测试阶段不依赖于搜索
放弃了对潜在推理路径树的探索
只使用了pass@1方法
而o1在单一思维链上自我纠正和回溯的能力
正是推理阶段计算量scale的自然结果
类似于学生的自我纠错
不过这项能力对不同的问题效果不同
像对于一些简单问题来说
延长思考时间帮助不大
但是对于复杂的数学或编程问题
则有可能显著的提升结果
而且验证数学和编程问题相对于英语作文更加容易
目前还不清楚OpenAI如何利用额外的测试时计算资源
只知道后台有相关设置
从按主题划分的胜率图表中可以看出
o1和推理模型在相对容易验证、但是生成答案较难的学科中
表现优于非推理模型
在验证和生成答案都比较困难的领域
这主要因为o1的训练
在很大程度上依赖于功能验证器（functional verifiers）在训练期间
给模型提供反馈
而o1 Pro则采用了自洽性（self - consistency）或者多数投票（majority vote）的方法
从表面看，这种方法的成本似乎很高
比方说如果有5个投票流
就需要生成5倍的token
这也是OpenAI为什么将ChatGPT Pro的订阅价格
大幅提高的原因之一
但是实际上OpenAI成本的增加
远低于价格上涨的幅度
因为在运行更长的平均序列长度
并且增加解码token与预填充token的比例时
推理系统更多还是受到带宽和容量的限制
而非FLOPs的限制
而且自洽性和多数投票
在大部分序列长度上会使用共享前缀
无需在KV缓存上花费额外的带宽或者内存
为了训练o1
OpenAI打造了名为“草莓训练”（Berry Training）的复杂系统
这个系统会通过蒙特卡洛树（Monte Carlo tree）生成海量的合成数据
伴随许多并发回合（rollouts）
然后
模型会基于过程奖励模型（PRM）
针对大约1000万个问题生成众多的变体
并且在多个点分支
每个问题会生成数千条答案“轨迹”（trajectories）
这些轨迹会共享前缀
实际上是一条通向答案的思维链
每条轨迹会包含数千个token
而训练o1这样的模型需生成数百万亿个token
之后
轨迹会通过功能验证器（functional verifiers）和优化奖励模型（ORM）进行修剪
由于过程奖励模型的效率较低
所以多数数据会选择通过优化奖励模型来实现
每个问题会经过多个并发回合完成
并且到最后才被修剪
如果过程奖励模型的表现更好
那么生成与保留优质轨迹的比例就会更高
但是优化奖励模型仍然占主导地位
所以会筛除掉大部分的数据
功能验证器在检查数学计算
或者运行代码来验证数据的正确性方面
发挥着重要的作用
但是运行复杂的功能验证系统依然面临着许多挑战
比如不同模型需要在不同的GPU上运行
计算结果能否精确路由、多模型权重的更新以及工作负载均衡等问题
而且由于功能验证器在GPU上的运行效果不佳
所以经常需要转移到CPU上
文章还提到
不同公司的下一代训练系统
在CPU和GPU资源之间的差异巨大
像英伟达的系统就有助于OpenAI运行复杂的功能验证器
而Anthropic在每FLOP成本和内存带宽容量成本方面
有很大的优势，但是因为缺少CPU资源
所以很难运行复杂的功能验证系统
在这种情况下
推理训练也就成为计算极其密集的工作了
仅1000万个推理问题
就可能会生成数百亿条轨迹和数百万亿的token
数据量也会随着问题集的扩展
呈指数级增长
而且与客户请求不完全重叠
造成推理部分的token数量
会远超预训练数据集
由于PPO和过程奖励网络PRN的工作方式
后训练阶段在每次反向传播之前
都需要进行多次前向传播
会涉及到多个模型
导致前向传播与反向传播的比例极高
从而对训练基础设施需求产生巨大影响
比如过去需要单一的大型全连接扩展架构
现在不再需要了
同时也让跨地理分布的数据中心训练更加容易
以往，在推理模型的后训练阶段
几乎需要与预训练相同的计算量
后训练FLOPS在许多情况下已经超过了预训练阶段
比如OpenAI正在训练一个介于GPT - 4o和Orion之间的模型
他们会先预训练一个基础模型
然后从中派生出两个模型
分别是一个传统的聊天模型
以及一个真正的推理模型
在从基础模型转变为推理模型的过程中
所需要的后训练FLOPs
将超过预训练所需要的计算量
因为Orion会被用来生成大量的“草莓训练”数据
以及用在各种验证器和奖励模型中
因此，文章认为
预训练的规模将继续扩大
而推理训练的兴起
使得后训练不再局限于微调
而且需要更多的计算量
训练的scaling law依然有效
我们再来聊聊o1推理架构中的token经济学
即使较小的推理模型
在使用Blackwell后服务的效率也会提升
但是尽管GPT - 4o和o1的架构、规模相同
token的定价差异却高达6倍
GPT - 4o mini和o1 mini差异更大
达到20倍
虽然部分的原因
是因为OpenAI为了盈利收取更高的费用
但是主要的原因还是成本变得更高了
文章以一个逻辑推理的提示词实验为例
推理模型
比如o1 - preview和o1 - mini
比同等规模的非推理模型会生成更多的输出token
即使推理token没有显示或者提供给用户
也会包含在计费的输出token中
由于推理模型的每个token成本明显更高
所以查询成本在o1-mini中会高出24倍
而在o1-preview中会高出57倍
而且序列越长
为了维持一定的交互速度
就必须减少一次性并行处理的批大小
这会导致推理模型的成本无法在更多的用户间分摊
意味着每个token成本会因为KV缓存的限制
高出5倍以上
那么
是什么导致推理模型的计算密度和内存需求增加
从而导致批大小减少和每GPU吞吐量降低了呢？
答案有两个方面
内存需求的增加
主要因为处理长序列长度
需要更大的KV缓存
而在使用全局查询注意力（GQA）的时候
KV缓存的大小会随着序列长度和批大小线性增长
当批大小恒定的时候
长序列长度就会占满内存容量
降低交互性或者限制最大的批大小
进而影响模型的FLOPs利用率
而每个token所需要的FLOPs
会随着序列长度的增加呈平方增长
所以长上下文模型在推理的时候
序列长度的增加会让系统很快达到FLOPS限制
由于序列长度的增加
会极大增加内存和FLOPs的需求
导致批大小的显著缩小
难以分摊集群的总成本
所以也会让每个token的服务成本明显提高
OpenAI虽然尝试通过改进注意力机制来缓解问题
但是没能从根本上解决
所以目前的挑战在于
需要能够在保持质量的同时
解决这些问题的长上下文架构
否则推理模型的每token成本就降不下来
而且生成的token数量也会更多
此外
序列长度的增加可能还会带来可靠性方面的问题
在训练中进行检查点保存
可以减少故障中断的时间
但是在推理过程中
静默数据的损坏和其他故障仍然可能发生
在Transformer架构中
如果生成新token的时候出错了
那么这个坏的token就会成为对话上下文的一部分
导致语法、语境或者格式错误
这个问题对长上下文模型尤其明显
因为长序列长度会导致错误累积
另外，许多错误可能源于模型本身
或者由于推理过程中的思维链一开始就走上了错误的轨迹
所以o1 pro才会采用自洽性和多数投票机制来应对这个问题
最后
在谈到计算领域Scaling Laws的时候
文章指出
OpenAI o1展示了推理模型的潜力
AI领域的“Scaling Law”之争
与过去对计算能力的扩展和摩尔定律的争论十分相似
曾经大家很热衷于讨论摩尔定律走向终结
但是现在因为英伟达等企业找到了新的扩展维度
比如用先进封装技术来提升I/O能力、突破光罩尺寸的限制、利用更大的硅片面积
以及芯片内外的并行计算和大规模高带宽网络
从而使得行业持续发展
所以，文章认为
AI领域的Scaling Laws
也会随着新技术范式的出现和扩展持续下去
在关于Scaling Laws的讨论中
预训练常常最受到大家的关注
但是它其实只是AI生命周期的一部分
预训练的目标非常单一
也就是正确预测下一个 token
但是
实现这个目标仍然远远没有达到大模型开发的最终目标
也就是「回答用户的提示词」或者「完成任务」。
由于训练模型缺乏足够复杂、高难度的提示词
所以数学就成为了微调模型的重点领域之一
一种方法
是聘请高技能的人类专家来设计提示词
或者在内部生成这些提示词
虽然模型可以通过提示词来生成思维链
但是仍然需要多重保障机制防止错误累积
另一种方法
是通过多个独立的模型来提高准确性
包括生成器、验证器模型、完成器和奖励模型等等
其中过程奖励模型（PRM）在训练思维链模型的时候更受青睐
但是OpenAI更多依赖结果奖励模型（ORM）
o1-preview的发布
引起了业界对于测试时计算
或者称推理时计算Scaling Laws的关注
其实测试时计算并不是一个新的概念
在棋类游戏和扑克中
测试时计算的理念已经存在了一段时间
比如，AlphaGo在测试时
会使用蒙特卡洛树搜索来决定下一步棋
通过更强大的计算力
推理模型可以思考更多的步骤
从而增加得出正确答案的可能性
不过
当前的推理模型受到了推理系统性能的限制
主要就是我们之前提过的
长上下文长度显著增加了内存和计算需求
但是一旦具备经济上的可行性
调整CoT的长度和计算资源使用
将成为测试时计算的关键技术
通过增加样本数量或者搜索实现
比如自洽性/多数投票、Best-of-N采样、蒙特卡洛展开等等方法
测试时计算还能得到进一步的scaling
不过，由于推理模型的成本过高
加上使用了大量token
所以对于企业来说
控制部署成本的上升变得至关重要
目前主要的实验室由于缺乏大规模的服务能力
以至于微软无法全面推出Copilot功能集合
OpenAI甚至关闭了Sora的注册入口
因此，文章指出了很关键的一点
那就是scaling预训练目前仍然可以大幅降低成本
而且比scaling测试时计算更加划算
同时
超大规模计算提供商也将继续建设更大的集群
比如说
马斯克计划建立100万块GPU的集群
而OpenAI和微软目前运行了数十万张推理GPT的GPU
也显示了scaling预训练仍然可以进一步的降低成本
好了
以上就是这篇文章的主要内容了
文章不仅深入剖析了o1 Pro的架构
还重点探讨了预训练scaling和推理scaling的发展
希望能够对大家有所帮助
原文内容很长
由于时间的关系
我们也还有很多的内容没有提到
建议大家有时间能去阅读一下原文
按照惯例链接呢我放到视频简介中了
感谢大家的观看
我们下期再见
