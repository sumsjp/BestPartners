大家好，这里是最佳拍档，我是大飞
在AI圈里
图灵奖得主杨立昆一直是一个比较特别的存在
当很多技术专家都坚信
沿着当前的技术路线
实现AGI只是时间问题的时候
杨立昆却一再提出异议
在与同行的激烈辩论中
他不止一次的表示
当前主流的技术路径无法引领我们走向 AGI
甚至现在 AI 的水平还不如一只猫
而图灵奖得主、Meta首席AI 科学家、纽约大学教授等等耀眼的头衔
以及沉甸甸的一线实践经验
也让我们谁都无法忽视这位AI专家的见解
在最近一次的公开演讲中
他再次详细阐述了自己的观点
那就是仅仅依靠文本训练
AI永远无法达到接近人类的智能水平
今天大飞就来给大家分享一下这次演讲的内容
看看杨立昆的这番言论
究竟是从何而来
在演讲的一开始
杨立昆就明确提出了自己观点的前提
那就是我们需要人类水平的 AI
为什么这么说呢
因为在未来
大多数人都将佩戴智能眼镜或者其他智能设备
人类将学会与这些设备进行对话
而这些系统也将承载各种AI助手
这条科技路线的最终结果
就是每个人都会有一支智能的虚拟团队在为他工作
不仅能够增强人类的智力
也能让人们变得更加具有创造性和更加高效
人人都将成为老板
只是手底下的员工不再是真正的人类
当然
当老板的肯定不希望手底下的AI都是人工智障
所以
我们需要让这些AI具备和人类差不多的智能水平
它们需要能够理解世界
能够记住事物
具备基本的直觉和常识
而事实上
每种动物其实都具有这样的模型
比方说，你家里的猫
它所具有的模型
远比任何一个AI所能构建或设计的模型都要复杂
所以说
你家里的猫也可以轻易地做到
现在很多最先进的AI也没法做到的事
比如拥有持久而且稳定记忆的脑系统
光这一点
当前的大语言模型就办不到
杨立昆觉得，之所以会出这些问题
主要还是AI的底层技术不够好
他指出
包括大型语言模型在内的 AI
其实都依赖于自监督学习技术
而自监督学习的核心在于训练一个、不针对任何特定任务构建的系统
也就是追求系统的通用性
因此
研究人员需要尽量以良好的方式来表示输入数据
而实现这一点的方法就是通过损坏数据
再让模型重建恢复
比如说，你可以截取一段文本
通过去掉一些单词
或者改变其中一些单词来破坏它
再让模型去重建
从而达到训练的目的
这个过程可以用在文本、DNA 序列、蛋白质或者其他任何内容上
甚至在某种程度上也可以用于图像和视频
经过反复的输入、破坏和重建
你就会训练出一个庞大的神经网络
这就是一个生成模型了
这个网络会计算输入和重建后的输出之间的距离
而这就是学习过程中要最小化的参数
通过这个过程
系统就学习到了输入的内部表示
或者更通俗一点来说，本质
而这个事物的本质
就可以用来执行后续各种的相似任务
从而实现功能泛化的目的
如今的AI开发者们
就是利用了自监督学习技术构建了自回归预测
但是这个预测并不是说AI可以推理或者预言未来
而是一种对文本的推测
大语言模型可以根据你给它输入的文本
来预测下一个单词
然后再将那个预测出的单词加入到输入中
再预测第二个单词
然后再将第二个结果加入到输入中
预测第三个单词，以此类推
这就是大语言模型所做的事情
杨立昆解释道
这并不是一个什么新的概念
其实自从香农（Shannon）时代以来就已经存在了
而且理论的雏形
可以追溯到 50 年代
但是
我们现在拥有了远超过去的算力
因此可以搭建出庞大的神经网络架构
让大语言模型可以在大量数据上进行训练
从而让它们涌现出一些特性
跨越了从理论到应用的鸿沟
到这里
杨立昆讲的大多还是眼下AI界公认的一些事实
他所说的推理和记忆能力
以及对自回归预测的改进
也确实是很多大模型公司的研究目标
站在大部分AI研究人员的视角来看
只要我们花时间去研究
就肯定就能在自回归预测的技术路线上有所突破
然而
杨立昆却这个想法泼了一盆冷水
他说
任何 10 岁的小孩都可以一次就学会
如何清理餐桌并且把碗筷放到洗碗机
根本不需要经过刻意的练习
而年满17 岁的人类
学会开车大约只需要经过 20小时的练习
但是
人类至今仍然没有拥有 L5 级的自动驾驶汽车
也没有出现能够清理餐桌和使用洗碗机的家用机器人
这些现象都说明
在自回归预测中缺少了一些重要的东西
否则我们早就应该能用现在的AI系统来完成这些事情
因此
自回归预测并不是一条合适的技术路线
在这个方向上会不断碰到一个问题
叫作莫拉维克的悖论（Moravec's Paradox）
这个悖论的意思是，对于人类而言
看似微不足道、甚至不被认为是智能的事情
实际上用机器完成起来却是非常困难的
而像操纵语言这样高级而复杂的抽象思维
似乎对机器来说却很简单
所以，我们其实是被AI误导了
它能做到那些高级的抽象思维
并不是因为它们和我们一样聪明了
而是这些事情对于它们而言本来就很简单
反而是我们人类陷入了思维定式
用人类的智力指标去硬套AI
杨立昆举了个例子
一个大语言模型通常是在 20 万亿个 token 上进行训练的
一般平均来说
一个 token 基本上是一个单词的四分之三
因此
这里总共是1.5 × 10的13 次方个单词
而每个 token 大约是3字节
那这样就需要 6 × 10 的 13 次方个字节
虽然对于任何一个人类来说
读完这些内容大约需要几十万年的时间
这基本是不可能完成的任务
但是
这就能证明AI拥有比人类更强的阅读能力吗？
其实不然
因为在接受信息的能力方面
人类其实和AI不相上下
一个四岁的孩子总共清醒的时间有16000 小时
而我们大脑中有 200 万个视神经纤维
每根神经纤维大约以每秒 0.5个字节到3个字节的速度在传输数据
因此这个数据总量大约是10 的 14 次方个字节
差不多与大模型的数量级相当
这说明，在接受信息的效率上
目前的大语言模型和人类其实只是打了个平手
只是各自能够快速接收的信息类型不一样罢了
而说到信息类型的广度
大语言模型可能还远远不及人类
我们能看，能摸
还能闻味道
而大模型目前还只是在吃文本
杨立昆表示
人类和大模型的对比可以告诉我们两点
第一，仅仅通过训练文本
AI永远无法达到接近人类的智力水平
第二
冗余可能是实现人类智能的一个关键
就拿刚刚我们例子中的视觉来说
视觉信息是非常冗余的
虽然每根视神经纤维每秒可以传输 1个字节的信息
但是我们视网膜中大约有 6000 万到 1 亿个光感受器
也就是说
在视觉信息到达大脑的时候
它接收到的信息实际上会被扩展大约 50 倍左右
显然文本暂时还做不到这一点
如果你有一系列的神经网络层
那么对于任何单一输入
你只能有一个输出
而在冗余信息加持下的人脑
可能会有多个解决方案
并且可能还会有某种方法
来同时处理这多个解决方案
不仅如此
如果人类对特定的感知有多个解释
大脑还会自发地在这些解释之间找到平衡点
还不会报错
得益于冗余
这些信息在人脑中组成了一个世界模型
我们将假设的一系列行动输入到模型中
而世界模型则将这些输入与各种函数结合
然后推导出世界的最终状态是什么样子的
在杨立昆看来
建立在冗余数据上的世界模型
一定会比自监督模型走得更远
为什么这么说呢？
首先
世界模型可以在不需要任何附加学习的情况下
完成新的任务
这是什么意思呢？
当人类面临新的情况的时候
我们会去想象行动的后果
然后将采取实现目标的一系列行动序列
而不是根据新的情况
从头去学习一遍新的知识
然后再去实现目标
所以说
人类在面对新情况时的一系列反应
实际上是根据过去的知识和经验
进行的规划行为
杨立昆认为
通过规划进行推理的过程
本质上比仅仅通过神经网络的多层运行更为强大
在优化控制理论领域
这被称为模型预测控制
我们可以利用世界模型来计算一系列控制指令的效果
然后再规划这个序列
让运动达到我们想要的结果
实际上
所有经典机器人学的运动规划
都是通过这种方式完成的
而现在
杨立昆想把这个思路带入到AI中
具体来说，我们可以构建一个 AI系统
它包含以下这些组件
分别是世界模型、可以针对任务配置的成本函数、寻找世界模型最佳动作序列的模块、短期记忆、感知系统等等
在这套系统中
模型不会像大语言模型那样
一个token一个token地预测文本
而是将单一动作拆分成一个动作序列
随后
AI会获取到初始世界的状态表示
然后输入对行动序列中第零个动作的假设
利用世界模型来预测下一状态
然后执行行动一，计算下一个状态
计算成本
最后通过反向传播和基于梯度的优化方法
找出最小化成本的两个动作
可以看到，在这个流程中
AI完全不需再输入任何新的知识
仅靠世界模型就可以完成一系列的预测
就和人类一样
除此以外
人脑中的世界模型还能带来一个重要的功能
那就是对冗余信息进行规划的能力
举个例子
如果你在计划一趟从纽约到巴黎的旅行
可以用你对世界、对身体的理解
以及你对到巴黎的想法
用低级肌肉控制来规划整个旅行
但是
如果说从每十毫秒要控制的肌肉数量
到所有去巴黎之前必须要做的事情
都需要你的主观意识去一一处理的话
那人就要疯掉了
因此，人脑要做的第一件事
就是以分层规划的方式对这些事情进行规划
要去巴黎，你首先需要去机场
搭乘飞机
那如何去机场呢？
假设你在纽约市
那就需要去叫出租车等等
虽然在某个时刻
你不得不将一些事情表达为低级的肌肉控制动作
比如走路
但是人类并不是在以低级别的方式规划整个过程
而是在进行分层规划
总得来说
世界模型能带给AI的好处相当多
问题是
我们到底要如何训练出一个世界模型呢？
杨立昆指出
传统的文本训练肯定没戏
虽然在像文本这样的离散空间中
你可以预测哪个单词会跟在一串单词之后
但是当涉及到冗余信息
比如视频画面中每一帧的信号时
自回归预测就歇菜了
比如，我们现在有这样一个视频
一个人想要录下杨立昆的这场演讲
在进度条走到他拿着相机拍摄的时候
我们按下暂停键
然后问大语言模型接下来会发生什么
它可能会预测
剩下的房间会有一面墙，会有人坐着
两边的人数可能会差不多
但是绝对不可能在像素级别上
准确预测你们每个人的样子、世界的纹理
以及房间的精确大小等所有细节
因为其中没有规律可言
随后杨立昆又提到了他的联合嵌入预测架构
JEPA
这套架构的核心理念就是放弃预测像素
而是让模型去学习对世界运作的抽象表示
然后在这个表示空间中
分别嵌入两个版本X和Y
经过编码器处理
然后训练AI根据X的表示来预测Y的表示
杨立昆说到
这个过程这实际上就是人类智力的表现
通过寻找某种现象的良好版本
能够让模型在遇到不良状况的时候
进行规划和预测
从而回到良好的版本上
他将这个过程称为科学的本质
不过
联合嵌入预测架构也并不是没有缺点的
如果只用梯度下降、反向传播等最小化预测误差的手段
来训练这样的系统，它就会崩溃
那我们要如何稳定地训练联合嵌入预测架构呢？
一种方法是估计信息量
然后测量来自于编码器的信息内容
目前有六种不同的方法来实现这一点
杨立昆在演讲中只简单介绍了两个方法
分别是MCR和蒸馏风格方法
在MCR方法中
训练人员可以提取出来自编码器的变量
确保这些变量具有非零的标准差
然后将变量放入一个成本函数中
确保权重能够被搜索到
这样，变量不仅不会崩溃
还会变成一个常数
而在蒸馏风格方法中
它的核心思想是只更新模型的一部分
而在另一部分不进行梯度的反向传播
并且通过一种有趣的方式来共享权重
这种蒸馏技巧的好处是
可以防止模型在空间表示中的崩溃
最后
杨立昆还捎带提了一下Meta的开源哲学
Meta认为
开源人工智能不仅仅是一个好主意
而且它对于文化得多样性
甚至对于民主的保存来说
都是必要的
而开源人工智能模型的可用性
正是推动人工智能初创生态系统发展的原因之一
好了
以上就是杨立昆这次演讲的主要内容了
如果用他自己的话概括一下就是
要放弃生成模型
转而使用嵌入预测架构；
放弃概率模型
转而使用基于能量的模型
以及放弃对比学习方法和强化学习
他相信，在这条技术路线下
我们终将可以打造出安全可靠、超越人类智力水平的通用人工智能
感谢大家观看本期视频
我们下期再见
