大家好，这里是最佳拍档，我是大飞
如果说AI大模型是当下科技行业的发动机
那么高带宽内存HBM
就是驱动这台发动机的燃油管路
从GPT-3到GPT-4
从图像生成到自动驾驶
每一次AI技术的突破
背后都离不开内存技术的迭代
今天大飞要给大家解读的
是一份来自韩国科学技术院KAIST的报告
《HBM路线图Ver1.7》。
这份由24位研究者共同参与的、长达370多页的报告
不仅详细规划了从2026年HBM4
到2038年HBM8的技术演进路径
更揭示了AI时代内存技术的核心突破方向
它想要告诉我们，未来13年
内存技术将如何重塑AI计算的格局
在开始聊技术之前
我们必须先搞明白一个问题
为什么HBM会成为当下AI芯片的标配呢？
要回答这个问题
我们得从传统计算架构的痛点说起
熟悉计算机原理的朋友都知道
冯·诺依曼架构的核心矛盾
在于计算单元和存储单元之间的速度鸿沟
GPU的计算能力
以每两年几倍的速度增长
但是传统内存的带宽增长却相对缓慢
这就导致了内存墙的问题，GPU再能算
也得等内存把数据传过来
于是大量的计算资源
被浪费在了等待数据上
而HBM的出现，正是为了打破这堵墙
与传统DDR内存相比
HBM采用3D堆叠技术
将多片DRAM芯片垂直堆叠
通过TSV和微凸点连接
再配合硅中介层与GPU封装在一起
这种设计带来了两个关键的优势
一是短距离互连
大幅降低了数据传输延迟
二是多通道并行设计
让带宽实现了指数级的提升
比如HBM3的单模块带宽
已经达到了819GB/s
而最新的HBM3E更是突破了1TB/s
这是传统DDR5内存的5-10倍
正是在这样的背景下
KAIST推出了这份HBM路线图报告
作为路线图中最先落地的一代产品
HBM4的核心突破在于定制化基片的引入
这个变革直接重构了传统内存与计算单元的连接方式
在HBM4之前，HBM的基片Base Die
主要承担信号缓冲和接口转换的功能
相当于一个被动的中间人
数据必须经过CPU或GPU
才能在HBM和其他内存之间传输
而HBM4的定制化基片
通过集成内存控制器MC
和低功耗内存接口LPDDR
实现了HBM与LPDDR的直接互连
CPU不再需要参与内存数据传输
这种架构的优势非常明显
一方面，内存容量得到了灵活扩展
HBM4的单模块容量达到36-48GB
配合LPDDR后
整个系统的内存容量可以轻松突破1TB
另一方面，数据传输路径被大幅缩短
从传统的DIMM到CPU，到GPU，再到HBM
简化为了DIMM直接到HBM
延迟降低了30%以上
为了支撑这个架构
HBM4在关键参数上实现了全面升级
报告显示
HBM4的I/O数量从HBM3的1024个翻倍到2048个
数据速率保持8Gbps
单模块带宽达到2.0TB/s
是HBM3的2.4倍
堆叠层数从12层提升至12-16层
单芯片容量从24Gb提升至32Gb
电源功耗则控制在43-75W之间
封装方面，HBM4继续采用微凸点技术
但是优化了TSV阵列布局
将TSV间距缩小到了25μm
进一步提升了互连密度
值得注意的是，HBM4的定制化基片
还首次集成了近存计算NMC的雏形
部分GPU的计算功能被迁移到基片上
让数据在内存附近就能完成简单的计算
减少了大量数据移动
KAIST的测试数据显示
在GEMM的工作负载中
HBM4的内存绑定任务
性能提升了3.5倍，计算资源利用率
从原来的30%提升至75%。
冷却方案上
HBM4采用直接芯片液冷D2C技术
通过将冷却液直接接触芯片表面
解决了3D堆叠带来的散热难题
与传统的散热片加风扇的方案相比
D2C液冷的散热效率提升了4倍
能将HBM的工作温度控制在65℃以下
有效缓解了热噪声对信号完整性的影响
如果说HBM4是架构转型
那么HBM5就是深度优化
它在HBM4的基础上
全面强化了近存计算能力
并且通过一系列的创新技术
解决了高堆叠带来的电源和散热问题
HBM5的核心架构是3D异构集成近存计算（3D NMC-HBM）
KAIST的研究者
将NMC处理器芯片和L2缓存芯片
直接堆叠在HBM DRAM芯片的上方
通过专用TSV互连和电源分配网络PDN连接
这种设计
让计算单元与内存的距离缩短到了微米级别
数据无需经过中介层
就能在计算单元和内存之间传输
延迟再降20%。
具体来说，HBM5的NMC处理器芯片
集成了10个GPU流处理器和0.75MB的L2缓存
支持直接访问HBM的DRAM阵列
专门处理矩阵乘法、激活函数等内存密集型的运算
测试数据显示
对于算术强度较低的GEMM任务来说
HBM5的性能相比HBM4提升了4倍
功耗却降低了29%，
真正实现了高能效比计算
为了解决高堆叠带来的电源完整性问题
HBM5采用了一系列的创新方案
首先
HBM5引入了分布式网格电源和接地TSV阵列
将电源TSV、接地TSV、信号TSV和热TSV
进行了不对称的分布式布局
在不影响信号传输的前提下
降低了电源网络的阻抗和电感
其次，HBM5在3D堆叠中
加入了专门的去耦电容芯片（Decap Die）
通过堆叠式电容和深槽电容
有效抑制了同时开关噪声
电源噪声的峰值
从HBM4的0.041V降低到了0.022V
降幅达46%。
此外，HBM5还在基片上
集成了温度传感器和有限状态机
能够实时监测的芯片温度
并且根据温度变化
动态调整电源供应和数据传输速率
进一步提升了系统稳定性
在关键参数上，HBM5的I/O数量
再次翻倍到4096个
数据速率保持8Gbps
单模块带宽达到4.0TB/s
堆叠层数提升到了16层
单芯片容量达到40Gb
单模块容量突破80GB
功耗方面
由于采用了更先进的制程和电源管理技术
HBM5的功耗控制在100W左右
仅比HBM4增加了33%，
但是性能提升了100%。
封装技术上
HBM5开始采用无凸点铜-铜直接键合技术
取代了传统的微凸点连接
这种技术将两个芯片的铜层直接键合
互连间距缩小到了10-15μm
不仅提升了互连密度和信号传输效率
还降低了接触电阻和热阻
为后续更高堆叠层数的HBM奠定了基础
冷却方案方面
HBM5升级为了浸没式冷却（Immersion Cooling）
将整个GPU-HBM模块浸入绝缘冷却液中
散热效率相比D2C液冷再提升50%，
能够轻松应对100W的功耗需求
如果说架构和封装是HBM5的硬件骨架
那么AI设计工具就是它的智能大脑
KAIST的报告用了大量篇幅介绍AI在HBM5设计中的应用
从TSV布局、去耦电容放置
到电源完整性优化
AI已经渗透到芯片设计的每一个环节
彻底改变了传统的设计流程
传统的HBM设计流程
高度依赖工程师的经验和反复仿真
一个复杂的PDN设计
可能需要几周的时间
而且很难达到全局最优
而HBM5引入了多种基于强化学习和Transformer的设计Agent
将设计周期缩短了99%以上
以TSV布局优化为例
KAIST提出了一种基于Transformer的强化学习算法
专门解决高堆叠HBM的IR压降问题
这个算法将TSV布局问题
建模为马尔可夫决策过程（MDP）
以TSV的直径、间距、位置作为动作空间
以IR压降的均匀性作为奖励函数
通过自注意力机制
捕捉TSV布局与IR压降之间的空间依赖关系
测试结果显示
通过这个算法设计的TSV布局
能够将HBM5的最大IR压降
从123.75mV降低至11.46mV
降幅达到90.7%，
而设计时间，可以从传统方法的3小时
缩短到1分钟以内
另一个典型案例
是基于Mamba强化学习的去耦电容放置优化
金炳默（Byeongmok Kim）团队提出的Mamba-RL算法
针对HBM5的PDN阻抗抑制问题
通过Mamba架构的选择性状态空间模型（SSM）
高效探索了去耦电容的最优放置位置
与传统的遗传算法和随机搜索相比
Mamba-RL不仅将PDN的自阻抗和转移阻抗
降低了40%以上
还将设计时间从30秒缩短到了0.1秒
而且泛化能力更强
能够适应不同的I/O通道接口和电源域配置
除了强化学习
生成式AI也在HBM设计中发挥了重要的作用
安贤俊（Hyunjun An）团队提出的PDNFormer模型
基于3D-CNN和多头注意力机制
能够快速估计多电源域、多层PDN的阻抗特性
这个模型在Intel i3-14100 CPU上的推理时间不到1毫秒
而传统的3D EM仿真工具
比如Ansys HFSS，则需要10000秒
效率提升了10000倍
而且平均绝对误差（MAE）仅为3.44dBΩ
精度完全满足设计要求
随着大模型推理对内存带宽和容量需求的持续增长
HBM6的核心目标是通过多塔架构（Multi-Tower Architecture）和混合中介层（Hybrid Interposer）技术
实现内存容量和带宽的二次飞跃
同时解决大规模封装带来的物理限制
HBM6的标志性架构是四塔HBM（Quad-Tower HBM
QT-HBM）
这种架构将四个DRAM堆叠体
以2×2的布局集成在一个基片上
每个塔都具备独立的TSV阵列和I/O通道
再通过硅中介层与GPU连接
每个塔的单模块带宽为1.536TB/s
四个塔的总带宽达到6TB/s
而一个GPU可以同时连接4个QT-HBM模块
总带宽突破24TB/s，是HBM5的6倍
容量方面
每个QT-HBM模块的容量为64GB
四个模块总容量达到256GB
配合L3缓存的嵌入式设计
整个系统的内存容量可以轻松突破1TB
足以支撑万亿参数模型的实时推理
为了解决大规模封装带来的中介层尺寸限制
HBM6引入了硅-玻璃混合中介层（Silicon-Glass Hybrid Interposer）技术
传统的硅中介层
由于晶圆尺寸和翘曲问题
面积难以超过3000mm²
限制了HBM模块的数量
而玻璃中介层具有低损耗、低翘曲、大尺寸面板制程等优势
能够制造出更大面积的中介层
但是玻璃的布线精度较低
难以实现细间距互连
KAIST提出了一个混合中介层方案
将硅中介层嵌入玻璃中介层的腔体中
硅中介层负责GPU与HBM之间的细间距互连
玻璃中介层负责模块之间的长距离互连
这种设计既发挥了硅中介层的高带宽优势
又利用了玻璃中介层的高扩展性
让GPU-HBM模块的数量从传统的8个提升到16个
总带宽突破128TB/s
HBM6的另一大创新
是L3缓存的嵌入式架构L3E
传统的GPU缓存层级为L1，L2，HBM
而HBM6将L3缓存直接集成在中介层上
通过混合键合技术与GPU和HBM连接
L3缓存的容量可以达到96MB
带宽高达16TB/s
是传统L2缓存的8倍
这种架构让KV缓存可以直接存储在L3缓存中
大幅减少了对HBM的访问次数
测试数据显示
L3E架构能够将HBM的访问量降低73%，
LLaMA3-400B模型的推理吞吐量
从1057 tokens/秒提升到8313 tokens/秒
提升了686%，同时能耗降低了40.4%。
在关键参数上
HBM6的I/O数量保持了4096个
但是数据速率翻倍到了16Gbps
单模块带宽达到8.0TB/s
堆叠层数提升到了16-20层
单芯片容量达到48Gb
单模块容量为96-120GB
功耗方面
由于采用了更先进的铜-铜键合和电源管理技术
HBM6的功耗控制在120W
仅比HBM5增加了20%，
但是带宽和容量实现了翻倍
信号完整性方面
HBM6引入了混合均衡器和生成式AI眼图估计工具
李正贤（Junghyun Lee）团队提出的生成对抗网络（GAN）模型
能够快速估计电源噪声对眼图的影响
推理时间仅为24.7秒
相比传统全瞬态仿真的216.8秒
效率提升了88.6%，
而且呢眼宽眼高等关键指标的平均
绝对百分比误差都小于1.5%
为信号完整性优化提供了高效的工具
进入到2035年
AI模型将进入到多模态加自主agent的时代
对内存的需求不仅体现在带宽和容量上
还要求内存具备异构存储、高能效比、高可靠性等特性
HBM7的核心突破
就在于混合内存架构和嵌入式冷却技术
将HBM与高带宽闪存HBF、3D堆叠LPDDR集成
同时解决了超高功耗带来的散热难题
HBM7的混合内存架构主要包含两个部分
分别是HBM-HBF集成和HBM-LPDDR集成
HBF是一种基于3D NAND的高速存储介质
它采用16层堆叠设计
单芯片容量达到64GB
单模块容量为2TB
虽然数据速率与HBM相当
但是成本仅为HBM的1/5
而且具备非易失性
HBM7将HBM与HBF
通过H2F链路直接连接
HBM负责存储中间计算结果和高频访问数据
HBF负责存储模型权重和低频访问数据
形成高速缓存+大容量存储的异构内存层级
这种架构让整个系统的内存容量突破17.6TB
而成本仅为全HBM方案的30%，
同时H2F链路的带宽达到12TB/s
确保了数据在HBM和HBF之间的高速传输
另一部分是HBM与3D堆叠LPDDR的集成
崔仁英（Inyoung Choi）团队提出的架构
将3D堆叠LPDDR模块与HBM模块
共同封装在玻璃中介层上
通过内存管理逻辑MML
实现统一内存访问
每个3D-LPDDR模块由16片DRAM芯片堆叠而成
容量为128GB
带宽为1024GB/s
8个模块的总容量达到1024GB
总带宽达到8192GB/s
MML能够智能分配数据的存储位置
将低功耗、低带宽需求的数据分配到LPDDR中
高带宽需求的数据分配到HBM中
在不牺牲性能的前提下
将系统能耗降低30%。
HBM7的另一大技术突破
是嵌入式冷却结构ECS（Embedded Cooling Structure）
随着HBM7的功耗提升到了160W
传统的浸没式冷却已经难以满足散热的需求
孙基英（Keeyoung Son）团队提出的ECS-TTL方案
在HBM的TSV中集成了流体TSV和热传输线TTL
冷却液通过流体TSV
在HBM、中介层和GPU之间循环流动
TTL则将HBM芯片内部的热量
快速传导到冷却液中
这种设计让散热效率提升了80%，
HBM的工作温度控制在59.7℃以下
相比HBM6降低了15℃
同时温度分布的标准差降低了18.5%，
有效缓解了热耦合带来的性能波动
关键参数方面
HBM7的I/O数量翻倍到了8192个
数据速率提升到了24Gbps
单模块带宽达到24TB/s
堆叠层数提升至20-24层
单芯片容量达到64Gb
单模块容量为160-192GB
功耗为160W
通过混合内存架构和嵌入式冷却
能效比达到150GB/s/W
相比HBM6提升了25%。
作为路线图的终极形态
HBM8的核心目标是实现内存中心计算HCC（HBM Centric Computing
），通过全3D集成技术
将GPU、HBM、HBF、LPDDR等组件
完全融合在一个封装内
形成计算-存储-网络的一体化架构
彻底消除数据移动的壁垒
HBM8的标志性技术是全3D集成（Full-3D Integration）
与之前的2.5D封装不同
HBM8采用垂直堆叠的全3D封装
GPU芯片位于内存堆叠的顶层
HBM、HBF、LPDDR芯片在垂直方向分层堆叠
通过同轴TSV（Coaxial TSV）实现垂直互连
这种设计会让芯片的占地面积减少70%，
互连长度缩短到微米级别
数据传输延迟降低了50%以上
更重要的是，全3D集成让不同组件
可以共享电源分配网络和冷却系统
系统的整体能效比提升了40%。
为了支撑全3D集成
HBM8采用了双面中介层（Double-Sided Interposer）技术
在PCB的上下两面都部署玻璃-硅混合中介层
上面的中介层连接GPU和HBM
下面的中介层连接HBF和LPDDR
通过垂直TSV实现两面中介层的互连
这种设计不仅解决了全3D集成的布线拥堵问题
还让内存扩展更加灵活
用户可以根据需求
在下面的中介层上增加HBF或LPDDR模块
实现容量的按需扩展
测试数据显示
HBM8的总内存容量可以达到200-240GB/模块
配合双面中介层
一个GPU可以支持32个HBM8模块
总容量突破6TB，总带宽达到1024TB/s
足以支撑10万亿参数模型的实时推理
内存中心计算的另一大突破是全内存网络（Full Memory Network）
HBM8通过交叉开关网络（Crossbar Network Switch）
将所有的内存模块
包括HBM、HBF、LPDDR
连接成一个统一的内存池
GPU、CPU、NPU等计算单元可以通过CXL、NVLink等接口
直接访问内存池中的任何数据
无需经过中间节点转发
这种架构让数据的访问延迟降低了60%，
同时支持内存的动态调度和负载均衡
计算资源的利用率提升了30%。
在热管理方面
HBM8升级了嵌入式双面冷却（Embedded Double-Side Cooling）技术
在双面中介层中都集成了micropin-fin和冷却通道
冷却液从顶部的GPU流入
经过中间的内存模块
从底部的中介层流出，形成闭环冷却
这种设计的散热效率达到每平方厘米200W
能够轻松应对HBM8的180W功耗需求
芯片的最高温度控制在65℃以下
温度分布均匀性提升了25%。
关键参数方面
HBM8的I/O数量翻倍到了16384个
数据速率提升到了32Gbps
单模块带宽达到64.0TB/s
堆叠层数保持20-24层
单芯片容量达到80Gb
单模块容量为200-240GB
功耗为180W，能效比达到355GB/s/W
相比HBM7提升了137%，
成为AI超级计算机的核心内存解决方案
回顾KAIST的这份HBM路线图
从HBM4到HBM8
我们看到的不只是硬件参数上的迭代
更是内存架构的一场根本性变革
内存技术的突破
需要与AI算法、封装技术、热管理技术的深度融合
当然
路线图中的技术方案还面临着许多的挑战
但是不可否认的是
这份路线图为我们描绘了一个清晰的技术方向
也让我们明白
AI的进步不仅要依赖于GPU的强大计算能力
更离不开内存这样的底层技术的支撑
正是这些共同的技术突破
才构筑起了AI时代的基石
感谢收看本期视频，我们下期再见
