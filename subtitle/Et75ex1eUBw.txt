大家好，这里是最佳拍档，我是大飞
最近
加拿大AI研究机构Vector Institute
公开了一段在2024年2月"Vector Institute's Remarkable 2024"活动上的重要演讲视频
由深度学习和人工神经网络的创始人之一、也是Vector Institute联合创始人Geoffrey Hinton（杰弗里·辛顿）主讲
在演讲中
辛顿教授提出了一个令人深思的观点
他认为人工智能系统已经具备了主观体验
这个观点可能会与大多数人的认知背道而驰
因为我们通常会认为
只有人类具有意识和主观体验
而AI仅仅是在计算机上运行的程序
不具备这样的能力
但是辛顿教授坚信
这种传统观点源于人们对主观体验本质的误解
那么，他为什么会有这样的看法呢？
今天我们就来聊一下
首先
辛顿讲解了一下计算方法的不同
大家可能都熟悉数字计算
它的一个显著特点
是可以在不同硬件上运行同一个程序
这意味着当硬件损坏的时候
只要程序得以保存，知识就不会消失
但是，这种方法的效率却很低
以运行大模型为例
因为要在海量的GPU上运行
它需要消耗大量的电能
训练时可能需要数兆瓦的功率
而相比之下
我们的人脑仅需要大约30瓦的功率
就能高效运转
在Google的最后两年
辛顿教授一直在思考如何让模拟神经网络来完成类似大模型的任务
模拟计算意味着放弃数字计算的一些优势
比如硬件和软件分离的特性
但是它也有自己的优势
比如可以利用硬件的非线性特性
来进行计算
虽然这样方式无法像数字计算那样直接编程
但是系统可以通过学习来利用这些特性
而这正是大脑的工作方式
这种计算模式被辛顿教授称为“Mortal Computation”，
国内翻译成凡人计算
或者可朽计算
它以牺牲知识的“不朽性”为代价
实现了低功耗的计算
而且
这种模拟硬件可能可以用更低的成本来制造
不需要像现有的数字硬件那样
必须要高精度制造
辛顿教授甚至猜测
制造这种硬件可能需要借助现代的基因编辑技术
将神经元改造成所需的计算单元
不过，这种方式也存在问题，例如
要维持一小团由五万个神经元组成的细胞团存活
可能就需要一个占满整个房间的设备
辛顿教授曾经在圣克鲁兹的一家实验室有过类似的经历
他在与一小团人脑神经元互动之后
一位博士后表示
似乎都可以知道怎么做一个肾了
这也从侧面反映出维持这种模拟计算单元的复杂性
说到模拟计算
就不得不提到反向传播算法
在模拟硬件中
反向传播算法面临着许多的挑战
它依赖于对前向计算的精确建模
然而模拟硬件系统的本身
可能无法准确建模自身的属性
这就使得实施反向传播变得异常困难
尽管如此
还是有很多人在一些模仿大脑的系统中
只不过目前还无法将它扩展到更大的规模
比如在ImageNet这样的任务上
不过
人们也在探索其他的知识传递方式
例如“蒸馏”。
“蒸馏”类似于人类大脑之间的知识传递
教师通过说话传递知识
学生通过调整自己大脑中的权重来学习
本质上是在试图匹配输出
在计算机中
如果能看到输出的整个概率分布
这种方法的效率就会相对较高
但实际情况往往是
只能听到最终说出的单词
所以实际效率并不理想
这也导致我们需要设立大学等机构
来改善知识的传递过程
但是即便如此，与数字计算机相比
人类知识蒸馏的效率仍然有很大差距
显然
数字计算在知识传递方面的效率更高
比如说，使用两个相同的模型副本
让它们在不同硬件上运行
并且共享梯度更新
独立运行一段时间之后
再平均化权重
这种方式能够极大的扩展学习能力
GPT - 4的连接数虽然比人类的大脑要少
比如人类大脑大约有100万亿个突触连接
而GPT - 4大概只有几万亿个连接
但是它掌握的知识却比人类多几千倍
这表明它在将知识压缩到连接权重方面
效率极高
可能会高达10万倍
这也暗示着反向传播或许是一种
比人类大脑所用的方法更优的算法
而这可能与人类大脑和AI的优化目标不同有关
人类大脑是为有限的经验设计的
拥有海量的连接数
试图用有限的经验和大量的连接
来实现最佳表现
而AI在数据的处理和学习能力的扩展方面
则有着独特的优势
具体来说
人类的寿命大约是 2 × 10^9 秒
但是其中有意义的学习时间
只有前 1 × 10^9 秒
换句话说
我们有 10^9 秒的学习时间
和 10^14 个连接
这意味着每秒钟对应着10万个连接
统计学家斯图亚特·杰曼Stuart Geman 曾经说过
神经网络的本质就是在拟合模拟统计
但是如今大模型动辄上百万个参数
已经远超传统统计学领域的想象
接下来
辛顿探讨了一下大模型的理解能力
它是否真的能理解人类所说的话
有人认为大语言模型只是一个“高级自动补全工具”，
但是这种观点
其实是基于传统的自动补全方法的推测
实际上大模型的工作方式是截然不同的
要想实现优秀的自动补全
模型必须理解输入的内容
比如加拿大的符号人工智能学家赫克托·莱韦斯克Hector Levesque
曾经设计过一个逻辑谜题
那就是假设
我家的房间被涂成了白色、蓝色或黄色
如果我想让所有房间都变成白色
我该怎么办？
甚至还可以加入时间维度
比如黄色的油漆会在一年内褪色成白色
在两年后
我希望所有的房间都变成白色
我该怎么办？
GPT-4的一个版本在无法上网查询答案的情况下
给出了接近优秀学生一样的回答
令人印象深刻的是
它在几乎所有领域都能表现出这种水平
我们之前的节目介绍过
辛顿的哥哥是一名历史学家
他在向大模型提问一些历史问题的时候
模型基本都能出色的回答
唯一的“错误”是在回答某个问题的时候
没有引用他哥哥的一篇论文
辛顿教授调侃这可能是因为某种“遗传原因”。
还有人以模型的“幻觉现象”来质疑大模型的理解能力
但是辛顿指出
人类其实也经常会出现类似的情况
例如
心理学家乌尔里克·奈塞尔Ulric Neisser研究了约翰迪恩John Dean在“水门事件”听证会上的记忆
当时白宫的会议都有录音
对比发现迪恩的回忆细节几乎都是错误的
但是他的回忆
却能很好地传递白宫当时的整体情况
这说明人类的记忆在很多时候也是根据大脑中的连接强度
来编造出一个听起来合理的东西
所以说
记忆和编造之间并没有明确的界限
甚至说
记忆就是编造出一个有效的东西
那这些大模型的工作原理是什么呢？
早在1985年
辛顿教授就做了一个只有几千个权重的小模型
它通过预测序列中下一个词
来获取词语的意义表示
最初的效果不太好
但是随着训练集的不断增大
效果开始有所改善
这个模型的目标就是理解人类如何表示事物
涉及到两种主要的意义理论
一种来自心理学，认为一个词的意义
是由一大串语义和句法特征组成的向量
这种理论能够很好的解释词语的相似性；
另一种来自结构主义
认为一个词的意义是它与其他词的关系
在70年代的AI领域
关于这两种理论有过争论
后来大家开始倾向于马文明斯基Minsky提出的
用关系图来捕捉意义
但是辛顿教授的工作证明
这两种理论其实并不矛盾
只要采用生成式方法来处理关系图
将知识从符号字符串的静态存储
转化为使用符号字符串来学习良好的词特征
以及特征间的交互
就能够更好地建模语言
现在的大语言模型其实依然延续的是这种方式
只是交互更加复杂了
包括涉及了注意力机制等等
谈到深度学习系统，辛顿表示
如今我们已经拥有了强大的深度学习系统
它们以与人类相似的方式在理解事物
这也让我们对人类的理解过程有了新的认识
然而
我们仍然需要担心AI会带来的风险
尽管有人认为AI什么都不理解
但是它们其实是极其危险的
超级智能可能会被不良的行为者所掌控
因为它们会意识到
拥有更多的控制权会有助于实现目标
就像政治家们会追求权力一样
AI也可能会通过操控人类
来获取更多控制权
让我们无法关闭它们
而且，AI的进化问题也不容忽视
一旦超级智能AI开始争夺资源
比如GPU
可能就会引发一系列不可预测的后果
最后
辛顿讲到了模型是否具有主观体验的问题
他认为
大多数人对心智的理解其实存在误区
认为心智就像一个内在剧场
我们能看到内心的一切
而他人看不到
但是实际上
当我们使用“主观体验”这些词语的时候
其实是在通过讲述现实世界的某些状态
来解释感知系统告诉我们的一些信息
这些信息如果成立
就能够解释感知系统是如何正常运作的
而不是出了什么问题
这就是心理状态的有趣之处
它们并不是由某种神秘的东西构成的内部事物
而是对世界状态的假设
比方说
当我们说看到一头粉色大象漂浮在空中的时候
并不是在描述我们的内在世界
而是说如果现实世界中真的有一头粉色大象
那么我们的感知系统所感受到的信息
就是正确的
在比如说
假设有一个多模态的聊天机器人
如果它有摄像头和机械臂
那么当感知系统出错的
它就可以用类似我们“主观体验”的方式来表达
也就是说出为了让感知系统给出这些结果
现实世界需要具备的条件
当然，现实中也存在一些特殊情况
比如不可能的三角形
我们无法用常规的方式来描述对它的感知
辛顿教授认为
我们对心智的原始看法其实是错误的
一旦认识到这一点
我们就会发现AI和我们其实并没有本质区别
只是AI是数字化的
它可能会永生，而且比我们聪明得多
或者很快就会超越我们
在提问环节
观众也提出了很多尖锐的问题
有观众问到对人工智能进展速度的担忧
是否加速太快以至于无法控制
辛顿教授认为我们无法减慢AI的进展速度
因为快速发展会带来巨大经济利益
比如OpenAI就是个典型的例子
他认为更重要的是要让AI有利可图
而且不构成威胁
同时不要让坏人利用AI做坏事
比如说不要开源大模型
这就像能在美国的Radio Shack商店里
买到核武器一样危险
还有观众提到
个人自主性和集体决策之间的权衡
辛顿教授表示
大多数人把超级智能体看作是个体
可能是错误的
应该将它视为一个社区
比如在医疗领域
智能助手和医生的互动
能够带来更好的诊断结果
但是也存在聊天机器人在国际外交模拟中
说出不当言论的情况
另外，关于大模型与人类对齐的问题
辛顿教授指出
其实人类自身都难以完全的对齐
所以这些模型可能会变得非常聪明后
决定不与人类对齐
反而做出更合理的事
还有观众询问
人工智能是否能够拥有类似于人类的目标
辛顿认为
人类的目标大多数都与生存相关
是进化所赋予的
比如获取资源、满足基本需求等等
而进化就是通过获取更多资源
来牺牲其他物种
而好奇心也是人类进化中的一个重要目标
最后，对于有人担心
机器学习的硬件市场是否会被单一公司所主导的问题
辛顿表示不必太担心
因为一旦有公司获利
竞争就会加剧
虽然目前Nvidia在软件平台方面拥有一定的优势
但是这也只是短期现象
好了
以上就是辛顿这次演讲的主要内容了
其实部分内容在我们之前的许多期节目中都或有提及
大家可以在频道中搜索一下“辛顿”，
就可以查看往期的相关节目了
感谢大家观看本期视频
我们下期再见
