大家好，这里是最佳拍档，我是大飞
我们频道已经做了非常多期关于AI的视频
包括对大语言模型工作原理的介绍
不过很多内容还是有一定学习的门槛
所以大飞一直希望能够用更通俗简单的方式
来讲一讲大模型和Transformer架构相关的知识
为了让更多的人能够接触和理解
今天我将尝试去掉机器学习中所有花哨的语言和术语
将一切简单的表示为数，这样的话
即便你只是个初中生
只知道如何对两个数字进行加法和乘法
应该也能够听得懂
我们将从如何构建一个简单的生成式网络出发
一步一步来探索模型的生成和训练
以及嵌入、分词器、自注意力、残差连接、层归一化到多头注意力等整个Transformer架构
内容比较长，希望大家可以耐心看完
咱们先从一个简单的神经网络说起
想象一下
神经网络就像一个超级智能的 “大脑”，
但它有自己独特的 “语言”，
那就是只能接受数字作为输入
也只能输出数字
这就好比我们要和一个只懂数字密码的 “朋友” 交流
我们得把我们想让它处理的信息都转化成数字
比如说，我们要对物体进行分类
像区分叶子和花
我们可以用物体的RGB颜色和体积这些数据来代表它们
这就好像给叶子和花都贴上了数字标签
这样这个 “智能大脑” 就能读懂它们了
现在我们来构建一个用来分类的神经网络
这个网络就像一座有很多楼层的大厦
有输入层、中间层和输出层
输入层就像是大厦的入口
有 4 个神经元
对应着我们的颜色和体积数据；
中间层就像大厦的中间楼层
有 3 个神经元；
输出层就像大厦的出口
有 2 个神经元
对应叶子和花这两个分类结果
那这个网络是怎么工作的呢？
这就好比是一个复杂的物流系统
我们要计算的是它的预测结果
也就是 “前向传播”。
首先，我们从最左边的输入层开始
把每个神经元的数字和下一层连接的权重相乘
就好像把货物从一个地方运到另一个地方
然后把这些乘积加起来
得到下一层神经元的值
比如说，我们有一片叶子的数据
经过计算
最后输出层可能会给出一个结果
显示这片叶子被分类为叶子
像蓝色圆圈的计算过程就是
（32 * 0.10）+（107 * -0.29）+（56 * -0.07）+（11.2 * 0.46）= -26.6
这里面有几个重要的术语
圆圈里的数字叫做神经元或者节点
就像物流系统里的一个个小站点；
线条上的彩色数字叫权重
就像连接各个站点的道路宽窄程度
它决定了能够运输的货物多少；
而一组神经元就是一层
就像大厦里的不同楼层
但是
这个网络还有一些小细节需要我们注意
比如说激活层
它就像一个神奇的 “魔法师”，
可以对每个圆圈里的数字做非线性变换
就像给货物施了魔法
让它们能适应更复杂的运输环境
这样网络就能处理更复杂的情况
拿RELU激活函数来说
它会把负数设置为零
而正数保持不变
这就好比在物流系统中
给不符合某些规则的货物做了特殊处理
只有符合规则的才能继续运输
如果没有激活层
网络中的所有加法和乘法
都可以压缩成一个等价的单层网络
这就像把一座多层的大厦简化成了一层
会丢失很多的信息
还有偏置
它是一个和每个节点相关的数字
会加到计算节点值的乘积上
就像给每个小站点都加上了一个特殊的标记
这个标记会影响货物的运输方向和数量
比如说
如果顶层蓝色节点的偏置为 0.25
那么节点中的值就会根据原始的计算值
-26.6
再加上这个偏置值，得出结果-26.35
另外
我们通常不会直接按照模型输出的数字来解释结果
而是会用 Softmax 函数把这些数字变成概率
就像给每个货物都贴上了一个概率标签
这样我们就能更清楚地知道每个分类的可能性了
比如在一个分类任务中
如果有三个类别
Softmax 函数会将输出层的三个数字
转换为三个概率值
这三个概率值之和为 1
这个我们可以根据这些概率值
来判断某个输入属于哪个类别
那这个网络的权重是怎么来的呢？
这就涉及到模型的训练
我们需要一些训练数据
就像我们要教一个小孩子认识叶子和花
我们得给他一些已经知道分类结果的叶子和花的数据
或者样本
开始的时候
我们会把权重都设为随机数
就像小孩子一开始什么都不知道
只能乱猜
然后，我们会给网络一个叶子的数据
希望输出层叶子对应的数值更大
就像我们希望小孩子能够正确地说出这是叶子
而我们计算的实际输出
和我们希望的输出之间
是存在差值的
这个差值的和就是损失
比如实际输出的两个神经元为0.6和0.4
而我们希望的输出分别为0.8和0.2
那么我们将得到
(0.8–0.6)=0.2和(0.2–0.4)= -0.2
忽略负号再相加之后
总共为0.4，所以损失就是0.4
当然，我们希望损失越小越好
所以要调整权重，让损失值降低
这就像我们要纠正小孩子的错误
让他越来越聪明一样
而这个过程就叫 “梯度下降”。
在训练过程中
我们可能还会遇到一些问题
比如说，我们在一个样本中
通过调整权重让损失变小了
但在另一个样本中损失可能变大了
所以我们要把所有样本的平均损失
作为最终目标来调整权重
每个这样的调整周期叫做一个 “epoch”，
通过不断重复这些周期
就能逐渐找到减少平均损失的权重
而且，训练深度网络是很困难的
因为梯度可能会失控
变成零或者无穷大
这就是 “梯度消失” 和 “梯度爆炸” 的问题
就像小孩子在学习过程中可能会遇到困难
有时候会学不进去
或者学得太猛也会出现问题
好啦
那这个简单的神经网络和语言有什么关系呢？
我们可以把它想象成一个可以预测字符的网络
因为英语有26个字母
所以我们可以把输出层的神经元数量
扩展到26个
每个神经元对应一个字母
我们给网络输入一些字符
比如 “humpty dumpt”，
然后让它来预测下一个字符
但是，网络只能接受数字输入
怎么办呢？
我们可以给每个字符分配一个数字代号
比如 a = 1
b = 2 等等
这样我们就可以输入字符的数字代号
让网络来预测下一个字符的数字代号
然后再把它对应回字母
这就好像我们把字母都变成了数字密码
让网络来破解下一个密码是什么一样
现在我们可以输入“humpty dumpt”，
让网络来预测下一个字符
并且逐步来构建一个完整的句子
假设网络给我们输出了“y”，
那么我们可以将“y”附加到现有的字符列表中
再喂给网络
让它继续请求预测下一个字符
如果网络训练的得当
它应该给出一个空格，以此类推
有聪明的观众可能会注意到
由于这个网络的输入层只有12个神经元
每个神经元对应着“humpty dumpt”中的一个字符
包括空格
所以我们无法将“Humpty Dumpty”直接输入网络
那么我们如何在下一次传递中输入“y”呢？
解决方案其实很简单
那就是去掉最开始的“h”，
然后发送12个最近的字符
因此，我们将发送“umpty dumpty”，
然后网络将预测出一个空格
接着我们将输入“mpty dumpty”，
它会产生一个“s”，
依此类推
在最后一行中
我们仅仅将“sat on the wal”喂给了模型
这样就丢失了很多信息
那么
如今最先进、最强大的网络是如何处理的呢？
其实也差不多是这样
因为我们可以输入到网络的长度是固定的
也就是由输入层的大小决定
所以这称为“上下文长度”，
也就是为网络提供的、用来进行预测的上下文
而如今的网络可以具有非常大的上下文长度
比如几千个单词
甚至还有一些方法可以输入无限长度的序列
但是总体来说
还是具有固定的、长上下文长度的模型性能更好
虽然我们现在已经拥有了一个能够生成语言的网络
但是这样做还有一些问题
比如说
我们输入的字符数量可能有限
而且我们对输入和输出的解释方式也不一样
那有没有更好的方法呢？
这就引出了一些新的概念
首先是嵌入
大家可能会注意到
我们之前给字符分配的数字是任意的
这样可能不太合理
我们可以通过训练
找到更合理的数字来表示字符
这就是嵌入
嵌入通常是用一个向量来表示一个字符
而向量里有多个数字
而且每个数字的位置也是特定的
就像我们给每个字母都找到了一个更合适的数字 “家”，
这个 “家” 里有好几个房间
每个房间都有一个数字
这样
我们就可以把 “humpty dumpt” 每个字符的向量
依次排列作为输入层的神经元
那这些向量是怎么找到的呢？
和训练权重的方法类似
我们通过调整这些向量
让损失最小化
就像我们要找到最适合每个字母的 “家”，
让它们在这个 “家” 里
能更好地帮助网络预测
我们以叶子和花朵的分类为例
如果只用一个数字来表示一个物体
可能就像只看颜色的红色通道一样
无法全面地了解物体的所有特征
而用多个数字组成的向量来表示字符
就像用多个维度来描述一个物体
能够更加准确地传达信息
我们可以通过不断地调整向量中的数字
观察网络对输入数据的分类效果
就像不断地调整一个工具的参数
直到它能更好地完成任务
当我们把每个字符都用合适的向量表示后
就可以把这些向量依次排列
作为输入层的神经元
这样网络就能更好地处理语言信息了
这里还需要注意的有两点
一个是我们要始终使用相同的嵌入
来表示一个特定的符号、字符或者单词
另一个是所有的嵌入向量必须具有相同的长度
否则我们就无法将所有字符组合稳定地输入网络
由于同一长度的向量集合可以称为矩阵
所以由嵌入组成的矩阵
我们就可以称为“嵌入矩阵（embedding matrix）”，
通过提供对应字母的列号
查看矩阵中的对应列
我们就能够获得用来表示这个字母的向量了
然后是子词分词器
我们之前是把字符作为语言的基本单位
但这样神经网络要处理的关系就很复杂
于是我们可以把单词分解成子词
比如把 “cats” 分成 “cat” 和 “s”，
这样模型就能更容易理解单词之间的关系
也能减少嵌入并输入到模型中的单一单位
而这个单位就被称为词元token
实际上
分词器的作用就是把输入的文本分解成词元
并且找到对应的嵌入向量
这就像我们把一个复杂的单词 “拆开”，
看看里面的小零件
然后给每个小零件都找到它在网络里的位置
例如，对于一些复杂的单词
比如 “unforgettable”，
我们可以将它分解为 “un”、“forget” 和 “table” 等子词
这样，模型在处理这个单词的时候
就不是把它作为一个整体
而是从子词的角度去理解它的结构和含义
通过这种方式
神经网络不需要将每个可能的单词作为一个独立的单元来处理
从而降低了词汇量的复杂性
也使得模型更加容易学习到单词之间的相似性和关联性
接着是自注意力机制
我们会发现，在预测一个单词的时候
往往需要考虑前面所有的单词
而且不同的单词对预测的影响还不一样
自注意力机制就是对所有单词的嵌入向量进行加权求和
但是权重不是固定的
而是根据要预测的单词和前面的单词来确定的
通过构建一些小型的单层神经网络
我们可以找到这些权重
这这就像我们在一个团队里
每个人对完成一个任务的贡献不一样
我们要根据任务的需要和每个人的能力
来给他们分配不同的权重
而且，还有多头注意力机制
这个就是并行多个注意力模块
然后把它们的输出连接起来
这就像我们有许多个团队
每个团队都在做同样的任务
然后我们把他们的成果都整合在一起
比如说，在一个句子 “我喜欢吃苹果
因为它很健康” 中
当我们预测 “健康” 这个单词的时候
自注意力机制会考虑到前面的 “苹果”、“喜欢”、“吃” 等单词的嵌入向量
并且根据它们对预测 “健康” 这个单词的重要性来分配权重
可能 “苹果” 这个单词对预测 “健康” 的权重比较大
因为它直接与 “健康” 相关
而多头注意力机制就像是有多个视角来看待这个句子
每个视角都有自己的注意力模块
然后把这些不同视角的结果整合在一起
得到一个更全面的对于句子的理解
还有Softmax函数
我们前面提到过
它可以把输出层的数字变成概率
让我们更好地理解结果
就像我们把一个模糊的可能性
变成了一个明确的概率值
让我们能够更清楚地知道每个选项的可能性大小
举个例子
我们在预测“dumpty”这个单词的时候
如果模型在预测“m”时犯了错误
它并没有将“m”作为最高值
而是将“u”视为最高值
“m”紧随其后
那么这个时候我们既可以使用“duu”来尝试预测下一个字符
但是后续内容的置信度可能很低
也可以试试用m来预测
所以这时候我们应该给每个选项赋予一个概率
如果u是50%，
而m是25%，那没什么问题
但是如果u和m的概率很接近
那么以接近50%对50%的概率去探索这两个选项
或许是个好主意
这就是softmax函数的意义
另外，还有残差连接
它就是把自注意力块的输出和原始输入相加
再传递给下一个块
这样可以帮助训练深层网络
这就像是我们在学习过程中
不仅要学习新的知识
还要回顾之前学过的知识
把它们结合起来，才能更好地进步
层归一化也是一个很重要的概念
它会对进入层的数据进行归一化处理
让数据更稳定，有助于训练深层网络
这就像我们在整理一堆杂乱无章的东西
把它们按照一定的规则整理好
这样我们就能更方便地使用它们
例如，在一个神经网络中
如果某一层的数据波动很大
通过层归一化可以将其稳定在一个合理的范围内
使得网络能够更好地训练
具体来说
层归一化会计算输入层中的所有神经元的均值和标准差
假设均值为M，标准差为S
那么层归一化的过程
就是将每个神经元的值替换为(x-M)/S
其中x表示某个神经元的原始值
不过
层归一化也会去除掉一些对学习目标有用的信息
所以会引入Scale和Bias两个参数
通过乘以scale
然后加上Bias来解决这个问题
另外就是Dropout
这是一种避免模型过拟合的技术
也称为“正则化技术”，
它通过在训练期间插入一个dropout层
随机删除一些神经元连接
让网络在训练的时候更具有冗余性
这就像我们在一个团队里
有时候会让一些人休息一下
这样其他的人就能有更多的机会去锻炼自己
也能让整个团队更具活力
所以在一个模型的训练过程中
如果网络出现了过拟合现象
我们可以通过增加 Dropout 的比例
来减少过拟合的程度
最后
我们来看看 GPT 架构和 Transformer 架构
GPT 架构是很多 GPT 模型使用的架构
它由很多模块组成
比如位置嵌入、Transformer块等等
其中除了Transformer块以外
其他的概念我们刚才都已经提过
这里的+号只是表示两个向量相加
而Transformer块是这个样子的
这里就必须要提到Transformer 架构
Transformer 架构推动大模型发展的很重要的一个创新
它最早是为了解决像语言翻译这样的任务而创建的
分别由编码器和解码器组成
编码器接收输入的文本
给出一个中间表示
然后解码器根据这个中间表示和已经生成的单词
生成下一个单词
这些编码器和解码器其实都是由几个块组成的
特别是夹在其他层之间的注意力块
如果我们现在再来尝试理解一下论文“Attention is all you need”中的transformer的图例
就会发现
左侧的垂直块组称为“编码器”，
右侧的垂直块组称为“解码器”。
这里的每个框都是一个块
它以神经元的形式接收一些输入
并且输出一组神经元作为输出
然后可以由下一个块处理
或者由我们解释
其中箭头显示块的输出去向
如图所见
我们通常会获取一个块的输出
并将它输入到多个块中
要注意的是
这里的前馈网络是不包含循环的网络
它包含两个线性层
每个层后跟一个RELU和一个dropout层
另外也许你会注意到解码器具有多头注意力
箭头来自编码器
这其实指的是注意力中的值和键来自于编码器的输出
其他没有变化
而图中的Nx只是表示这个块被链式重复了N次
你可以想象成在背靠背的堆方块
并且将前一个块的输入传递到下一个块
需要注意的是，假设N=5
我们是否将每个编码器层的输出都送到相应的解码器层呢？
并不是的
基本上我们只会运行一次编码器
然后采用这个表示
并将相同的表示送到5个解码器层中的每一个
对于Add & Norm块
你只需要这么理解它就行了
其实无论是GPT架构还是Transfomer架构
都包含了我们前面提到的很多模块
它们的工作原理也是基于例如嵌入、自注意力机制、Softmax 等模块的组合和相互作用
就像我们盖一栋大楼
需要很多不同的材料和工具
这些模块就是我们盖楼的材料和工具
它们组合在一起
才能让大楼稳稳地立起来
例如，在GPT架构中
位置嵌入模块会给每个输入的文字或单词赋予一个位置信息
这有助于模型更好地理解文本的顺序
Transformer 块则包含了多头注意力机制、层归一化等模块
这些模块协同工作
使得 GPT 架构能够高效地生成语言
而在 Transformer 架构中
编码器和解码器中的各个模块相互配合
能够更好地处理输入文本和生成输出文本
比如，编码器中的自注意力机制
可以帮助理解输入文本的语义
而解码器中的自注意力机制
则可以根据已经生成的单词和编码器的输出
生成下一个单词
好了，今天的内容就到这里
大飞我尽量用通俗的语言和初中的数学知识
来讲解了大语言模型的基本工作原理
希望大家能够有更多的了解
感谢大家的观看，我们下期再见
