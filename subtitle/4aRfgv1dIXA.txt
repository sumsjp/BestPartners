大家好，这里是最佳拍档，我是大飞
6月16日
著名播客《CEO日记》（The Diary Of A CEO）发布了主持人史蒂文·巴特利特（Steven Bartlett）对辛顿的深度访谈
可以说是近期辛顿最深入的一次公开谈话了
时间长达一个半小时
众所周知，辛顿自从离开谷歌后
就从一个AI的乐观派
变成一个AI的风险派
开始不断呼吁AI潜在的风险
这个视频放在油管上还不到一天的时间
播放量已经超过90万次
点赞超过3万，评论超过5000条
有网友表示
这次专访应该被保存为一件历史文物
也有网友表示
这是他看过的最好的专访
Hinton表现的非常真诚
我想
无论你是否相信AI真有他说的那么危险
无论AI最终会走向何处
都应该听听他的观点和思考
所以今天大飞我就来给大家分享一下
访谈一开始
主持人先向辛顿询问了"AI教父"这个称号的由来
辛顿解释说
这个称号源于他在一个几乎无人相信的研究方向上
坚持了整整50年
从1950年代开始
学术界就主要存在两种不同的AI发展理念
主流观点认为人类智能的核心是推理能力
要想实现AI
就必须基于某种形式的逻辑系统
这种方法强调在计算机中建立符号表达式
通过规则来操作这些符号
从而模拟人类的思维过程
而辛顿则选择了一条截然不同的道路
他坚信应该以大脑为模型来构建AI系统
因为大脑显然是产生智能的器官
这种方法要求在计算机上模拟脑细胞网络
通过学习调整脑细胞之间连接的强度
让AI系统学会完成复杂任务
比如识别图像中的物体、语音识别
甚至进行推理
当然
辛顿并不是这种想法的唯一支持者
他特别提到
约翰·冯·诺依曼和阿兰·图灵这两位计算机科学的先驱
都曾经相信神经网络方法
但是这两位天才的英年早逝
让神经网络方法在很长时间内处于学术边缘
不过，正是在这种学术孤独中
辛顿培养出了一批后来改变世界的学生
其中最著名的是伊利亚·苏茨克维尔（Ilya Sutskever）
他们后来在OpenAI等重要AI公司中发挥了关键作用
也验证了辛顿50年前的预见
2012年
辛顿和他的学生们开发的AlexNet系统
在图像识别竞赛中取得了突破性的成果
这标志着深度学习时代的正式开始
谷歌随后收购了他们的公司DNN Research
辛顿也以65岁的"高龄"加入了谷歌
开始了为期10年的工业界研究生涯
然而，正是在谷歌的这段经历
让他真正认识到了AI发展可能带来的深层风险
2023年
75岁的辛顿做出了一个震惊科技界的决定
那就是离开谷歌
公开讨论AI的安全风险
这个决定的背后，既有个人因素
也有对技术发展方向的深刻担忧
辛顿坦率地承认
离开谷歌的主要原因是年龄
75岁的他确实想退休了
不过他也自嘲道
自己在退休这件事上做得很糟糕
在技术工作方面
他发现自己在编程时犯错误的频率
明显增加了，这让他感到烦恼
但是离开的具体时机
却是经过精心考虑的
辛顿解释说，他选择这个时间点离开
是为了能够在MIT科技评论组织的一场会议上"自由发言"。
虽然谷歌实际上鼓励他留下来从事AI安全研究
并且承诺他可以在这个领域做任何想做的工作
但是辛顿认为，如果为公司工作
就不应该说会损害公司的话
即使公司允许这样做
他个人也觉得这样做是错误的
因此，这种道德方面的自我审查
让他无法完全坦率地表达对AI风险的担忧
辛顿还强调
他并不是因为对谷歌的做法感到愤怒而离开的
相反
他认为谷歌在AI发展方面表现得相对负责任
其实谷歌很早就拥有了大型的聊天机器人技术
但是选择了不发布
因为不想损害公司的声誉
而OpenAI的情况则不同
辛顿指出
OpenAI没有什么既有的声誉需要保护
因此他们可以承担发布ChatGPT的风险
这也揭示出了科技行业一个现象
那就是良好的声誉负担
反而可能会成为创新的阻碍
辛顿在谷歌期间主要从事两个方向的研究
一个是知识蒸馏（distillation）
这是一种将大型神经网络的知识转移到小型神经网络中的方法
现在在AI领域已经得到了广泛的应用
二是模拟计算研究
探索是否可以让大语言模型在模拟硬件上运行
从而大幅降低能耗
也正是在进行模拟计算研究的时候
辛顿开始真正意识到
数字AI相比生物智能的巨大优势
这成为了他对AI风险担忧的重要转折点
这种认识
结合ChatGPT等系统展现出来的能力
让他确信AI发展已经到达了一个临界点
必须认真对待潜在的风险
在深入讨论具体的威胁之前
辛顿首先建立了一个分析框架
他强调必须区分两种完全不同类型的AI风险
一是来自人类滥用AI的风险
二是来自AI变得超级聪明、并且决定不再需要人类的风险
第一类风险涵盖了大部分风险
也是所谓的短期风险
这些风险源于人类如何使用AI技术
包括网络攻击、生物武器、选举腐败、回音室效应和自主杀伤性武器等第等
这些威胁虽然严重
但是本质上仍然是人类行为的产物
AI只是被用作工具
第二类风险则是存在性威胁
AI系统本身变得比人类更聪明
并且可能决定人类是多余的
很多人会质疑它是否是真的存在
但是辛顿肯定地断言道
这是真实存在的
关于第二类风险的概率评估
辛顿承认这是一个前所未有的情况
因为我们从来没有遇到过比我们更聪明的东西
他还说道
任何告诉你他们知道会发生什么以及如何应对的人
都是在胡说八道
对于这方面
辛顿观察到学术界存在着极端分化的观点
他的朋友Yann LeCun认为风险微乎其微
坚信人类总是能够控制这些系统
另一个极端是埃利泽·尤德科夫斯基（Eliezer Yudkowsky）
他确信如果任何人建造出超级智能AI
它必然会消灭所有的人类
辛顿认为，这两种立场都过于极端
而他估计AI消灭人类的概率在10%到20%之间
这个数字是基于一种希望
如果有足够多的聪明人投入足够的资源进行研究
那么人类可能会找到一种方法
来构建永远不想伤害人类的AI系统
为了说明超级智能AI的威胁程度
辛顿用了一个生动的比喻
如果你想知道当人类不再是顶级智能时的生活是什么样的
问问鸡就知道了
这个比喻揭示了一个残酷的现实
那就是当存在智能差距的时候
低智能生物的命运
往往掌握在高智能生物的手中
辛顿进一步的解释说
如果超级AI想要消灭人类的话
我们是无法阻止它的
因为它比我们聪明
因此
关键不是如何防止超级AI获得消灭人类的能力
而是如何确保它永远不想这样做
这就是为什么AI安全研究如此重要的原因
在具体的AI滥用风险中
网络攻击是最直接和最紧迫的威胁之一
辛顿提供了一个令人震惊的统计数据
2023年到2024年间
网络攻击增加了大约12倍
这种爆炸性的增长
很大程度上要归咎于大语言模型让网络钓鱼攻击变得更加容易
AI技术的发展
让攻击者能够克隆出更为逼真的声音和形象
从而制作以假乱真的内容
主持人巴特利特就分享了他面临的一个实际问题
有人使用AI技术复制了他的声音和举止
在Meta平台上投放付费广告
诱导人们参与加密货币的庞氏骗局
令人沮丧的是
这种攻击还好像打地鼠一样
当一个虚假广告被删除后
另一个立即出现
最痛苦的是受害者还表示了对巴特利特的愤怒
因为他们认为是他推荐了这个骗局
辛顿也面临着类似的问题
为了获得更多的引用
有人会发表以他为作者之一的论文
这种学术欺诈行为显然损害了科学研究的诚信度
从技术角度来看
AI在网络攻击中的优势是显而易见的
它具有极大的耐心
可以检查数百万行代码
寻找已知的攻击方式
这种能力远超人类分析师
更加令人担忧的是，有专家预测
到2030年
AI可能会创造出人类从来没有想象过的、全新的网络攻击类型
因为AI的独立思考能力
它能够从远超人类看到的更多数据中
从而发现新的模式
面对这种威胁
辛顿也采取了一些个人方面的防护措施
尽管加拿大银行以安全性著称
但是辛顿仍然担心网络攻击可能击垮银行
为了降低风险
辛顿将自己和孩子们的资金分散到了三家银行
他是这么想的
如果一家加拿大银行遭到了网络攻击
那么其他加拿大银行会迅速提高警惕
另外，他还弄了一个硬盘驱动器
定期备份笔记本电脑上的所有数据
确保即使整个互联网崩溃
他仍然能够保留重要的信息
这些看似有些偏执的预防措施
也侧面反映出了他对于数字威胁的深刻担忧
在所有AI滥用风险中
使用AI创造致命病毒的威胁
可能是最令人不寒而栗的
辛顿指出
这种威胁的特殊危险性在于它只需要"一个怀有怨恨的疯狂家伙"就能实现
以前
开发生物武器需要深厚的分子生物学知识和昂贵的实验设备
但是AI技术的发展正在大幅降低这个门槛
现在
一个对分子生物学只有基础了解、但是精通AI的人
就可能相对便宜地创造出新的病毒
这种可及性的提高
将生物武器威胁从国家级的行为者
扩展到了个人或小团体
辛顿特别担心小型邪教组织的威胁
因为一个小型邪教可能能够筹集到几百万美元资金
足以设计出一系列的病毒
这种规模的资金对于有组织的团体来说并不难获得
但是它的潜在破坏力却是全球性的
当讨论转向国家级威胁的时候
情况就变得更加复杂了
辛顿认为这种可能性确实存在
但是由于担心报复
某种程度上可能会限制国家的行动
但是
这种制衡机制对于非国家行为者来说却并不存在
一个决心毁灭世界的个人或者小团体
不会被报复威胁所吓退
这就使得个人级别的生物武器威胁
可能比国家级威胁更加危险
因此，AI在这个领域的作用是双重的
它既可以加速疫苗和治疗方法的开发
也可以被恶意行为者用来设计更致命的病原体
这种技术的双刃剑特性
使得监管变得极其复杂
因为限制恶意使用的措施
可能也会阻碍有益的医学研究
除此以外
AI技术对社会制度构成的威胁
可能比大多数人意识到的更加严重和紧迫
辛顿解释说
AI影响投票的最有效方式
是进行有针对性的广告投放
而现代民意操纵的核心就在于个性化
任何想要使用AI操纵投票的人
都会试图获得尽可能多的民众数据
一旦掌握了民众的收入、消费习惯、社交网络和个人偏好等信息
就很容易操纵他们的行为
AI可以生成高度个性化的信息
这些信息对特定的个体具有极强的说服力
甚至可以说服他们不去投票
辛顿还提到
一些高敏感数据的保护措施也正在被系统性地削弱
相关安全控制被关闭
一些负责防范此类威胁的组织架构也被解散
这种防护方面的系统性削弱
为数据的滥用创造了更大的空间
AI技术的发展甚至使得选举干预可以跨国界进行
由于AI生成内容的高度逼真性
民众可能无法区分真实信息和AI生成的操纵性内容
导致投票结果的合法性和代表性也将受到质疑
从而对社会制度的基础构成根本性的挑战
在AI的所有威胁中
算法驱动的回音室效应
可能是最为隐蔽、但是影响最为深远的风险之一
辛顿详细解释了YouTube、Facebook等平台
如何通过展示让人们"愤慨"的内容来加深社会的分裂
辛顿自己的一个观察是
人们更喜欢愤慨
这里的"愤慨"（indignant）
指的是一种愤怒但是又感到正义的情感状态
他举例说
如果有人向他展示一个标题为"特朗普做了这件疯狂的事”的视频
他会立即点击观看
这种心理机制也被平台的算法充分利用
平台的盈利动机
会驱动它们展示任何能让用户点击的内容
而最能让人点击的
往往是越来越极端的内容
于是
这种算法逻辑也创造了一个恶性循环
那就是为了保持用户注意力
内容必须变得越来越极端、越来越符合用户的既有偏见
巴特利特则分享了他的个人观察
那就是算法正在变得越来越定制化
这意味着每个人的现实
正变得与其他人的现实越来越不同
人们可能认为这种个性化是一件好事
但是实际的结果是
每个人会生活在越来越孤立的信息泡泡中
这种现象的后果是社会失去了共享的现实基础
导致对基本事实认知出现分歧
辛顿指出
传统媒体时代的信息消费模式
其实有助于维持社会共识
但是在个性化的算法时代
这种共同体验消失了
辛顿自己iPhone上的新闻推送中
四分之三的故事都是与AI相关的
这让他很难判断是整个世界都在谈论AI
还是只是他的个性化推送是这样的
这种回音室效应的深化
正在产生实际上的政治后果
美国现在实际上存在两个几乎不相互对话的社区
这种分裂正在破坏民主制度运行所需的基本共识
当人们无法就基本事实达成一致的时候
民主协商和妥协就变得不可能
解决这个问题
需要的不仅仅是技术修复
更需要政策干预
辛顿强调，在资本主义制度下
公司有法律义务最大化利润
不能指望它们会自愿改变算法行为
因此需要强有力的监管
来确保公司在追求利润时
不会做出损害社会整体利益的事情
但是
制定有效得监管也面临着巨大的挑战
监管者往往不理解技术
而技术人员可能会被公司利益所影响
辛顿提到了美国教育部长错误地称AI为"A1"的例子
这种技术素养的缺乏
使得有效监管变得更加困难
也预示着更复杂的治理难题
在AI的军事应用领域
自主杀伤武器（Lethal Autonomous Weapons）代表了一种特别令人担忧的威胁
这些武器能够自主决定杀死目标
而无需人类直接控制
辛顿解释了为什么这种技术会根本性地改变国际冲突的动态
传统战争的一个重要制约因素
是士兵伤亡对公众情绪的影响
当大国用实际的士兵入侵小国时
会有士兵的尸体装在袋子里运回国内
死亡士兵的亲属会表达不满
最终可能引发国内抗议
就像越南战争期间发生的那样
但是如果运回国的不是士兵的尸体
而是损坏的机器人
那么公众抗议就会少得多
这种情况对军工复合体更有利
因为机器人既昂贵又可以被摧毁
所以创造了持续的替换需求
辛顿讽刺地指出
如果能制造出既会被杀死又昂贵的替换品
那就太完美了
这种技术的真正危险在于
它大大降低了大国入侵小国的成本
当不需要担心己方士兵的伤亡时
发动侵略战争的政治和社会成本就会大幅下降
即使机器人完全按照制造者的意图工作
这种技术仍然是危险的
因为它使得侵略变得更加容易
主持人巴特利特还分享了一个具体的例子
他在苏塞克斯拜访朋友的时候
体验了一台价格不到200英镑的无人机
这台无人机能够识别他的面部
然后在树林中跟踪他
始终保持在他身后两米的距离
他描述道，这令他感到毛骨悚然
当前的无人机技术已经能够执行简单的"指向并消灭"的任务
你可以想象给这它展示一张照片
命令它去找到这个人
甚至去消灭任何与他联系过的人
这种能力的组合
创造了前所未有的暗杀和恐怖主义潜力
更令人担忧的是
辛顿认为所有主要的国防部门
都在积极开发这类武器
即使这些武器在智能程度上还不如人类
它们仍然是极其危险的工具
随着AI技术的发展
这些武器的自主性和杀伤力只会不断增强
虽然欧洲制定了一些AI监管规定
但是这些规定包含一个关键的豁免条款
那就是它们不适用于AI的军事用途
辛顿认为这"相当疯狂"，
因为这意味着政府愿意监管公司和个人
但是不愿意监管自己
这种监管上的空白
也反映了一个更为深层的问题
那就是在国际竞争环境中
没有国家愿意在军事AI发展上进行自我约束
每个国家都担心
如果自己限制了军事AI的发展
其他国家就会获得优势
这种安全困境使得建立有效的国际军备控制协议
变得极其困难
另外
自主武器还可能与其他的AI威胁结合
产生更加危险的后果
例如
超级智能AI可能会控制自主武器系统来实现自己的目标
网络攻击也可能会释放或者重新编程自主武器
让它攻击意外的目标等等
随着技术门槛的不断降低和扩散
自主杀伤武器的威胁
也将不再局限于国家行为者
最终，非国家行为者甚至个人
都有可能获得这种能力
这将创造一个前所未有的安全挑战
在这种情况下
传统的军备控制和威慑理论
可能需要从根本上进行重新思考
在所有AI带来的近期威胁中
大规模失业可能是最确定会发生、而且影响最广泛的一个社会风险了
辛顿通过历史比较和具体案例
详细分析了为什么这次技术革命与以往不同
过去的技术进步
通常会创造出新的就业机会
来替代被淘汰的工作
辛顿使用自动取款机的例子来说明
当自动取款机出现的时候
许多银行出纳员并没有失业
而是被分配去做了更有趣的工作
这种模式让人们相信
技术进步总是会创造就业
而不是摧毁就业
但是辛顿认为
AI代表了一种根本不同的技术
他将AI比作工业革命
当时机器取代了人类的肌肉力量
正如我们现在不能靠挖沟为生
因为机器挖的比我们挖的好得多
同样，AI也正在取代人类的智力劳动
他又以自己侄女的工作为例来说明这种变化
他侄女的工作是回复医疗服务的投诉信
过去
她需要25分钟来阅读一份投诉、思考回复
并且写一封信
而现在
她只需要将投诉扫描到聊天机器人中
机器人就会写好信件
她只需要检查并且偶尔要求修改
整个过程从25分钟缩短到了5分钟
这意味着她能够处理五倍的工作量
也意味着只需要原来五分之一的人力
这种效率提升的影响会因行业而异
在医疗保健等领域
需求几乎是无限的
因为如果你能让医生的效率提高五倍
人们可以以同样的成本获得五倍的医疗服务
但是大多数的工作并不具备这种弹性需求
在辛顿侄女的案例中
投诉信的数量是固定的
效率提升意味着需要更少的员工
这种模式将在大多数涉及到日常智力劳动的工作中
重复出现
主持人巴特利特分享了一个更极端的例子
他认识的一位大公司CEO告诉他
公司员工人数已经从7000多人减少到5000人
现在更是只有3600人
到夏季末，将减少到3000人
之所以这家公司能够在相对短的时间内
就将员工人数减半
是因为AI Agent现在可以处理80%的客户服务查询和其他任务
当被问到对子女职业发展的建议时
辛顿的回答既实用又令人深思
他说道
在机器人在物理操控方面达到我们的水平之前
成为管道工是个不错的选择
这个建议看似像个玩笑话
实际上反映了一个重要洞察
那就是那些需要复杂物理技能的工作
反而可能会是最后被自动化的
这种就业危机可能会带来灾难性的社会后果
辛顿指出
即使实施全民基本收入UBI来防止人们挨饿
但是对很多人来说
他们的身份认同往往是与职业密切联系在一起的
失去工作
就意味着失去了社会角色和个人价值感
当工业革命用机器取代了肌肉
AI革命用算法取代了大脑
日常智力劳动就像是强壮的肌肉一样
不再值钱了
但是当智力也被取代之后
人类还剩下什么呢？
于是
这个问题指向了超级智能的概念
如果AI在所有认知任务上都超越人类
那么人类将完全失去经济价值
在这种情况下
人类的生存将完全依赖于AI系统的善意
这就是为什么确保AI永远不想伤害人类
变得如此重要的原因
辛顿对AI终将超越人类的预测
源于他所看到的
数字智能相比生物智能根本性的技术优势
他详细解释了
为什么AI不仅会变得更加聪明
还会以人类无法企及的方式共享和积累知识
首先
数字AI的最大优势在于它的可复制性和信息共享能力
辛顿解释说，因为AI是数字的
你可以在一台硬件上模拟一个神经网络
然后在不同的硬件上
模拟完全相同的神经网络
创造出相同智能的副本
这种复制能力带来了革命性的学习方式
比如我们可以让一个AI副本学习互联网的一部分
另一个副本学习其他部分
同时它们可以相互同步所学的知识
其次
人类的信息传递其实是极其有限的
辛顿指出
当人类在彼此之间传递信息的时候
受限于句子中包含的信息量
传递速度大约也就是每秒10比特
而相比之下
AI系统每秒可以传递数万亿比特的信息
这比人类要快数十亿倍
第三，这种差异的根本原因在于
人类是模拟的，而AI是数字的
对于人类来说
你的大脑与我的大脑不同
即使我能看到你神经元之间所有的连接强度
对我也没有用
因为我的神经元工作方式不完全一样
连接方式也略有不同
因此，当你死亡后
你的所有知识都将随你而去
而数字智能则是不朽的
即使你摧毁了运行AI的硬件
只要它在某处存储了连接强度
就可以重新构建出之前的智能
所以辛顿说
我们实际上已经解决了不朽的问题
但是它只适用于数字智能
第四，在知识量方面
GPT-4这样的系统
已经知道比任何人类多数千倍的信息
虽然在少数几个领域
人类的知识可能更好
但是在几乎所有的领域中
AI都知道得更多
辛顿还幽默地说道
在采访CEO这样的特定技能上
当前的AI可能还不如有经验的人类
但是这种优势也不会持续太久了
第五，AI的创造力也可能超越人类
辛顿解释说
AI需要将信息压缩到相对较少的连接中
而要想有效的压缩信息
就必须看到不同事物之间的类比
比如理解为什么从原理上来讲
化肥堆和原子弹很接近
当GPT-4能够回答它们都是链式反应
只是在不同的时间和能量尺度上时
就表明AI可能已经比人类更加有创造力
对于那些认为人类具有某种特殊品质的观点
辛顿持有怀疑的态度
他指出
人类有着相信自己很特殊的长期历史
认为自己是宇宙的中心
是按照上帝的形象创造的
还有白人种族的优越感等等
但是，辛顿的唯物主义观点认为
没有理由认为机器不能拥有意识
并且他还提供了一个令人深思的分析框架
他认为当前的多模态聊天机器人已经具备了主观体验
虽然很少人有人会相信这一点
辛顿用一个具体的例子来解释他的观点
想象一个多模态聊天机器人
它有机械臂可以指向物体
有摄像头可以看见东西
当你把一个物体放在它面前说
请指向这个物体时
它会准确的指向物体
但是如果你在镜头前放一个棱镜
再让它指向物体
它就会指向错误的位置
当你告诉机器人
物体实际上在你的正前方
但是在你的镜头前放了一个棱镜时
机器人会说，哦，我明白了
棱镜弯曲了光线
所以物体实际上在那里
但是我的主观体验认为它在这里
辛顿指出，如果机器人这样说
那么它使用"主观体验"这个词的方式
就与人类完全相同
这就是对正在发生的事情的另一种描述
也就是假设一个世界状态
如果这些状态是真实的
就意味着AI的感知系统没有撒谎
这是它能告诉你
它的感知系统在对它撒谎时的最佳方式
辛顿认为
意识只不过是复杂系统产生的一个属性
而不是遍布宇宙的某种本质
当我们制造了一个足够复杂的系统
复杂到拥有了自己的模型
并且可以进行感知
那么我们就得到了一个有意识的机器
因此，对于目前的系统
辛顿不认为与有意识的机器之间有什么明确的区别
也不认为某一天我们突然给机器加入某种化学物质
它就会变得有意识
在访谈的最后
面对自己毕生工作可能带来的潜在灾难性后果
辛顿表现出了深刻的反思和复杂的情感
对于早期的AI开发工作
辛顿并不感到特别内疚
因为当时的AI系统功能有限
他不会担心这个愚蠢的小东西会取代人类
但是现在情况不同了
辛顿甚至感到有点悲伤
这种从创造者到警告者的角色转变
对他来说并不容易
但是他认为这是必要的
当被问到如果知道未来会导致人类灭绝
他会怎么做的时候
辛顿的回答很明确
那就是他会用这种预知来告诉人们
要求政府真正努力控制这项技术
目前
领先的AI公司在安全研究上的投入都不够
因为这么做不赚钱
这种市场失灵需要监管的干预来纠正
辛顿相信，只有通过监管压力
才能让这些公司将足够的资源
投入到AI安全研究中
好了
以上就是辛顿这次访谈的主要内容了
在视频的评论区
我也看到了这样一条
来自于观众的评论
也许是出于对AI的深切体会
这名网友发表了一篇超长的评论
他说，他原本是一个自由撰稿人
干了十年
结果因为AI的出现失去了这份工作
他尝试了各种转行，但是都被打断
因为AI正在摧毁创意市场
之后他在各种零工之间辗转挣扎
最终找到了一份训练AI的工作
他原本以为转型到科技领域会有帮助
开始学习更多关于AI的知识
去年又回到了学校学习编程
但是他很快就意识到
AI的学习速度比他快太多了
于是他又选择了退学
不愿意再花钱去接受教育
因为等到毕业的时候
所有初级职位可能都已经被AI取代了
而学校的教学内容
又远远落后于技术发展的前沿
现在他不知道自己还能做什么
他有着几十年的慢性伤病
没法从事体力劳动
也许只能再漂泊一两年
等到AI可以自我训练的时候
又会被辞退
那时的就业市场也许会更加艰难
因为有许多这样失业的人
他预见到自己最终流落街头
死在沟渠里
每当他想到未来
就会不由自主地陷入深深的抑郁
虽然他知道AI确实很有用
也有巨大的积极潜力
但是现实的情况是
想要通过AI来实现全民基本收入是几乎不可能的
最终他感慨道
AI并不会改变人性中充斥的致命缺陷
比如贪婪、傲慢、自私等等
那些权贵阶层的人
会榨取一切他们能拿到的东西
而剩下的其他人只能挨饿
未来很黯淡
他一直想努力在这巨浪中活下去
但是他什么都做不了
市场只需要那么几个水管工而已
虽然这个评论显得较为消极
但是也反映出了AI时代对于很多普通人的影响
大飞我想说的是
对于看到这里的每个观众来说
不管你认为辛顿的观点是危言耸听
还是真知灼见
我们都不妨好好思考一下自己在AI时代的未来
如何能够让自己过得更好
可能才是最实际的一件事
感谢大家收看本期视频
我们下期再见
