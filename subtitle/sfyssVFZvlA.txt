大家好，这里是最佳拍档，我是大飞
我想请大家先跟我做一个简单的思想实验
现在，请你想象一下
当你盯着一张复杂的财务报表
或者一张画着螺旋图案的纸时
你的眼睛是怎么工作的？
你的视线是从左上角的第一个像素开始
一行一行、机械地扫描到右下角吗？
显然不是
人类的视觉系统是非常高效且势利的
我们的视网膜中央凹
会根据大脑的指令
跳跃式地捕捉重点
当我们在看表格时
我们会根据表头和数据的对应关系
在行与列之间进行逻辑跳跃
当我们在看一个螺旋时
我们的视线会顺着线条的逻辑旋转
这种视觉过程，是由语义驱动的
是有因果关系的
我们的每一次注视
都不仅取决于位置
更取决于上一次我们看到了什么
以及我们要找什么
然而，长期以来
我们主流的视觉语言模型
那些号称能理解图片的大模型
却在用一种非常反直觉的方式工作
它们大多采用了一种叫做光栅扫描的顺序
把一张二维的图片
强行切成一个个小方块
然后按照从左到右、从上到下的死板顺序喂给模型
这种做法
就像是强迫一个阅读理解的高手
必须按像素点去读书一样
它忽略了图像内部天然的逻辑结构和语义关联
正是基于这个深刻的洞察
DeepSeek团队刚刚发布了他们的最新技术报告
DeepSeek-OCR 2
这篇论文提出了一个极其性感的概念
Visual Causal Flow
视觉因果流
他们不仅将OmniDocBench榜单的成绩刷到了新高
更重要的是
他们提出了一种全新的编码器架构
DeepEncoder V2
试图赋予机器一种基于因果推理的视觉能力
今天
我就来为大家拆解一下这篇论文
看看DeepSeek是如何在多模态的底层架构上
动了一次漂亮的手术
首先，我们需要审视一下
传统的视觉编码器出了什么问题
在DeepSeek-OCR的第一代
以及目前市面上绝大多数的视觉模型中
视觉编码器通常扮演着一个翻译官的角色
最经典的做法是使用CLIP或者ViT作为底座
这些模型通过双向的注意力机制
让每一个图像块都能看到其他的图像块
从而提取出一个全局的特征表示
这听起来很完美，对吧？
类似于人类的周边视觉
一眼看清全局
但是
当这些特征被送入到大语言模型中
进行下一步处理时，问题出现了
大语言模型是天生的序列生物
它们是在一维的文本数据上训练出来的
习惯于从前向后、基于上文预测下文的因果推理
当我们把二维的图像特征
简单粗暴地展平
并加上固定的位置编码后
我们就人为地引入了一种归纳偏置
模型会误以为图片左上角的信息
在逻辑上一定优先于右下角的信息
对于自然风景图，这也许还能凑合
但对于文档、表格、公式这种高度结构化的图像
这种假设简直就是灾难
比如一个从下往上画的流程图
或者一个复杂的数学公式布局
它们的逻辑顺序
往往和空间坐标没有直接的线性关系
DeepSeek-OCR 2的核心突破
就在于它不仅发现了这个问题
还给出了一个极具想象力的解决方案
他们设计了DeepEncoder V2
这里的神来之笔是
他们抛弃了传统的CLIP类视觉编码器
转而使用了一个小型的语言模型结构
来充当视觉编码器
这听起来是不是有点反直觉呢？
用语言模型来看图，能行么？
别急
我们慢慢拆解这个设计的精妙之处
在DeepEncoder V2的架构图中
我们可以看到整个流程被分为了两个关键阶段
首先是视觉的Token化
这一步
他们依然保持了对计算效率的极致追求
并没有使用沉重的ViT-Large
而是沿用了上一代DeepSeek-OCR的设计
结合了一个8000万参数的SAM-base和一个卷积层结构
这个模块的作用非常纯粹
就是将高维的像素信息
压缩成初步的视觉Token
这里有一个非常重要的细节
通过卷积和窗口注意力机制
他们实现了16倍的Token压缩率
这意味着
即使是一张1024x1024的大图
经过这一步处理后
Token数量也被控制在了一个非常可控的范围内
接下来的部分，才是真正的好戏
语言模型即视觉编码器
DeepSeek团队在这里使用了一个基于Qwen2-0.5B魔改而来的架构
为什么要用Qwen2？
因为它的参数量大约是5亿
这和传统的、大约3亿参数的CLIP ViT-Large
在量级上是相当的
不会带来过大的计算负担
但是，架构的内在逻辑却完全变了
在这个新的编码器中
输入不仅仅是刚才提到的视觉Token
还加入了一组全新的东西
叫做Causal Flow Query，因果流查询
我们可以把这两组Token
想象成两种不同身份的特工
第一组特工是视觉Token
它们是图像的原始特征表示
在注意力机制的设计上
DeepSeek通过定制的掩码
让这些视觉Token保持了双向注意力
也就是说
每一个视觉Token都能看到所有的其他视觉Token
无论它们在图像的哪个位置
这就保留了ViT架构的优势
确保了模型拥有全局的感受野
不会因为局部关注而丢失整体信息
第二组特工
也就是新引入的因果流查询
或者是论文中提到的可学习查询
这组Token被拼接在视觉Token的后面
它们的行为模式完全不同
它们采用的是因果注意力
这意味着，第N个查询Token
只能看到它之前的Token
而不能看到未来的Token
这个设计简直是天才般的混合
请大家在脑海中构建这样一张图
左边是视觉信息
全向互通，像一个全知全能的数据库
右边是查询序列，按顺序排队
像一个正在进行逻辑推理的侦探
每一个查询特工
不仅能看到它之前所有的查询结果
还能查阅左边那个全知全能的视觉数据库
通过这种级联因果感知的设计
DeepEncoder V2实际上是在做一件前人没做过的事
它在编码阶段
就开始对视觉信息进行重排序
这些可学习的查询向量
不再是受限于固定的空间位置
而是根据图像的语义逻辑
动态地去抓取视觉Token中的信息
如果图片是一个螺旋
这些Query就会学习到沿着螺旋抓取特征
如果图片是一个表格
它们就会学习到按行或按列抓取特征
最终，送入到大模型解码器里的
不再是那堆原始的、带有空间偏见的视觉Token
而是这组经过了因果重排序和信息蒸馏的查询Token
这实际上将二维图像的理解过程
分解成了两个正交的维度
编码器负责阅读逻辑的推理
即决定先看哪儿后看哪儿
而解码器则负责在这些已经理顺了的信息上
进行具体的视觉任务推理
这就像是把怎么读和读懂了什么
分派给了两个专业的部门
大大减轻了后续语言模型解码器的压力
说到这里
很多做工程的朋友可能会担心一个问题
那就是Token的数量
我们都知道，多模态模型的推理成本
很大程度上取决于视觉Token的数量
像Gemini 1.5 Pro或者GPT-4o
在处理高分辨率文档时
往往需要消耗成千上万个Token
这对显存和推理速度都是巨大的考验
DeepSeek-OCR 2在这里展示了他们极其强悍的工程优化能力
或者说是抠门的艺术
他们采用了多视图裁剪的策略
但是却把最终喂给大语言模型的视觉Token数量
死死地按在了256到1120个之间
这怎么可能呢？
一张1024x1024的图
通常切片后会有几千个Token
DeepSeek的策略是这样的
对于全局视图，比如1024x1024
他们只用256个Query来表示
如果需要更清晰的细节
他们会引入局部裁剪视图
每个局部视图由144个Query来表示
最极端的情况下
也就是6个局部视图加上1个全局视图
总共也就才1120个Token
这个数字非常有意思
论文中特意提到
Gemini-3 Pro的最大视觉Token预算
也是1120左右
这说明，DeepSeek在设计之初
就瞄准了行业最顶尖的效率标准
相比之下
DeepSeek-OCR的第一代在高达模式下
需要1156个Token
新版本在保持甚至降低Token数的情况下
性能却大幅提升
这完全归功于DeepEncoder V2更高效的信息压缩和重组能力
接下来
我们来看看这套复杂的系统是如何训练出来的
DeepSeek团队为我们展示了一个精细的三阶段训练流水线
第一阶段
编码器预训练（Encoder Pretraining）
这一步就像是教这个新的大脑
学会基本的看图规则
他们冻结了其他的组件
专门训练DeepEncoder V2
为了让这个由语言模型改造而来的编码器
适应视觉任务
他们使用了预测下一个Token的任务
这不仅让模型学会了识别物体
更重要的是
让那些因果查询Token开始学习
如何根据语义去重组视觉信息
第二阶段：
查询增强（Query Enhancement）
到了这一步
他们把那个30亿参数的DeepSeek-MoE解码器拉了进来
这时候，视觉Tokenizer被冻结了
重点是联合优化大语言模型风格的编码器和解码器
这个阶段引入了更高分辨率的数据
并且通过一种叫四阶段流水线并行的工程手段
动用了160张A100显卡进行大规模训练
这一步的目标
是让编码器输出的Query
能更完美地对齐解码器的语义空间
第三阶段
解码器专项训练（Decoder Specialization）
最后，为了追求极致的训练效率
他们冻结了整个DeepEncoder V2
只训练最后的DeepSeek-LLM解码器
这样做的好处是显而易见的
因为编码器不动了
视觉特征可以预先计算
大大加快了数据吞吐量
这个阶段主要是通过大量的OCR数据
强化模型对文字、公式、表格的转写能力
聊完了架构和训练
我们必须要拿数据说话
DeepSeek-OCR 2的实战效果到底如何？
论文中选用的竞技场是OmniDocBench v1.5
这是一个非常硬核的文档解析基准测试
涵盖了杂志、论文、报告等各种复杂的版面
结果显示
DeepSeek-OCR 2在各项指标上都取得了显著的进步
总分达到了91.09%，
相比第一代提升了3.73%。
可能有人觉得3%不算多
但在高水平的竞赛中
这已经是巨大的飞跃
特别值得关注的
是一个叫做阅读顺序R-order的编辑距离指标
这个指标衡量的是模型输出的内容顺序
是否符合人类的阅读逻辑
这个值越低越好
在这个指标上
DeepSeek-OCR 2从第一代的0.085
下降到了0.057
这直接证明了DeepEncoder V2的设计初衷是成功的
它确实学会了更好的视觉因果流
能够更聪明地判断，读完这一栏
下一眼该看哪儿
而不是傻乎乎地按像素坐标去扫描
当然，作为一个客观的解读
我们也必须看看它的不足
论文中很诚实地展示了改进空间
在报纸这一类别上
DeepSeek-OCR 2的表现相对较弱
编辑距离超过了0.13
论文解释说
这可能是因为现有的训练数据中
报纸类的样本太少，只有25万份
而且对于这种超高密度的文本
目前的多视图裁剪策略
可能还需要进一步增加局部视图的数量
除了学术指标
对于工业界的朋友来说
实用性可能更加重要
论文给出了一个非常关键的数据
重复率（Repetition Rate）
在使用视觉模型进行长文档OCR时
模型经常会因为注意力发散
而开始复读机模式
DeepSeek-OCR 2通过更强的因果推理能力
将在线用户日志数据的重复率
从6.25%降低到了4.17%。
这意味着，在实际的生产环境中
它的稳定性得到了极大的增强
最后
我想聊聊这篇论文背后更深远的意义
也就是他们在讨论部分提到的
通向原生多模态
DeepSeek-OCR 2并没有止步于做一个更好的OCR工具
DeepEncoder V2的成功
实际上验证了一个大胆的猜想
也许我们根本不需要为每一种模态
设计一个专门的编码器
如果我们把Transformer语言模型
看作是一个通用的序列处理器
那么只要我们能把不同模态的数据映射到同一个嵌入空间
再配合特定的可学习查询
一个单一的、基于大语言模型架构的编码器
就有可能同时处理所有的感官输入
想象一下，未来的DeepSeek模型
同一个Encoder，配置不同的Query
一组Query负责看视频
一组Query负责听音频
它们在同一个参数空间内
进行信息的压缩和重组
然后统一交给解码器去推理
这将极大地统一现有的多模态架构
让全模态不再是简单的模型拼接
而是真正的原生融合
总结来说
DeepSeek-OCR 2不仅仅是性能上的升级
它通过引入DeepEncoder V2
用语言模型的架构去改造视觉编码器
成功地将因果推理引入了视觉感知的最前端
同时
它打破了二维空间和一维序列之间的隔阂
让机器的视觉更接近人类的意图驱动模式
这篇论文虽然名为OCR
但是它所展示的视觉因果流思想
可能会成为未来多模态大模型设计的一个重要范式
它告诉我们
不要被数据的物理形态所束缚
真正的智能
在于如何构建信息背后的逻辑流
好了，今天的解读就到这里
有关于对这篇论文的任何想法
欢迎在评论区留言
感谢收看本期视频，我们下期再见
