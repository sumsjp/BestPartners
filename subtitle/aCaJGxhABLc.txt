大家好，这里是最佳拍档，我是大飞
对于搞AI的同学来说
了解底层的算力架构是很重要的
尤其是今年大语言模型的崛起
让GPU无比紧俏
那么底层的算力优化也就变得越来越必要了
今天我们就来聊聊GPT背后所使用的算力网络
也是让英伟达站上万亿市值的基础
InfiniBand
它到底是一项怎样的技术？
为什么会倍受现在这些AI公司的追捧？
而人们经常讨论的“InfiniBand与以太网”之争
又是怎么回事呢？
InfiniBand，简称IB
是一种能力很强的通信技术协议
它的英文直译过来，就是“无限带宽”。
Infiniband的诞生故事
还要从计算机的架构讲起
大家都知道
现代意义上的数字计算机
从诞生之日起
一直都是采用的冯·诺依曼架构
在这个架构中，有CPU
包括运算器、控制器，有存储器
包括内存、硬盘
还有I/O也就是输入/输出设备
上世纪90年代早期
为了支持越来越多的外部设备
英特尔公司率先在标准PC架构中引入PCI总线的设计
而PCI总线，其实就是一条通道
不久后，互联网进入高速发展阶段
线上业务和用户规模的不断增加
给IT系统的承载能力带来了很大挑战
当时，在摩尔定律的加持下
CPU、内存、硬盘等部件都在快速升级
而PCI总线，升级速度缓慢
大大限制了I/O性能
成为整个系统的瓶颈
为了解决这个问题
英特尔、微软和SUN公司
主导开发了“Next Generation I/O
简称NGIO”的技术标准
而IBM、康柏以及惠普公司
则主导开发了“Future I/O
简称FIO”。
IBM这三家公司
还在1998年合力搞出了PCI-X标准
1999年
FIO Developers Forum和NGIO Forum进行了合并
创立了InfiniBand贸易协会，IBTA
很快，2000年
InfiniBand架构规范的1.0版本正式发布了
简单来说，InfiniBand的诞生目的
就是为了取代PCI总线
它引入了RDMA协议，具有更低的延迟
更大的带宽，更高的可靠性
可以实现更强大的I/O性能
说到InfiniBand
有一家公司我们是必须提到的
那就是大名鼎鼎的Mellanox
中文名为迈络思
1999年5月
几名从英特尔公司和伽利略技术公司离职的员工
在以色列创立了一家芯片公司
将其命名为Mellanox
Mellanox公司成立后，就加入了NGIO
后来，NGIO和FIO合并
Mellanox随之加入了InfiniBand阵营
2001年
他们推出了自己的首款InfiniBand产品
可是让人没想到的是，2002年
InfiniBand阵营突遭巨变
这一年，英特尔公司“临阵脱逃”，
决定转向开发PCI Express
也就是PCIe
而另一家巨头微软
也退出了InfiniBand的开发
尽管SUN和日立等公司仍选择坚持
但是InfiniBand的发展似乎已然蒙上了一层阴影
2003年开始
InfiniBand转向了一个新的应用领域
那就是计算机集群互联
这一年
美国弗吉尼亚理工学院创建了一个基于InfiniBand技术的集群
在当时的全球超级计算机500强测试TOP500中排名第三
2004年
另一个重要的InfiniBand非盈利组织诞生
那就是开放Fabrics联盟，OFA
OFA和IBTA是配合的关系
IBTA主要负责开发、维护和增强Infiniband协议标准
而OFA负责开发和维护Infiniband协议和上层应用API
到了2005年
InfiniBand又找到了一个新的场景
就是存储设备的连接
老一辈的网络工程师一定记得
当年InfiniBand和FC是非常时髦的SAN技术
再后来
InfiniBand技术逐渐深入人心
开始有了越来越多的用户
市场占比也不断提升
到了2009年，在TOP500榜单中
已经有181个采用了InfiniBand技术
不过
当时千兆以太网当时仍然是主流
占了259个
在InfiniBand逐渐崛起的过程中
Mellanox也在不断壮大
逐渐成为了InfiniBand市场的领导者
2010年
Mellanox和Voltaire公司合并
InfiniBand主要供应商只剩下了Mellanox和QLogic
不久之后，2012年
英特尔公司出资收购了QLogic的InfiniBand技术
返回到InfiniBand的竞争赛道
2012年之后
随着高性能计算HPC需求的不断增长
InfiniBand技术继续高歌猛进
市场份额不断提升
到了2015年
InfiniBand技术在TOP500榜单中的占比首次超过了50%，
达到257套，占比为51.4%。
这标志着InfiniBand技术首次实现了对以太网技术的逆袭
InfiniBand成为了超级计算机最首选的内部连接技术
2013年
Mellanox相继收购了硅光子技术公司Kotura和并行光互连芯片厂商IPtronics
进一步完善了自身产业布局
2015年
Mellanox在全球InfiniBand市场上的占有率达到了80%。
他们的业务范围
已经从芯片逐步延伸到网卡、交换机/网关、远程通信系统和线缆及模块等全领域
成为世界级的网络提供商
面对InfiniBand的赶超
以太网也没有坐以待毙
2010年4月，IBTA发布了RoCE技术
也就是基于融合以太网的远程直接内存访问
将InfiniBand中的RDMA技术“移植”到了以太网中
2014年
他们又提出更加成熟的RoCE v2版本
有了RoCE v2
以太网大幅缩小了和InfiniBand之间的技术性能差距
结合本身固有的成本和兼容性优势
又开始反杀回来
在这张2007年到2021年的TOP500技术占比图中
我们可以看到
2015年开始
25G以及更高速率的以太网崛起
迅速成为行业新宠
一度压制住了InfiniBand
2019年，英伟达公司豪掷69亿美元
成功的收购了Mellanox
击败对手英特尔和微软
二者分别出价60亿和55亿美元
成功地收购了Mellanox
对于收购原因
英伟达CEO黄仁勋是这么解释的
这是两家全球领先高性能计算公司的结合
我们专注于加速计算
而Mellanox专注于互联和存储
现在看来
老黄的决策是非常有远见的
也正如大家所见，AIGC大模型崛起
整个社会对高性能计算和智能计算的需求发生了井喷
想要支撑如此庞大的算力需求
必须依赖于高性能计算集群
而InfiniBand
在性能上是高性能计算集群的最佳选择
将自家的GPU算力优势与Mellanox的网络优势相结合
就等于打造了一个强大的“算力引擎”。
在算力基础设施上
英伟达毫无疑问占据了领先优势
如今，在高性能网络的竞争上
就是InfiniBand和高速以太网的缠斗
双方势均力敌
不差钱的厂商
更多会选择InfiniBand
而追求性价比的
则会更倾向高速以太网
剩下还有一些技术
例如IBM的BlueGene、Cray
还有Intel的OmniPath
基本属于第二阵营了
介绍完InfiniBand的发展历程
接下来
我们再看看它的工作原理
为什么它会比传统以太网更强
它的低时延和高性能
究竟是如何实现的
前面我们提到
InfiniBand最突出的一个优势
就是率先引入RDMA
Remote Direct Memory Access
远程直接数据存取协议
在传统TCP/IP中，来自网卡的数据
要先拷贝到核心内存
然后再拷贝到应用存储空间
或者从应用空间将数据拷贝到核心内存
再经由网卡发送到Internet
这种I/O操作方式
需要经过核心内存的转换
它增加了数据流传输路径的长度
增加了CPU的负担
也增加了传输延迟
而RDMA相当于是一个“消灭中间商”的技术
RDMA的内核旁路机制
允许应用与网卡之间的直接数据读写
将服务器内的数据传输时延降低到接近1us
同时，RDMA的内存零拷贝机制
允许接收端直接从发送端的内存读取数据
绕开了核心内存的参与
极大地减少了CPU的负担
提升CPU的效率
应该说
InfiniBand之所以能迅速崛起
RDMA居功至伟
从InfiniBand的网络拓扑结构中可以看出
InfiniBand是一种基于通道的结构
组成单元主要分为四类
分别是HCA（Host Channel Adapter
主机通道适配器
TCA（Target Channel Adapter
目标通道适配器）
InfiniBand link，也称连接通道
可以是电缆或光纤
也可以是板上链路
以及组网用的InfiniBand交换机和路由器
通道适配器就是搭建InfiniBand通道用的
所有传输均以通道适配器开始或者结束
从而确保安全
或者在给定的QoS级别下工作
使用InfiniBand的系统可以由多个子网组成
每个子网最大可由6万多个节点组成
子网内部由InfiniBand交换机进行二层处理
而子网之间则使用路由器或网桥进行连接
InfiniBand的二层处理过程也非常简单
每个InfiniBand子网都会设一个子网管理器
生成16位的本地标识符LID
InfiniBand交换机包含了多个InfiniBand端口
并且根据第二层本地路由标头中包含的LID
将数据包从其中一个端口转发到另一个端口
除了管理数据包外
交换机不会消耗或生成任何数据包
简单的处理过程
加上自有的Cut-Through技术
InfiniBand将转发时延大幅降低至100ns以下
明显快于传统的以太网交换机
在InfiniBand网络中
数据同样采用串行方式
以最大4KB的数据包形式进行传输
InfiniBand协议同样采用了分层结构
各层相互独立，下层为上层提供服务
其中
物理层定义了在线路上如何将比特信号组成符号
然后再组成帧、数据符号以及包之间的数据填充等等
详细说明了构建有效包的信令协议等
链路层定义了数据包的格式以及数据包操作的协议
比如流控、路由选择、编码、解码等
网络层通过在数据包上添加一个40字节的全局的路由报头GRH
来进行路由的选择
对数据进行转发
在转发的过程中
路由器仅仅进行可变的CRC校验
这样就保证了端到端的数据传输的完整性
传输层再将数据包传送到某个指定的QP
也就是Queen Pair中
并指示QP该如何处理这个数据包
可以看出
InfiniBand拥有自己定义的1-4层格式
是一个完整的网络协议
而端到端的流量控制
是InfiniBand网络数据包发送和接收的基础
可以实现无损网络
说到QP，我们需要多提几句
它是RDMA技术中通信的基本单元
QP，也叫队列偶，就是一对队列
包括发送工作队列SQ
Send Queue和接收工作队列RQ
Receive Queue
用户调用API发送接收数据的时候
实际上是将数据放入QP当中
然后以轮询的方式
将QP中的请求一条条的处理
InfiniBand物理链路可以用铜缆或光缆
针对不同的连接场景
也可能需要使用专用的InfiniBand线缆
InfiniBand在物理层定义了多种链路速度
例如1X
4X，12X
每个单独的链路是四线串行差分连接
以早期的单数据速率SDR规范为例
1X链路的原始信号带宽为2.5Gbps
4X链路是10Gbps，12X链路是30Gbps
因为采用了8b/10b编码
所以1X链路的实际数据带宽为2.0Gbps
由于链路是双向的
因此相对于总线的总带宽是4Gbps
随着时间的推移
InfiniBand的网络带宽不断升级
从早期的SDR、DDR、QDR、FDR、EDR、HDR
一路升级到NDR、XDR、GDR
最后
我们再来看看市面上的InfiniBand商用产品
英伟达收购Mellanox之后
于2021年推出了自己的第七代NVIDIA InfiniBand架构——NVIDIA Quantum-2
整个平台包括
NVIDIA Quantum-2系列交换机、NVIDIA ConnectX-7 InfiniBand适配器、BlueField-3 InfiniBand DPU
以及相关的软件
其中呢NVIDIA Quantum-2系列交换机采用了紧凑型的1U设计
包括风冷和液冷版本
交换机的芯片制程工艺呢为7纳米
单芯片呢拥有570亿个晶体管
比A100的GPU呢还多
采用了64个400 Gbps端口
或者是128个200 Gbps端口的灵活搭配
提供总计51.2T bps的双向吞吐量
NVIDIA ConnectX-7 InfiniBand适配器
支持PCIe Gen4和Gen5
具有多种外形规格
可以提供400Gbps的单或双网络端口
根据行业机构的预测到2029年
InfiniBand的市场规模将达到983.7亿美元
相比于2021年的66.6亿美元增长14.7倍
在预测期2021-2029内的复合年增长率为40%
好了今天
我们简单地回顾了一下
InfiniBand的发展历史和基本原理
相信在高性能计算和人工智能计算的强力推动下
infiniband的发展前景呢还是令人非常期待的
究竟他和以太网谁能笑到最后
还需要时间来告诉我们答案
感谢大家观看本期视频
我们下期再见
