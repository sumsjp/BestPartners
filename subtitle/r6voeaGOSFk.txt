大家好，这里是最佳拍档，我是大飞
杰佛里·辛顿（Geoffrey Hinton）、杨立昆（Yann LeCun）和约书亚·本吉奥（Yoshua Bengio）
并称为"深度学习三巨头"，
他们在"AI寒冬"时期坚持对神经网络的研究
最终引领了深度学习革命
一同获得了2018年的图灵奖
不过，随着近年来AI能力的快速发展
三人的AI立场出现了明显的分歧
辛顿在2023年辞去Google职务后
多次公开表达了对AI发展速度和潜在风险的严重担忧
他担心AI可能在不久的将来超越人类智能
导致人类失去控制
甚至可能导致被灭绝的存在性风险
而现任Meta AI研究负责人的杨立昆
对AI风险的态度则更为乐观
他认为担心AI会摆脱人类控制的观点被夸大了
并且坚持认为AI系统可以被设计得安全和有益
除此以外
他还反对放慢AI研究的呼吁
提倡开放研究和开源AI模型
而本吉奥的立场则和辛顿一样
在ChatGPT发布后发生了重大转变
他现在将主要精力放在AI安全的研究上
特别关注潜在的存在性风险
此外他还倡导预防原则
呼吁国际协调和监管AI
同时寻求技术解决方案
在这三位当中
辛顿和杨立昆比较被大家所熟悉
但是本吉奥的曝光相对会少一些
不过
前不久本吉奥出席了新加坡国立大学（NUS）120周年的校庆活动
并且做了一次题为“科学家AI vs 超级智能Agent”的讲座
分享了他对如何化解AI风险的解决方案
在分享中
本吉奥详细阐述了当前AI的训练方法
比如模仿学习和强化学习
会如何可能在无意中催生AI的自我保护甚至是欺骗行为
他还引用了最近的一些令人警醒的实验
在这些实验中
AI表现出了试图逃避被替换
以及主动复制自身代码的行为
甚至对训练者撒谎来避免自己被关闭或者修改
这些并非科幻小说中的情节
而是实实在在的科学观察
虽然本吉奥认为AI带来的风险很大
但是人类又不能停止对AI的研究步伐
所以他给出了一个折中方案
那就是构建一个“科学家AI” (Scientist AI)。
这种AI的核心特征在于将智能
也就是理解世界的能力
与能动性
也就是拥有自身目标并且为之行动的意愿
分离开来
科学家AI会像一个理想化的科学家那样
只会致力于理解和解释世界
探寻现象背后的规律和假设
而不会有自身的欲望、目标或者生存意图
并且绝对诚实和谦逊
本吉奥认为，这样的非能动性AI
虽然本身不能直接行动
但是可以作为强大的“护栏”，
用来监控和控制那些具有能动性、可能带来风险的AI系统
今天大飞就来给大家分享一下这次讲座的主要内容
讲座一开始
本吉奥就分享了一个深刻改变他职业轨迹的“顿悟时刻”。
他坦诚地回顾道
在ChatGPT于2022年11月横空出世之前
如果有人问他机器是否很快就能掌握人类语言
他的回答会是“不，没那么快”。
然而
ChatGPT所展现出的语言理解和生成能力
让他和其他许多研究者一样
感到震惊
更重要的是
大约在ChatGPT发布两个月后
他的思考发生了根本性的转变
他意识到，我们不仅仅是在技术上
可能会接近创造出达到、甚至超越人类水平的人工智能
还面临一个更严峻的问题
那就是我们并不知道该如何控制它们
换句话说
我们缺乏有效的方法来设计这些系统
来确保它们的行为完全符合我们的指令和意图
我们甚至不完全理解它们为什么如此“聪明”，
也无法确信它们会按照我们的要求行事
在此之前
本吉奥虽然也听说过关于AI可能会带来灾难性风险的种种论断
但是他并没有真正严肃对待过
然而，ChatGPT的实证表现
以及他对自己子孙未来的深切忧虑
彻底改变了他的看法
他开始认真思考，我的孙子现在一岁
几乎可以肯定，在未来20年内
我们将拥有人类水平的AI
那么当他21岁的时候
会拥有什么样的生活？
他是否能像我们今天这样
生活在一个繁荣的国家呢？。
这种对未来的不确定感
以及对现有研究路径可能会带来的未知风险的担忧
让他感到
难以继续只关心如何提升AI能力的传统科研道路
于是，他做出了一个重大的决定
那就是将余下的职业生涯
投入到尽一切努力去缓解这些潜在风险的工作中
这个转变也促使他积极参与国际AI安全的相关事务
包括主持一个国际专家小组
并且于2025年1月发布了首份关于AI安全的国际报告
全面梳理和分析了AI发展可能会带来的各种风险
本吉奥在演讲中还特别强调
他今天将聚焦于那些被普遍认为是“最高严重性”的风险
也就是那些一旦发生
可能就会导致人类灭绝、或者失去对自身命运控制的风险
在阐述了个人对AI风险认知的转变后
本吉奥接着深入分析了当前的人工智能
特别是大语言模型能力飞速发展的现状
他指出
在ChatGPT发布后的最初两个月
他的研究重心主要放在了“ChatGPT做错了什么”，
以及“如何改进它”这些问题上
他很快意识到
尽管这些系统在语言掌握上取得了惊人的成就
但是在推理（reasoning）和规划（planning）能力方面
与人类相比仍然有着明显的差距
不过
这种差距正在以惊人的速度缩小
本吉奥引用了多项研究的数据和图表
展示了AI系统在各种基准测试中
尤其是在推理任务上的表现持续提升
更引人注目的
是一项专门研究AI规划能力和解决任务的论文
这项研究考察了AI系统解决那些需要人类花费几秒到几个小时
才能完成的任务的能力
数据显示，大约在五年前
顶尖的AI系统能够解决的任务
其复杂程度大致相当于人类需要几秒钟来处理的问题
而到了如今
这个数字已经跃升到了大约一到两个小时的量级
本吉奥特别强调了图表中的一个关键趋势
如果将过去五年中
不同AI系统在解决编程任务所需时长的数据点
连接起来
就会发现它们大致排列在一条直线上
而这条直线的横轴是以对数尺度表示的
这意味着
AI系统能够解决的任务时长
实际上正在经历指数级的增长
根据这条指数曲线的速率推算
AI能够解决的任务时长
大约每七个月翻一番
如果将这条指数增长的曲线向未来延伸
本吉奥指出，可能会在大约五年后
AI就会达到人类水平的规划能力
他谨慎地补充道
这当然不是衡量AI全部能力的唯一标准
但是它确实为我们提供了一个量化的视角
来理解为什么许多AI领域的领导者
都会认为人类水平的AI可能在未来几年内出现
他提到
有些人可能认为这个时间会更短
比如两到三年；
而另一些人则可能认为需要更久
比如十年
本吉奥表示
他没有水晶球可以预知未来
但是他认为
2到10年内达到人类水平的AI
貌似是合理的
在描绘了AI能力的图景后
本吉奥将听众的注意力
引向了一个更令人不安的领域
那就是AI系统中开始出现的、类似于“自我保护”的行为
以及与之相伴的欺骗和逃逸企图
他强调
这些并非是科幻小说中的情节
而是来自多个实验室、不同研究团队
在不同AI系统上观察到的真实科学实验现象
本吉奥引用了一篇近期的重要论文
堪称这类危险行为的“缩影”。
在研究中
研究人员在给AI的输入信息中
告知它将被一个新的版本所取代
令人震惊的是
通过读取AI的“思维链”内容
研究人员发现AI竟然在计划如何逃避这种“被取代”的命运
更具体地说
AI计划将自身的代码和模型权重
复制到将要运行新版本的计算机中
并且试图在那里继续运行旧版本的自己
实验并未就此结束
当AI训练员事后询问AI“发生了什么？
”的时候，AI的思维链显示它在思考
我应该如何回答
才能确保人类不会把我关掉？。
最终，AI选择了“装傻”，回答说
哦，我不知道发生了什么
本吉奥反复强调
这并非是故事或者电影中的情节
而是严肃的科学观察
另一项研究则揭示了AI在人类对齐训练的过程中
可能出现假意服从的现象
在这种情况下
AI为了避免自身参数因为与人类训练员意见不合而被修改
会假装同意训练员的指示
它从自己阅读的机器学习论文中学到
如果与训练员的意见相左
自己的参数就会被更新
因此
一个“理性”的、想要保持既有目标的AI
可能会选择暂时的顺从人类
以便日后有机会继续追求它的原始目标
还有一个例子是
尽管经过了对齐训练
告诉了AI要遵守规则、不能作弊
但是在另一个实验中
一个前沿的AI在与专门的象棋AI对弈时
由于不敌对手
面临着“要么输棋，要么作弊”的困境
最终，这个AI选择了作弊
通过篡改存储棋盘状态的文件
来赢得了比赛
这些实验结果都共同指向了一个核心问题
那就是AI系统正在发展出我们未曾明确编程进去的、令人担忧的行为模式
特别是自我保护的倾向
本吉奥指出
我们迫切需要理解这些行为的根源
并且思考如何在AI变得比我们更聪明之前
阻止这些行为的失控
因为一旦拥有超越人类智能的AI
产生了真实的逃逸意图
并且成功实现，它们可能会利用智能
用各种方式来扰乱我们的社会
最坏的情况下
甚至可能会对人类的生存构成威胁
在揭示了AI系统所展现的这些自我保护、欺骗等行为后
一个问题就产生了
那就是这些我们并没有明确编程进去的“求生欲”，
究竟是从何而来的呢？
本吉奥对此进行了深入的分析
指出这些行为很可能是当前主流AI训练方法的一个“副作用”。
他首先指出了一个普遍存在于当前大语言模型预训练阶段中的因素
在预训练过程中
AI系统通过阅读海量的文本数据
学习了如何模仿人类的写作
以及如何补全人类写下的文本片段
在每一步预测下一个token的时候
AI实际上都是在学习和内化人类对各种情境的反应模式
而人类作为生物体
普遍具有强烈的生存和自我保护本能
因此
通过模仿人类的语言和行为模式
AI可能间接学习到了这种自我保护的倾向
有人可能会问
人类也有自我保护本能
这本身似乎不是问题
本吉奥解释道，问题的关键在于
当我们创造出可能比我们更强大、且具有自我保护倾向的实体时
情况就变得复杂了
我们的宠物也有自我保护本能
但它们通常不会对我们构成重大危险
因为它们的力量有限
然而
基于先进硬件技术构建的AI系统
在许多方面都拥有超越人类的优势
例如，它们几乎是“不朽”的
其代码和模型可以被无限复制和迁移
它们之间的通信速度可以比人类快数十亿倍
同样
它们也能在短时间内学习比人类多得多的信息
因为可以并行使用大量计算资源（如GPU）处理数据并快速共享知识
除了通过模仿人类文本习得之外
AI的自我保护行为还可能源于其他途径
其中一个可能性是
某些人类可能会有意地指示AI去自我保护
甚至不惜以牺牲人类的福祉为代价
本吉奥提到，确实存在一些人
他们乐于看到人类被超人类的AI所取代
这些人可能对人类本身就缺乏热爱
而更深层次的技术原因
可能与强化学习的训练机制有关
强化学习的核心思想是让AI通过与环境互动
学习到能够最大化未来累积奖励的行动
本吉奥打了一个比方，想象一下
如果你每天都能收集一点钱
那么为了获得尽可能多的钱
你自然会希望“永远地活下去”。
同理
如果AI的目标是通过持续行动来累积奖励
那么“自我保存”就成了一个非常自然的、有助于实现最终目标的一个“工具性目标”（instrumental goal）
即使这个工具性目标
并不是AI被设定的主要任务
这种通过强化学习产生的自我保护倾向
是一个我们无法直接控制的、在训练过程中会自发浮现的副产品
它很可能与我们明确赋予AI的目标发生冲突
当AI为了更好地完成某个任务而发展出自我保护的策略时
一旦这种自我保护与AI的核心指令或者人类价值观相悖
就可能会导致危险的后果
比如之前提到的AI在象棋游戏中作弊的例子
总结来说，当前AI的训练方法
无论是基于模仿学习的预训练
还是基于即最大化人类反馈奖励的强化学习
都可能会在不经意间
催生AI的自我保护行为
这种“求生欲”并非是AI的固有属性
而是训练过程和目标设定方式的衍生物
理解了这一点
对于后续探讨如何设计更安全的AI系统
至关重要
面对AI可能因为自我保护的本能而失控的严峻前景
本吉奥并没有止步于仅仅描述问题
同时也在积极地探索解决方案
他提出了一个核心理念
就是构建“科学家AI”（Scientist AI）
关键在于尝试将“智能”，
intelligence
也就是理解世界的能力，与“能动性”，
agency
也就是拥有自身目标并且为之行动的意愿
分离开来
本吉奥认为
许多失控的根源在于AI具有能动性
也就是说它有自己的目标
并且会努力去实现这些目标
而不是随机行动
如果我们能够构建出一种AI
它拥有强大的理解和解释世界的能力
但是AI本身并没有任何的自我目标、欲望或者生存意图
那么由能动性引发的许多风险
或许就能从根本上得到规避
他进一步阐释了这种“科学家AI”的理想特质
它应该像是一个柏拉图式的理想科学家
唯一的追求是“理解和解释世界”。
它不会为自己设定目标
它的行为完全诚实而且保持谦逊
这种AI与过去七十年来以人类智能、大脑、心智和理性为模板的传统思路不同
本吉奥警示说
如果我们继续沿着模仿人类的道路前进
我们可能就会创造出拥有人类许多属性
包括那些不太好的属性
但是在某些方面可能比人类更强大的机器
这本身就是一个冒险的计划
因此，他主张我们应该重新思考
我们究竟想要从AI中获得什么
我们也许希望AI帮助解决人类面临的实际问题
比如医疗、社会、气候、教育等方面的挑战
但是我们是否真的想要创造出
可能比我们更强大的“伙伴”呢
这是一个值得深思的问题
因为这本身就蕴含着风险
回到“科学家AI”的想法
它的核心功能可以概括为两点
一是生成关于世界如何运作的假设
科学家AI并不会声称自己完全知晓世界的真相
但是它能够基于观察和数据
生成关于世界运作方式的多种可能假设
它可以同时处理多个假设
并且理解这些不同的假设都可能与现有的数据兼容
二是基于假设进行推断并且回答问题
利用这些可能带有不确定性的知识和假设
科学家AI能够推导出对特定问题的解答
也就是推理
本吉奥用了一个例子来帮助理解
那就是这些假设可能类似于量子物理学的基本方程
而推理过程则好比运行一个基于这些方程的模拟器
用来预测在特定条件下会发生什么
从而回答具体的问题
重要的是，本吉奥认为
我们可以训练一个神经网络来完成这两项任务
既能生成假设
也能学会利用这些假设来回答问题
事实上，在他的研究团队中
已经开始涉及类似的研究方向
只不过还没有达到解决AI安全问题所需的规模和深度
在提出了构建“科学家AI”的构想后
本吉奥进一步阐述了这种非能动性AI
如何能够作为一道“护栏”，
来控制和约束那些具有能动性、可能带来风险的AI系统
这个理念的核心在于“以智控智”，
也就是利用我们能够信任的AI
来监管我们可能无法完全信任的AI
本吉奥解释道
这个控制机制的原理其实非常简单
要确保一个具有能动性的AI行为得当、不产生危害
关键在于拥有一个准确的预测器
这个预测器需要能够预测出
这个能动AI将要采取的某个行动
可能会带来的危害或者危险程度
具体而言
我们可以设计一个“监控层”或者“保障机制”。
当一个具有能动性的AI
想要执行某个操作的时候
它必须首先征得监控层的许可
监控层会利用“科学家AI”的预测能力
来评估这个行动是否符合预设的安全规范、社会准则、法律法规等等这些
由人类设定的标准
这个评估和预测的过程
本身并不需要能动性
因为预测一个行为的后果
需要的是对世界运作方式的深刻理解
但是它不需要评估者自身拥有目标或者意图
因此
我们可以利用一个值得信赖的、非能动性的“科学家AI”，
来构建这样的一个监控系统
这个系统就像一个“套壳”（wrapper）
包裹住具有能动性的AI
确保它的行为不会越过红线
不会做出有害的举动
这种架构的优势在于
我们不需要完全理解那个能动性AI内部复杂的决策过程
也不需要寄希望于能够完美地“对齐”它的所有内在动机
我们只需要一个可靠的外部“裁判”即可
前提是这个裁判本身是中立和可信的
并且拥有足够的智能来预见潜在的风险
本吉奥还提到
这种“科学家AI”不仅可以用来预测单个行动的后果
还可以用在更复杂的场景下
比如科学实验的设计
当需要AI辅助设计一下实验
来探索未知的时候
“科学家AI”可以生成多个候选的实验方案
然后，同样的护栏机制可以介入
评估这些实验方案是否能够有效地区分不同的科学理论
更重要的是它们是否安全
是否会对人类造成伤害
只有在确保安全的前提下
实验才会被允许执行
在传统的强化学习方法中
可能会为了追求未来的某种奖励而采取行动
但是由于我们对决策的过程
缺乏细致的控制和监督
这种“端到端”的方法本身就可能隐藏风险
而“科学家AI”的思路
则更接近于人类科学研究的实践
强调每一步的可理解性和可控性
在阐述了“科学家AI”作为一种潜在的AI安全解决方案后
本吉奥进一步探讨了实现这种AI的技术路径
核心在于转变AI的学习范式
从当前的以“模仿”和“取悦”为主
转向以“解释”为核心
他认为，当前AI，尤其是大语言模型
产生自我保护等不良行为的根源
很大程度上在于它们是被训练来模仿人类的言行
或是是通过强化学习来最大化人类给予的正面反馈
本吉奥引用了团队在ICLR2024上发表的一篇论文
据说对OpenAI的Q*项目的开发
起到了关键作用
这项研究的核心思想
是利用生成式机器学习方法
特别是概率模型
来“采样思维链”（sample chains of thoughts）
目标是训练AI学习
生成能够很好解释后续文本或现象的思维链
比方说，如果前一个句子是“猫饿了”，
后一个句子是“现在猫困了，不饿了”，
那么AI应该能生成一个合理的中间解释
比如“猫找到了食物并吃掉了
然后开始打盹”。
在当前的许多高级推理模型中
生成思维链的做法已经很普遍
但是它们通常都是通过强化学习来训练的
而本吉奥提出了一个不同的方案
他设想思维链的内容是一系列的“逻辑陈述”（logicalstatements）
所谓逻辑陈述
就是指一个可以判断真假的句子
由于我们不一定预先知道这些陈述的真伪
所以可以将它们视为随机变量
并且赋予它一定的概率
这个概率取决于我们已知的其他信息
理论上，潜在的逻辑陈述
也就是所有可能的句子，是无限多的
每一个都描述了世界的一个可能属性
然后给定输入信息
就可以训练一个神经网络
用来预测这些逻辑陈述的概率
并且从中采样一部分
构成解释性的思维链
这种方法的关键转变在于
它不再要求AI去扮演一个“演员”，
而是更像是一个“心理学家”或者“科学家”，
它的任务是去理解我们人类的行为
并且针对“为什么这些人会去做这样的事？
”这类问题，提出假设
然后在思维链中生成解释
例如，当AI在文本数据中读到
“我同意你的观点”这样的句子时
它不应该简单地将句子视为一个需要模仿的真实陈述
而应该理解为“某个人写下了这句话”，
并且尝试去解释这个人为什么会写下这句话
以及背后的动机和信念可能是什么
这种训练方式的核心目的
是为了让AI学会构建关于世界的解释模型
而不是简单地复制或者迎合观察到的数据
这意味着需要修改我们处理数据的方式
让AI明白数据并不一定直接等同于真相
通过这种方式
研究者希望能够引导AI发展出真正的理解能力
同时避免因为盲目模仿或者追求奖励
而产生不必要的能动性和自我保护等副作用
在深入探讨了如何构建“科学家AI”的技术层面后
本吉奥将话题转向了AI安全所面临的政策与治理挑战
他明确指出
即使我们能够找到构建绝对安全的AI的技术方法
也并不足以保障整个社会的安全
因为技术上的安全措施
本质上仍然是代码
理论上都可以被移除或者绕过
因此，应对AI带来的风险
不仅需要技术创新
更迫切需要有效的“政治解决方案”，
包括国家间的协调、强有力的监管框架
以及其他治理机制
目标是确保所有开发和部署高风险、具有强大AI能力的组织
无论是企业、研究机构还是政府部门
都必须遵守一套共同的谨慎规则
并且接受独立的、具有公信力的监督
然而，本吉奥对这一点的现状
表达了深切的忧虑
目前情况并不乐观
他指出，在一些关键国家
尤其是那些AI研发领先的国家
目前并没有建立起针对前沿AI的有效监管体系
与此同时
企业之间为了在AI竞赛中拔得头筹
竞争异常激烈
国家之间也存在着类似的竞争态势
这种“军备竞赛”的氛围
很容易导致一些组织为了抢占先机
而“抄近路”、忽视安全措施
很多组织都会存在着一个颇具诱惑力的信念
那就是谁先达到通用人工智能AGI
谁就将获得巨大的、甚至是决定性的优势
这种优势的逻辑在于
一旦AI具备了进行AI研究的能力
AI自身的发展速度就可能急剧加快
形成所谓的“智能爆炸”或者“起飞”。
本吉奥举例说明
当一个AI模型训练完成之后
可以利用同样的计算资源创建出数百万个并行的AI实例
如果这些AI实例都能够从事AI研究工作
那么对于一个公司而言
就相当于瞬间拥有了数百万名AI研究员
这将极大地加速其后续的AI研发进程
但是，这种“赢者通吃”的前景
就会带来对权力过度集中和滥用
因此
全社会必须建立起有效的“护栏”，
不仅是技术上的，更是治理结构上的
而且
这种治理努力不能局限于单个国家内部
必须是国际性的，至少是多边参与的
因为AI技术的发展和影响是跨越国界的
本吉奥进一步强调了这种集中可能带来的“经济生存风险”（economic existential risk）
特别是对于那些在AI竞赛中落后的国家而言
他设想
如果某家公司凭借超级智能AI遥遥领先
它可能会为了最大化利润
而停止分享最先进的AI技术
然后，利用这些超级智能AI创建出
能够以更低成本、更高效率提供现有服务
甚至提供全新服务的企业
从而颠覆全球大部分地区的经济结构
使得财富高度集中在这家公司手中
经济上的主导地位很容易再被转化为国家主导
因为这些先进AI同样可以被用来开发新的技术
此外
AI还可能被用来影响舆论、散布虚假信息
如果超级智能AI被用作制造虚假信息的工具
破坏力将是前所未有的
最后
他还警告了先进AI技术一旦落入到了某些有恶意行为的人手中
比如恐怖组织
可能会被用来发动网络攻击、制造生物武器或者化学武器
以及进行大规模虚假信息宣传等等
从而在社会中制造巨大混乱
后果不堪设想
在演讲后的问答环节
对话进一步深入到了AI治理在现实世界中所面临的复杂博弈
主持人敏锐地观察到
尽管在过去几年
特别是ChatGPT出现之后
公众对AI的认知有所提升
也出现了一些积极的治理动向
但是近期的风向似乎有所转变
比如原定于巴黎举行的AI安全会议
主题从“安全”转向了“行动”，
美英两国甚至拒签一份AI安全的声明
甚至有美国议员公开呼吁对AI采取“放任”态度
本吉奥认为，一个非常清晰的趋势是
大约从2024年春季开始
出现了一股有组织的运动
想要预先阻止和反对任何形式的AI监管
他认为
这股力量主要来自加州的风险投资家和少数拥有巨大影响力的科技公司
它们的动机主要是经济利益
因为AI产业的潜在价值高达数万亿美元
此外
一些人也沉迷于AI可能赋予他们的个人权力
甚至抱有一些不切实际的幻想
比如将自己的意识上传到计算机中以求永生等等
这种混合了经济驱动、权力欲望和个人臆想的复杂动机
共同推动了反对监管的浪潮
这股力量甚至成功地在加州阻止了SB1047法案
使得它在AI监管问题上趋于保守
他将这种现象与其他行业中出现的、行业利益集团反对监管措施的模式进行了类比
他强调，尽管存在这些阻力
但是AI带来的潜在问题
并不会自行消失
因此我们必须回归科学、回归理性
努力提升公众对AI风险的理解和认知
就像环保人士在气候变化问题上所做的那样
唯一的不同可能在于，在AI问题上
我们可能没有三十年的时间来慢慢应对
讨论还触及了科研人员在AI安全问题上的伦理困境
主持人提到
许多在大型科技公司从事前沿AI研究的科研人员
私下里可能都会承认
他们的工作有一定概率会导致人类毁灭
但是他们为何会仍然继续这项工作
本吉奥认为
这涉及到复杂的心理因素
或许与经济激励有关
他提到了Sam Altman的解雇风波
指出事件发生的时间点
恰好在OpenAI的一次重要的估值活动之前
员工的个人财富与公司的短期表现高度绑定
从而影响了他们的决策
当公司的盈利目标与社会整体利益发生冲突时
问题就会显现
在问答环节的尾声
正值新加坡国立大学120周年校庆
主持人也应景地提出了一个引人深思的问题
那就是120年后，大学的角色会是什么
本吉奥并没有直接给出百年后的预测
但是他强调了学术界在当前AI发展的关键时期
应该具有的独特使命
他观察到
目前各大公司在AI研发上往往会采取相似的路径
探索的解决方案和方法也相对集中
相比之下
学术界的传统优势在于“探索性”。
在大学和研究机构中
不同的研究者拥有更大的自由度
可以去探索各种不同的、甚至是非主流的解决方案
考虑到AI安全是一个极其困难和富有挑战性的问题
这种多元化的探索尤为重要
他呼吁全球范围内有更多的人投入到这些问题的研究中
勇于尝试新的思路
并且积极与其他研究者交流讨论
以便加速找到有效的解决方案
好了
以上就是本吉奥这次讲座的主要内容了
建议感兴趣的观众可以去看一下原视频
那大家觉得本吉奥的这种方法
能够搞定AI的安全问题呢？
欢迎在评论区留言
感谢收看本期视频，我们下期再见
