大家好，这里是最佳拍档，我是大飞
我们之前做过一期节目
介绍过英伟达在推出第一代 Blackwell B200 系列处理器的时候
遇到了产量问题
并且还出现了疑似服务器过热的报告
不过，根据SemiAnalysis的最新报道
Nvidia 的第二代Blackwell B300 系列处理器似乎即将问世
它们不仅具有更大的内存容量
性能提高了50%，
而TDP仅仅增加了 200W
今天我们就来给大家简单介绍一下
不知道大家是否还记得GB200和B200是什么时候发布的
其实才刚刚过去六个月
英伟达就推出了全新一代的AI GPU
GB300和B300
新的GB300 和 B300虽然表面上听起来
好像只是渐进式的提升
但是实际的效果远超预期
这两款芯片对推理模型的推理和训练性能
能够带来巨大的提升
重要的是，随着B300的推出
整个供应链正在进行重组和转型
既得利益者可能会被重新分配
B300 GPU是基于台积电 4NP 工艺节点的全新流片
这个节点是专门针对英伟达进行优化过的 4nm 级节点
性能有所增强
因此，在产品层面上
B300能够提供比 B200高 50% 的 FLOPS
除了有200W 的额外功率增加以外
GB300 和 B300 HGX 的 TDP 分别可以达到 1.4KW 和 1.2KW
而相比之下
GB200 和 B200 的 TDP 分别为1.2KW 和 1KW
其余的性能提升将来自架构的增强和系统方面的增强
比如 CPU 和 GPU 之间的功率浮动
这个功率浮动指的是 CPU 和 GPU 之间的动态功率分配
除了 FLOPS 的增加以外
内存还从 8层HBM3E升级到了 12层
每个 GPU 的 HBM 容量增加到 288GB
不过引脚的速度没有变化
因此内存带宽仍然为每 GPU 8TB/s
需要注意的是
三星至少在未来 9 个月内
无法进入 GB300 或 B300的供应链
这次新发布的GB300 和B300
可以说是专门为推理模型进行推理而构建的
由于长序列长度的增加
导致KV Cache也随之扩大
从而限制了关键的批处理大小和延迟
因此显存的改进
对于OpenAI o3这类大模型的训练和推理至关重要
这张图展示了英伟达H100和H200
在处理1
000个输入token和19
000个输出token时的效能提升
这与OpenAI的o1和o3模型中的思维链（CoT）模式非常相似
而H100和H200的Roofline模拟
是通过FP8精度的Llama 405B模型完成的
从H100到H200的升级
主要在于使用了更大、更快的显存
这样带来了两个效果
首先，更高的带宽使得交互性能
普遍提升了43%左右
比如从H100的3.35TB/s
提升到了H200的4.8TB/s
而更大的批处理规模
也让每秒的token生成量提升了3倍
进而让成本也降低了大约3倍
其间的差异主要还是因为 KVCache 限制了总的批处理大小
但是对于运营商而言
H100和H200之间的性能与经济差异
远远超过技术参数的数字那么简单
首先
以前的推理模型时常会因为请求响应时间过长
而影响使用体验
但是现在有了更快的推理速度之后
用户的使用意愿和付费倾向都将会显著提高
其次，成本降低3倍所带来的效益
可以说极为可观
仅仅通过显存的升级
硬件就能实现3倍的性能提升
这种突破性的进展远远超过了摩尔定律、黄氏定律
或者任何已知的硬件进步速度
最后
对于性能最顶尖、具有显著差异化优势的模型来说
能够因此获得更高的溢价
像目前的一些SOTA模型的毛利率
已经超过了70%，
而面临开源竞争下的普通模型
利润率仅有不到20%。
目前来看
只有推理模型可以突破单一的思维链限制
以及通过扩展搜索功能来提升性能
比如o1的Pro模式和o3
从而让模型能够更加智能地解决问题
提高每个GPU所带来的收益
当然
英伟达也并非唯一一家能够提供大容量显存的厂商
像ASIC和AMD都具备这样的能力
而AMD更是凭借更大的显存容量
比如MI300X的192GB、MI325X的256GB
以及MI350X的288GB，占据了优势地位
不过，老黄手里还有一张绝对王牌
那就是NVLink
NVL72在推理领域的核心优势在于
它能让72个GPU以超低的延迟协同工作
同时共享显存
而这也是全球唯一具备全连接交换（all-to-all switched connectivity）和全规约运算（all reduce）能力的加速器系统
因此
英伟达的GB200 NVL72和GB300 NVL72
对以下这些关键能力的实现极其重要
包括实现更高的交互性以及更低的思维链延迟时间；
支持
72个GPU的分散KV Cache
支持更长的思维链来提升智能水平；
相比传统的8 GPU服务器
NVL72具备更好的批处理扩展性；
以及支持更多的样本并行搜索
从而提升模型的准确性和性能
总体而言
NVL72可以在经济效益上实现10倍以上的提升
尤其是在长推理思维链场景中
而且
NVL72还是目前唯一能够在高批处理下
将推理长度扩展到10万个token以上的解决方案
针对新发布的GB300和B300
英伟达这次还重构了整个供应链
在之前GB200的时候
英伟达会提供完整的Bianca主板
这个主板包含了Blackwell GPU、Grace CPU、512GB 的LPDDR5X内存
以及集成在同一PCB上的电压调节模块VRM
同时还提供交换机托盘和铜质背板
但是随着GB300的推出
供应链的结构和产品内容将发生重大的调整
在新的GB300方案中
英伟达只会提供三个核心组件的供应
分别是搭载在「SXM Puck」模块上的B300芯片
用BGA封装的Grace CPU
以及由美国初创企业Axiado提供的基板管理控制器HMC
用来取代原有的Aspeed方案
所以对于终端客户来说
将需要直接采购计算板上的其他组件
同时，二级内存的方案
将从焊接式的LPDDR5X
改为可更换的LPCAMM模块
主要由美光公司供应
交换机托盘和铜质背板仍然由英伟达全权负责
这样一来
相比之前只有纬创 Wistron和富士康工业互联网FII能够制造Bianca板卡的局面
SXM Puck的方案打破了原有的市场格局
它的采用为更多的OEM和ODM厂商参与计算托盘的制造
创造了机会
显然，纬创在ODM领域受到的影响最大
Bianca主板份额将会显著下降
而富士康工业互联网通过独家生产SXM Puck及其插座
抵消了Bianca主板业务的损失
同时
英伟达也正在寻求Puck和插座的其他供应商
但是目前还没有确定新的订单
其次，另一个重大转变是VRM供应链
尽管SXM Puck上仍然保留了部分VRM组件
但是主要的板载VRM
将由超大规模数据中心运营商和OEM
直接从供应商采购
这个转变将会带来市场格局的重塑
为新的供应商创造更多的机会，同时
芯源系统公司MPWR的市场份额将会因商业模式的转变而下降
所以这个消息一出来
芯源系统公司的股价就掉了37%，
可见市场反应相当迅速
第三
英伟达在互联技术也取得了突破
GB300平台搭载了800G ConnectX-8网卡
可以在InfiniBand和以太网上提供双倍的扩展带宽
相较于上一代的ConnectX-7网卡
ConnectX-8具有许多明显的优势
比如带宽提升了2倍
PCIe通道数从32个增加到了48个
以及支持风冷MGX B300A等创新性的架构设计
以外
ConnectX-8网卡还原生支持SpectrumX
不需要再借助低效的、之前400G产品使用的Bluefield 3 DPU方案
在2024年的第三季度
受到GB200和B200发布延迟的影响
大量订单转向了英伟达价格更高的新一代GPU
截至上周
所有的超算中心都已经决定采用GB300的方案
这个决策主要基于两点考虑
一
GB300提供了更高的FLOPS算力和更大的显存容量
二、客户拥有更多的系统定制自主权
在此之前
由于上市的时间压力以及机架、散热和供电密度的重大调整
超算中心很难对GB200服务器进行深度的定制
这就迫使Meta完全放弃了同时向博通和英伟达采购网卡的计划
转而完全依赖英伟达
类似地
谷歌也放弃了自研网卡的方案
转而采用英伟达的解决方案
但是
这对于那些擅长优化从处理器到网络设备
甚至到螺丝和钣金等各个环节成本的超算中心数千人的研发团队来说
就显得很尴尬了
其中，亚马逊的案例最具代表性
他们选择了一个次优的配置
导致总拥有成本
也就是TCO超过了原来的参考设计
由于使用了PCIe交换机和需要风冷的200G弹性网卡
亚马逊无法像Meta、谷歌、微软、甲骨文、xAI和Coreweave那样
部署NVL72机架
由于受到内部网卡方案的限制
亚马逊只能被迫采用NVL36的架构
却因为更高的背板和交换机成本
推高了每个GPU的支出
总体而言
因为受到定制化方面的限制
导致亚马逊的配置方案并不理想
而GB300的推出
可以说是为超算中心提供了更大的自主权
比如可以自主定制主板、散热系统等等
这就使得亚马逊能够开发自己的定制主板
把之前需要风冷的组件
比如Astera Labs PCIe交换机
整合进水冷系统
随着更多的组件采用水冷设计
再加上K2V6 400G网卡将在2025年第三季度
实现规模化量产
亚马逊有望重返NVL72架构
从而显著提升TCO效率
不过，这也带来了一个显著的挑战
那就是超算中心需要投入更多的资源来进行设计、验证和确认工作
这无疑是超算中心要面临的最复杂的系统设计
虽然部分超算中心能够快速地完成设计
但是设计能力较差的团队会明显落后
总的来说
尽管市场传闻有公司取消了订单
但是SemiAnalysis观察到
由于设计进度较慢
微软可能是最晚部署GB300的公司之一
他们在第四季度仍在采购GB200
而随着部分组件从英伟达转移到ODM
客户的总采购成本也将出现较大的差异
这不仅会影响ODM的收入
更重要的是会导致英伟达全年的毛利率产生波动
伴随着新的GB300和B300
英伟达也推出了新的定价策略
从而直接影响到了Blackwell系列的利润
虽然显存方案从SK海力士和美光的8层HBM3E堆栈
升级到了12层
显存容量获得了显著的提升
但是这个升级
直接导致英伟达的BOM成本
增加了大约2,500美元
这部分成本的增加主要来自更高的容量、堆栈层数增加所带来的每GB显存溢价
以及封装良率下降带来的额外成本
而且随着推理模型对于显存容量和带宽需求的增加
封装良率的下降趋势也将越来越明显
总体来说
GB300的平均售价相比于GB200
会提高大约4,000美元左右
其中HBM的成本会增加大约2
500美元，增量的利润率不足40%，
而GB200整体的利润率将维持在70%的中低水平
不过
由于英伟达减少了整体的供应内容
转而让超算中心自行采购
因此英伟达也能实现成本上的平衡
首先
英伟达不再提供每个Grace CPU配套的512GB LPDDR5X内存
这抵消了大部分额外的HBM成本支出
其次，PCB的成本节省最为显著
综合各项因素，在平均销售价格提升4
000美元的同时
英伟达的物料成本仅仅增加了1
000美元多一点
相对于GB200
GB300的增量毛利率可以达到73%，
这意味着在良率能够保持稳定的情况下
GB300的利润水平将基本持平
这个结果虽然看起来好像很一般
但是值得大家注意的是
HBM的升级周期通常会导致利润率下降
比如H200、MI325X
但是这一次英伟达打破了这个惯例
同时
随着各项工程技术问题的逐步解决
整体的良率将会进一步地提升
等度过Blackwell初期的产能爬升以后
整体利润率预计将会在明年逐步得到改善
好了
以上就是SemiAnalysis对英伟达最新的GB300、B300的介绍了
根据报道
B300 将在 B200 上市大约半年后上市
还是相当值得期待的
各大科技巨头也已经盯上了这个性能怪兽
不过呢就在上周
天风国际的分析师郭明錤却在研报中曝出
B300/GB300的DrMOS存在着严重的过热问题
很可能会影响B300/GB300的量产进度
具体新产品的发布进度如何呢
我们也会保持持续的关注
感谢大家观看本期视频
我们下期再见
