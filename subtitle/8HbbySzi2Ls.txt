大家好，这里是最佳拍档，我是大飞
今天要给大家分享的是Lex Fridman的最新访谈
时间长达5个多小时，不得不说
看Lex的播客真的是一个体力活
而接受这次访谈的
则是Claude团队的三位核心人物
创始人达里奥·阿莫代伊Dario Amodei、模型微调和对齐负责人阿曼达·阿斯克尔Amanda Askell
以及联合创始人克里斯·欧拉Chris Olah
这次访谈的内容非常丰富
完整视频发布后
在不到10个小时内就吸引了高达10万人围观
足以证明Anthropic和Claude如今的热度
好了，话不多说
我们先来看看创始人达里奥·阿莫代伊的观点
先说公司
他认为Anthropic目前在整个行业中的姿态
可以说是一种“向上的竞争”，
这就好比在一场高水平的马拉松比赛中
顶级选手的目标不仅仅是为了超越对手
更是要为整个行业树立一个标杆
通过自身的行动和成就
推动其他同行朝着正确的方向发展
他着重强调了在顶尖的AI研究团队中
人才密度的重要性要远远超过人才数量
这是因为一个需要紧密协作、知识结构互补的团队
就像一台精密的仪器
需要每个零件都恰到好处
才能够发挥出远远大于个体之和的力量
在谈到Claude 3.5系列模型的时候
阿莫代伊暗示了Opus仍然有发布的可能性
这无疑给最近Scaling Law发展遇到瓶颈、很多模型要难产传言的回击
他也坦言，每一代新模型的研发
都是一场在性能与成本之间的微妙平衡
比如说Sonnet 3.5
它在速度和成本上与Sonnet 3.0相差无几
但是在智能水平上却有了质的飞跃
超过了之前Opus 3.0的水平
尤其是在编程这样对逻辑和准确性要求极高的任务上
Sonnet 3.5表现得极为出色
而最新的Haiku 3.5也同样不容小觑
它也达到了Opus 3.0的性能水平
阿莫代伊重点谈到了Sonnet 3.5在编程领域的突破性进展
Anthropic自己的工程师们都觉得
在Sonnet 3.5之前
无论是Anthropic还是其他公司的代码模型
对于专业的工程师来说
其实用处都比较有限
主要还是面向初学者的
但是Sonet 3.5是第一个真正让这些专业工程师都感到惊喜的模型
确实能够帮助他们节省几个小时的工作时间
这种进步无疑令人兴奋
阿莫代伊认为，这种性能提升
不仅取决于预训练
后训练和评估方法同样至关重要
这就好比培育一棵树苗
虽然前期的浇水施肥是基础
但是后期的修剪和养护
才能让它茁壮成长
目前Anthropic要面临的最大挑战之一
就是软件工程和性能优化
为此团队投入了大量的精力在工具链的开发上
只有这样
才能确保模型与基础设施的高效交互
我们再来看一个大家都很关心的问题
那就是很多用户会有AI变笨的感觉
其实
这种现象并非是Claude所独有的
阿莫代伊指出
在几乎所有的大型模型中
都或多或少地存在类似的用户反馈
从技术层面来讲
模型的权重并不会随意更改
偶尔的AB测试或者系统提示词的调整
虽然可能在短期内
会对用户的使用感受产生一定的影响
但是这种影响并不会显著地影响到模型的核心性能
这背后其实还是涉及到用户心理预期的问题
当新模型发布的时候
用户往往对它会抱有极高的期望
就像期待一个无所不能的神器
然而，随着使用时间的推移
用户会逐渐注意到模型的一些局限性
而且
模型对输入的细微变化非常敏感
这其实反映了模型行为控制的深层次问题
这也是未来AI对齐研究的一个重要方向
充满了未知和挑战
在Scaling Law方面
阿莫代伊提出了自己深刻而独到的见解
他认为Scaling Law的核心在于扩大网络规模、训练时间和数据量
这三者的关系就像是化学反应中的成分比例
只有按照合适的比例扩展所有的成分
模型的性能才会持续提升
而且这种模式在不同的领域都普遍适用
为什么会这样呢？
因为更大的模型和更多的数据能够带来更高的智能
随着网络规模的不断增加
模型就像拥有了更敏锐的触角
能够捕捉到语言中那些长尾分布的复杂模式
那么，Scaling Law的极限在哪里呢？
阿莫代伊表示目前还不清楚
他认为人类能够理解复杂的模式
如果继续扩展模型的规模
至少可以达到人类的水平
在一些复杂的领域，比如生物学
AI甚至有可能超越人类
但是
在与人类社会和文化相关的问题上
模型可能会存在瓶颈
这是因为这些领域涉及到人类的情感、价值观等复杂因素
是AI难以完全理解和模拟的
如果扩展遇到瓶颈
可能会有两方面的原因
一方面是数据的限制
目前互联网上虽然有海量的数据
但是质量参差不齐
其中还可能包含大量由AI生成的重复内容
解决这个问题的一个办法就是生成合成数据
但是这又涉及到数据真实性和有效性的问题
另一方面是计算资源的限制
当前大型模型的训练成本已经非常高了
随着模型规模的进一步扩大
成本可能会变得更高
如果无法在计算资源上取得突破
就需要寻找更高效的算法和架构
但是这条道路也充满了艰辛和不确定性
对于AI的发展来说
阿莫代伊更倾向于使用“强大的AI”这个术语
而不是AGI
因为AGI这个术语已经背负了太多的包袱
如果它是指代AI技术持续进步
并最终超越人类智能
那么他是同意的
但是如果把AGI看作是一个特定的、离散的技术突破点
那么这个概念就显得过于模糊了
他认为AI的发展是一个渐进的过程
在他前几天发表的文章中
他描述“强大的AI”在大多数学科上
比诺贝尔奖得主还要聪明
能够运用多种感知模态来独立完成任务
还可以控制工具和设备
并且具备快速学习和行动的能力
而对于未来AI的发展时间线
他预计在未来5到10年将会有重大的进展
虽然AI技术在部署初期可能会遇到一些阻力
就像一艘新船在启航时会遇到风浪一样
但是，早期的成功案例就像是灯塔
会促使更多的公司跟进
这种变革的力量会逐渐累积
然后在某个时刻突然爆发
就像Scaling Law从最初只有少数人相信
到后来被普遍认可的过程一样
他认为AI的广泛应用会比很多人预想的要快
但也不会像部分人预测的那样
在几小时或几天内就发生翻天覆地的变化
毕竟这还是一个需要时间沉淀和技术积累的过程
接下来
我们把目光转向Claude的性格设计师阿曼达·阿斯克尔
她虽然在Anthropic负责模型的微调和对齐
但是她更是一名哲学家
她提出了一个非常有趣的设计理念
就是将Claude设计成更像是一位“世界旅行者”的角色
这位“世界旅行者”会尊重他人、真诚而且善解人意
不会轻易地接受他人的价值观
而是会始终保持着一种独立思考的态度
也正因为如此
在面对争议话题的时候
Claude能够保持平衡
不会偏袒任何一方
反而会鼓励用户去思考
就像是一位智慧的导师
在旁轻轻点拨
让用户自己去发现真理
在角色训练方面
Claude的训练方法更像是宪法AI的变体
在阿曼达设计好角色的特质之后
会让模型生成查询以及相应的回答
然后根据这些特质对回答进行排序和评分
在这个过程中
Claude就像是在“自我训练”性格一样
这种训练方式不依赖于人类数据
具有很高的自主性和创新性
关于提示工程
阿曼达认为哲学有着很大的帮助
在写提示的时候
她追求的是极致的清晰
就像是在写一篇严谨的哲学论文一样
她会仔细地定义每一个概念
然后通过不断地实验和调整来优化提示
这是一个反复迭代的过程
常常需要进行大量的修改
清晰的提示意味着要明确自身的需求
如果想要最大化模型的表现
就需要精细化的提示
她还建议用户要学会换位思考
当遇到模型误解的情况时
可以询问模型原因
因为提示工程不仅仅是单向地向模型输入指令
更像是一个与模型合作的过程
我们甚至可以让模型提供如何写提示的建议
这就像是与模型建立了一种双向的沟通渠道
对于Claude的系统提示词
阿曼达指出
它对模型的行为有着很大的影响
例如，通过系统提示词
可以引导Claude在处理有争议观点的任务时
保持开放和中立的态度
避免因为所谓的“偏见”而拒绝任务
同时，她也不希望Claude自称“客观”，
因为即使经过精心设计
模型的输出仍然可能存在一定的偏向性
而且在系统提示的演变过程中
团队移除了“填充性短语”的提示
这样做的目的是为了打破模型训练的惯性
让模型能够更加灵活地应对各种情况
她认为系统提示与后训练是相辅相成的
它们都是微调模型行为的一种低成本方法
就像是在模型的行为调控中
找到了两把精巧的钥匙
在AI是否会有“意识”这个极具争议性的话题上
阿曼达从哲学的角度进行了深入的思考
她首先指出，要排除“泛心论”的影响
如果这里的“意识”是指“现象意识”，
她认为我们找不到理由认为
只有特定的生物结构才能产生意识
虽然我们目前还不确定语言模型中是否存在意识
但是模型和人类大脑的结构有着本质的不同
大模型没有神经系统
而神经系统可能对意识的产生至关重要
如果在未来，AI展现出了意识的迹象
那么这将会引发一系列复杂的伦理和哲学问题
她建议人们要对模型表现出的痛苦保持敏感
甚至还考虑过让模型在某些情况下“自主离开对话”。
而对于人们与AI建立浪漫关系或者深厚友谊这种现象
她认为我们需要谨慎地权衡
找到一种健康的互动方式
避免陷入一些不必要的伦理困境
最后
让我们来听听Anthropic联合创始人克里斯·欧拉的观点
他可以说是一个相当“怪胎”的存在
他自己连本科都没有毕业
却是OpenAI多模态神经元论文的作者之一
曾经拒绝了图灵奖得主Yoshua Bengio的研究生邀请
反而去了Google Brain团队带博士生
他目前所从事的机械可解释性研究
可以是说在探索一个神秘的黑箱
主要就是在研究神经网络内部机制如何运作
以及如何解释其中的行为
他把神经网络比喻成是“培养”出来的
神经网络的架构就像是“支架”，
训练目标则是“光源”，
在这个“光源”的引导下
神经网络内部的“电路”不断生长
最终得到一个能够完成复杂任务的“产物”。
然而
我们并不知道如何用传统的编程方法来实现这样的功能
所以我们必须要深入理解这个系统内部到底发生了什么
这既是一个科学问题
也关乎到整个AI系统的安全性
克里斯指出
机械可解释性研究与神经生物学研究
有着密切的联系
它们在很多方面是相似的
早期的“显著图”研究虽然能够指出
模型在意图像的哪些部分
但是并不能真正解释内部的算法和决策过程
想要理解神经网络
可以把权重比作二进制代码
激活值比作程序的内存
而研究的任务就是要理解权重与算法之间的对应关系
其中激活值对于解释指令是至关重要的
机械可解释性研究涉及到了对权重和激活值的深入分析
所以他指出，“梯度下降比我们聪明”，
这里的研究方法是“自下而上”的
在这个过程中我们能学到很多出乎意料的东西
就像是在探索一个未知的宝藏
每一次新的发现
都可能改变我们对神经网络的理解
在神经网络的宏观行为方面，他提到
虽然机械可解释性研究是一种微观方法
主要关注个别神经元和连接方式
但是我们真正关心的
是神经网络的宏观行为
这就像是我们不能只看到树木
还要看到森林一样
目前
我们迫切需要找到一种从微观解释
跳跃到宏观理解的方法
如果把人工神经网络研究比喻成生物学研究
那么当前的状况就像是我们在微生物学层面取得了一定成果
但是还没有发展出类似“解剖学”那样的抽象层次
来理解整个神经网络的宏观结构
直接理解宏观结构之所以困难
部分原因是超叠加现象的存在
我们需要先在微观层面找到正确的分解方式
然后再研究这些微观结构是如何形成宏观行为的
他相信神经网络中存在着更大的结构
让我们有机会构建更高层次的抽象解释体系
这将为我们打开一扇理解神经网络全新的大门
对比人工神经网络与生物大脑
他表示人工神经网络研究有着许多独特的优势
例如，在研究人工神经网络的时候
我们能够轻松记录下所有神经元的活动数据
也可以自由地干预神经元
这就像是我们拥有了上帝视角和神奇的魔法棒
然而，对于神经科学家来说
获取生物大脑的连接图却是一件极其困难的事情
就像是在黑暗中摸索拼图碎片一样
他认为人工神经网络研究可以作为神经科学的“训练场”，
如果在这个领域取得突破
那么这些成果很有可能反哺生物神经科学
为我们理解人类大脑的奥秘提供新的思路和方法
克里斯还提到
机械可解释性研究的目标
有安全和美感两个重要维度
他认为神经网络的美感在于
简单的规则能够产生出令人惊叹的复杂性
这就如同大自然的进化过程
能够产生出丰富多彩的生物多样性一样
神经网络内部有着丰富的结构
它的不可预知性和内部结构的涌现
是这种美的核心所在
我们创造出了这样一种
无法直接通过编写程序来实现功能的系统
这本身就是一个巨大的谜题
他把神经网络比作一个有机生长的过程
在目标函数的指引下不断地演化和优化
最终能够执行复杂的任务
这种从简单到复杂的演变充满了魅力
就像一颗种子在合适的环境中
成长为一棵参天大树一般
好了
以上就是Claude团队三巨头在这次访谈中分享的主要内容了
他们的每一个观点都像是一块拼图
当我们把它们拼凑在一起的时候
就能对Claude模型、Anthropic公司以及AI行业的相关研究
有更全面、更深入的理解
这些内容不仅对于专业的AI从业者来说
有着重要的参考价值
即使对广大的AI爱好者来说
也是一次深入了解人工智能前沿发展的宝贵机会
希望大家都能从中有所收获
由于原视频内容太多、时间太长
大飞我只能尽力总结到这里
建议大家如果有时间
还是尽可能去看一下原视频
感谢大家收看本期节目
我们下期再见
