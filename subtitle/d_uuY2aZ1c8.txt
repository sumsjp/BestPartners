大家好，这里是最佳拍档，我是大飞
上一期视频我们介绍了黄仁勋在CES上最新发布的内容
其中有一个Cosmos世界模型平台
广受大家的关注
正好英伟达除了开源了模型之外
还公开了 Cosmos 的技术报告
今天大飞就来给大家解读一下
我们先再来介绍一下Cosmos
简单来说
Cosmos 是一个世界模型平台
上面有一系列开源、开放权重的视频世界模型
参数量从 4B 到 14B 不等
这些模型的作用非常明确
就是为机器人、自动驾驶汽车这些需要在物理世界中运行的 AI 系统
生成大量照片级真实的、基于物理的合成数据
来解决这些领域数据严重不足的问题
这次英伟达的 Cosmos 平台一次发布了 8 个模型
这些模型在 2000 万小时的视频数据上进行训练
分为扩散连续 token和自回归离散 token两种模型
支持通过文本生成视频
以及文本+视频生成视频两种生成方式
Cosmos一共包含了三种规格的模型
分别是Nano、Super、Ultra
与VideoLDM基准相比
Cosmos世界模型在几何准确性方面表现更优
而且在视觉一致性方面可以持续超越VLDM
姿态估计成功率最高甚至可以飙升到14倍
黄仁勋在CES的主题演讲中也表示
机器人技术的ChatGPT 时刻即将到来
但是因为并不是所有的开发者都具备训练自己的世界模型的专业知识和资源
所以英伟达创建了Cosmos
目的是为了让物理 AI 普及化
让每个开发者都能用上通用机器人技术
英伟达这次公开的技术报告
主要就是介绍了Cosmos 世界基础模型平台
简称WFM的工作原理
它一共包含了四大功能模块
分别是扩散模型、自回归模型、视频分词器
以及视频处理与编辑流程
在论文中
研究人员主要关注的是视觉世界基础模型
因为在这种模型中
观察结果可以以视频形式呈现
扰动可以以各种形式存在
简单来说
研究人员提出了一个预训练加后训练的范式
将WFM分成了预训练 WFM 和后训练 WFM
为了建立预训练 WFM
他们利用了大规模的视频训练数据集
让模型可以接触到各种不同的视觉体验
从而能够生成高质量、具有一致性的3D视频
得到一个通用的世界模型
而为了建立后训练 WFM
他们使用从特定物理AI环境中收集的提示-视频对数据集
对预训练WFM进行微调
从而得到用于特定目的的WFM
这种预训练和后训练相结合的策略
为构建物理AI系统提供了一种高效的方法
由于预训练WFM提供了良好的基础
后训练的数据集可以相对较小
构建出来的世界基础模型大概是这样的
假设x_0:
t为从时间0到t的真实世界视觉观测序列
而c_t为对世界的扰动
那么WFM能够根据过去的观测x_0:
t和当前扰动c_t
来预测时间t+1的未来观测值
而对于视频来说，x_0:
t就是一个RGB视频
c_t可以是多种形式的扰动
比如物理AI的动作、随机扰动
或者是描述扰动的文本等等
除此以外，我们都知道
数据决定了一个AI 模型的上限，因此
为了构建一个高上限的预训练WFM
研究人员还开发了一个基于Ray框架的视频数据处理流程
用它来定位具有丰富动态效果和高视觉质量的视频部分
来促进模型学习编码在视觉内容中的物理知识
这个视频数据处理流程包括5个主要步骤
分别是分割、过滤、标注、去重
以及分片
这些步骤都经过了专门的优化
从而提高数据质量
并且满足模型训练的需求
利用这个编辑流程
研究人员从长达 2000 万个小时、分辨率从720p到4k不等的原始视频集合中
提取了大约 1 亿个视频片段
片段长度从 2 秒到 60 秒不等
其中涵盖了各种物理AI应用
最主要的类别
包括20%的自然动态
16%的空间意识和导航
16%的手部动作和物体操作
11%的驾驶和10%的人体动作和活动
对于每个训练片段
他们使用视觉语言模型VLM
为每256个帧提供一个视频描述
因为视频处理是计算密集型的工作
所以研究人员还利用了GPU上的 H.264 视频编码器和解码器
来进行解码和转码
在报告中
研究人员探讨了两种用来构建预训练 WFM 的可扩展方法
这两种方法分别是基于 transformer 的扩散模型和自回归模型
扩散模型是通过逐步去除高斯噪声视频中的噪声来生成视频
而自回归模型则是基于之前的生成内容
按照预设顺序来逐段生成视频
这两种方法都能够将困难的视频生成问题
分解为更加容易解决的子问题
对于基于扩散的WFM
预训练包括两个步骤
分别是从文本到世界生成预训练
以及从视频到世界生成预训练
而对于基于自回归的 WFM
预训练包括两个步骤
分别是基本的下一个token生成
和文本条件的视频到世界生成
但是不管怎样
扩散模型和自回归模型
都是使用token来表示视频的
只不过前者使用向量形式的连续 token
后者使用整数形式的离散 token
研究人员注意到
视频的token化其实是一个非常复杂的过程
因为视频包含了丰富的视觉世界信息
所以为了便于学习世界基础模型
就需要将视频压缩为紧凑的token序列
同时还有最大限度地保留视频中的原始内容
因为世界基础模型训练的计算复杂度
会随着 token 数量的增加而增加
为此，研究人员提出了Cosmos分词器
这是一组视觉分词器
其中包括用于图像和视频的连续和离散分词器
Cosmos分词器可以提供卓越的视觉重建质量和推理效率
并且提供一系列压缩率
来适应不同的计算限制和应用程序需求
具体来说
Cosmos分词器采用了轻量化而且计算高效的架构
并且结合了时间因果机制
通过使用因果时间卷积层和因果时间注意力层
模型可以保留视频帧的自然时间顺序
从而通过单一统一的网络架构
实现图像和视频的无缝分词
通过在高分辨率图像和长时视频上直接训练分词器
可以不受类别或宽高比的限制
包括1:1、3:
4、4:3、9:16和16:9等
而且在推理阶段
它对时间长度不敏感
能够处理超出训练时时间长度的视频分词
评估结果表明
Cosmos分词器在性能上显著超越了现有的分词器
不仅质量更高
而且运行速度最快可达12 倍
此外
它还可以在单块80G显存的英伟A100 GPU上
一次性编码长达8秒的1080p视频和10秒的720p视频
而且不会耗尽内存
接下来
研究人员针对预训练的 WFM 进行微调
从而获得可以适用于各种物理 AI 任务的后训练 WFM
在后训练中
研究人员以相机姿态作为输入提示词
从而创建出了一个可导航的虚拟世界
用户可以通过移动虚拟视点来探索所创建的世界
随后
研究人员在由视频动作序列组成的各种机器人任务中
对WFM进行微调
结果表明，通过利用预训练的WFM
可以根据机器人采取的行动
更好地预测世界的未来状态
研究人员在论文展示了两个任务
分别是基于指令的视频预测
以及基于动作的下一帧预测
对于基于指令的视频预测
输入是机器人当前的视频帧以及文本指令
输出是预测的视频
研究人员创建了一个名为Cosmos-1X的数据集
其中包含了大约200小时的、由EVE机器人捕捉的第一视角视频
包括导航、折叠衣物、清洁桌面、拾取物体等等
基于动作的下一帧预测
输入是机器人的当前视频帧
以及当前帧与下一帧之间的动作向量
输出是预测的下一帧
目的是为了展示机器人执行指定动作的结果
团队使用了一个名为Bridge的公开数据集
包括大约20
000个第三人称视角的视频
展示了机器人手臂在厨房环境中执行不同任务的过程
为了展示了如何对预训练的WFM进行微调
研究人员还创建了一个适用于自动驾驶任务的多视角世界模型
他们策划了一个内部数据集
称为真实驾驶场景RDS数据集
包含大约360万个20秒的环视视频片段
这些视频都是通过英伟达的内部驾驶平台录制的
随后，研究人员使用RDS数据集
对Cosmos-1.0-Diffusion-7B-Text2World模型进行微调
打造出了一个多视角的世界模型
最后
因为英伟达开发WFM 的预期用途
是作为物理AI的构建者
所以为了在使用的时候
更好地保护开发人员
研究人员还开发了一个功能强大的防护系统
其中包括一个用来阻止有害输入的前置防护系统
以及一个通过视频内容安全分类器和面部模糊过滤器
来阻止有害输出的后置防护系统
总的来说
我们可以把世界基础模型WFM
看做是一种能够模拟物理世界的强大神经网络
它可以从文本/图像输入数据
生成详细的视频
并且通过将当前状态与动作相结合
来预测场景的演变
WFM不仅能够帮助开发人员想象许多不同的环境
模拟未来
做出更好的决策
另一方面
构建世界模型通常需要收集大量的数据集
不仅耗时，成本也高
而WFM可以通过生成合成数据来增强训练的过程
此外，物理测试的风险巨大
比如一台价值数十万美元的机器人原型
任何失误都可能带来重大的损失
而有了WFM模拟的3D环境
研究人员就可以在受控的环境中
训练和测试物理AI系统
总之，物理模拟世界的一切
英伟达Cosmos都能帮你生成出来
正如老黄在CES会上强调的一个新概念
以后会需要三台计算机
分别是一台是DGX，用来训练AI
另一台AGX
用来部署AI
最后一台，便是Omniverse+Cosmos了
在未来
也许每一个工厂都会拥有数字孪生
通过将Omniverse和Cosoms结合
来生成一大批的未来场景
好了
以上就是对这篇论文的大概解读了
感谢大家的收看，我们下期再见
