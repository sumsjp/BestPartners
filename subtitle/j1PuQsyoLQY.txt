大家好，这里是最佳拍档，我是大飞
这几天最引人关注的
莫过于“华为大模型抄袭”的话题了
事件的主角
是华为在6月30日正式开源的盘古大模型（Pangu Pro MoE）
起因则是由HonestyAGI在GitHub中发布的一项研究
通过论文式的推理
证明盘古大模型抄袭了阿里巴巴的Qwen-2.5 14B模型
在研究中
HonestyAGI提出了通过分析模型的注意力参数的标准差
来识别所谓的“模型指纹” （LLM-Fingerprint）
并且基于这个方法对华为Pangu Pro MoE模型的来源
进行了分析
研究发现
华为Pangu Pro MoE模型与Qwen-2.5 14B的标准差相关性高达92.7%，
因此认为盘古模型可能是基于Qwen-2.5“升级改造”而来的
而非从头训练
说难听点就是直接抄袭了Qwen-2.5
除此之外，HonestyAGI还发现
盘古大模型官方在GitCode中发布的代码中
甚至还包含了Qwen 2024的许可证
不过
HonestyAGI的这项研究一经发布
便引发了不少的争议
有人认为它通过种种证据
已经实锤了盘古大模型的抄袭
但是另一方面
也有人对这种“模型指纹”方法的专业性提出了质疑
更是指出HonestyAGI的这篇论文可能是AI写的
那么这项研究到底够不够靠谱
盘古大模型团队到底有没有抄袭呢
今天大飞就来带大家深入了解这件事的来龙去脉
我们先来看看HonestyAGI具体指出了哪些证据
首先，在论文中
HonestyAGI提出的模型 “指纹” ，
指的是一种用来识别和区分模型的技术
目的是为模型提供一个独特的标识
来解决模型知识产权保护、来源追溯和相似性分析等等问题
不同的模型 “指纹” 方法
在实现方式和应用场景上也会各有差异
所以HonestyAGI团队针对大语言模型
通过分析各层的注意力参数
比如Q、K、V的标准差来形成 “指纹”。
再通过对每个Transformer层提取相关矩阵并计算标准差
然后跨层进行归一化来生成特征签名
就可以用来识别模型的谱系
这个方法的特点包括几点，1、健壮性
能够在大量持续的训练后
还能保留指纹的特征
2、内在性
指纹特征是由模型架构自然产生的
3、简单性
只需对参数矩阵使用torch
std()计算即可
通过对盘古模型的QKV偏置分析
结果显示
Pangu与Qwen2.5-14B的Q、K、V投影偏差模式几乎相同
早期层均出现了特征性峰值后的收敛
而这个设计可以说是Qwen 2.5之前独有的特色
多数的开源模型
包括Qwen3现在都已经弃用了
紧接着
团队对注意力层的归一化权重进行分析后
进一步印证了它们之间的相似性
盘古和Qwen2.5-14B在各层的表现趋势非常一致
呈现出了类似的初始化方式和收敛过程
这也让它们在行为上明显区别于Qwen2-57B-A14B和Qwen3-30A3B这样的其他模型
除此之外
团队还研究了每一层的激活值大小
为此
HonestyAGI从The Pile测试集中
随机抽取了1000个批次的样本
并且计算了每层的激活范数
同时采用了统一的归一化方法
每个批次包含8条序列，长度为1024
目前初步的结果显示
盘古模型和Qwen模型的表现依然非常相似
说明它们在计算方式上有很大的相似之处
为了验证“模型指纹”是不是存在一定的偶然性
HonestyAGI还对Qwen和混元A13B进行了对比
结果发现两者在不同层级的内部表现差异很大
说明它们的架构和学到的知识表示完全不同
显然
HonestyAGI提供的这组对比数据
是为了证明它的测试方法是可信的
不过在论文发表后
许多人对HonestyAGI团队提出的“模型指纹”方法
在技术层面上也提出了质疑
有人认为这个方法过于简单粗暴
存在cherry picking
也就是选择性挑选数据的空间
而且使用参数的标准差来判定模型的相似度
这种方法并不科学
也有人认为，在深度学习领域
模型结构的创新固然重要
但是模型的参数更多是依赖算力和数据
华为有足够的算力来重新训练大模型
因此没必要套用Qwen的参数
还有人指出
“模型指纹”的方法存在着多处的技术缺陷
比如仅公开了少量的模型对比样本
缺乏大规模的基准测试，同时
相同参数量的 MoE 模型可能会因为架构约束
产生相似的曲线
不能仅凭相似曲线就判定抄袭
除此之外
对于HonestyAGI这个研究团队
网友们也提出了质疑
原论文作者除了这篇文章以外
没有其他任何的科研成果
联名的五个人既没有留邮箱
也无法在Google Scholar上找到任何的信息
作者自称是哥斯达黎加大学的韩国学生
却使用的是outlook邮箱
而且说话有明显中式英语的味道
所以大家怀疑论文作者的身份是假的
更重要的一点是
论文引用的参考文献中
竟然还存在多处不实的参考文献
以至于有人认为
全篇文章都极有可能是用AI来生成的
对于这次抄袭风波，涉事“主角”，
盘古Pro MoE模型背后的团队
也在第一时间站出来进行了回应声明
在声明中强调了他们是全球首个面向昇腾硬件平台设计的、同规格的混合专家模型
创新性地提出了分组混合专家模型（MoGE）的架构；
并且部分基础组件的代码实现
参考了业界的开源实践
涉及其他开源大模型的部分开源代码
严格遵循了开源许可证的要求
到这里，客观的来说
HonestyAGI团队用来计算大模型之间的相似性的方法
确实过于简单了些
在严谨性和科学性上存在一定的偏颇
仅凭这项研究
或许还并不能实锤华为盘古大模型的抄袭
但是紧接着的一则爆料
将这次风波推向了高潮
就在昨天7月6日
一篇名为《盘古之殇：
华为诺亚盘古大模型研发历程的心酸与黑暗》的文章在GitHub上走红
仅用一天时间就揽获了6.3K的Star
这篇文章的作者
自称是华为诺亚方舟实验室盘古大模型团队的内部员工
文章以第一人称视角
详细披露了华为盘古大模型研发背后的真实情况
不仅揭露了团队面临的技术挑战和工作压力
更是直指内部存在的"套壳"行为和管理乱象
为了证明自己所言的可信度
作者首先详细列举了诺亚方舟实验室的组织架构和人员构成
比如现任主任王云鹤，前主任姚骏
以及唐睿明、尚利峰等实验室的负责人
特别提到了他们隶属于被称为 "四野" 的组织体系
其中基础语言大模型团队属于 "四纵"，
而王云鹤领导的小模型实验室则是 "十六纵"。
这种军事化的组织命名方式
暗示了华为内部的企业文化风格
作者还描述了在苏州研究所的集中攻关经历
各地的团队成员被长期集中在甪直镇的酒店
这里我稍微补充一下
这个甪直镇位于江苏省苏州市的吴中区
位于苏州市的东部
华为苏州研发中心就在这附近
团队长期处于所谓的战时状态
每周六默认加班
甚至一两个月无法回家
值得注意的是
作者提到诺亚实验室原本是以研究为主
但是在加入大模型项目后
彻底转向为了交付型的团队
每天陷入例会、评审和汇报的循环中
连实验申请都需要层层的审批
这种组织定位的转变
也为后续的技术研发埋下了效率方面的隐患
关于盘古模型的早期发展
作者透露内部代号为 "盘古智子"，
最初只有内部申请才能使用的网页版
后来迫于压力才接入 Welink 进行公测
在技术研发层面
爆料内容揭示了华为在昇腾芯片上的艰难探索
早期使用的昇腾 910A 芯片
仅支持 FP16 的精度
与英伟达 GPU 普遍采用的 BF16 相比
训练的稳定性存在着明显的差距
这种硬件限制直接影响了模型的训练效果
比如 2023 年重点研发的 38B MOE 模型和 71B 稠密模型
由于算力不足和架构方面的问题
效果未达预期
真正的技术痛点出现在 tokenizer
也就是分词器这个环节
71B 和 135B 模型使用的分词器效率极低
每个汉字、符号甚至空格
都要单独占用一个 token
导致算力的浪费严重
当时小模型实验室有一个自研词表
团队尝试为 135B 模型更换分词器
经过复杂的 embedding 初始化和超过 1T 数据的续训练
才勉强成功
但是模型效果并没有得到显著的提升
这个案例暴露出了华为早期技术选型的失误
以及在关键组件上依赖内部其他团队的风险
当阿里、智谱等公司在 GPU 上快速推进大模型训练的时候
华为的昇腾生态还处于襁褓之中
公司内部曾经尝试训练 230B 的稠密模型
但是因为硬件的兼容性和算法问题失败了
导致项目陷入绝境
在这种压力下
团队不得不对 38B MOE 模型进行 "降级处理"，
移除 MOE 结构后得到了 13B 稠密模型
再通过架构调整
比如采用 ROPE 位置编码、RMSNorm 归一化和更换词表
勉强推出了第二代的38B 稠密模型
这些操作虽然展现了团队的应急能力
但是也反映出华为在核心技术上的被动追赶
真正引发行业震动的
还是爆料中提到的多次 "套壳" 事件
最典型的当属 135B V2 模型的研发过程
小模型实验室在短期内就声称
通过改造旧版的135B 模型
让指标平均提升了10 个百分点
但是实际的情况是
他们直接使用了阿里千问 1.5 的 110B 模型参数
通过增加层数、扩展 FFN 维度
并且添加少量的自研机制
拼凑出了 135B 的参数规模
令人惊讶的是
新模型的参数分布与千问高度相似
甚至代码中还保留着 "Qwen" 的类名
这种行为被作者称为 "套壳应用的第一次杰作"，
而管理层由于技术认知上的局限
对这种做法采取了默许的态度
更严重的问题在于资源分配的不公
小模型实验室在套壳过程中
几乎不受流程的约束
能够随意的获取算力和数据
而负责全栈自研的四纵团队
却面临严格的版本管理和模型血缘追溯的要求
例如
当四纵团队辛苦研发的 38B V3 模型数据和代码
被小模型实验室直接调取的时候
后者甚至要求前者配合适配到 "一键运行" 的程度
这种 "只许州官放火" 的双重标准
导致一线研发团队的不满情绪被激化
随后
署名权的争议进一步暴露了内部管理的混乱
在 135B V3（Pangu Ultra）的技术报告中
许多实际贡献者因为名额的限制
被排除在了作者名单之外
而一些没有技术贡献的管理层人员却被挂名
这种不尊重知识产权的行为
成为压垮团队士气的最后一根稻草
持续不断的内部问题
也导致了核心人才的加速流失
作者提到
许多骨干成员在目睹套壳造假和资源不公之后
心灰意冷
陆续加入了字节跳动、Deepseek、月之暗面等团队
一位离职的同事更是直言
在这里工作是技术生涯的耻辱
反映出资深技术人员对华为研发环境的彻底失望
人才流失带来的技术断层也很快显现
当 Deepseek 发布 V3 和 R1 模型后
华为被迫启动了718B MOE 模型的研发
但是小模型实验室再次选择了套壳的策略
直接加载了 Deepseek V3 的参数进行续训
甚至连文件目录都没有修改
这种急功近利的做法与四纵团队坚持的全栈自研
形成了鲜明的对比
却因为短期效果更好
而得到了管理层支持
进一步挫伤了研发团队的积极性
作者特别提到
华为的流程管理在此时呈现出了魔幻般的矛盾
一方面要求严格的可追溯性和合规性
另一方面却对套壳的行为视而不见
这种双重标准导致真正做事的团队被繁琐的流程所拖累
而投机者却能轻松获取资源和荣誉
形成典型的 "劣币驱逐良币" 效应
在文章结尾，作者宣布即将离职
并且申请从技术报告作者名单中移除
称这些署名是 "无法抹除的污点"，
可见自我切割的决心
值得注意的是
作者多次强调对华为的复杂情感
既有对 "打造世界第二选择" 的最初认同
也有对内部管理混乱的绝望
好了
以上就是这次事件的基本概况了
总结一下
首先HonestyAGI的论文确实疑点太多
本身提出的方法也并不完全站得住脚
所以至少到目前
并没有实锤能证实华为盘古模型的抄袭
其次
后面这个匿名者的爆料可信度还是很高的
至少应该肯定是内部人士
虽然仅从他的所言
我们并不能说他一定就是对的
因为没有提到训练的其他方面
比如合成数据的选择
也不能说小模型团队的选择就一定是错的
但是至少能感觉到的是
华为内部的管理一定是存在很大问题的
内部政治斗争和人才流失的情况也非常严重
另外
就从我当年和华为合作的几次经验来看
华为的套壳文化其实由来已久
如果大家去看看当年华为刚开始做华为云的时候
在短短的时间内服务就几乎包罗万千
稍微仔细一点就能看到大量的套壳痕迹
而真正需要自己扎实开发的
比如计费系统
却做的跟屎一样
云主机的计费服务连明细都整不明白
每次结算的时候我都得找客服人工核对
因为这部分功能无壳可套
当然，我也并不是要刻意抹黑华为
因为国内几乎绝大部分科技公司都是如此
套个开源的项目或者换个皮
就说是自己的
还记得我一年前做的节目CEC-IDE么
对吧？
当然了
套壳这种事情现在放在国内呢
估计也都不叫个什么事了
大家都是见怪不怪了
我是觉得呢
还在国内大厂打工的牛马工程师们
日子确实是越来越不好过了
我身边也有很多朋友、前同事
也是深陷泥沼，不能自拔
可惜我也是爱莫能助
只能是希望大家都能早日摆脱苦海
抓紧搞个副业吧
那大家是怎么看华为盘古抄袭千问的事情呢
欢迎在评论区留言
感谢观看本期视频，我们下期再见
