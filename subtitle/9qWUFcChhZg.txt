大家好，这里是最佳拍档，我是大飞
2025年，AI行业最热门的论断
应该莫过于Agent
也就是智能体之年的说法
几乎所有的行业会议、媒体报道都在重复这个观点
好像明天早上你一起来
我们就能看到无数的智能体在互联网上存在了
帮助我们完成所有工作
但是就在这样的狂热氛围中
安德烈·卡帕西（Andrej Karpathy）却泼来了一盆冷水
在旧金山AI Startup School的演讲中
他提出了自己截然不同的见解
不仅提醒我们要警惕短期的炒作
更是系统性地勾勒出了未来十年AI的发展路径
尤其指出数字基础设施需要经历一场根本性的变革
今天我们就来总结一下他这次演讲中的核心观点
看看在这场智能体的热潮背后
到底隐藏着哪些值得我们去关注的深层逻辑
演讲一开始
安德烈·卡帕西首先抛出了一个重要的论断
那就是我们正在经历软件行业70年以来
最剧烈的范式转变
随后
他根据人类与计算机交互的不同方式
将软件的发展划分为了三个阶段
首先是1.0，人类编写代码的手工时代
在这个时代
计算机就像是一个严格遵循指令的机器
每一行代码都由人类程序员所精心编写的
从早期的FORTRAN语言到现在的Python、Java
程序员用精确的语法来告诉计算机该"怎么做"。
GitHub上数以亿计的代码库
就是这个时代的辉煌成果
但是这种模式存在着明显的局限性
那就是人类的思维速度和代码的复杂程度
形成了瓶颈
一个复杂的功能可能需要成百上千行的代码
而且一旦需求变化，修改的成本极高
接下来是2.0
由神经网络开启的数据编程时代
随着深度学习的兴起
这个时代的核心不再是人类编写的显式代码
而是通过数据训练生成的神经网络权重
比如我们熟悉的图像识别模型AlexNet
它并不是程序员一行行写出来的识别的规则
而是通过数百万张图片的训练
让模型自己"学会"如何识别特征
卡帕西特别指出
当时很多人只是把神经网络当作一种高级的分类器
却忽略了它代表的范式革命
现在的Hugging Face Model Atlas等平台
其实就像是2.0时代的GitHub
存储的不是代码而是训练好的模型
开发者可以直接调用这些"数据编写的程序"。
第三个阶段就是如今大语言模型催生的软件3.0时代
卡帕西认为
这是一场最具有颠覆性的变革
神经网络从专用的工具变成了通用的计算机
而我们与它交互的语言
正是人类日常使用的自然语言
比如以前做情感分类
需要写代码或者训练特定的模型
现在只需要给大语言模型一个提示
让它直接判断这段文本的情感倾向即可
这种用自然语言编程的方式
让编程的门槛降到了历史最低
几乎每个会说话的人都能成为"程序员"，
这就是所谓的"提示工程"（Prompt Engineering）
为了帮助大家理解大语言模型的本质
卡帕西做了一个精妙的类比
那就是可以把大语言模型是看做一种全新的操作系统
这个类比包含了多层的含义
也揭示了大语言模型在技术生态中的核心地位
首先，他借用了吴恩达曾经说过的
"AI是新的电力"的比喻
指出OpenAI、DeepMind等公司
其实就像是电力公司
投入巨资来建设所谓的智能发电厂
也就是AI数据工厂，训练大模型
然后通过API向用户提供服务和收费
用户对大语言模型的需求也很类似于电力
需要低延迟、高可靠性
甚至出现了OpenRouter这样的"智能转换开关"，
让用户可以在不同模型之间无缝切换
当多个大模型同时宕机的时候
就如同经历全球范围的"智能断电"，
将会影响整个数字世界的运转
而更深一层的其实还是技术架构方面的类比
卡帕西指出
大语言模型本身就像是计算机的CPU
负责核心的推理；
上下文窗口相当于内存
存储当前任务的相关信息；
而围绕大语言模型构建的整个系统
就像是操作系统一样调度资源
完成多步骤的任务
比如处理一个复杂的数据分析请求
大模型需要先读取数据
然后执行计算，最后输出结果
这种架构正在重新定义软件的构建方式
以前需要多个模块配合的任务
现在可以通过大模型的上下文协调来完成
卡帕西还观察到
当前的大模型的市场格局
其实也很类似于早期操作系统之争
既有闭源的商业巨头
比如GPT-4、PaLM
也有开源的挑战者，比如Llama生态
这就很像当年的Windows与Linux
闭源系统凭借者技术优势占据主流地位
而开源社区则通过协作快速创新
这种双轨制的发展能够很好的推动大模型生态的不断进化
既能保证商业落地的稳定性
又能保持技术创新的活力
不过
虽然大语言模型已经展现出了惊人的能力
但是卡帕西还是提醒大家
必须清醒认识到它的局限性
他将大语言模型比作"随机的人类模拟器"，
它既有超越人类的优势
也有类似人类的认知缺陷
我们先说优势，显而易见的
在知识储备方面
大模型的训练数据覆盖了整个互联网级别的文本
包含人类历史上几乎所有的公开知识
这是任何单个学者都无法比拟的
其次是上下文窗口代表的强大的短期记忆
虽然长期记忆
也就是模型权重是固定的
但是在单次交互中
模型能够处理数万token的信息
这相当于瞬间可以记住一本厚书的内容
最后是跨领域的一个泛化能力
它得益于通用的训练方式
从而让大语言模型无论是在代码编写还是创意写作领域
都展现出了惊人的能力
但是，大模型的缺陷也同样十分显著
第一就是"幻觉"的问题
大模型会编造不存在的事实
而且无法区分真实与虚构的概念
比如它有时会坚持说"爱因斯坦获得过三次诺贝尔奖"，
但是实际上爱因斯坦只获得过一次
第二是"锯齿状的智能"，
模型会在某些领域表现出专家级的能力
在另一些简单问题上却很容易犯低级错误
第三是"顺行性的遗忘症"，
模型在每次交互后都会重置上下文
这样就无法像人类一样积累经验
必须依赖于外部的记忆工具
第四是安全性脆弱
模型很容易会受到提示注入攻击
比如通过恶意指令让模型泄露敏感的信息
这很容易在实际应用中构成重大的风险
面对大语言模型的不完美
卡帕西提出了现阶段的最佳策略
那就是开发"部分自治应用"，
从而构建人机协作的新范式
而不是追求那种不切实际的全自动化
以代码编辑器Cursor为例
它展现了部分自治应用的核心特征
首先是智能的上下文管理
它会自动将整个代码库的信息嵌入到模型中
让模型理解项目的全貌；
其次是多模型的编排
它可以同时调用聊天模型、代码Diff工具等等
从而实现一些复杂的功能；
最重要的是专用的界面设计
通过可视化的代码高亮对比
让用户可以快速的审查AI给出的建议
并且通过快捷键轻松的选择接受或拒绝
最后
还有一个非常关键的"自治程度滑块"的设计
用户可以根据任务的风险等级
来调整AI的自主权
包括从以人为主导的代码补全
到以AI为主导的文件修改
从而在安全和效率之间找到一个最佳的平衡点
另一个案例是Perplexity
它在信息检索中也应用了类似的逻辑
通过打包多个来源的信息
再调用多个模型来进行交叉验证
然后再通过带有来源引用的界面设计
让用户可以方便地查看推理过程
这样就从低自治程度的快速搜索
到高自治程度的深度分析
都可以灵活的切换
这些应用的共同点是
不追求AI独立完成所有的任务
而是让人类专注在决策和验证
让AI负责重复性、规律性的工作
从而形成高效的协作闭环
卡帕西还强调，人机协作的关键
应该还在于如何利用人类的视觉处理和逻辑判断的优势
比如界面中的可视化反馈
能够让用户在毫秒级的时间内
识别出AI输出的正确性
这比纯文本的效率要高无数倍
同时
"自治滑块"设计也符合了心理学中的"控制感"需求
用户不会因为AI的不可控而产生焦虑的情绪
反而能够通过分级授权来逐步建立信任
这种模式不仅适用于工具类的应用
未来还将会渗透到几乎所有的软件领域
成为数字化工具的一个标配
在分享自己开发MenuGen应用的经历时
卡帕西还揭示了一个关键的现象
那就是大语言模型会让编码变得前所未有的简单
但是现在部署却成为了新的瓶颈
作为一个几乎没怎么用过Swift语言的开发者来说
卡帕西通过大模型在几小时内
就可以完成应用的核心功能
这在传统开发中可能需要几周的时间
这种"氛围编程"Vibe Coding的现象
让任何人都能够快速将想法转化为可运行的代码
极大降低了创新的门槛
但是问题出在从Demo到产品的转化阶段
他花费了整整一周时间来处理DevOps任务
包括用户认证、支付集成、域名配置、云端部署等等
这些繁琐的手动操作与AI带来的效率提升
形成了强烈的反差
而这背后的根本原因在于
现有的数字基础设施是为两类用户设计的
分别是通过界面操作的人类
和通过API交互的传统程序
而AI智能体作为第三类用户
既不像人类那样可以依赖视觉进行点击
也不像传统程序那样依赖于固定的API
它们需要的其实是机器可读的结构化信息
而我们的网站、云平台上
到处都充满的其实是复杂的HTML和手动配置的流程
这些对于AI来说就好像迷宫一样了
比如你让AI去填写一个网页表单
它需要解析网页上的视觉元素、去定位点击的位置
这种模拟人类操作的方式不仅低效而且易错
因此也成为了AI创新落地"最后一公里"的障碍
面对这个结构性的问题
卡帕西提出了一个系统性的解决方案
核心思想是"双向奔赴"，
人类需要主动改造现在的基础设施
让AI能更加高效地进行交互
而不是等待AI来适应人类世界
他提出的第一个具体方案是创建lm
txt文件
这个文件类似于网站的robots
txt，但是这是专门为AI智能体设计的
这个文件可以用简洁的Markdown语言
来说明网站的功能、操作接口和数据结构
比如"本网站提供天气查询服务
可以通过/api/weather?
city=[城市名]来获取数据"。
相比让AI去解析复杂的网页DOM结构
这种更加直接的机器语言交互
能够将交互效率提升几个数量级
其次
现有的很多文档中到处充满着"点击此处""下拉菜单"这些面向人类的操作指令
这些对AI都毫无意义
卡帕西建议将文档改造成"双语模式"，
既有人类可读的步骤说明
也包含AI可执行的API调用或者命令行指令
比如Vercel和Stripe已经开始提供为大模型专门优化过的文档
将操作指南转化为结构化的API文档
让AI能直接解析并且执行部署流程
这种文档转型不仅可以方便智能体
也能够提升人类开发者的效率
因为结构化的信息更加易于检索和复用
第三个方案是构建一些桥梁工具
将现有的以人类为中心的信息
转化为对AI友好的格式
比如有些工具能将GitHub的仓库页面
转换为包含完整目录结构的纯文本块
让大语言模型无需解析网页就能够分析代码；
还有工具能将Excel表格的可视化数据
转化为结构化的JSON
方便AI进行数据分析
这些工具看似简单
却解决了智能体与现存系统交互的核心障碍
让遗留系统不需要进行大的、彻底的重构
就能够接入AI生态
卡帕西还强调
虽然未来的多模态模型可能学会模拟人类的点击行为
但是这是一条昂贵且低效的路径
就像让人类学习用脚打字
还不如设计一个适合手的键盘一样
让AI适应人类界面
还不如改造界面来适应AI
人类主动迈出50%，
包括提供机器可读的接口、结构化的文档、翻译工具
AI就能用剩下的50%完成更多高效的交互
所以
这种策略能够极大加速智能体的落地
避免陷入一种技术空想主义
那在演讲的最后呢
卡帕西用他在自动驾驶行业的亲身经历
对整个行业呢也发出了一些警示
2013年呢
他就体验到了Waymo近乎完美的自动驾驶的演示
当时认为商业化指日可待
却没想到12年后仍然没有完全解决
这说明
技术演示与实际产品之间存在巨大的"可靠性鸿沟"，
智能体的发展同样需要以十年为尺度的耐心
因此
针对当下流行的"2025年是智能体元年"的说法
他明确表示了自己的担忧
这种炒作很容易让创业者忽视底层基础设施的缺失
盲目追求全自动化的产品
最终陷入落地的困境
历史经验告诉我们
任何颠覆性的技术
都需要经历"期望膨胀期-泡沫破裂期-稳步发展期"的阶段
AI智能体也不例外
他还用钢铁侠的战衣
来类比现阶段的产品策略
我们都知道，钢铁侠的战衣里
既有托尼·斯塔克直接操控的增强模式
也有自主战斗的智能模式
卡帕西认为
当前的大模型可能更适合作为增强工具
通过优秀的人机接口让人类高效的监督AI
而不是追求不可控的全自主系统
这种"部分自治"的策略
既能够发挥大模型的优势
又能够规避它的缺陷
因此可以说是一条最务实的发展路径
最后，卡帕西指出
这次AI革命的独特之处在于
大语言模型不像电力、计算机那样
首先是掌握在政府或者巨头手中
而是通过互联网快速普及到了全球数十亿的用户
每个人都能够通过提示工程参与编程
每个创业者都有机会在新的基础设施上构建新的应用
所以说
这是一场真正的全民技术革命
我们正站在历史的转折点上
但是，作为开发者和观察者来说
我们既要拥抱大语言模型带来的效率革命
也要警惕炒作的陷阱
脚踏实地去构建真正可用的技术生态
也许这并不是一条最令人感到兴奋的路径
但是可能是通往AI未来的、最可靠的一条必经之路
好了
以上就是安德烈卡帕西这次演讲的主要内容了
感谢大家的观看，我们下期再见
