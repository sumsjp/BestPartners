大家好，这里是最佳拍档，我是大飞
9月26日
知名播客BG2和英伟达的CEO黄仁勋进行了一场独家对话
这次访谈的标题叫做“NVIDIA：
OpenAI、计算的未来和美国梦”，
时长超过100分钟
堪称黄仁勋近期信息密度最高、干货最足的一次分享
在这场对话中
黄仁勋系统性地解释了华尔街与硅谷之间存在的巨大认知分歧
并且详细拆解了英伟达看似坚不可摧的商业护城河
以及他对全球人工智能竞赛、大国博弈和未来社会形态的完整思考
今天我们就来回顾一下这场访谈的内容
可能大家还记得
一年前市场上有个挺流行的担忧
AI的预训练需求好像暂时放缓了
会不会导致之前建的算力中心过剩呢？
当时黄仁勋给出了一个非常大胆的预测
推理（Inference）需求的增长不是100倍
也不是1000倍
而是会达到“10亿倍”的量级
现在一年过去了，事实证明
连黄仁勋当时的这个预测
都还是低估了实际的需求增长
为什么会这样呢？
黄仁勋在访谈里系统性地提出了一个关键观点
那就是AI的算力需求
其实是由三种“缩放定律”（Scaling Laws）所共同驱动的
而不是大家之前以为的只有一种
第一种是“预训练缩放定律”（Pre-training Scaling Law）
这也是大家最熟悉的一种
简单说就是模型越大、用的数据越多、训练时间越长
模型就越智能
过去几年
不管是GPT系列还是其他大模型
其实都是靠这个定律驱动算力增长的
比如GPT-3用了千亿参数
训练时消耗的算力达到了每天几百PFlops
这背后就是预训练缩放定律在起作用
但是黄仁勋强调
这只是算力需求的“第一引擎”，
真正关键的是另外两种
第二种是“后训练缩放定律”（Post-training Scaling Law）
后训练指的就是模型在预训练之后
通过类似“练习”的方式
来精通某项特定技能的过程
这个过程会结合强化学习
让AI通过大量“试错”和“推理”来优化自己
举个例子，我们想让AI学会写代码
光给它看海量的代码库还不够
还得让它不断尝试编写代码、调试错误、运行测试
直到能写出符合要求的程序
这个“练习”的过程，就是后训练
而这个过程本身
需要消耗的推理计算量非常大
第三种是“推理时思维缩放定律”（Inference-time Thinking Scaling Law）
这也是黄仁勋认为市场最没理解透的一点
堪称“革命性”的算力驱动因素
他在访谈里说道
旧的推理方式是一次性的
你问AI一个问题，它会直接给你答案
但是新推理方式是‘思考’，
也就是在回答之前
AI会先自己琢磨、查证、梳理逻辑
这种“思考”不是简单的一步计算
而是一个复杂的过程
AI在生成最终答案前
会进行多轮的内部推理、研究事实
甚至调用外部工具
这个过程简单的话会形成“思维链”，
复杂一点的话还会形成“推理树”，
而且任务越复杂
AI“思考”的时间就越长
答案质量就越高，而每一次内部循环
都是一次计算消耗
这三种缩放定律放在一起
就彻底改变了我们对“训练”和“推理”的传统认知
过去大家觉得
算力主要消耗在“训练”阶段
但是现在看来
未来AI系统的大部分算力消耗
会发生在“使用”阶段
想象一下
当一个AI Agent能像人类员工一样
自主完成复杂的任务
它背后“思考”所消耗的计算资源
会是过去简单任务的亿万倍
这也能解释为什么OpenAI、谷歌这些公司
一边发布更强大的基础模型
一边把重心转向能执行复杂任务的Agent系统
因为这才是算力需求的真正未来
不过，这种指数级的增长需求
与华尔街的线性预测模型之间
形成了黄仁勋所说的“巨大的认知分歧”。
访谈主持人布拉德·格斯特纳直接抛出了这个问题
一方面
Sam Altman和Sundar Pichai这些行业领袖
都在谈论“万亿级”的算力投资；
但是另一方面
覆盖Nvidia的25位华尔街分析师
普遍预测Nvidia的增长率会在2027年“断崖式下跌”到大约8%。
为什么会有这么大的认知差距呢？
黄仁勋给出了一个三层的宏大叙事框架
不仅阐释了Nvidia的增长逻辑
也回答了“增长从哪来”、“增长能持续多久”这两个核心问题
第一层是“物理定律层面的转变”，
通用计算的时代已经结束
未来是加速计算和AI计算的时代
黄仁勋说，摩尔定律已经走到尽头
靠CPU性能提升来推动计算发展的模式
已经行不通了
这意味着
全球现有的、价值数万亿美元的、基于通用计算
也就是CPU的数据中心基础设施
在下一轮更新的时候
必须转向加速计算架构
比如用GPU、TPU这些专门为AI设计的芯片
这不是“创造新的市场”，
而是“存量市场的替换”，
仅仅把旧的CPU数据中心替换成加速计算数据中心
就已经是一个庞大的市场空间了
第二层是“现有应用的迁移”，
把互联网的核心工作负载从CPU迁移到GPU
就足以驱动数百亿美元的需求
黄仁勋举了个例子
像搜索、推荐引擎、电商购物这些
支撑Meta、谷歌、亚马逊、字节跳动等科技巨头核心业务的系统
过去都是跑在CPU上的
但是现在
它们正在全面转向用AI和GPU
因为AI能提供更好的个性化体验
效率也更高
这个过程不是在“造新的东西”，
而是用更先进的技术
重塑一个已经存在的、服务全球40亿人的庞大市场
黄仁勋说道
这就像从煤油灯转向电力
从螺旋桨飞机转向喷气式飞机一样
第三层是“未来的增量”，
指的是AI作为“智能工厂”，
对全球GDP的赋能
这是最让人兴奋的部分
也是AI创造全新价值的地方
黄仁勋把AI工厂类比成工业革命中的“马达”，
马达替代了体力劳动
而AI工厂则通过生成Token来增强人类的智力劳动
他还做了一个经济学估算，首先
全球GDP中
大约50%-65%和“智力劳动”相关
比如设计、研发、咨询、编程这些需要动脑的工作
总价值大概是50万亿美元；
其次
AI会对这50万亿美元的经济活动进行“增强”，
假设一家公司有一个年薪10万美元的员工
公司愿意额外花1万美元给这个员工配备AI服务
换来2-3倍的生产力提升
这笔投资是非常划算的
黄仁勋说
Nvidia内部已经给每一位芯片设计师和软件工程师
都配了AI助手
结果生产力的提升非常明显；
最后
如果全球50万亿美元的“智能GDP”，
每年需要价值10万亿美元的“AI Token生成服务”来增强
假设这些服务的毛利率是50%，
那么就需要价值5万亿美元的AI基础设施
来支撑这些服务的运行
这三层框架一出来
Nvidia的增长逻辑就从“卖芯片”的简单故事
变成了和全球经济结构变迁同频共振的宏大叙事
它清晰地表明，Nvidia的增长
既来自旧设施的替换
也来自现有业务的升级
还来自未来AI创造的新价值
这也能解释为什么Nvidia的收入和数据中心的电力消耗高度相关
因为算力越多
需要的电力就越多
比如阿里巴巴的吴泳铭就说过
他们的数据中心电力消耗会在本年代末增长10倍
而AI生成的Token数量每几个月就翻一番
背后都是对算力的无尽需求
以及对Nvidia AI基础设施的依赖
聊完英伟达的增长逻辑
我们再来看这次访谈里另一个重磅话题
那就是Nvidia和OpenAI的“星际之门”（Stargate）计划
最近大家可能都注意到一个新闻
那就是英伟达要投资OpenAI千亿美元
来搞“星际之门”项目
很多人觉得这只是一次普通的商业合作
Nvidia卖芯片
OpenAI买算力
但是黄仁勋在访谈里明确的说道
这是一次“战略绑定”，
背后的逻辑比大家想的深得多
首先
黄仁勋毫不掩饰对OpenAI未来的看好
他认为
OpenAI很可能将成为下一个数万亿美元级别的超大规模公司（Hyperscale Company）
什么是超大规模公司？
就是像Meta、谷歌、微软这样
既能服务消费者市场
又能服务企业市场
甚至成为全球基础设施一部分的公司
黄仁勋认为
OpenAI未来也会达到这个级别
它的AI服务会渗透到个人生活、企业办公、工业生产等各个领域
就像现在的互联网一样普及
所以
能在OpenAI成为“巨无霸”之前进行投资
黄仁勋觉得是“他们能想象到的最聪明的投资之一”。
而且特别关键的一点是
这笔投资不是Nvidia强制要求的
而是OpenAI主动给Nvidia的机会
这说明OpenAI也认可Nvidia的战略价值
想和它深度绑定
其次，这次合作的深度和广度
远超外界想象
黄仁勋把双方的合作拆成了三个层次
第一个层次是“现有云合作的延续”，
Nvidia会继续和微软合作
为OpenAI在Azure云平台上构建价值“数千亿美元”的算力集群；
同时，还会和甲骨文、软银合作
建设数个吉瓦（Gigawatts）级的数据中心
第二个层次是“帮助OpenAI自建基础设施”，
这是这次新合作的核心
过去OpenAI的算力主要靠租云服务商的资源
但是现在它要自己建AI基础设施了
而Nvidia会从最底层开始参与
包括芯片设计、软件开发、系统集成
甚至整个AI工厂的规划和运营
是“全栈式”的支持
第三个层次是“建立直接的战略关系”，
黄仁勋把这种关系类比成Nvidia和Meta的Mark Zuckerberg、谷歌的Sundar Pichai、微软的Satya Nadella、xAI的Elon Musk之间的直接合作
这意味着OpenAI的规模已经大到
不需要再通过云服务商做“中间人”了
而是可以和最核心的技术供应商直接、平等地对话
形成深度绑定的战略伙伴关系
为什么OpenAI要做这样的调整呢？
黄仁勋点出了其中的核心原因
那就是OpenAI面临“双重指数级增长压力”。
一方面是“用户增长指数”，AI越好用
应用场景越多
用户数量和使用频率就会指数级增长；
另一方面是“计算增长指数”，
就像我们前面聊的
每个用户每次和AI交互
因为“思考”的引入
需要的计算量也在指数级增长
这两个指数叠加在一起
意味着OpenAI的算力需求
会以“指数的指数”的速度增长
单靠租云服务已经满足不了了
所以必须同时推进“租云”和“自建”两条路
才能够确保算力供给跟得上需求
其实呢与Openai的紧密合作
背后
反映出的正是英伟达坚固的护城河
现在市场上有很多竞争对手
比如说AMD英特尔
还有一些公司在做ASIC芯片
主持人直接问黄仁勋
Nvidia的竞争护城河是在扩大还是缩小呢？
黄仁勋的回答则是
Nvidia真正的护城河
不是某一款芯片的性能优势
而是一种被称为“极限协同设计”（Extreme Co-Design）的系统级创新能力
首先
黄仁勋解释了为什么Nvidia要搞“年度发布周期”。
过去芯片的更新是18-24个月一次
但是现在改成了每年一次
因为摩尔定律失效以后
晶体管的性能不再大幅提升
如果不能快速提升整体性能
AI生成Token的成本就会持续上升
而要持续降低Token成本
唯一的办法就是“系统级的创新”，
这就是“极限协同设计”的由来
黄仁勋说
极限协同设计要求同步优化模型、算法、系统和芯片
让它们像一个整体一样工作
而不是各自为战
这和传统的“盒子内的创新”完全不同
过去是只想着把CPU做得更快
但是现在要同步升级构成AI数据中心的所有核心组件
具体来说
“极限协同设计”体现在三个层面
在芯片层面
Nvidia会并行研发革命性的CPU、GPU、网络芯片和NVLink；
系统层面
会把这些芯片以最优化的方式整合起来
确保它们之间的数据传输效率最高
不会出现“某一个组件拖后腿”的情况；
软件层面
会提供从底层驱动到上层应用库的完整软件栈
让开发者能轻松用上整个系统的能力
不用自己去解决硬件兼容、数据传输这些复杂问题
正是这种跨所有层面的协同设计
才让Nvidia从Hopper架构到Blackwell架构
在一年内实现了性能提升30倍的突破
这绝对不是靠单一芯片的技术进步能做到的
其次
这种系统能力还体现在“规模化部署”的巨大挑战上
黄仁勋举了个例子
Elon Musk的xAI公司要部署Colossus 2的集群
需要用到50万个GPU
这不是简单地把50万个GPU堆在一起就行
要考虑电力供应、散热、数据传输延迟、系统稳定性等一系列的问题
任何一个环节出问题
整个集群都没法正常工作
而客户之所以敢下数百亿美元的订单给Nvidia
就是因为Nvidia的架构经过了市场验证
能确保这么大规模的系统稳定运行
这种“经得住考验的规模化部署能力”，
本身就是一道很高的信任壁垒
所以黄仁勋说
现在的竞争已经不是“我的芯片比你的快”，
而是“我的整个AI工厂
比你的AI工厂效率更高”了
这种从“组件思维”到“系统思维”的跃迁
正是Nvidia能远超竞争对手的根本原因
毕竟
竞争对手可能能做出一款性能不错的芯片
但是要想做到“芯片、系统、软件”全链条的协同创新
还要能支撑几十万GPU的规模化部署
难度要大得多
黄仁勋甚至说
即使竞争对手把他们的ASIC芯片免费送给客户
客户还是应该选择Nvidia的系统
很多人听到这话会觉得不可思议
免费的芯片都不要？
但是黄仁勋的逻辑
其实紧扣了数据中心的“现实约束”，
那就是电力和空间都是有限的
黄仁勋解释说，数据中心建设中
土地、电力、建筑这些投入的成本非常高
当一家企业拿到宝贵的2吉瓦电力配额的时候
它的核心目标不再是“节省芯片成本”，
而是“用这些电力创造最大的商业价值”。
他用简单的算术算了一笔账
如果每瓦性能
或者说每瓦能生成的Token数量
是竞争对手的两倍
那么客户用同样的数据中心
就能产生两倍的收入，谁不想要呢？
再具体一点
假设竞争对手的ASIC芯片
性能和Nvidia上一代的Hopper GPU差不多；
而Nvidia新一代的Blackwell GPU
性能是Hopper的30倍
这意味着，在同样的电力消耗下
用Blackwell的客户能够获得30倍的潜在收入
在这种情况下
为了节省一点芯片成本而放弃30倍的收入
这种机会成本显然“高得离谱”。
任何理性的CFO
都会选择“每瓦性能”最高的解决方案
因为这直接决定了企业收入的上限
除此之外
黄仁勋还分析了ASIC的“生态定位”。
他认为
ASIC适合那些“功能固定、市场规模有限”的领域
比如视频转码器、智能网卡等等
这些任务的算法很少变
用ASIC能做到很高的效率
但是对于AI这种“工作负载多样且快速变化”的领域
ASIC的“专用性”反而成了致命弱点
因为AI需要处理的任务实在是太多了
聊天、写代码、生成图片视频、做数据分析、制定商业计划
等等等等
而且底层算法还在不断演进
这就要求计算平台必须具备高度的“可编程性”，
能够快速适配新任务、新算法
而这正是GPU和CUDA生态的核心优势
这也能解释为什么谷歌虽然有自己的TPU
但同时也是Nvidia GPU的大客户
因为在一个复杂的计算集群里
既需要TPU这样的“专用辅助”芯片
也需要GPU这样的“通用主力”芯片
通过合理组合来实现整体最优
而Nvidia通过开放NVLink Fusion等接口
允许英特尔等公司的芯片接入自己的生态
这正是“平台化”和“生态化”思维的体现
非但不靠封闭来阻挡对手
反而是靠开放来扩大生态
让更多伙伴参与进来
一起把AI算力的市场做大
在这场对话中
黄仁勋还花了大量篇幅讨论了全球人工智能竞赛
尤其是美国与中国的关系
黄仁勋首先提出了一个观点
AI基础设施已经成为和能源、通信同等重要的国家战略资源
所以全球范围内正在掀起“主权AI”（Sovereign AI）的建设浪潮
什么是主权AI？
就是每个国家都需要拥有自己的AI基础设施
用自己的数据和文化训练AI模型
确保AI能服务于本国的特定需求
不管是工业生产、制造业升级
还是国家安全
黄仁勋认为
虽然各国会使用GPT、Gemini这些全球领先的模型
但是同时必须建立自己的主权AI能力
因为AI不仅是技术
还承载着文化、价值观和历史
一个国家不能把核心的智能需求
完全依赖于其他国家的技术
就像每个国家都会有自己的电网、通信网络一样
未来也会有自己的AI基础设施网络
而这为Nvidia等基础设施提供商
创造了全球性的全新市场
关于美国的对华技术政策
黄仁勋提出了坦率的批评
他认为
美国采取“小院高墙”式的对华技术封锁
比如限制Nvidia向中国出口高端芯片
看起来是在遏制中国AI发展的做法
但是实际上不仅徒劳无功
反而更是一种危险的“单方面裁军”。
他指出了这种政策的两个主要后果
第一，会催生强大的竞争对手
把拥有95%市场份额的Nvidia排除出中国市场
相当于把整个中国市场拱手让给华为等本土企业
这些企业会在“没有强竞争”的环境下
会靠着“垄断利润”加速技术研发和产能扩张
最终成长为Nvidia在全球市场的强劲对手
第二，严重低估了中国的能力
黄仁勋曾经警告说
外界普遍认为中国造不出高端芯片
或者技术上落后美国数年
这些想法都是“疯狂”的
实际上
中国拥有世界上最渴望成功、最勤奋的企业家
还有充满活力的内部竞争生态
中国在芯片和AI领域和美国的技术差距
其实是以“纳秒”来计算的
不是大家想的“几年”。
当然这里的纳秒是打了引号的
意思是强调中美之间的差距很小
那么，正确的路径应该是什么呢？
黄仁勋认为
让美国最优秀的企业在中国市场和本土企业直接竞争
才最符合美国的国家利益
这样做不仅能为美国企业创造经济价值
还能让美国通过技术影响力
在全球AI格局中保持话语权；
更重要的是
竞争能倒逼美国科技企业不断创新
保持在技术最前沿
黄仁勋说道，一个自信、强大的国家
应该秉持‘放马过来’（Bring it on）的态度
相信自己的体系和人民能在竞争中胜出
在谈到美国的核心优势时
黄仁勋的回答更是尖锐
他说
美国拥有一个世界上任何国家都没有的独特品牌声誉
那就是来到美国，实现美国梦
作为一个从中国台湾移民到美国
从餐馆洗碗工成长为万亿市值公司CEO的亲历者
黄仁勋对“美国梦”的理解非常深刻
这种“让每个人都有机会通过努力改变命运”的信念
是美国吸引全球顶尖人才的根本原因
比如过去几十年
全球最优秀的科学家、工程师、创业者都愿意去美国
因为那里有更好的机会、更开放的环境
但是黄仁勋警告说，近些年来
这个核心优势正在受到严重的挑战
他观察到一个非常危险的信号
顶尖中国AI研究者来美国的意愿
已经从三年前的90%骤降到现在的10%-15%。
这不是一个小变化
而是关乎美国未来的“生存危机”级别的早期预警
因为AI行业的竞争
本质上是人才的竞争，没有顶尖人才
再先进的技术也难以持续领先
所以黄仁勋呼吁美国政策制定者
必须谨慎区分“与中国竞争”和“对中国人强硬”这两个概念
“与中国竞争”是在技术、市场上的良性比拼
而“对中国人强硬”则是把优秀的中国人才拒之门外
这会摧毁美国最宝贵的资产
也就是美国作为全球人才灯塔的品牌形象
展望未来
黄仁勋认为人工智能将从根本上改变社会
他坚信
人工智能会带来巨大的生产力提升
而不是大规模的失业
那种认为AI会摧毁就业的观点
前提是“我们再也没有新的想法了”，
但是他认为智力不是零和游戏
周围聪明的人和工具越多
能想到的新点子、能解决的新问题就越多
创造的岗位也会越多
每项工作都会改变，有些会消失
但是经济整体会增长
在他看来
AI本身就是最伟大的均衡器
过去
一个人想利用计算机创造经济价值
至少得学习Python编程
现在，他们只需要学习人类语言
技术鸿沟正在被技术本身填平
对于未来的具体形态
他预言在未来五年内
人工智能与机器人技术的融合将成为现实
每个人都会像电影星球大战中一样
有自己的“R2-D2”机器人
成为生活中的伙伴和向导
云端的人工智能和实体世界的机器人将无处不在
生物学的复杂性将被揭示
每个人都将拥有自己的“数字孪生”，
用于预测健康状况和疾病
面对这种指数级加速的变化
黄仁勋给出的建议很简单
那就是登上那列火车
不要试图去预测火车未来会到哪个站点
因为当它呈指数级加速的时候
任何预测都是徒劳的
唯一的策略就是趁现在它还相对较慢时跳上去
然后随着它一起经历指数级的旅程
也许，从芯片公司到AI基础设施公司
英伟达的进化本身就是登上这列火车的最好证明
而对于整个世界来说
这趟旅程才刚刚开始
好了，感谢观看本期视频
我们下期再见
