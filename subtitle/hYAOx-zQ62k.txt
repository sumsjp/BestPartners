大家好，这里是最佳拍档，我是大飞
最近OpenAI的更新有点过于频繁了
昨天OpenAI正式宣布
开放GPT-3.5微调API
同时GPT-4版本也即将推出
这意味着
人人都可以打造自己的个性化「类ChatGPT应用」，
可以说
这是OpenAI自「插件应用商店」以来
做出的最大产品更新
将是有史以来最大的LoRA云服务
既然这次更新跟微调有关
我们就先从一些简单的微调概念出发
最后再来介绍一下OpenAI的这次更新内容
首先我们说一下什么是微调
通常我们说的微调
指的是基于已经训练好的语言模型
使用小规模、特定的数据集继续训练它
让它更适应于特定的任务或领域
可能听上去不是太好理解，没关系
我后面会更细致的介绍
因为AI这个领域的概念比较多
大多也相对抽象
所以大家可能会感到疑惑
像微调、提示工程（Prompt Engineering）、嵌入（Embedding）、智能体（Agent）这些概念是什么意思
有什么区别？
我们大概打个比方吧
如果我们把大语言模型比喻为一个已经训练好的家政阿姨
她懂中文，会做家务
但是对你家里的情况不了解
那么，微调
就相当于阿姨第一次到你家干活的时候
你要花一小时时间告诉她家里的情况
比如物件的摆放、哪些地方不能动
哪些地方要重点照顾
嵌入
就相当于你省去了对阿姨进行二次培训
而是在家里贴满纸条，这样
阿姨需要做什么事，就先找纸条
一看到纸条就知道该怎么做了
而提示工程，就是你给阿姨的指令
告诉阿姨应该做什么事情
同时为了让阿姨能圆满完成任务
在指令里通常需要包含很多背景信息
然后阿姨借助她自己的语言能力和推理能力
理解你的指令
做出推理，最终给出回应
另外阿姨记性不太好
你一次不能说太长的内容
也就是所谓的上下文窗口长度限制
智能体
就好比让阿姨去修家里坏了的电视
阿姨不会修
但是阿姨知道该找修电视的
于是阿姨把修电视的叫过来把电视修了
那这个修电视的就是智能体
能帮助大语言模型处理它能力之外的事
再打个比方
如果我们把大语言模型比作大学生
那么提示工程，就好比考试的问题
问题中提供了一定的背景知识和要求
然后学生按照问题的要求和背景知识
进行推理，最终给出答案
限于考卷的长度
你的问题和学生的答案都不能太长
也就是上下文窗口长度限制
而微调
就好比大学生要突击某门考试
考试前给这个学生一些
这门课的指定格式的问题和答案
让学生记住这些知识
并且考试的时候按照上面的格式回复
嵌入，就像记笔记，是一种短期记忆
当考试的时候，你把笔记带上
遇到问题先翻看笔记
对于笔记上有的内容可以得到准确的答案
另外由于上下文窗口长度的限制
你每次只能抄笔记本上几段的内容
智能体，就好比考数学
学生可以带高级计算器
遇到不会算的问题
学生直接用计算器就可以得到答案
计算器就相当于智能体
那么我们再来说说微调有哪些好处？
第一点，可以大幅缩减Prompt的长度
如果你原本需要在Prompt中加入one-shot、few-shot例子
或者需要很多文字去描述格式
基于微调就可以在Prompt中省掉这些了
但是按照OpenAI公布的价格
由于现在调用微调模型的价格是GPT-3.5的8-9倍
所以这么做未必会更便宜
当然如果通过微调能达到GPT-4的效果
那还是合算的
因为微调模型的价格只有GPT-4价格的一半
再加上Prompt更简洁了，还是划算
第二点
可能会比单纯采用提示的结果更好
由于借助微调
可以针对你自己的私有数据进行微调
而这些数据你无法通过Prompt完整提供
或者通过Prompt很难描述清楚
这种情况下
微调的生成结果会比单纯Prompt效果更好
第三点，稳定的按照特定格式输出
微调后，对于输出格式有要求的
比如代码生成、输出JSON格式、输出特定语言
比如中文
微调在输出特定格式的效果上
会比单纯Prompt更稳定
第四点，降低请求的延迟
这部分提升
其实来源于Prompt变短了
或者说你不需要用GPT-4
只需要用微调后的GPT-3.5了
那么微调可能会影响模型其他方面的能力么？
如果你针对某一个任务对大语言模型进行了微调
可能会影响模型在其他任务上的表现
所以，在微调前
最好准备一些测试数据集，微调后
用测试数据集测试一下性能
如果不够好
那么针对不好的地方还要进一步微调
同时
我们要知道微调并不能解决幻觉（Hallucination）的问题
我们都知道，ChatGPT喜欢胡说八道
对于不知道的答案也会乱说一通
微调并不能很好的解决这个问题
可能对于你微调数据集的问题会得到很好的结果
但是对于微调数据集之外的结果
还是一样会有幻觉问题
那微调会带来哪些其他要考虑的问题么？
首先是微调的隐形成本
微调的结果严重依赖用来训练的数据集的质量
如果数据质量不高效果也不会太好
而且微调是要有特定格式要求的
不是你一个普通文档就能去微调的
所以，在微调前
你是需要花大量时间对数据进行整理、筛选的
另外，你还要准备多套测试的数据集
用来测试微调后的结果
而且，要做好一次微调效果不理想
要进行多次微调的准备
这部分试错的成本需要考虑进去
其次是数据安全问题
美国用户可能还好
或者普通中小企业也没问题
但是对于数据安全要求高的企业
或者国内的用户
将微调后的模型托管在OpenAI
这个数据安全性是不得不考虑的一个重要问题
虽然OpenAI也承诺
说不会使用这些微调数据
但是这个风险始终存在
最后是受限于OpenAI的对齐和内容审查
对于想通过微调去做点违法事情的
比如训练黑客相关的应用
或者是输出一些有害的内容
是无法通过审查的
对于微调后的数据
OpenAI会采用基于GPT-4的内容审查模型
再去过滤审查一遍的
那有哪些应用场景是适合使用微调的呢？
1
需要稳定的输出特定格式、风格的时候
比如GitHub Copilot的代码生成
OpenAI自己微调的Function Calling
可以稳定输出Json格式
以及你可以模仿某个名人、动漫角色的说话风格
或者某位作家的写作风格
总之
当你需要特定格式、风格的输出
而通过Prompt有很难描述或者受长度限制时
就可以考虑微调
2
当你需要在Prompt能放入更多内容的时候
因为基于In-Context-Learning的方式去写Prompt的话
需要在Prompt中输入大量的背景说明和few-shot示例
这样其他内容就被挤压了
但是
如果将原本放在Prompt中的示例作为微调的数据集
对模型进行微调，那么
就不需要在Prompt中加入few-shot了
可以节约大量的Tokens在Prompt上
例如一些需要很长的Prompt才能描述清楚的输出格式
一些Embedding无法很好满足的场景
比如分块后的数据比较大
无法正常放入Prompt
3
对于一些大语言模型没有
或者能力很弱的任务，通过微调
能让大语言模型产生一些新的能力
比如
OpenAI的ChatGPT就是基于原来GPT3.0的模型微调来的
还有Function Calling也是微调的结果
又或者你新发明了一种编程语言
但是GPT没训练过
你可以微调后让它也能帮你生成这门语言的代码
甚至是一些没有训练过的行业数据
比如中医数据等
那微调的具体价格是什么样呢？
这次微调GPT-3.5的成本可以分为两部分
初始训练成本和使用成本
训练是每1千token 0.008美元
使用输入是每1千token 0.012美元
使用输出是每1千token 0.016美元
例如
一个gpt-3.5-turbo微调任务的训练文件
为10W个token，大约75
000个单词的话
那么训练3个epoch的预期成本为2.40美元
按照这个计算
微调的GPT3.5 Turbo生成成本
是基本模型生成成本的8倍
因此用户确实必须处于OpenAI提到的
“将提示大小减少90%”的范围内
才能从中获得成本效益
此外，需要注意的是
每个训练示例限制为4096个token
训练时，长度超过这一限制
将被截断为4096个token
因此
要确保整个训练示例适合上下文
请检查消息内容中的总token数是否低于4
000个
每个文件当前限制为50MB
这次OpenAI官方也给出了GPT-3.5 Turbo微调的代码样例
总结一下，大概就是4个步骤：
准备数据、上传文件、微调GPT模型和使用
第一步，准备数据
数据样例就是对话数据，注意
目前的微调都是有监督微调SFT
第二步，上传文件
目前通过API接口上传
这里给的是curl命令示意
本身可以通过代码完成
官方没有说明文件大小
数据样例就是对话数据
第三步，创建微调任务
也十分简单
大家看一下就能明白
第四步，一旦模型完成了微调过程
你就可以通过API调用的方式来使用它
OpenAI还计划在未来推出一个微调UI
其中包含一个仪表板
用于检查正在进行的微调工作负载的状态
对函数调用和GPT-4、gpt-3.5-turbo-16k微调的支持
也计划于今年秋季推出
大家对OpenAI这次开放微调
有什么看法呢
欢迎在评论区留言
感谢观看本期节目，我们下期再见
