大家好，这里是最佳拍档，我是大飞
在年前1月底我参加的一个AI生成视频主题的技术沙龙上
有一个有趣的问题
AI视频生成领域会多快迎来 “Midjourney时刻”？
当时的选项分别是半年内、一年内、1到2年或者更长
不过，就在昨天
OpenAI公布了准确答案，20天
2月16日凌晨
OpenAI发布了自己的首个AI视频生成模型—Sora
昨天大飞的微信朋友圈完全被它刷屏
应该说，这是一个历史性的里程碑
通过将扩散模型结合OpenAI大获成功的transformer
在视觉领域实现了与大语言模型类似的突破
毫无疑问
视觉生成领域将会产生一次重大的技术和商业革命
有趣的是
明星AI 公司Stability AI昨天原本发布了一个新的视频模型SVD1.1
但是由于与Sora撞车
其官方推文已被火速删除
今天我们这个视频
大概会来介绍一下Sora是什么
它是怎么工作的
以及对Sora的一些思考
简单来说
Sora是一个通过文字来生成视频的AI大模型
首先，我们必须得承认
Sora在多个方面重新定义了AI视频生成模型的标准
最关键的有四点
1、它将视频时长从当前的5-15秒
直接提升到了1分钟
这个长度完全可以应对短视频的创作需求
从OpenAI发表的文章看，如果需要
超过1分钟也是毫无任何悬念
2、它可以生成多个镜头
并且各个镜头具有角色和视觉风格的一致性
3、不仅可以用文字prompt生成视频
还支持视频到视频的编辑
当然也可以生成高质量的图片
Sora甚至还可以拼接完全不同的视频
合二为一并且保持前后连贯
4、它是扩散模型
更是扩散+Transformer的视觉大模型
并且产生了涌现现象
对现实世界有了更深刻的理解和互动能力
具有了世界模型的雏形
在OpenAI官方网站公布了数十个示例视频
充分展示了Sora模型的强大能力
人物的瞳孔、睫毛、皮肤纹理
都逼真到几乎看不出一丝破绽
真实性与以往的AI生成视频相比
绝对是史诗级的提升
AI视频与现实的差距，更加难以辨认
无人机视角下的东京街头景色
让Sora在复杂场景展现
人物动作自然度等方面的优势展露无遗
在山道上穿梭的复古SUV
可以看到真实性很高，如果不说
我相信很多人会以为是某个3A游戏大作
更夸张的是
Sora可以在两个输入视频之间逐渐进行转场
在完全不同主题和场景构成的视频之间创建无缝的过渡
看到这里，我们不禁想了解
Sora是如何工作的呢？
在OpenAI昨天发布的技术文档中
无论从模型架构还是训练方法
其实都没有发布什么天才级的创新技术
更多是现有技术路线的优化
但跟一年多以前横空出世的ChatGPT一样
OpenAI的秘诀都是屡试不爽的Scaling Law
也就是缩放定律
当视频模型足够“大”的时候
就会产生智能涌现的能力
问题在于
大模型训练的“暴力美学”几乎已经人尽皆知
为什么这次又是OpenAI？
我们已经知道
AI生成视频的技术路线主要经历了四个阶段
分别是循环网络RNN、生成式对抗网络GAN、自回归模型和扩散模型
今天
领先的视频模型大多数是扩散模型
比如 Runway、Pika 等
自回归模型由于更好的多模态能力与扩展性也成为热门的研究方向
比如谷歌在2023年12月发布的VideoPoet
而Sora则是一种新的 diffusion transformer 模型
从名字就可以看出
它融合了扩散模型与自回归模型的双重特性
Diffusion transformer架构由加利福尼亚大学伯克利分校的威廉·皮布尔斯 William Peebles 与纽约大学的 Saining Xie 在2023年提出
那么如何训练这种新的模型呢？
在技术文档中
OpenAI 提出了一种用 patch
也就是视觉补丁
作为视频数据来训练视频模型的方式
这是从大语言模型的token汲取的灵感
Token优雅地统一了文本的多种模式
包括代码、数学和各种自然语言
而patch则统一了图像与视频
OpenAI训练了一个网络来降低视觉数据的维度
这个网络接收原始视频作为输入
并输出一个在时间和空间上都被压缩的潜在表示（latent representation）
Sora在这个压缩的潜在空间上进行训练
并随后生成视频
OpenAI还训练了一个相应的解码器模型
将生成的潜在表示映射回像素空间
OpenAI表示
过去的图像和视频生成方法通常会将视频调整大小、裁剪或修剪为标准尺寸
而这损耗了视频生成的质量
例如分辨率为256x256的4秒视频
而将图片与视频数据 patch 化之后
无需对数据进行压缩
就能够对不同分辨率、持续时间和长宽比的视频和图像的原始数据进行训练
这种数据处理方式为模型训练带来了两个优势
第一，采样灵活性
Sora可以采样宽屏1920x1080p视频、垂直1080x1920视频
以及介于两者之间的所有视频
直接以其原生宽高比为不同设备创建内容
并且能够在以全分辨率生成视频之前
快速地以较低尺寸制作原型内容
这些都使用相同的模型
第二，改进了框架与构图
OpenAI根据经验发现
以原始长宽比对视频进行训练可以改善构图和取景
比如
常见的将所有训练视频裁剪为正方形的模型
有时会生成仅部分可见主体的视频
相比之下，Sora的视频取景有所改善
在语言理解层面，OpenAI发现
对高度描述性视频字幕进行训练可以提高文本保真度以及视频的整体质量
为此
OpenAI应用了DALL·E 3 中引入的re-captioning technique
首先训练一个高度描述性的字幕生成器模型
然后使用它为训练数据集中的视频生成文本字幕
此外，与DALL·E 3 类似
OpenAI还利用GPT将简短的用户提示转换为较长的详细字幕
然后发送到视频模型
这使得Sora能够生成准确遵循用户提示的高质量视频
以及支持“图像生成视频”与“视频生成视频”。
甚至能让Sora执行各种图像和视频编辑任务
创建完美的循环视频、动画静态图像、及时向前或向后扩展视频等
在Sora的技术文档里
OpenAI并没有透露模型的技术细节
而只是表达了一个核心理念，scale
OpenAI在2020年首次提出了模型训练的秘诀
Scaling Law
根据 Scaling Law
模型性能会在大算力、大参数、大数据的基础上像摩尔定律一样持续提升
不仅适用于语言模型
也适用于多模态模型
OpenAI 就是遵循这一套“暴力美学”发现了大语言模型的涌现能力
并最终研发出划时代的ChatGPT
而Sora 模型也是如此
凭借Scaling Law
它毫无预兆地在2024年2月就打响了视频的 “Midjourney 时刻”。
OpenAI表示
transformer 在各个领域都表现出了卓越的扩展特性
包括语言建模、计算机视觉、图像生成以及视频生成
这张图就展示了训练过程中
在相同的样本下
随着训练计算规模的增加
视频质量显著提高
OpenAI发现
视频模型在大规模训练时表现出许多有趣的新功能
使Sora能够模拟现实世界中人、动物和环境的某些方面
这些属性的出现对3D、物体等没有任何明确的归纳偏差
这纯粹就是模型缩放现象
因此，OpenAI将视频生成模型
命名为“世界模拟器”（world simulators）
或称之为“世界模型”，
可以理解为让机器像人类理解世界的方式一样学习
英伟达科学家Jim Fan这样评价Sora
他认为Sora是一个数据驱动的物理引擎
它是对许多世界的模拟
无论是真实的还是幻想的
模拟器通过一些去噪和梯度数学来学习复杂的渲染、‘直观’物理、长期推理和语义基础
而世界模型的概念
是Meta首席科学家杨立昆提出的
我们也曾做过多期节目介绍
没想到的是
OpenAI仅仅通过早就熟稔于心的Scaling Law
让Sora具备了世界模型的能力
并且还这个世界模型还具有三个特点
第一个是3D一致性
Sora可以生成带有动态摄像机运动的视频
随着摄像机的移动和旋转
人和场景元素在三维空间中一致移动
第二个是远程相关性和物体持久性
视频生成系统面临的一个重大挑战是在采样长视频时保持时间一致性
OpenAI发现Sora通常能够有效地对短期和长期依赖关系进行建模
例如，模型可以保留人、动物和物体
即使它们被遮挡或离开框架
同样
它可以在单个样本中生成同一角色的多个镜头
并在整个视频中保持其外观
第三个是与世界互动
Sora 有时可以用简单的方式模拟影响世界状况的动作
例如
画家可以在画布上留下新的笔触
并随着时间的推移而持续存在
尽管Sora在技术和性能表现上有了巨大的提升
它仍有不少的局限性
比如在理解复杂场景的物理原理、因果关系、空间细节、时间推移上存在弱点
例如它不能很好地表现玻璃碎裂
还有在吹蜡烛之前和吹蜡烛之后
火苗没有丝毫变化
它也搞反了人在跑步机上跑步的方向
目前OpenAI只是提供了生成的视频展示
还未向外正式开放Sora的使用
而是精心挑选了一批"受信任"的专业人士做测试
不过随着Sora的发布
也引发了人们对滥用视频生成技术的担忧
这个我们回头会专门做一期节目来讲讲
最后
我想再说说我对Sora这次发布的看法
虽然OpenAI有很多被人诟病的地方
包括年前的宫斗剧
但是在AI技术领域，确实牛逼
在别人还在使劲忙乎如何生成10s内低分辨率视频的时候
OpenAI直接干到60秒高清视频
甚至时间再长也完全有可能
一下子把对手甩到没影了
很有可能
在国内大厂还在宣传自己的模型逼近GPT-3.5效果的时候
GPT-5哪天就又一次这么悄无声息的发布了
又把所有人都干死了
我反复想问的是，为什么又是OpenAI
为什么他们能够如此彻底的贯彻缩放法则
如此彻底的坚信暴力美学
大力出奇迹
这背后又依靠的是什么样的信念和行动
更何况
科学家的信念离实现中间还有一个长期的工程落地的阶段
但是OpenAI又如此快的突破这个过程
再结合最近萨姆奥特曼的7万亿美元芯片计划
对将来会发生什么
大飞我真的是无法猜测
但是我知道的是
现在中美两国在AI方面的差距
绝对不是某些国内大佬所说的正在接近
反而是飞速的拉开
在算力方面的匮乏
在技术创新方面的突破
在底层大模型方面的缺失
更是会加快这个速度
而且国内现在AI领域的方向就是在号召人们不要做底层大模型
要在应用层找机会
我觉得这就像是鸵鸟把头扎进沙子里一样
自欺欺人
如果把AI比作原子弹
我们可能再也不会有当年搞原子弹时的决绝了
也不会有钱学森那样的一批人物出现了
机会一旦失去了，就是失去了
付出的代价将是长远的、沉痛的
时代的洪流正在朝我们奔涌袭来
只不过我们还一无所觉而已
好了，本期视频就到这里
感谢大家的观看，我们下期再见
