大家好，这里是最佳拍档，我是大飞
1月1日
OpenAI的研究员Jason Wei在CIS 7000上做了一个演讲
主要是关于缩放法则
或者称为扩展定律的Scaling Laws
Jason Wei毕业于斯坦福
曾经在Google Brain工作过三年
是思维链CoT的作者
也为o1模型做出了很大贡献
在社交媒体上也比较活跃
他的这次演讲
内容还是非常深入浅出的
所以想给大家分享一下
其实在过去的几年间
人工智能能取得如此令人瞩目的突破
应该说Scaling Laws扮演了核心引擎的角色
那么它究竟是如何推动了人工智能的发展
以及是否将继续推动向前发展呢？
今天就让我们来看看Jason Wei是如何看待的
在2010 年到 2017 年
也就是Transformer 架构和深度学习
还没有得到广泛应用之前的这段时间里
人工智能的进步主要依赖于针对特定的评估基准
比如对ImageNet进行优化
研究人员会尝试各种方法
比如构建更优的架构、引入归纳偏差、改进优化器以及精心调整超参数等等
目标是在基准测试中超越基线性能
比如在 ImageNet 数据集上
力求以一半的计算量
实现比基线高出 5%的性能提升
然而
Transformer 的出现改变了这个局面
它为学习多种类型的关系提供了强大的工具
使得Scaling成为人工智能发展的新方向
那么，什么是Scaling呢？
在人工智能领域
Scaling并非指的是简单地增加计算资源、数据量或者模型大小
更准确地说
它是将自身置于一种沿着连续轴移动
并且期望持续获得性能改进的情境之中
通常情况下
这个连续轴会涉及计算量、数据量或者模型大小等关键的因素
以大语言模型的发展为例
可以看到Scaling无处不在
在许多的相关研究论文中
都有关于Scaling Laws的图表展示
这些图表清晰地呈现了随着模型参数数量、训练数据量以及计算资源的增加
模型性能的变化趋势
不过，在早期
Scaling却面临着许多巨大的挑战
首先，从技术和运营层面来看
分布式训练需要深厚的专业知识
构建一个高效的分布式训练系统
需要聘请大量的专业工程师来应对复杂的技术难题
同时
机器学习的研究人员也需要时刻警惕
可能出现的损失发散和硬件故障问题
确保训练过程的稳定
此外
计算成本高昂也是一个不容忽视的问题
大规模的Scaling需要投入大量的计算资源
这对于许多研究机构和企业来说是一个沉重的负担
其次，在心理层面
研究人员长期以来习惯于利用归纳偏差来改进算法
他们从提出假设并验证性能提升的过程中获得乐趣
因此对于单纯的Scaling工作
可能缺乏足够的热情
而且，人类学习的高效性
也让人们对于让机器通过大规模数据进行学习的必要性产生了质疑
比方说
一个人不需要像训练GPT - 3时那样
通过阅读海量文本就能学会写一段英语
这就使得人们思考
是否真的需要让机器从如此庞大的数据中学习
再者，科学研究的激励机制
在当时也与Scaling所需要的工程工作不太匹配
学术会议更倾向于接受具有新颖算法的研究成果
而只是扩大数据集和计算资源的工作
往往难以得到足够的认可
那既然Scaling面临着如此多的困难
为什么我们仍然要坚持走这条路呢？
因为在非Scaling的范式下
模型的每一次改进都需要全新的独创性思维
这就需要投入大量的研究精力
而且成功并不是必然的
具有很大的不确定性
相比之下
以Scaling为中心的人工智能虽然成本高昂
却提供了一种相对可靠地提升模型能力的方法
特别是当我们衡量模型能力的标准
具有较高的通用性时
这种大规模的投资往往是值得的
比如
我们希望模型能够在多个领域和任务中表现出色
那么通过Scaling来提升模型的通用能力
就是一个合理的选择
接下来
Jason Wei深入探讨了第一个Scaling范式
那就是去Scaling下一个词的预测
这个范式始于 2018 年
至今仍然在发挥着重要的作用
这个范式的核心原理
就是通过大规模的多任务学习
来实现对下一个词的精准预测
想象一下，语言模型面对一个句子
比如“在周末，达特茅斯的学生喜欢”，
它会对词汇表中的每个单词
从A到Z打头
计算出一个出现的概率
然后根据实际的下一个词来调整这些概率
从而不断地进行学习和优化
通过这种方式
语言模型能够学习到多种能力
在语法学习方面
例如在预训练过程中遇到“在我的空闲时间
我喜欢去｛编码，香蕉｝”这样的句子
模型会逐渐认识到在这个语境下动词“编码”的可能性更高
从而学习到相应的语法规则
而在世界知识的获取上
当遇到“阿塞拜疆的首都是｛巴库
伦敦｝”这样的句子时
模型会提高“巴库”的权重
进而积累关于世界地理的知识
对于电影评论的情感分析
比如“我一直全神贯注
非常投入，这部电影真的是 ｛好
坏｝”，
模型可以学习到如何判断情感倾向
在翻译任务中
“神经网络在俄语中的单词是 {нейронная, сетьпривет}”
模型能够掌握不同语言之间词汇的对应关系
甚至在空间推理方面
通过“艾洛去厨房泡茶
祖克站在艾洛旁边，思考他的命运
然后祖克离开了 ｛厨房
商店｝”这样的句子
模型可以学习到空间位置的推理能力
在数学运算上
对于“三加四加八等于 {15
11}”这样的例子
模型也能逐渐学会正确的计算结果
2020 年
卡普兰等人发表的论文推广了下一个词预测中的Scaling范式
提出了Scaling Laws
这个定律表明
随着模型大小、数据集大小以及训练计算资源的增加
下一个词的预测能力
也就是语言模型的性能会平稳地提升
研究人员通过使用七个数量级的计算量进行训练验证
发现这个趋势非常稳定
而且没有出现性能饱和的现象
这个发现极大地增强了研究人员继续扩大规模的信心
那么
为什么Scaling能够取得如此好的效果呢？
对于小型的语言模型而言
由于参数有限，记忆成本非常高
所以在知识编码方面必须非常谨慎
而大型的语言模型拥有大量的参数
在学习尾部知识和记忆大量事实方面
具有更大的优势
例如
小型模型可能无法存储和利用一些较为罕见的知识
但是大型模型可以轻松应对
此外
小型模型在单次前向传递中的计算容量较低
主要学习一阶相关性
而大型模型在拥有更多计算资源的情况下
可以学习复杂的启发式方法
从而更好地处理各种任务
然而
尽管Scaling Laws具有一定的可预测性
但是 ChatGPT 的成功仍然让许多人感到惊讶
因为对下一个词的预测
实际上是一种大规模的多任务学习
不同任务的能力提升速度并不相同
我们可以将下一个词的预测准确性
看作是多个子任务准确性的加权总和
例如语法准确性、世界知识准确性、情感分析准确性、数学能力准确性、空间推理准确性等等
当模型整体性能提升时
不同任务的提升幅度可能会有很大的差异
比如，GPT-3.5的语法已经近乎完美
在后续训练 GPT- 4的时候
语法方面的性能提升可能微乎其微；
而在数学能力方面
GPT-3 和 GPT-2 表现较差
但是 GPT-4 却有了巨大的飞跃
这种现象被称为涌现能力或相变
以翻译任务为例
当给定提示“我喜欢踢足球和网球”，
并要求翻译成西班牙语的时候
较小的模型 Adam 和 Babbage 可能只是重复答案
无法正确完成翻译
而最大的模型 Curie 却能够突然学会并且完美地执行这项任务
这表明在模型规模达到一定程度后
一些原本难以完成的任务会突然变得可行
模型的能力出现了质的提升
但是
仅仅通过Scaling下一个词的预测
就想要实现AGI的想法可能会面临巨大的挑战
因为对某些词的预测非常困难
需要进行大量的计算和复杂的推理
比方说
在面对一个数学问题“什么是 ((8 - 2) * 3 + 4) ^ 3 / 8 的平方？
”时，为了预测下一个词
也就是正确答案 A、B 或 C
模型实际上需要完成整个数学计算过程
这对于单纯的下一个词预测来说是一个巨大的瓶颈
为了解决这个问题
研究人员提出了思维链提示的方法
这种方法类似于我们在解决数学问题时向老师展示解题的过程
要求语言模型在给出最终答案之前输出推理链
实践证明
这种方法在数学应用题基准测试中效果显著
能够大幅提升模型的性能
并且随着模型规模的扩大
性能提升效果更加明显
不过
思维链提示也存在一定的局限性
在互联网上的大部分数据中
模型训练所依据的推理过程往往是事后总结的
而不是真实的思维过程
例如，大学数学作业的解决方案
通常是经过整理和完善的
与我们实际的思考过程可能存在着差异
我们真正希望模型能够模拟人类的内心独白或者思想流
比如“我先看看我们应该采取什么方法
哦，我试试这个
实际上好像错了
我再试试别的方法
我算一下这个
好了，答案对了
这是我的最终答案
”但是目前的训练数据还难以完全满足这一要求
于是这就引出了第二个Scaling范式
那就是在思维链上去Scaling强化学习
这个范式的核心思想是训练语言模型在给出答案之前进行思考
除了像传统的扩展训练计算量之外
还增加了一个新的维度
也就是扩展语言模型在推理时可以思考的时间长度
OpenAI 发布的 o1 模型就是这个范式的典型代表
在解决化学问题的时候
o1 模型会首先明确问题
比如“首先
让我们理解一下问题是什么”，
然后逐步分析问题
确定存在哪些离子
考虑不同的计算策略
比如计算 pH 值时会思考 ka 和 kb 值的关系
不断回溯和调整思路
最终得出正确答案
在填字游戏、数独等具有验证不对称性的问题上
o1 模型也表现出色
那什么是不对称性问题呢
就是验证一个解决方案比生成一个解决方案要容易得多
比方说，在解决填字游戏时
o1会先思考横行可能的答案
然后根据竖行的线索进行验证和调整
逐步找到正确的答案
在竞赛数学和竞赛代码等需要大量思考才能够获得良好表现的问题上
o1 模型相比 GPT - 4o 有了巨大的提升
在竞赛数学数据集上的 pass@1 准确率
会随着训练计算量的增加而提高
并且在推理时给模型更多的时间思考
也能在基准测试上取得更好的成绩
这表明在思维链上Scaling强化学习的范式
为模型处理复杂问题提供了更强大的能力
从长远来看
我们希望人工智能能够帮助我们解决人类面临的一些最具挑战性的问题
比如听力疾病、环境保护等等
在未来
我们可以想象为一个非常具有挑战性的问题
提供一个提示
例如撰写一篇关于让 AI 更安全的最佳方法的研究论文
语言模型可以在推理时分配大量的计算资源
经过长时间的思考和分析
可能需要在数千个 GPU 上处理一个月
最终返回一个全面的答案和研究成果
另外
随着Scaling Laws在人工智能领域的广泛应用
它也深刻地改变了 AI 文化
在数据方面
过去研究人员主要致力于改进神经网络
来学习特定的 XY 关系
而现在的重点更多地转向了收集更好的 X 和 Y 集合
例如
谷歌的 Minerva 论文揭示了通过在大量的数学数据和档案数据上
对现有语言模型进行持续训练
可以显著提高模型的数学性能
这表明数据的质量和多样性在模型训练中越来越重要
在评估方法上
目前行业中存在一个急需解决的问题
那就是缺乏能够准确捕捉语言模型能力边界的评估方法
从一些基准测试的发展情况来看
比如GPQA
它是近年来最具挑战性的基准测试之一
但是在大约一年内就被 o1 模型给跑饱和了
这说明现有的评估基准很容易被前沿模型达到性能上限
难以真正衡量模型的全部能力
我们需要更加完善和多样化的评估方法
来准确评估模型的性能和进步
在模型类型上
出现了从单一任务模型向高度多任务模型的转变
过去
每个自然语言处理任务都需要单独的模型
而现在一个模型可以尝试完成多种不同的任务
不过，这也带来了一些挑战
比如在不同任务和维度上衡量模型的优劣
变得更加复杂
一个模型可能在某些方面表现出色
比如能成为竞赛级的程序员和数学家
但在一些简单的比较问题上
比如判断 9.11 和 9.8 哪个更大的时候
却可能出现错误
这说明
我们不能仅仅依靠单一的指标
来评估模型的整体性能
需要综合考虑多个方面的因素
在团队规模上也发生了显著的变化
在 2015 年，像迪德里克·金马（Diederik Kingma）和吉米·巴（Jimmy Ba）这样的两个人
就可以写出具有开创性的论文
比如被广泛引用《Adam：
一种随机优化方法》论文
然而
如今构建像 o1 或者Gemini 这样的大型模型
则需要一个庞大的团队
这反映了随着人工智能技术的发展
项目的复杂性在不断增加
需要更多的专业人员参与到模型的研发和训练过程中
最后
Jason Wei展望了一下人工智能的未来
认为AI在多个方向上有着巨大的发展潜力
比如在科学和医疗保健领域
减少模型的幻觉方面
多模态，以及AI对工具的使用
另外
未来要重点关注的一个点在于人工智能的应用落地
虽然目前在某些技术领域已经取得了很大的进展
但在实际部署和广泛应用方面仍然存在一定的差距
比如在全球范围内大规模部署自动驾驶技术
仍然面临许多挑战
我们需要进一步缩小技术前沿与实际应用之间的差距
让人工智能真正造福人类社会
好了
以上就是Jason Wei这次演讲的主要内容了
希望能让大家更好地理解Scaling Laws的历史和发展
感谢大家的观看，我们下期再见
