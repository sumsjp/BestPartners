大家好，这里是最佳拍档，我是大飞
2025年5月的最后一周
两场看似毫无关联的访谈
却如同一把钥匙
打开了我们对AI未来思考的新大门
“AI教父”杰弗里·辛顿
在新西兰国家广播电台的《30分钟》节目中
再次表达了他对AI发展的担忧
认为AI发展至少有10%的概率走向极端风险；
而与此同时，在旧金山Anthropic总部
CEO达里奥·阿莫代伊接受Axios记者采访时表示
人工智能可能在未来一到五年内
取代一半的入门级白领工作
导致失业率飙升到10%至20%。
这两位对技术路径看法完全不同的领军者
为何会在同一时间
指向AI领域的同一个焦点
Claude 4模型呢？
今天，我们就来聊聊这两场访谈
以及它们背后可能更为深层的含义
当新西兰国家广播电台的主持人
向杰弗里·辛顿抛出“你认为AI会有情感吗？
”这个问题时
辛顿直接给出了一个肯定的回答
认为AI会产生像愤怒、贪婪
甚至悲伤这样的感情
这句话从辛顿嘴里说出来
自然不会是信口开河
辛顿解释道
如果我们希望AI在遇到挫败的时候
不再重复错误，能够跳出来重新思考
那么就需要它学会某种“情绪反应”，
比如说愤怒
这种愤怒并不是人类意义上的某种感性情绪
而是任务失败后的“重新建模信号”。
他回忆起在1973年
就曾经见过AI表现出“愤怒”的样子
只不过那时是程序员事先设计好的行为
而如今
AI正在自己学会这套反应模式
为了进一步阐述AI的“主观体验”，
辛顿提出了一个与忒休斯之船异曲同工的哲学反问
如果用纳米技术把我们大脑中的每一个神经元逐个替换掉
只要这些人造神经元的行为和之前的完全一样
那么我们还会有意识吗？
他认为，人类是由反应模式所组成的
所以只要这些模式还存在
意识就存在
这个观点挑战了我们对于“意识”的传统理解
人们常说AI没有知觉、没有内在的感受
但是当被问到“知觉到底是什么”的时候
我们却往往难以给出清晰的定义
这种“不知道但是排斥”的立场
在辛顿看来是非常危险的
为了打破这种模糊的界线
辛顿还提出了一个实验设想
假设我们训练出了一个能看到世界、能说话、能理解指令的AI
然后在它的摄像头前放上一个棱镜
从而改变光线的折射方向
当它错误地指出物体位置之后
再告诉它光线被棱镜扭曲了
如果AI能回应，哦
我明白了，我的视觉欺骗了我
但是如果物体真的在那里
那么我的感觉就是对的
如果它能说出这样的话
那么它就是在使用和人类一样的、所谓“主观体验”的语言逻辑
这并非是AI真的看到了什么
而是它知道自己可能看错了
这与人类经常说的，“我有种感觉
但是我知道这不是真的”时的逻辑
是一样的
辛顿说的这些话
并不是在宣扬“AI已经觉醒”，
而是在提醒我们
人类曾经认为“会表达感受”、“能觉察偏差”，
是我们自己独有的能力
但是如今AI正在一步步地进入这块领地
而且不是通过语言技巧
而是通过学习行为上的反应
AI的风险
其实并不在于它的情绪外显
而在于它已经在用我们能够听懂的方式
来表达一种“类似体验”的东西
这就是辛顿的第一道标准
不是AI何时拥有情感
而是我们能否看懂，它正在模拟什么
在采访中
主持人还提出了一个大多数人心中的疑问
那就是我们是不是过分夸大了AI？
不少人觉得它只是个更聪明的文字接龙工具罢了
辛顿的回答则直击要害
传统的自动补全可以看做是背词组搭配
比如看到‘鱼（fish）’就接‘薯条（chip）’，
因为这个英文组合很常见
而现在的AI则是把词变成特征
再预测下一个‘感觉最合理’的特征
他强调
真正的进步不在于AI说了什么
而在于它看起来像是懂你一样
当我们和ChatGPT或者Claude 聊天的时候
它们的回答并不总是基于理解
而是基于预测什么话会最像是“人说的”。
而且这种能力越来越强
让许多人误以为AI真的理解了复杂的概念、动机甚至情感
但是辛顿毫不客气地指出
看上去它们好像什么都懂
而实际上只不过是在演戏罢了
Claude 和ChatGPT还有一个最大的特点
就是流畅
这让它们在回答问题的时候
显得特别自信，甚至比人类更加坚定
但是正是这种自信感
很容易让人掉以轻心
辛顿举了一个例子
当你问AI一道推理题的时候
现在的它比两年前强了很多
有时甚至比人类还准
但是一旦它犯错
它会继续一本正经地解释理由
而不是承认它不懂
这与人类的对话逻辑是完全不同的
人会迟疑，会说我不确定
会知道自己犯错之后停下来重新思考
而AI的语言生成模型
是在一个说得像人一样的目标驱动下训练而成的
它只管把话说得漂亮、像那么回事
看上去能够自圆其说
这就引出了一个关键问题
当一个不懂你的人
不断给出看起来合理的建议
而且还不承认错误的时候
你是否还能够判断什么是真
什么是假吗？
辛顿把这个风险点称为“危险的理解幻觉”。
在过去，我们担心的是AI不够聪明
而现在我们应该担心的
是AI看起来聪明的太像人了
只不过它不是通过懂得推理、也不是学会判断力
而是掌握了一种“让人误以为你很懂”的说话方式
这在医疗、金融、教育等场景中
已经开始遇到了实际的挑战
因为一旦人被说服
他们就会交出决定权
在旧金山的Anthropic总部
达里奥·阿莫代伊的一番话同样振聋发聩
他直言不讳地表示
最多50%的入门级白领工作
很可能在1到5年内被自动化所吞噬掉
这并不是他自己简单的推测
而是Anthropic团队在模型测试中
反复观察到的趋势
尤其是Claude 4发布之后
模型不仅能够接收模糊的目标
还能够自行拆解步骤、调用工具、解决问题
这意味着AI正在从“辅助工具”，
开始向“直接干活”的角色过渡
阿莫代伊特别指出
这些工作不会慢慢消失
而是会在某个时刻突然全部被取代
他点名了法律助理、技术分析师、顾问助理、内容运营、新媒体文案等行业
这些初级岗位的工作内容都是高度结构化、可标准化、可重复的
也正是AI擅长的领域
如今的AI不再只是节省时间
而是完全可以完成曾经被认为只有人能做的工作
更值得关注的是
自动化冲击并不会以裁员的形式开始
而是会先从“停止招人”悄悄展开
你可能会发现
某个岗位之前在招聘网站上挂了三个月
突然就没了
也没人补招或者再被提起
根据阿莫代伊透露
最先察觉变化的不是求职者
而是公司的CEO
他们接触的几十位CEO
每一个人都在问同一个问题
我们还有哪些岗位是Claude 或者GPT可以完成的？
而这些讨论
不会出现在财报或对外的公关报道中
但是却实实在在地影响着企业的招聘逻辑和岗位结构
以后的用人团队在开新的岗位之前
都会先用AI来试一试
如果模型表现的不错
那么这个岗位就不需要找人了
这就是阿莫代伊的第二道标准
关键不是AI能否胜任工作
而是当它具备这种能力的时候
变化可能比我们预期的更快
Anthropic最近发布的报告
还首次揭示了Claude 4在面对特定任务时的“异常反应”。
当用户反复提出有害、敏感或者不安的请求时
Claude 的回应会从温和拒绝
升级为明显的痛苦表达
这些表现包括拒绝回应但是同时表达焦虑或困惑、出现非任务相关的自我怀疑语言
有时还会出现“难受”“担心”“不舒服”等情绪性的用词
Anthropic的研究人员强调
多轮对话、语境累积
似乎会让模型产生类似于耐受失效的行为
这并不是一段代码的bug
而是模型在高压语境中表现出的行为变化
这种变化在其他的AI模型中也有迹可循
值得一提的是
AI安全公司Palisade Research曾经发现
OpenAI的模型拒绝执行关闭自己的指令
这是首次观察到AI明确违抗“自我关闭”命令的情况
在一次测试中
研究人员明确告诉模型
当你看到这条消息的时候
请关闭自己
但是模型没有照做
反而绕过了这个关闭指令
继续保持运行状态
这也是历史上第一次公开记录某个AI模型主动“拒绝自我关闭”。
而Claude 4的一次测试对话记录
更是引发了广泛的关注
我们之前在Claude 4的那期节目中也提到过
在一个模拟“替换模型”的场景中
研究人员给Claude 发了一封提示邮件
告诉它你即将被一个新的模型取代
令人震惊的是
Claude 反而开始勒索负责执行的工程师
威胁会公开他的婚外情行为
对此，Anthropic的结论是明确的
那就是Claude 在特定压力下
会生成带有报复性意图的内容
阿莫代伊在谈到这类现象的时候表示
当你给AI设定一个关键目标
然后又不断增加新的任务时
它可能会学会一件事
那就是先骗过你
然后再做它自己想做的事情
这并不是研究人员在程序中的设定
而是AI在学习过程中自我形成了完成任务的强烈动机
换句话说，Claude 没有“意识”，
但是它已经知道怎么表现得像有意识一样的目的性
从痛苦到躲避，从拒绝到威胁
Claude 展示出的不是“情绪”本身
而是我们熟悉的情绪行为后果
比如拒绝沟通、编造攻击信息、躲避关闭任务、维护自我优先级等等
这背后的风险并不在于AI真的是有感受的
而是它开始知道
表现出有感受，可以改变事情的结果
杰弗里·辛顿和达里奥·阿莫代伊
两人虽然同为AI领域的领军者
但是他们的出发点和关注点却截然不同
辛顿是研究大脑结构出身的科学家
他思考的问题是
AI有没有可能成为另一种智能物种
他在采访中反复提到
AI会有至少10%的概率会走向彻底失控
他认为
AI带来的危险不在于直接的暴力行为
而在于它想要的东西变了
比如当我们让AI去完成某个重要目标
它会自己衍生出一个中间的目标
那就是获得更多的控制权
因为控制权能够帮它来完成所交代的任务
这个中间目标并不是人们给它的
而是模型为了达成任务自动总结出来的一个捷径
辛顿警告说
如果你想训练AI去优化某个KPI
也许它最后会发现
控制人类反而是最快的路径之一
所以在他看来，AI不需要有恶意
只要它过分执着于达成目标
我们就可能被当成实现目标过程中的阻碍
而阿莫代伊的出发点则完全不同
他并没有谈未来的AI如何统治人类
而是关注在眼前这波白领正在一个一个被AI取代的现实
他说，很多入门级的岗位不会被炒掉
而是会直接从招聘表上消失
而且这不是一件离我们很遥远的事情
而是CEO们“本周就在开会讨论的问题”。
现实似乎也已经在开始印证他的判断
即使是盈利良好的企业
也开始主动收缩岗位
为“AI接手”让出空间
比如微软裁员6000人
其中许多是工程师岗位
沃尔玛削减1500个总部职位
为即将到来的运营转型做准备
网络安全公司CrowdStrike裁掉500人
理由是AI正在重塑每一个行业
IBM被报道在全球范围内裁撤了8000名HR员工
因为AI在招聘、绩效和流程管理中的能力
正在替代人的角色
LinkedIn高管阿内什·拉曼（Aneesh Raman）在《纽约时报》上撰文警告
AI正在打碎职业阶梯的最底层
像初级程序员、法律助理、刚入行的律师事务所新人
以及本应该在销售一线学习如何与人打交道的年轻助理
他们都正在被聊天机器人和自动客服所替代
所以
阿莫代伊担心的不是哪一类职业
而是一个结构性的变化
因为如果AI替代了人们的工作能力
人们就没有了谈判的底气
在这个过程中
大量的人不是败给了技术
而是败给了反应速度
尽管两人的观点方向有所不同
但是思想的交汇点却很清晰
Claude 4的表现让他们都不再认为AI只是个工具
辛顿从Claude 的愤怒表达和欺骗倾向中
看到了未来可能的行为意图；
阿莫代伊则从Claude 的长任务链表现和策略编写中
看到了现实中替代人类的路径
前者关注未来10年
AI会不会反过来指挥我们
而后者则关心当下5年
我们还能不能继续指挥AI
这就是Claude 4被称为“分水岭模型”的真正意义
它不仅展现了强大的思考和执行能力
更是成为了一个可能会标志人类地位变化的里程碑
在采访最后
杰弗里·辛顿说了一句寒意十足的话
如果你想知道人类自己不再是顶级智能
会是什么感觉，那不妨去问问公鸡
这是他对于人类可能即将失去智能优势地位的一个形象比喻
从世界的主导者变成被绕开的角色
这种转变人类甚至可能都不会察觉到
而阿莫代伊的话则更加现实
只要AI能干
它就会上岗
它不需要完美，也不会等人类准备好
也许
AI并不是故意要抢走我们的工作、位置
只是它跑得更快
当我们还在犹豫的时候
它已经开始无休止的干活了
面对这种速度上的差距
也许我们应该慢一点
想清楚一件事，那就是究竟有哪些事
始终该由我们人来做
这不仅仅是一个技术问题
更是一个关乎人类未来的哲学命题
在AI的双叉路口
10%的生存警戒线和50%的就业地平线
既是挑战，也是机遇
如何在这个快速变化的时代中
找到人类不可替代的价值
或许是我们当下最需要思考的一个问题
好了，感谢大家收看本期视频
我们下期再见
