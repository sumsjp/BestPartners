大家好，这里是最佳拍档，我是大飞
最近
谷歌现任首席科学家杰夫·迪恩Jeff Dean
和出走又回归的Transformer作者之一诺姆·沙泽尔Noam Shazeer
与知名播客主持人德瓦克什·帕特尔Dwarkesh Patel展开了一次对谈
视频刚刚发布几个小时
就有超过20万网友在线围观
由于两人都是谷歌的骨灰级员工
经历了从MapReduce到Transformer的整个阶段
所以谈话的信息量巨大
而且又都是老相识，对谈非常放松
Noam Shazeer甚至提到
当初入职谷歌只是为了捞一笔就跑
却没想到成了改变世界的那个人
果然是个耿直boy
现在
他们作为Gemini的三位联合负责人中的两位
正在引领着谷歌迈向人工智能的新高度
今天大飞就来帮大家总结一下对谈的主要内容
建议大家有空还是能去看下原视频
让我们把时间拨回到过去
看看他们是如何与谷歌结缘的
杰夫·迪恩说是自己主动联系谷歌的
而诺姆·沙泽尔的经历则更有意思
1999年，他在招聘会上看到谷歌
当时以为谷歌已经是一家非常庞大的公司了
凭直觉认为去了可能也无自己的用武之地
所以并没有申请
直到2000年
他抱着试一试的心态向谷歌发送了简历
有趣的是，当他加入谷歌后
看到墙上记录的每日搜索查询数量图表
呈现出了指数增长的趋势
他开始觉得这些人应该会非常成功
于是心想，不如在这里挣一笔钱
然后开开心心去搞自己感兴趣的AI研究
刚加入谷歌的时候
诺姆·沙泽尔就被安排了一位无所不知的导师
正是杰夫·迪恩
随着谷歌的不断发展壮大
公司的规模变化给他们带来了深刻的感受
早期，谷歌人数还比较少
大家彼此都很熟悉
能够记住每个人的名字
但是随着公司的扩张
逐渐出现了一些他们不了解的项目
比如像杰夫·迪恩突然收到一封关于启动“鸭嘴兽计划Project Platypus”的邮件
把他直接搞蒙了，这是什么鬼？
当然他们后来也意识到
即便不能了解公司的每一个细节
但是能在较高的层面上知晓公司的动态
建立良好的人脉网络
对于获取更多信息也是有帮助的
在科技领域
摩尔定律一直是影响技术发展的重要因素
过去，按照摩尔定律
大约每18个月就能获得性能更快的硬件
这就让系统设计在一定程度上
非常依赖硬件的自然升级
但是近几年来，情况逐渐发生了变化
通用CPU的扩展性已经不如从前
制造工艺的改进周期
从每两年一次延长到了三年
多核处理器等架构改进带来的性能提升
也是大不如前
与此同时，专门的计算设备
比如机器学习加速器和GPU却大量涌现
如今，算术运算已经变得非常廉价
而数据移动则相对昂贵
深度学习之所以能够蓬勃发展
很大程度上就是因为可以利用矩阵乘法来构建模型
其中乘法的运算量巨大
但是数据通信量相对较小
谷歌的TPU就是顺应这种趋势的产物
它本质上是一个降低精度的线性代数机器
早期
TPU v1甚至不确定是否能服务8位整数的量化模型
但是随着技术的发展
人们不仅可以用更低的精度进行训练
推理精度也得到了提高
现在已经出现了
甚至有人将模型量化到两位或者一位
这种量化趋势不仅需要硬件的支持
更需要算法设计者和芯片设计师的协同设计
才能提高吞吐量和成本比
接下来
杰夫·迪恩回顾了一下自己的学术生涯
1990年还在读本科的时候
他在一门关于并行计算的课程中接触到了神经网络
在关于平行反向传播的毕业论文中
他决定实现几种不同的并行化神经网络反向传播训练方法
在当时，一个有3层
每层分别10、21、10个神经元的神经网络
就已经算很大了
因此他在32个处理器的机器上进行了实验
采用了模式分割法和网络分割法
比较和对比了两种方法下模型并行和数据并行的效果
最终，尽管这篇论文只有8页
却成为1990年的最佳本科论文
被明尼苏达大学图书馆保存至今
虽然当时由于计算能力的限制
神经网络还无法有效地处理实际的问题
但是他坚信
神经网络是解决问题的正确方向
直到2008年末到2010年左右
随着摩尔定律带来的计算能力提升
神经网络才开始在实际应用中崭露头角
时间来到2007年
杰夫·迪恩参与的一项语言建模研究
为后来大语言模型的发展埋下了伏笔
当时
谷歌的机器翻译研究团队在DARPA竞赛中取得了优异成绩
但是翻译系统存在效率问题
翻译一个句子需要12个小时
杰夫·迪恩发现
问题在于系统没有为高吞吐量进行设计
翻译过程涉及大量的磁盘寻道
为了解决这个问题
他花了两到三个月时间
与团队合作设计了一种内存中的压缩n-gram数据表示
他们使用来自大约2万亿个单词的数据
构建了一个数据结构
将信息存储在200台机器的内存中
并且创建了批处理API
实现了同时请求10万个短语的翻译
然后并行返回结果
从而将翻译的时间
从一整夜缩短到大约100毫秒
这项成果不仅提升了谷歌翻译的性能
也为语言模型在其他领域的应用奠定了基础
比如后来的文本完成、拼写纠正等等功能
说到拼写纠正
诺姆·沙泽尔在2001年构建的拼写纠正系统也堪称经典
他向整个公司发送了系统的演示链接
能够准确识别各种错误拼写
像“scrambled Uggs Bundict”，
系统能正确纠正为“scrambled eggs Benedict”，
而且每次都完美命中
展示了早期语言模型在实际应用中的强大能力
随着人工智能技术的不断发展
AI在谷歌的业务中也开始扮演越来越重要的角色
杰夫提到
谷歌对自己的定位不仅仅是一家信息检索公司
更是想致力于让世界上所有的信息
变得可以访问和有用
这其中就包括创建新的信息
比如
根据用户的需求生成信件、视频摘要等等
此外，AI的多模态能力
也让它能够理解和处理各种不同类型的信息
包括文本、图像、音频
甚至像自动驾驶汽车上的激光雷达传感器数据、基因组信息等等
在上下文搜索方面
谷歌正在进行积极的探索
目前的大语言模型虽然在一定程度上能够理解上下文
但是存在幻觉和事实性问题
这是因为模型在训练过程中
融合了数万亿的token
导致信息模糊
而谷歌搜索拥有整个互联网索引的上下文
虽然是浅层搜索
但是如果能够将两者结合
让模型关注到数万亿的token
甚至个人信息
就能更好地解决这些问题
不过，这面临着巨大的计算挑战
因为朴素的注意力算法是二次方的
所以要想将模型的上下文窗口
从数百万个token扩展到数万亿个token
需要一系列算法近似来实现
在内部编程模型的应用上
谷歌已经取得了一定的成果
他们使用内部代码库对Gemini模型进行了进一步的训练
根据统计
现在谷歌内部提交到代码库中的代码
有25%是由基于AI的编码模型生成的
从未来的发展来看
研究人员可以利用AI自动生成实验代码
提高研究效率
甚至有可能实现让AI自动编写代码的设想
即指定分布式AI库
比如
有人在Reddit上试着使用了谷歌的新编码模型
要求它实现一个没有外部依赖的SQL数据库
这个模型不仅生成了SQL解析器、分词器、查询规划系统以及数据存储格式
还能够处理简单的查询
这充分证明了AI在软件开发方面的能力
展望未来，杰夫迪恩认为
研究员和软件工程师的工作模式可能会发生巨大变化
AI将成为他们强大的助力
从高级规范或者句子描述中生成初步的结果
帮助他们专注于更高层次的设计和问题解决
但是同时，这也带来了新的挑战
比如如何管理大量由AI生成的任务
以及如何设计更好的界面
来跟踪这些任务的进展等等
在芯片设计领域
如今AI也发挥着重要作用
传统的芯片设计流程漫长
从决定制造芯片到将芯片交给台积电
再到最终放入数据中心
整个周期大约需要22个月
其中设计过程占据了较长时间
而通过自动化的搜索过程
有望将设计时间缩短到只需几个人月
不过
制造时间仍然是一个关键的限制因素
新的前沿节点由于金属层增多
制造时间往往要延长到三到五个月
推理计算如今在AI系统中
也开始占据越来越重要的地位
目前来看
即使是一些大型的语言模型
每token的运算成本也非常低
比如每token执行一万亿次运算
成本大约是10的负18次方美元
相当于获取一百万个token只需要一美元
这比阅读书籍或者与人员交谈
成本要低得多
因此，推理时计算具有很大的潜力
会让AI系统变得更聪明
但是目前在延长推理时间方面
还需要解决一些问题
比如设计新的算法
更好地利用增加的计算资源来提高答案的质量等等
同时
推理计算也会影响数据中心的规划
需要考虑如何专门化硬件
来满足不断增长的推理计算需求
提到数据中心，杰夫指出在训练方面
谷歌已经支持了多数据中心的模型训练
例如，在Gemini 1.5的训练中
他们使用了多个城市区域的算力
在数据中心之间建立了延迟较高
但是带宽很高的连接
这种方式能够有效地利用多个数据中心的资源
提高训练效率
但是随着规模的扩大
这样做也面临一些挑战
比如如何在系统中更好地使用异步性
以及如何解决大规模调试的问题
其中
大规模调试是AI研究和开发中的一个重要环节
Noam Shazeer提出一个有意思的观点
那就是bug有时候也是好东西
因为模型在训练时可能会遇到各种各样的bug
但是由于噪音的容忍度
模型可能会自我调整
从而产生未知的效果
甚至有的bug会产生正面影响
随着规模的扩大
bug反而会让研究人员发现新的改进机会
在小规模实验中
研究人员可以通过简单易懂的代码库进行实验
快速得到初步结果
但是在大规模集成实验中
不同改进之间可能存在着相互作用
导致实验结果不如预期
比方说，视频数据输入方式的改进
以及模型参数更新方式之间
可能存在复杂的相互影响
研究人员需要不断尝试
将各种改进叠加在一起
观察它们在大规模上的协同效果
找出影响模型效果的最突出因素
并且解决这些问题
AI的持续学习和模块化发展也是未来的重要方向
杰夫·迪恩一直是稀疏模型的忠实粉丝
他认为模型的不同部分应该擅长不同的事情
比如Gemini 1.5 Pro模型和其他混合专家模型
某些部分会针对特定的token激活
而其他部分则不激活
这样可以构建出功能更为强大且推理高效的模型
不过
目前的模型结构仍然存在一些局限性
比如结构过于规则化
各个专家的大小大致相同
路径合并较快
缺乏针对特定任务的明显分支等等
Jeff Dean还提出了一个更加具前瞻性的设想
那就是在未来
应该让模型的各个组件能够更加独立地开发
一小群人专注在特定的语言或者任务子集
通过创建高质量的训练数据来训练模型的模块化部分
然后将这些模块化部分集成到更大的模型中
从而增强模型在特定领域的能力
在模型的内存管理方面
对于拥有数百亿或者数千亿参数的模型来说
目前在运行的时候需要将整个模型加载到内存中
而对于混合专家模型来说
虽然某个专家在特定的时刻可能没有被使用
但是为了效率
仍然需要检索它的内存
未来
可能会根据专家的使用频率和计算成本
更加灵活地分配内存资源
比如将使用频繁的专家副本
放在高性能内存中
而将使用较少的专家分配到普通内存中
从而实现更好的负载平衡
从更宏观的角度来看
杰夫设想谷歌未来可能会提供一个大型的基础模型
然后针对不同的设置提供自定义版本
这些版本可以添加不同的模块
并且有访问限制
例如
谷歌内部员工可以使用在内部数据上训练的特定模块
而其他公司也可以根据自身需求添加对自己有用的模块
并且通过云API提供服务
这种模式可以让模型的应用更加灵活
满足不同用户的需求
随后
三人还聊到了开放研究成果的问题
谷歌在过去发布了许多的研究成果
比如Transformer论文
对整个行业的发展起到了巨大的推动作用
但是同时谷歌也面临着
如何平衡发布研究成果与保持竞争优势的问题
目前来看
有些技术对谷歌的产品来说可能非常关键
也许就不会发布；
有些技术在应用到产品之后
可能会根据情况来决定是否发布相关信息；
还有一些技术则会公开出版
从而推进整个领域和社区的发展
回顾杰夫·迪恩和诺姆·沙泽尔在谷歌的25年
他们经历了谷歌从早期到如今的辉煌发展
见证了人工智能领域的巨大变革
在谷歌最初的四五年里
杰夫·迪恩参与了搜索、爬虫和索引系统的工作
看着系统的流量快速增长
索引大小不断地扩大
更新频率逐渐提高
这种成长让他感到非常快乐
如今，他与Gemini团队一起工作
看到模型能力的快速提升
同样也感到非常兴奋
诺姆·沙泽尔也有同感
早期他在谷歌认识了很多人
社交氛围浓厚
构建的产品被数百万人使用
而现在在谷歌新大楼的名为“梯度穹顶”的微型厨房区域中
人们可以自由地、面对面地交流想法
还有遍布全球的团队通过Gemini聊天室分享成果
这种工作环境让他非常享受
聊到这里，两人都不禁有些手舞足蹈
展望2030年
杰夫迪恩认为世界对计算能力的需求将大幅增长
随着AI模型的不断发展
推理计算的需求将变得极其庞大
一方面，提高模型的质量
可能需要扩大推理计算量
使得生成相同数量输出的请求计算量增加；
另一方面
AI服务的使用量将大幅增加
更多的人会发现并且频繁使用基于聊天的对话界面
模型本身可能也会变得更大
因此
未来需要极其高效的硬件来满足这些需求
当然
谷歌也在不断投资研发新的硬件
从而保持在这个领域中的优势
最后，主持人问道
对于普通的打工人来说
如何获得像他们这样成功的职业生涯？
杰夫·迪恩认为
了解新的有趣领域的最好方法
就是关注行业动态
与同事交流，以及关注研究论文
同时，尽量与不同领域的人合作
才能实现知识转移
共同完成一些个人无法单独完成的事情
这样做不仅有助于解决实际的问题
还能丰富自己的知识和技能储备
诺姆·沙泽尔则强调了谦逊的重要性
要能够放弃不好的想法
接受更好的想法
在资源分配方面
则需要平衡自上而下和自下而上的方式
在激励协作的同时，保持灵活性
此外，向大家阐明有趣的研究方向
也能引导团队朝着有价值的方向前进
好了
以上就是二人这次访谈的主要内容了
希望对大家能有所帮助
感谢大家的观看，我们下期再见
