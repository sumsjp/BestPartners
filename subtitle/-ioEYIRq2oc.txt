大家好，这里是最佳拍档，我是大飞
在全球顶会NeurIPS 2024中
伊利亚·苏茨克维尔Ilya Sutskever在从OpenAI离开后
首次在公开场合登台演讲
并且用短短16分钟的发言震撼全场
他向全世界宣告，预训练结束了
数据如同化石燃料一般，难以再生
AI未来的发展方向
将是具备自我意识的超级智能
虽然他的演讲不长
我相信很多人也已经看过了
但是作为我们频道来说
大飞我还是觉得有必要做这么一期节目
也许过两年回头来看
这场演讲又可能具有划时代的重要性
但是站在现在来看
我们又将走向何方呢？
就在11月底的时候
NeurIPS 2024公布了时间检验奖
Ilya和GAN之父伊恩·古德菲洛Ian Goodfellow获奖
所以在演讲一开始
Ilya首先感谢了论文《利用神经网络进行序列到序列学习（Sequence to Sequence Learning with Neural Networks）》的两位合著者
奥里奥尔·维尼亚尔斯Oriol Vinyals和Quoc Le
并且放出了三人在十年前
2014年蒙特利尔NeurIPS 会议上的一次演讲合照
Ilya说，那是一个更加纯粹的时代
而如今
三位青葱少年已经长成了这般模样
接下来
Ilya重温了一下10年前演讲的PPT
他们的工作
可以用以下三个要点概括，分别是
这是一个基于文本训练的自回归模型
它是一个大型神经网络
它使用了一个大规模的数据集
当时他们提出了一个名为「深度学习假设」的概念
指的是如果你有一个10层的大型神经网络
它就可以在几分之一秒内
完成任何人类能做的事情
为什么要强调在几分之一秒内呢？
如果你相信深度学习的基本假设
也就是人工神经元和生物神经元是相似的
并且你也相信
真实神经元的速度比人类快速完成任务的速度更慢
那么只要全世界有一个人
能够在不到一秒内完成某项任务
那么只要把一个10层的神经网络的连接
嵌入到你的人工神经网络中
它就应该也能做到
而这，就是他们的动机
之所以专注在10层神经网络
是因为在那个时候
这就是他们能够训练的神经网络
当然，如果能够突破10层
就可以完成更多的事
然后，Ilya用这张PPT
描述了他们的主要想法
核心观点就是
如果你有一个自回归模型
并且它能够足够好地预测下一个Token
那么它实际上会抓取、捕获
并且掌握接下来任何序列的真实分布
在当时来说
这绝对是一个相对新颖的观点
尽管它并不是第一个被应用在实践上的自回归神经网络
但是Ilya认为
这是第一个令他们深信不疑的自回归网络
如果把它训练得足够好
那么你就会得到想要的任何结果
于是，他们选择尝试了翻译
这个如今看来平凡无奇
在当时却极具挑战性的任务
接下来
Ilya展示了一些可能很多人从来没见过的古老历史
LSTM
不熟悉的人可能会觉得
LSTM是Transformer出现之前
深度学习研究者所使用的工具
它可以被看作是一个旋转了90度的ResNet
但是更加复杂一些
我们可以看到积分器（integrator）
如今被称为残差流（residual stream）
还涉及到一些更为复杂的乘法操作
Ilya还想强调的一点是
他们当时使用了并行化
不过并不是普通的并行化
而是流水线并行化（pipelining）
每层神经网络都分配一块GPU
从今天来看，这个策略并不明智
但当时的他们并不知道
于是，他们使用8块GPU
实现了3.5倍的速度
而正是从这里，Scaling Laws开始了
最终
Ilya放出了那次演讲中意义最为重大的一张PPT
它可以说是Scaling Laws的开端
如果你有一个非常大的数据集
训练一个非常大的神经网络
那么是可以保证成功的
从广义上来说
后来发生的事情也的确如此
接下来
Ilya又提到了一个真正经得起时间考验的想法
那就是联结主义（connectionism）
也是深度学习的核心思想
联结主义认为
如果你愿意相信人工神经元在某种程度上有点像生物神经元
那么你就会相信
超大规模的神经网络并不需要达到人类大脑的级别
就可以来完成几乎所有人类能做的事
但是它与人类的大脑仍然不同
因为人类大脑会弄清楚自己如何配置
而神经网络使用的是最优的学习算法
需要与参数数量相当的数据点
在这一点上，人类仍然更胜一筹
Ilya前面讲的所有这些
其实都是为了最终引出了「预训练时代」。
这个时代
可以用GPT-2、GPT-3和Scaling Laws定义
此处
Ilya还格外感谢了前同事亚历克·拉德福Alec Radford
贾里德·卡普兰Jared Kaplan和达里奥·阿莫代伊Dario Amodei
这项技术
也是推动我们今日见到的所有AI技术进步的核心驱动力
但是，Ilya指出
我们所知道的这条预训练路线
毫无疑问会终结
为什么这么说？
尽管计算能力正在通过更好的硬件、更优的算法和更大的集群不断增长
但是数据量并没有增长
因为我们只有一个互联网
甚至可以说，数据是AI的化石燃料
它们是以某种方式被创造出来的
而如今，我们已经达到了数据峰值
不可能再有更多数据了
当然了
Ilya也不是说现在数据就用完了
而是指虽然目前现存的数据
仍然能够支持我们走得很远
但是我们只有一个互联网
那接下来会发生什么呢？
Ilya给出了以下预测
首先，智能体会有一些突破
这些能自主完成任务的AI智能体
就是未来的发展方向
其次，还会有一些模糊的合成数据
但是这到底意味着什么，还不清楚
不过很多人都取得了有趣的进展
最后，就是推理时计算了
最引人瞩目的例子，就是o1
在预训练之后
我们接下来该探索什么？
Ilya举了一个来自生物学的例子
这张图
展示了哺乳动物的身体大小与大脑大小之间的关系
在生物学中，一切都很混乱
但是上面这个紧密的联系
却是一个罕见的例子
从人类及其近亲的进化分支上看
包括尼安德特人、能人等等
大脑与身体比例的缩放指数都是不同的
这意味着在生物学中
确实存在着不同比例缩放的先例
而如今我们所扩展的
可以说是第一个我们知道该如何去扩展的事物
这个领域中的每个人
都会找到解决办法
而我们在相关领域也取得了惊人的进步
10年前这个领域的人
还会记得当时是多么的无能为力
而对于那些过去2年刚刚进入深度学习的人
可能都是无法感同身受的
最后Ilya谈到的
就是超级智能superintelligence了
它是目前公认的发展方向
也是很多研究人员正在尝试构建的东西
从本质上来说
超级智能与现在的AI完全不同
虽然目前我们拥有出色的大语言模型和聊天机器人
但是它们也表现出了某些奇怪的不可靠性
比如它们时常会感到困惑
但是却能够在评估中表现出远超人类的能力
虽然我们还不知道如何去调和这一点
但是最终
或者说迟早会实现这样的目标
那就是AI将真正具备实际意义上的智能体特性
并且将真正的学会推理
由于推理会引入更多的复杂性
因此一个会推理的系统
推理量越多，就会变得越不可预测
相比之下
我们熟知的深度学习还都是可以预测的
举个例子，那些优秀的国际象棋AI
对于最顶尖的人类棋手来说
就是不可预测的
所以，我们将来不得不面对的
是一些极其不可预测的AI系统
它们能够从有限的数据中理解事物
同时也不会感到困惑
同样，自我意识也是有用的
它构成了我们自身的一部分
同时也是我们世界模型中的一部分
当所有这些特性与自我意识结合在一起的时候
就会带来与现有系统完全不同的性质和特性
它们将拥有令人难以置信的惊人能力
当然
虽然无法确定超级智能将如何实现、什么时候实现
但是Ilya相信，这终将发生
至于这种系统可能会带来的问题
就留给大家自己去想象了
毕竟，我们无法预测未来
任何事情都有可能发生
在问答环节
Ilya回答了观众的几个问题
第一个是
是否有其他属于人类认知的生物结构值得去探索
Ilya觉得这取决于我们所看到的抽象层面
一直以来都有很多人渴望从生物学中获得启发
甚至可以说
受生物学启发的AI已经非常成功了
比如深度学习
但是另一方面
生物学上的灵感其实非常有限
就像使用神经元已经是AI从生物学获得的全部灵感了
但是更详细的生物学灵感却一直很难获得
第二个问题是关于模型的自动纠错
在未来
模型如果能进行自我的自动纠错
那是否就意味着它能够识别什么时候出现了幻觉？
Ilya给出了肯定的答复
但是他同时也指出
把它叫作自动纠错有点过于贬低了
实际上它要比自动纠错宏大得多
但是不管怎么说
模型会在某一天意识到幻觉的出现
也会获得自我纠错的能力
最后，来自多伦多大学的观众
问Ilya大语言模型能否泛化超出分布的多跳推理？
这次Ilya没有正面回答这个问题
他反问道什么叫超出分布的泛化？
它意味着什么？
什么叫分布内，什么又叫超出分布呢？
在深度学习之前
人们使用字符串匹配和n-gram
而对于机器翻译
人们使用统计短语表
这些东西的代码复杂性其实难以想象
对于那个时候来说
如果模型记住了一些内容
从而在比赛中获得了高分
那么可能就会说它在分布内
但是现在人们对于泛化的标准已经大幅提高了
所以对于大模型来说，在某种程度上
它可能确实不如人类做得好
但是与此同时
它们肯定在某种程度上
也能够泛化到分布之外
好了
以上就是Ilya这次演讲的主要内容了
应该说
Ilya曾经是暴力Scaling的早期倡导者之一
认为通过增加数据和算力来scale up
就能够显著改善模型的性能
但是现在
Ilya可能已经有了新的想法
也曾经透露过
SSI正在研究一种全新的替代方法来扩展预训练
对于他在演讲中的预言
我更愿意解读为
并不是现在预训练就结束了
也并不是现在数据就用完了
而是对于超级智能来说
只靠预训练终究是不够的
终将会结束
当然，对于Ilya的结论
业界也有其他大佬表示不同意
谷歌大佬洛根·基尔帕特里克Logan Kilpatrick发推文内涵Ilya
说认为预训练结束
恐怕是因为你缺乏想象力
前Meta具身智能团队的高级总监德鲁夫·巴特拉Dhruv Batra也表示
Ilya错了
在他看来，人类的数据还没有用完
我们只是用完了人类书写的文本而已
但是我们拥有的视频数量
依然远超我们的处理能力
只是目前还没有解决视觉领域的预训练问题罢了
另外
如果考虑到未来机器人能够有采集海量多模态数据的能力
包括视觉、嗅觉、触觉等等
那么也许数据还富裕得很
那大家是如何看待Ilya的演讲和结论呢
觉得预训练究竟是否会终结呢？
欢迎在评论区留言，感谢大家的观看
我们下期再见
