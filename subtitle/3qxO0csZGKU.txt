大家好，这里是最佳拍档，我是大飞
前两天13号
谷歌发布了Gemma系列的最新模型Gemma 3
引发了许多关注
这次Gemma 3不仅带来了一些技术上的革新
也预示着AI在小模型领域发展的一些新的方向
今天，我们就来简单介绍一下
看看它究竟有哪些新的特性
以及将会带来什么样的影响
Gemma 3是谷歌在开源语言模型领域的一个最新力作
也是从Gemini系列模型衍生出来的一个成果
这次发布的Gemma 3提供了多种参数规模的模型
分别为1B、4B、12B和27B
专门为在手机、笔记本电脑和高端GPU等标准消费级硬件上运行大模型而进行了优化
拓展了模型在移动端和边缘端的应用场景
从功能特性上看
Gemma 3引入了多项新的功能
首先是多模态能力
Gemma 3模型可以与定制版的SigLIP视觉编码器兼容
这就意味着模型能够将图像和视频作为输入
实现分析图像、回答与图像相关的问题、对比图像、识别物体
甚至读取和解析图像中的文本等等功能
比方说
当用户上传一张日语的空调遥控器照片
并且询问如何调高室内温度的时候
Gemma 3能够根据图像中的“暖房”两个字
分析出在日语中“暖房”意味着“加热”，
进而判断出带有加号(+)的按钮
可能是用来在选择加热模式后调整温度的
这个功能的实现
就得益于SigLIP视觉编码器
SigLIP编码器是一种基于Vision Transformer的模型
使用CLIP的变体进行训练
输入为调整尺寸后的896×896像素的方形图像
然后在视觉助手任务的数据上进行微调
为了处理非方形宽高比和高分辨率图像时出现的问题
比如文本不可读或者小物体丢失等等
研究人员引入了推理阶段的自适应窗口算法
也就是平移和扫描
Pan and Scan，简称P&S技术
这个算法会将图像分割为大小相等的非重叠区域
覆盖整个图像
将各个区域调整为896×896像素后传入编码器处理
从而有效解决了前面说的问题
其次
Gemma 3这次也具备了长上下文处理能力
支持最长128Ktoken的上下文长度
不过1B规模的模型例外
仅支持32K token
在处理复杂任务的时候
长上下文窗口能够让应用程序处理和理解海量信息
比如在一些需要对长篇文档进行分析、总结的场景中
Gemma 3就能够凭借长上下文处理的能力
更好地完成任务
为了实现长上下文处理
研究人员在架构上进行了改进
他们在每个全局注意力层之间
交错设置了多个局部注意力层
并且为局部层分配了仅1024个token的较小处理范围
按照5层局部层对应1层全局层的模式排列
打个比方
就好比我们在阅读一篇很长的文章
局部注意力能够帮助我们关注某个段落的细节
而全局注意力则让我们把握文章的整体脉络
通过这种方式
模型既能处理局部信息
又能够理解全局内容
从而提高了对长文本的处理能力
而且模型只有全局层需要处理长上下文
有效降低了长上下文中KV缓存占用过高的问题
同时
研究者还将全局自注意力层的RoPE
也就是旋转位置嵌入基频
从10k提升到1M
而局部层的频率保持在10k
并且采用了位置插值方法
从而扩展了全局自注意力层的适用范围
第三
Gemma 3拥有了更为广泛的语言支持
它可以为超过35种语言提供开箱即用的支持
并且为超过140种语言提供预训练支持
这使得它能够更好地走向全球
满足不同地区用户的需求
Gemma 3 还使用了与Gemini 2.0相同的分词器SentencePiece tokenizer
具有数字分割、空白保留和字节级编码等功能
最终的词汇表包含262K个条目
这种设计对非英语语言的处理更加平衡
在性能表现方面
Gemma 3在多项基准测试中
相较于上一代实现了全面的提升
在LMArena竞技场中
Gemma 3拿下了1338的ELO高分
其中27B参数的模型表现尤为突出
击败了o1-preview、o3-mini high、DeepSeek V3等模型
成为仅次于DeepSeek R1的最优开源模型
成功打进全球Top 10
在标准基准测试中
对比之前的模型版本和Gemini 1.5
Gemma 3在各种零样本基准测试
比如MMLU-Pro、LiveCodeBench、Bird-SQL (dev)、MATH和HiddenMath等任务中
都展现出了良好的性能
例如，在MATH测试中
Gemma 3 - 27B的得分达到了89.0
相较于Gemma 2 - 27B的55.6分有了大幅提升
数学性能暴涨33-45分
与闭源的Gemini 1.5和2.0相比
Gemma 3 - 27B基本上略逊色于Flash版本
这次Gemma 3的训练过程也具有许多特色
在预训练和后训练过程中
它使用了蒸馏技术
并且通过强化学习和模型合并的组合进行了优化
这种方法有效地提升了模型在数学、编码、指令跟随方面的性能
在预训练阶段
研究团队采用了与Gemma 2类似的知识蒸馏方法
为每个token采样256个logit
并且按照教师模型的概率分布进行加权
学生模型则通过交叉熵损失函数
来学习教师模型的分布
训练数据方面
Gemma 3模型的训练token规模要大于Gemma 2
27B参数模型训练使用了14T tokens
12B版本训练使用了12T tokens
4B模型训练使用了4T tokens
1B模型训练使用了2T tokens
增加的token数量
主要是为了适应预训练阶段同时使用图像和文本的混合数据
并且研究人员还增加了多语言数据的比例
从而提高语言的覆盖范围
此外，团队还应用了多种过滤技术
来降低不当或不安全内容的风险
并且移除个人信息和其他敏感数据
在后训练阶段
Gemma 3主要使用了4个组件来进一步提升性能
一是从更大的指令模型中
提取Gemma 3预训练检查点；
二是基于人类反馈的强化学习(RLHF)，
让模型的预测能与人类偏好保持一致；
三是机器反馈强化学习(RLMF)，
增强数学的推理能力；
四是强化学习执行反馈(RLEF)，
提高编码的能力
这些方法显著提升了模型在数学、编程、指令跟随方面的能力
才使得Gemma 3能在LMArena拿下1338得分
从生态方面来看
Gemma 3带来的不仅仅是模型本身性能提升
还有与工具的无缝集成
比如可以将ShieldGemma 2完美集成到现有的工作流程中
并且它还支持灵活的开发工具
如Hugging Face Transformers、Ollama、JAX、Keras、PyTorch、Google AI Edge、UnSloth、vLLM和Gemma
cpp等等
开发者们可以在Google AI Studio中立即体验Gemma 3的全部功能
也可通过Kaggle、Hugging Face下载模型
此外
开发者还能根据具体的需求来定制Gemma 3
以及通过改进的代码库支持高效微调和推理
无论是Google Colab、Vertex AI
甚至消费级GPU
都能轻松地训练和微调模型
Gemma 3还提供多种部署环境
包括Vertex AI、Cloud Run、Google GenAI API、本地环境和其他平台
而且可以根据应用和基础设施选择最佳方案
英伟达也针对Gemma 3进行了深度优化
从Jetson Nano到最新的Blackwell芯片
同时在NVIDIA API中已经推出Gemma 3
只需要一个API调用就可以快速进行原型开发
除此之外
Gemma 3也针对Google Cloud TPU进行了优化
并且通过开源ROCm堆栈与AMD GPU集成
以及提供在CPU上执行的解决方案
在安全和隐私方面
Gemma 3也进行了多项改进
研究团队持续完善了内部的安全流程
与Gemini团队的安全标准保持了一致
在训练阶段
团队对预训练数据进行了严格的安全过滤
降低预训练和微调模型生成有害内容的可能性
对于微调模型
还结合使用SFT和RLHF技术
引导模型避免产生不良行为
在隐私方面
研究团队评估了模型的记忆化率
也就是模型生成内容与其训练数据匹配的比例
结果显示
Gemma 3模型的长文本记忆率
显著低于先前的模型
在Gemma 3系列中
4B、12B和27B模型的记忆化率差异很小
而1B模型的记忆化率最低
同时
研究团队使用谷歌云敏感数据保护SDP服务
来评估模型生成内容中可能包含个人信息的比例
测试结果表明
所有Gemma 3模型的记忆化输出中
都没有发现个人信息
这表明这些输出中的个人数据比率极低
低于检测的阈值
不过，Gemma 3也还有很多不足
比如在处理某些极端复杂的多模态任务时
性能可能还无法达到一些专业定制模型的水平
另外就是在面对一些非常罕见、或者特殊的语言表达时
可能会出现理解偏差
回顾Gemma系列的发展历程
自从诞生一年以来
Gemma系列模型的下载量已经超1亿次
涌现了超过6万个Gemma衍生模型
而Gemma 3可以说是谷歌在AI开源方面
又迈出的重要一步
它的多模态能力、长上下文处理以及良好适配性
都为未来AI在消费级硬件设备上的应用
拓展了更为广阔的空间
相信在未来
小模型也一定会找到突破性的应用场景
为我们的生活和工作带来更多的便利
感谢大家观看本期视频
我们下期再见
