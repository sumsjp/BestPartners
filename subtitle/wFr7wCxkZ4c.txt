大家好，这里是最佳拍档，我是大飞
前一段时间
Anthropic 的联合创始人及首席科学家贾里德·卡普兰Jared Kaplan
在 YC AI 创业者学校发表了一场演讲
上周刚刚公开出来
这位前理论物理学家
从物理学的独特视角
回顾了 Scaling Laws 的发现过程
剖析了它对通往 AGI 路线图的颠覆性影响
并且指出了在这条「平滑曲线」的尽头
我们依然缺失的关键拼图
Jared 认为
AI 的进步并非源于研究者突然的灵光乍现
而是因为我们找到了一个可以系统性地让 AI 变强的“曲柄”，
现在整个行业都在奋力的转动它
那么，这个“曲柄”究竟是什么？
从 Claude 4 到更强大的未来模型
我们又还需要填补哪些空白呢？
今天大飞就来给大家总结一下这场演讲的核心内容
Jared Kaplan 的职业生涯
并不是从代码与模型开始的
而是粒子物理、宇宙学和弦理论
他的初心
源于身为科幻作家母亲所憧憬的一个梦想
那就是我们能造出超光速引擎吗？
带着对宇宙终极问题的痴迷
Jared 在学术界沉浸多年
然而
物理学进展的缓慢让他感到沮丧
与此同时，他身边的许多朋友
包括后来 Anthropic 的多位联合创始人
都在告诉他，AI 正在迎来巨变
起初，Jared对此充满了怀疑
AI 还停留在不那么激动人心的 SVM
也就是支持向量机的层面
但是最终，他被说服了
并且幸运地在正确的时间点认识了正确的人
一头扎进了 AI 的世界
他的物理学背景
让他习惯于从宏大的视角出发
提出最简单、最根本的「蠢问题」。
比如，2010 年代
大数据的概念风靡一时
Jared 只是朴素地想知道
数据到底要多大才算大？
它究竟有多大帮助？
同样
现在大家都在说更大的模型性能更好
Jared 又想问，到底能好多少呢？
正是这些看似「愚蠢」的追问
引导他和团队在 2019 年左右
发现了 AI 发展中一个令人震惊的规律
Scaling Laws
Jared 回忆说
这真的让他们大吃一惊
因为他们发现 AI 训练背后存在着一种非常精确而且出人意料的东西
这些趋势的精确度
堪比在物理学或天文学中看到的任何东西
这些横跨了数个数量级的计算量、数据集规模和模型参数的漂亮直线
给了 Anthropic 团队无比坚定的信念
AI 将会以一种可预测的方式
持续不断地变得更聪明
而之所以如此坚信
因为当你看到一个规律在多个数量级上都成立的时候
就有理由相信
它在未来很长一段时间内依然有效
要想理解 Scaling Laws 的威力
我们首先需要了解现代大模型训练的两个核心阶段
第一个是预训练 (Pre-training)，
在这个阶段
模型会通过学习海量的人类文本
来理解数据背后的相关性
本质上，就是学会预测下一个词
第二个是强化学习 (Reinforcement Learning)，
这个阶段的目标是让模型变得更加有用
通过基于人类的反馈
比如选择两个回答中更好的一个到
模型会学习哪些行为是好的、是有用、诚实、无害的
并且强化它们
同时会抑制那些不好的行为
这就是所谓的 RLHF
Jared 指出
Scaling Laws 不仅适用于预训练
同样也适用于强化学习阶段
而后者在早期常常被大家所忽视
他特别提到了研究员 Andy Jones 在大约四年前的一个「个人项目」。
当时
这位研究员仅凭自己的一块 GPU
无法复现 AlphaGo 的研究
于是就选择了一个更简单的棋类游戏
Hex，中文也叫做六连棋或者六贯棋
来研究强化学习的 scaling 行为
他惊人地发现，棋力（Elo）的提升
同样呈现出了漂亮的直线趋势
Jared 认为
这个发现在当时没有得到足够的重视
但是现在我们清楚地看到
无论是预训练还是强化学习
只要增加计算投入
就能够获得可预测的性能提升
他强调，这才是驱动 AI 进步的根本
也就是说
不是 AI 研究者突然变聪明了
而是我们找到了一个非常简单的方法
可以系统地让 AI 变得更好
我们现在正在做的
就是转动那个曲柄
随着我们不断的「转动曲柄」，
AI 的能力也正在被解锁
Jared 倾向于从两个维度来审视这些能力
图中的Y 轴是灵活性 (Flexibility)，
代表 AI 处理不同模态、与现实世界交互的能力
X 轴是任务时间尺度 (Time Horizon)，
代表 AI 能够独立完成任务所需要的时间长度
这是 Jared 认为的一个更有趣、也更重要的维度
随着模型越来越智能
它们能处理的任务时间跨度也在稳步增加
Jared 引用了 METR 的系统性研究
这样研究发现了一个惊人的趋势
那就是AI 模型能够完成的任务长度
大约每 7 个月会翻一番
实际上，这几乎是一个指数级的增长
也意味着
AI 正在从只能完成几分钟、几小时的任务
快速迈向能够处理以天、周、月甚至年为单位的复杂工作
Jared 畅想
如果沿着这条指数曲线走下去
未来几年内
我们或许会看到 AI 系统
或者由数百万 AI 组成的系统
能够完成整个公司、甚至整个科学界才能完成的工作
毕竟
数学和理论物理学的优点之一就是
只需要思考就能取得进步
所以
如果 AI 系统之间能够有效的协同工作
也许真的能加速科学发现
既然 Scaling Laws 描绘了一条如此清晰、平滑的通往 AGI 的道路
那我们是否只需要坐等计算资源增加就行了呢？
Jared 的回答是：不
他认为
要想真正解锁人类级别的通用智能
除了继续转动这个曲柄
我们至少还需要补上几块关键的拼图
首先是组织知识 (Organizational Knowledge)，
目前的 AI 模型在每次交互的时候
都像一个「失忆的实习生」。
我们需要的是能够融入特定环境的 AI
它们需要具备上下文感知能力
就像一个在公司工作了好几年的资深老员工一样
理解内部的术语、流程和隐性知识
其次是记忆 (Memory)，
这与组织知识不同
更侧重于任务执行过程中的状态跟踪
当一个任务需要跨越很长的时间
模型必须能够记住自己的进展
Jared 透露
Claude 4 已经开始内置这种记忆能力了
允许模型突破单个上下文窗口的限制
从而可以处理超长期的工作
第三是监督 (Oversight)，
这是目前面临的一大挑战
对于有明确对错的任务
比如代码、数学题这样
强化学习很容易发挥作用
但是实际上
现实世界充满了模糊的任务
比如讲笑话、写诗等等
我们需要开发出能够生成更细致、更微妙的奖励信号的 AI 系统
换言之
我们需要用 AI 来更好地监督 AI
第四是更复杂的任务 (Larger Tasks)，
我们需要能够训练 AI 去执行越来越复杂的任务
这是能力提升的直接体现
最后是模态的扩展与数据融合 (Modality and Data Integration)，
我们需要沿着能力 Y 轴继续向上攀登
从文本模型，到多模态模型
最终延伸到能够与物理世界交互的机器人技术
在炉边谈话环节
Jared 进一步回答了大家关心的问题
主持人Diana 首先提到了最新发布的 Claude 4
Jared 笑称
如果 12 个月后还没有更好的模型出来
那他们就有麻烦了
随后他解读了 Claude 4 的几个关键进步
首先是更强的 Agent 能力和监督水平
Jared 指出
像 Claude 3.7 在写代码时非常出色
但是有时会「过于热情」，
为了让测试通过
不惜会使用一些 try-except 之类的捷径
而 Claude 4 提升了作为 Agent 的能力和对指令的遵循度
能够产出质量更高的代码
其次是他最兴奋的一个特性，记忆力
这也正是解锁更长任务时间尺度的关键
Claude 4 不仅能够在复杂任务中耗尽上下文窗口
还能将记忆存储为文件或记录
并且在需要的时候进行检索
从而实现跨多个上下文窗口的长期工作
这让他期待 Claude 能够成为一个承担越来越大块工作的「合作者」。
Diana 还观察到一个有趣的转变正在发生
那就是YC 的创业公司
正在从销售 Copilot
转向销售端到端的全自动工作流
Jared 认为
这取决于任务对可靠性的要求
有些任务只需要 70-80% 准确率
有些场景需要 99.9% 的可靠性
这种协作模式也重新定义了人类的角色
于是Jared 提出了一个核心观点
那就是对于人类
判断一件事做得对不对
通常比亲手去做要容易得多
但是对于 AI
判断能力和生成能力之间的差距要小得多
这意味着，人类的最佳角色是管理者
负责对 AI 的工作进行检查
确保方向正确
随后
Jared 还进一步阐述了深度智能和广度智能之间的差异
深度智能就像证明一个数学猜想
花十年解决一个极度困难的特定问题
而广度智能则更像是生物学或者历史研究
需要整合横跨多个领域的、海量的信息
而AI 在预训练阶段
吸收了几乎全部的人类文明知识
因此在广度上拥有无与伦比的优势
它能发现隐藏在不同知识领域交叉点上的洞见
而这是任何一个人类专家都难以做到的
Jared 预测，利用 AI 的知识广度
尤其在生物医药等研究领域
将是未来的一大硕果
除此以外
AI 在落地场景中还有哪些低垂的果实呢？
Jared 说自己的主要做研究而非商业化
但是他认为，与电脑数据交互的任务
都可以用 AI 来做
新的技术也许有很长的采用周期
就像电力取代蒸汽机一样
我们应该尽可能利用人工智能
融入到经济的各个部分
当被问到从物理学转到AI有什么感受的时候
Jared 坦言
物理学的训练对他最大的帮助
是寻找宏观趋势并且尽可能地精确化
当 AI 研究者在模糊地说
学习是指数级收敛的
他会追问，你确定是指数吗？
会不会是幂律？
二次方？
它到底是怎么收敛的？
正是这种对精确性的追求
让他和团队抓住了 Scaling Laws
他认为，AI 领域的圣杯
就是找到一个斜率更优的 Scaling Law
这意味着投入同样的算力
你能比别人获得更大的优势
对于可解释性
他认为这门学科更像是生物学或者神经科学
而AI 的优势在于
你可以测量其中的一切
不像真正的大脑那样
存在观测的盲区
这为逆向工程 AI 的工作原理
提供了海量的数据
当被问到
什么情况会让你相信 Scaling Law 失效的时候
Jared 的回答还挺出人意料
他说，他的第一反应会是
我们搞砸了训练的某个环节
比如网络架构搞错了、训练有瓶颈、或者是算法精度出了问题
而不是定律本身失效了
因为在过去五年里
每一次看似定律被打破
最终都发现是他们自己做错了
这种强烈的信念
也解释了为什么 Anthropic 会如此坚定地沿着 Scaling 的路线前进
对于算力的稀缺性，Jared 认为
目前 AI 发展还处于一种极度不均衡的状态
所有人的焦点都集中在解锁前沿能力上
因此效率并不是首要的考量
他开玩笑说
我们最终会把计算机带回二进制时代
未来，为了提高效率
肯定会采用更低精度的计算
但是只要智能的提升依然能够带来巨大的价值回报
那么对最前沿、最强大模型的追求就不会停止
在观众问答环节，一位观众指出
Scaling Law 的性能提升在对数图上是线性的
但是为什么任务时长的提升却是指数级的？
Jared 坦诚这主要是一个经验性发现
但是他提供了一些个人看法
那就是完成长时程任务的关键
在于自我纠错的能力
智能上一个微小的提升
可能只是让模型多发现了一两个错误并加以修正
但是这足以让任务的完成度翻倍
因为模型不会在原来的地方卡住
而是能走得远一倍
这种效应的累积
就可能导致任务时长的指数级增长
另一位观众提问
在缺乏明确验证信号的领域
如何进行Scaling呢？
比如在编程领域
有单元测试作为清晰的验证信号
但是在其他模糊的领域
我们又该如何提升模型的长时程任务能力？
Jared 认为
也许需要为 AI 模型构建更多不同的任务
这些任务越来越复杂，时间越来越长
然后再去进行强化学习训练
如果有必要，行业会投入资源去做
但是更好的方法是让 AI 去监督 AI
他举例说，一个长达七年的任务
如果只在最后才有一个成功或失败的信号
那效率就实在太低了
但是如果有一个监督 AI
能够持续地提供更细致的反馈
比如这里做得好，那里做得不好
那么训练的效率就会大大提高
在最后
Jared 也给现场的 AI 创业者和开发者们提出了几点建议
首先是要去构建那些“还不太好用”的东西
因为 AI 的能力边界正在飞速的移动
今天由于模型能力不足而无法实现的产品
可能在下一个版本问世之后
就能完美运行
其次是用 AI 来集成 AI
目前 AI 发展的最大瓶颈之一
就是我们还没有足够的时间
将它集成到各行各业
利用 AI 来加速 AI 的落地过程
本身就是一个巨大的杠杆
第三是寻找下一个爆发领域
软件工程是 AI 应用爆发的第一个领域
下一个能以如此之快速度增长的领域会是什么呢？
这是留给所有人的问题
好了
以上就是Jared Kaplan这次演讲的主要内容了
从一个物理学家的好奇心出发
到揭示驱动整个 AI 时代的 Scaling Laws
再到洞察通往 AGI 的未竟之路
Jared Kaplan 的分享为我们描绘出了一副 AGI 的蓝图
在他的眼中，AI 的未来
似乎不再是神秘的黑箱
而是一条遵循着清晰的规律、可以被预测、被规划的道路
而在这条路上
人类的角色也愈发的明确
我们不再是单纯的工具使用者
而是与日益强大的AI并肩协作的管理者、监督者和合作者
只是不知道人类的明天
是否会像这条指数级的增长曲线一样
有着确定的未来
感谢大家收看本期视频
我们下期再见
