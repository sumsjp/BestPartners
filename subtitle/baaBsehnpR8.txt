大家好，这里是最佳拍档，我是大飞
在马斯克发布Grok 3之后
OpenAI也随即采取行动
发布了SWE-Lancer编码基准测试
直接让AI模型来挑战真实的外包任务
而且这些任务的总价值高达100万美元
有趣的是，测试结果显示
Anthropic的Claude 3.5 Sonnet在这个基准上的赚钱能力上
竟然超越了OpenAI自家的GPT-4o和o1模型
这到底是怎么一回事呢？
今天我们就来聊聊这个价值百万的基准测试
我们先介绍一些背景
在SWE-Lancer出现之前
测试模型代码能力的基准
主要有SWE-Bench和SWE-BenchVerified这两个
在我们频道里也经常出现
但是实话说
这两个测试基准一直存在一个比较大的局限性
那就是它们主要是针对孤立任务进行测试的
很难反映现实中软件工程师的复杂工作情况
大家想想，现实里的软件工程师
他的工作基本会涵盖到整个技术栈
也会涉及到比较复杂的跨代码库交互和权衡
比如要开发一款功能丰富的APP
那么既要考虑前端界面的设计和用户交互体验
又要处理后端的数据存储、服务器逻辑
以及与各种第三方服务的对接
还要确保不同模块之间的协同工作
避免出现兼容性方面的问题
而以往的测试基准
是无法全面模拟这样复杂场景的
也就难以准确的评估AI模型在实际软件工程中的能力
为了解决这些问题
于是OpenAI推出了SWE-Lancer
它是一个全新的、更贴近现实的基准测试
用来评估AI模型的编码性能
这个测试的数据集非常有特点
包含了来自著名外包网站Upwork的1488个自由软件工程任务
这些任务都来自Expensify开源仓库
在现实世界中的总报酬价值为100万美元
也就是说
如果AI模型能够完美地完成这些任务
就相当于能像人类软件工程师一样
获得百万年薪
这无疑是对AI模型编码能力的一次极具挑战性的考验
SWE-Lancer基准测试包含了两种主要的任务类型
分别是独立开发者IC SWE任务和SWE管理任务
IC SWE一共有764个，价值414775美元
主要是来模拟个体软件工程师的职责
要求模型生成代码补丁来解决实际的问题
比如
在处理一个“允许在原生设备上使用键盘快捷键”的任务时
模型需要根据给出的问题文本描述
给出重现步骤、期望行为、问题修复前的代码库检查点以及修复目标
并且生成相应的代码补丁
之后
会由专业工程师创建的端到端测试
来验证这个补丁是否有效
如果通过测试
模型就能获得相应的报酬
而SWE管理任务有724个，价值585225美元
要求模型扮演软件工程经理的角色
从多个解决任务的提案中挑选出最佳方案
举个例子
在一个关于在iOS上实现图像粘贴功能的任务中
可能会有不同的开发者提出各种实现方案
有的方案侧重于优化性能
有的则注重代码的简洁性
而模型需要综合考虑各种因素
选择最适宜的方案
如果模型选对了，同样可以获得报酬
SWE-Lancer的测试方法也十分独特
采用了端到端的测试方法
与传统的单元测试不同
端到端测试能够模拟真实用户的工作流程
验证应用程序的完整行为
这种方法不仅能够更为全面地评估模型的解决方案
还能够避免一些模型通过作弊来通过测试
就拿修复一个导致用户头像在“分享代码”页面与个人资料页面不一致的漏洞
这个价值1000美元的开发任务来说
传统的单元测试可能只能分别验证头像上传和显示这两个独立功能
比如检查头像能否成功上传到服务器
在个人资料页面能否正常显示等等
但是端到端测试则会模拟用户登录、上传头像、切换账户以及查看不同页面的完整流程
通过这种方式
测试不仅能够验证出头像是否正确显示
还能够确保整个交互过程的连贯性和正确性
比如检查切换账户后
头像在不同页面的显示是否同步
是否会出现加载异常等等问题
在评估过程中
SWE-Lancer还引入了一个重要模块
就是用户工具
这个工具允许模型在本地运行应用程序
并且模拟用户的行为来验证解决方案
比如在处理Expensify应用中报销流程的相关任务时
模型会借助用户工具来模拟用户进行费用录入操作
它会按照真实用户的操作习惯
在本地运行的应用程序中
依次输入各项费用明细
比如金额、日期、费用类型等信息
然后点击提交按钮
观察应用程序能否正确地记录费用数据
以及后续的审批流程是否能够顺利启动
通过这样的模拟操作
模型能够判断自己对报销流程问题的解决方案是否有效
比如是否修复了费用录入后数据丢失的漏洞
或者是否优化了审批流程中的卡顿现象等等
那么，如今前沿的AI模型们
在SWE-Lancer测试中的表现究竟如何呢？
OpenAI使用了GPT-4o、o1和Claude 3.5 Sonnet等模型进行了测试
结果显示
所有模型在完整的SWE-Lancer数据集上
获得的报酬都远低于100万美元的潜在总报酬
这表明，即使是当前最先进的AI模型
在面对真实的自由职业软件工程任务时
仍然面临着巨大的挑战
具体来说，在IC SWE任务中
表现最好的Claude 3.5 Sonnet的通过率仅为26.2%，
只能正确解决不到三分之一的开发任务
获得了89000美元的报酬，报酬率为21.5%。
而GPT-4o的通过率仅为8.6%，
o1为20.3%。
在SWE管理任务中
Claude 3.5 Sonnet的表现要稍好一些
通过率达到了47.0%，获得了314000美元的报酬
报酬率为53.7%
而GPT-4o在这类任务中的通过率为38.7%
o1为46.3%
通过进一步分析不同任务类型和难度级别对模型表现的影响
可以发现模型在不同任务类型和难度级别上的表现
存在着显著差异
在价值较低、相对简单的任务中
模型的通过率相对较高；
而在价值较高、难度较大的任务中
模型的通过率则明显下降
例如
在SWE-Lancer Diamond数据集中
价值超过1000美元的任务
模型的通过率普遍低于30%。
这说明
尽管模型在处理一些基础任务时
能够表现出一定的能力
但是在面对复杂的、高价值的软件工程任务时
它们与人类的高级软件工程师相比
仍然有较大的差距
研究人员还对一些因素进行了深入探究
比如尝试次数对模型性能的影响
研究团队使用通过率指标（pass@k）对GPT-4o和o1进行了评估
结果发现，所有模型的通过率
都会随着尝试次数的增加而持续提升
这种趋势在o1模型中特别明显
比如增加了6次尝试之后
解决任务的比例提高了近两倍
GPT-4o在允许6次尝试
也就是pass@6的时候
达到了与o1首次尝试相同的得分为16.5%。
这表明，适当增加模型的尝试次数
可以在一定程度上提高其解决问题的能力
但是也意味着在实际应用中
可能需要更多的计算资源和时间来获得更好的结果
另外
增加测试计算的资源也对模型性能有影响
在高质量数据集的IC SWE任务中
启用o1和用户工具的实验表明
增加推理计算量
能够将通过率从低计算量的9.3%，
提升到高计算量的16.5%，
相应的报酬也从16000美元增加到29,000美元
报酬率从6.8%提升到了12.1%。
而且增加测试计算资源
能特别提高在较难、且报酬较高问题上的性能表现
这说明
强大的计算资源对于提升AI模型在复杂任务中的表现具有重要的作用
但是同时也面临着成本和资源消耗的问题
此外
移除用户工具也会对模型的表现产生影响
在IC SWE任务中
移除用户工具对pass@1的通过率影响较小
不过，研究人员观察到
较强的模型能够更为有效地利用用户工具
因此在此消融实验下会经历更大的性能下降
例如，GPT-4o这样相对较弱的模型
在用户工具等待运行的90到120秒期间
往往会完全放弃使用该工具
而表现最好的模型会考虑到这种延迟
设置合理的超时时间
并且在结果可用的时候进行复查
这体现了不同模型在利用外部工具解决问题的策略和能力上
存在着显著的差异
从这次测试结果来看
虽然AI模型在软件工程任务上取得了一定进展
但是要达到可信部署的标准
还需要继续提高可靠性
目前
即使是表现最优的Claude 3.5 Sonnet
大部分的解决方案仍然会存在错误
不过
AI Agent在问题定位方面表现突出
能够通过在整个代码库中进行关键词搜索
以惊人的速度准确定位相关的文件和函数
不过
它们对问题如何跨越多个组件或文件的理解
往往有限，不能够解决根本的原因
从而导致解决方案不正确或者不够全面
另外，研究人员还发现
AI Agent很少会因为尝试重现问题
或者因为找不到正确的修改位置而失败的情况
总的来说
OpenAI这次公开的SWE-Lancer编码基准测试
为评估AI模型的编码能力提供了一个更为真实、更加全面的平台
通过这次测试
我们也看到了当前AI模型在软件工程领域的优势与不足
尤其是在面对复杂任务的时候
还存在着很多挑战
也许在这类任务上
AI模型还无法战胜富有经验的软件工程师
但是我们也必须要注意到
一旦有了可量化的标准评估手段
AI模型的进化可能会远超我们的想象
只要在朝着这条路上一直前进
AI超越绝大部分软件工程师
依然只是时间问题
好了，感谢大家收看本期视频
我们下期再见
