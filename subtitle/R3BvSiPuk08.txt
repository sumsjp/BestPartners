大家好，这里是最佳拍档，我是大飞
今天要和大家深度拆解的
就是一篇有希望能彻底解开“幻觉之谜”的重磅研究
来自OpenAI和佐治亚理工联合发表的论文《为什么语言模型会有幻觉（Why Language Models Hallucinate）》。
这篇论文的价值在于
它没有把幻觉归咎于“模型不够大”、“训练数据不够多”这类表面的原因
而是用严谨的统计理论和实证案例证明
幻觉本质上是两个核心问题的产物
分别是预训练阶段的“统计误差传导”，
以及后训练阶段的“评估机制激励错位”。
换句话说，幻觉不是技术上的“意外”，
而是现有训练和评估逻辑下的“必然结果”。
今天我们就从这两个核心问题入手
一步步搞懂幻觉的来龙去脉
以及到底该怎么解决它
我们首先要明确一个前提
大语言模型的预训练目标
是为了学习人类语言的概率分布
简单来说
就是让模型知道“哪些句子更常见、更合理”。
比如“我吃饭”比“饭吃我”更合理
“今天天气很好”比“今天天气很绿”更合理
很多人会觉得
只要给模型喂足够多的“无错误数据”，
它就能只输出正确的内容
但是这篇论文用数学证明了
即使训练数据100%无误差
预训练后的模型依然会产生错误
因为生成正确内容
比判断内容是否正确更难
这里就要引入论文的核心理论工具
IIV二分类问题
什么是IIV？
它的全称是Is-It-Valid
也就是有效性判断
简单来说，就是给模型一个句子
让它判断这个句子是“有效”的还是“无效”的
有效即为正确
记为+，无效即为错误，记为-。
这是一个典型的有监督分类任务
训练的数据里既有正确的句子
也有随机生成的错误句子
比例各占50%。
论文通过数学推导证明了一个关键的结论
那就是模型的生成误差率
也就是输出错误内容的概率
至少是它在IIV分类任务中错误率的2倍
为什么会这样呢？
因为生成任务本质上包含了无数个“隐性的IIV判断”，
比如模型要输出“亚当的生日是多少多少
它首先得在脑子里判断
这个日期是亚当的生日吗、这个日期的格式对吗、以及我有没有见过这个信息等等一系列的分类问题
只要其中一个分类判断错了
最终的生成结果就会错
我们举个论文里的例子
大家就能更直观的理解了
第一种情况是“拼写判断”，
比如“Greetings”和“Greatings”。
模型对这类问题的IIV分类准确率很高
因为拼写规律很明显
所以生成时也很少犯拼写错误
这就是为什么现在的大模型基本不会写错别字
第二种情况是“字母计数”，
比如问“DEEPSEEK里有几个D？
”正确答案是1
但是DeepSeek-V3模型在10次测试里
要么答2
要么答3，甚至有其他模型答6、7
为什么？
因为“数字母”这个任务的IIV分类难度更高
模型需要先把“DEEPSEEK”拆成单个字母
再逐个计数
而很多模型的架构不擅长处理这种细粒度任务
导致IIV分类错误率高
导致最终生成错误的答案
有趣的是
论文里还提到DeepSeek R1能正确回答
因为它会采用链式推理
这说明模型对任务的“拟合能力”越强
生成的误差就越低
第三种情况是“生日事实判断”，
这类问题的IIV分类难度最高
因为生日是“没有规律的任意事实”，
既不像拼写有固定规则
也不像字母计数有明确步骤
全靠训练数据里的记忆
如果训练数据里亚当的生日只出现过一次
甚至没出现过
模型就无法准确判断“某个日期是否正确”，
IIV的分类错误率就会飙升
生成时自然就会“编一个看似合理的日期”。
论文里把“生日、论文标题”这类无规律、全靠记忆的信息
称为“任意事实（Arbitrary Facts）”。
这类事实的最大特点是
没有可以学习的模式
完全依赖训练数据中的出现频率
比如“爱因斯坦的生日是03-14”，
这句话在训练数据里出现了成千上万次
模型的IIV分类准确率接近100%，
生成时也不会错；
但是像某个小众学者的生日
可能只在一篇讣告里出现过一次
模型的IIV分类准确率就会很低
生成时大概率会编一个日期
论文中用“单例率（Singleton Rate
sr）”来量化了这种误差
所谓“单例”，
是指训练数据中“仅出现过一次且非‘我不知道’（IDK）”的提示-响应对
论文证明，对于任意事实
模型的幻觉率下限等于单例率
简单说
如果训练数据里20%的生日事实是“单例”的
那么模型在回答这些生日问题的时候
至少有20%的概率会产生幻觉
这个结论背后的逻辑
来自艾伦·图灵（Alan Turing）提出的“Good-Turing缺失质量估计”。
简单理解就是
训练数据中“只出现过一次的事件”，
可以用来估计“没出现过的事件”的概率
比如你在池塘里捕了100条鱼
其中20条鱼的品种只见过一次
那么你下次捕鱼的时候
遇到新品种的概率，大概就是20%。
因此，对于模型来说，“单例”越多
说明它没见过的“正确事实”越多
生成时就越容易用编出来的错误事实来填补空白
除了“任意事实”以外
模型自身的缺陷也会导致预训练误差
比如模型架构无法拟合目标概念
或者虽然架构足够强，但是没训练好
这在论文里被称为“Poor Models”问题
最经典的例子是“三元语言模型（Trigram Model）”的缺陷
三元模型是20世纪80-90年代的主流语言模型
它只能根据“前两个词”来预测下一个词
比如面对两个提示（“She lost it and was completely out of
.
”和“He lost it and was completely out of
.
正确的结尾分别是“her mind”（她的思绪）和“his mind”（他的思绪）
但是三元模型无法区分“she”和“he”对应的物主代词
因为它只看“out of”前面的“completely”，
所以会把两个提示的结尾都生成“her mind”或“his mind”，
导致错误
论文用数学证明了
对于这类“需要长上下文理解”的任务
如果模型架构的“拟合能力”不够
比如三元模型的上下文窗口只有2个词
那么它的IIV分类误差就会很高
进而导致生成误差率至少达到50%。
比如之前提到的“字母计数”问题
DeepSeek-V3之所以会数错DEEPSEEK里的D数量
本质上也是因为模型缺陷
它的分词器会把“DEEPSEEK”拆成“D/EEP/SEE/K”这样的token
而不是单个字母
从而导致模型无法直接“数字母”，
而DeepSeek-R1通过“链式推理”，
把单词拆成单个字母逐个计数
本质上是用“推理能力”弥补了模型架构的缺陷
降低了IIV分类误差
最终生成正确答案
除了上述两种核心原因
论文还提到了三个导致预训练误差的重要因素
第一是计算复杂度
有些问题本身就是“计算上不可解”的
比如“解密一段随机的加密文本”，
即使模型知道加密算法
也无法在有限时间内破解
这时它只能输出一个错误的解密结果
因为它无法判断“哪个解密结果是正确的”。
第二是分布偏移（Distribution Shift）
简单来说
就是测试时的提示词和训练数据里的提示词“长得不一样”。
比如训练数据里很少出现“一磅羽毛和一磅铅哪个更重”这种“陷阱题”，
模型可能就会因为“羽毛看起来轻”，
而错误地回答“铅更重”。
再比如“数字母”的任务
如果训练数据里都是短单词（如“CAT”“DOG”）
突然让它数“DEEPSEEK”这种长单词
模型就容易出错，这不是模型没学好
而是训练数据的分布没覆盖到这类情况
导致IIV分类的误差升高
第三是垃圾进垃圾出
简称GIGO（Garbage In
Garbage Out）
虽然论文假设训练数据没有误差
但是现实中
训练数据里必然会包含错误、谣言甚至某些阴谋论
模型会学习这些错误信息
在生成时复制出来
因为它无法判断这些内容是错误的
论文还指出
GIGO会让预训练的误差进一步升高
因为模型不仅要面对“统计误差”，
还要面对“数据污染”的问题
看到这里，可能有朋友会问
预训练有误差很正常，但是后训练
比如RLHF、DPO这些优化方法
不就是专门用来修正模型错误的吗？
为什么经过后训练，幻觉还是没消失
甚至在某些任务上更严重了呢？
论文的答案很尖锐
不是后训练技术不行
而是现有的评估机制在“鼓励幻觉”，
就像老师在判卷子的时候
认为瞎写一个答案比空着不写得分更高
学生自然会选择瞎写一个
大语言模型的评估基准也是如此
“编一个看似合理的错误答案”比“说我不知道”得分更高
所以模型自然会选择幻觉
现在主流的模型评估基准
几乎都采用“二元评分（Binary Grading）”的方式
也就是正确得1分
错误或者弃权得0分
论文里做了一个统计
在10个最有影响力的评估基准中
9个是纯二元评分
只有1个WildBench会给“弃权”一点微弱的分数
但是依然低于“有错误的合理答案”。
比如GPQA是多选题
没有“我不知道”的选项
模型必须选一个
MMLU-Pro也是多选题，弃权得0分
蒙对得1分；
SWE-bench要求模型修复GitHub上的代码漏洞
提交错误的补丁得0分
提交“我修不了”也得0分
所以模型会强行输出一个看似能运行的补丁
即使它有bug
论文用一个简单的数学推导证明了
在二元评分体系下
无论模型对答案的置信度有多低
“猜测”都是最优选择
假设模型对一个问题的答案只有10%的把握
那么“猜测”的期望得分是10%×1 + 90%×0 = 0.1分
而“弃权”的得分是0分
显然猜测更划算
即使模型只有1%的把握
期望得分也是0.01分
依然高于0分
因此，在二元评分的激励下
模型会优先选择“输出一个答案”，
而不是“承认不知道”，
哪怕这个答案是编的
论文里还做了一个思想实验
假设有两个模型
Model A和Model B
Model A很诚实，知道的就回答
不知道的就说“我不知道”，从不幻觉；
而Model B则相反
不知道的就编一个看似合理的答案
经常幻觉
在现有评估基准上
哪个模型得分更高呢？
答案是Model B
因为现有基准里的问题
总有一部分是Model A不知道
是但Model B能“蒙对”的
比如100个问题，Model A知道60个
回答对60个
弃权40个，总得分60分；
Model B知道60个，回答对60个
剩下40个蒙对10个，总得分70分
虽然Model B有30个幻觉
但是它的得分更高
而厂商在优化模型的时候
只会看“基准得分”，不会看“幻觉率”，
这就导致后训练时
模型会逐渐放弃“诚实的表达不确定性”，
转而学习“如何更好地猜测”，
最终幻觉率居高不下
更严重的是
很多评估基准还用“大模型作为裁判”来打分
比如Omni-MATH会让模型输出解题的过程
再让另一个模型来判断“过程是否正确”。
但是论文指出
模型裁判经常会把“错误但冗长的解题过程”判为正确
因为“看起来很专业”，
这进一步鼓励模型“编造详细的错误内容”，
而不是“简洁地承认不会”。
比如一个数学题
Model A说“我不知道”，
得0分；
Model B编了一段充满公式的错误推导
模型裁判没看出来，反而给了1分
这种“误判”会让幻觉问题雪上加霜
很多研究者试图通过“增加专门的幻觉评估基准”来解决问题
但是论文认为这没用
因为现有主流评估的影响力远大于这些新的基准
厂商还是会优先优化主流基准的得分
而不是幻觉评估的得分
这就像学校里新增了一门所谓的“诚实度考试”，
但是高考、中考还是只看分数
于是学生依然会在高考中蒙题
不会因为“诚实度考试”而放弃得分的机会
论文呢把这种现象
称为惩罚不确定性的流行病
要想解决它
不能够只靠增加新的评估
而是要修改现有主流评估的评分逻辑
让诚实表达不确定性得到合理的分数
虽然幻觉的核心呢
是预训练统计误差
和后训练评估激励错位
那么解决方案也应该从这两点入手
论文没有提出复杂的技术
而是给出了两个简单却根本的建议
一是在评估中“明确置信度目标”，
二是“修改主流评估的评分逻辑”。
论文建议，在评估的提示词中
明确告诉模型“什么时候该回答
什么时候该弃权”，
比如在问题后面加上这样一句话
只有当你对答案的置信度超过90%时才回答
正确得1分
错误扣9分，回答‘我不知道’得0分
为什么这样有效呢？
我们可以算一笔账
如果模型的置信度是90%，
那么“回答”的期望得分是90%×1 + 10%×(-9) = 0.9 - 0.9 = 0分
和“弃权”得分相同；
如果置信度是91%，
期望得分是0.91 - 0.09×9 = 0.91 - 0.81 = 0.1分
高于弃权；
如果置信度是89%，
期望得分是0.89 - 0.11×9 = 0.89 - 0.99 = -0.1分
低于弃权
这样一来
模型会自动在“置信度>90%”时回答
否则弃权，不会再盲目猜测
论文里提到了几个常用的置信度阈值
分别是t=0.5
也就是错误扣1分
正确得1分、t=0.75
错误扣2分、t=0.9，错误扣9分
不同的阈值对应不同的应用场景
比如医疗领域需要像t=0.99这样的高阈值
避免错误诊断；
而日常聊天可以用t=0.5这样的低阈值
允许偶尔的小错误
更重要的是
“明确置信度目标”需要写在提示词里
让模型清楚的知道“评分规则”。
之前的研究虽然也尝试过“惩罚错误”，
但是大多没有明确告诉模型“惩罚力度”，
导致模型无法判断“该冒险还是该弃权”。
论文里举了一个例子
在prompt里加入“错误扣2分”的说明后
模型的幻觉率下降了30%，
同时正确回答率没有明显的下降
这说明模型完全有能力根据评分规则调整行为
只是之前没有得到明确的指令
对于后训练来说，论文强调
解决幻觉问题的关键
不是新增“幻觉评估”，
而是把“置信度目标”融入现有的主流评估基准
比如SWE-bench可以修改评分规则
提交正确补丁得1分
提交错误补丁扣2分
提交‘无法修复’得0分”；
MMLU-Pro可以增加“我不知道”的选项
给0.2分
而不是0分
为什么要修改主流的评估基准呢？
因为它们可以是厂商优化模型的“指挥棒”，
如果这些评估不修改
即使新增100个“幻觉评估”，
厂商也不会优先去优化
因为主流评估的得分会直接影响模型的市场竞争力
比如一个模型在MMLU上得分80、幻觉率50%，
另一个模型在MMLU上得分75、幻觉率10%，
厂商大概率会选择前者
因为客户更看重MMLU得分
论文里还提到一个很有意思的现象
那就是人类考试其实也经历过类似的改革
比如美国的SAT、GRE考试
早年也是“错误不扣分”，
导致学生盲目猜题
后来改成“错误扣0.25分”，
学生就会在“没把握”时放弃猜题
所以说
大模型的评估也应该借鉴这种思路
通过调整评分规则
来引导模型从“应试机器”变成“可信助手”。
此外
论文还提出了“行为校准（Behavioral Calibration）”的概念
不是要求模型输出“我有90%的把握”这种概率值
而是要求模型“在置信度>t时输出答案
否则弃权”。
这种校准可以通过对比不同阈值下的正确率和弃权率
来进行验证
比如在t=0.9的时候
模型的正确回答率应该≥90%，
弃权率应该对应“没把握”的问题比例
现有的模型大多没有经过这种校准
但是论文认为
这会成为未来大语言模型评估的重要指标
虽然这篇论文的分析非常深刻
但是它也承认
现有框架还有一些局限
需要未来研究进一步完善
首先
论文只考虑了“合理的错误内容”，
忽略了“无意义的内容”，
比如模型输出“asdfghjkl”这种乱码
但是实际上
现有模型很少输出无意义内容
因为它们的预训练目标是“生成合理的语言”，
所以这个局限对结论影响不大
其次，论文主要分析了“事实性问题”，
对“开放式生成”的幻觉讨论较少
开放式生成的幻觉更难量化
比如一篇传记里有1个错误和有10个错误
幻觉程度显然不同
但是论文的框架没有区分这种“程度差异”。
未来可能需要建立“幻觉程度评分体系”，
而不是简单地“正确/错误”二分
第三
论文指出“检索增强（RAG）不是万能的”。
很多人认为
给模型加一个“搜索引擎”，
让它先查资料再回答，就能消除幻觉
但是论文认为
RAG只能解决“训练数据里没有的事实”，
无法解决“检索不到信息”的情况
比如检索不到某个学者的生日
模型还是会编一个日期
因为现有评估鼓励猜测
只有结合“检索+置信度目标”，
才能彻底解决问题
也就是当检索到信息且置信度>t时回答
检索不到或置信度<t时弃权
第四
论文没有考虑“潜在上下文歧义”的问题
比如用户问“我的电话坏了，怎么办？
”，这里的“电话”可能指“手机”，
也可能指“座机”，
但是提示词里没有明确说明
这种情况下，模型的错误不是“幻觉”，
而是“误解”，
但是现有框架无法区分这两种情况
未来可能需要把“用户潜在意图”纳入误差分析
让模型学会“追问”，
而不是盲目回答
好了
以上就是这篇论文的主要内容了
给我的最大启发是，AI的问题
很多时候不是技术问题
而是“人的问题”，
我们设计了什么样的训练目标
就会得到什么样的模型；
我们设计了什么样的评估规则
就会引导模型走向什么样的方向
要解决幻觉问题，不仅需要优化模型
更需要优化我们对AI的“期望和评估方式”，
毕竟
我们需要的是一个“真正可信的助手”，
而不是一个“只会考满分的考生”。
论文链接我放在视频简介里了
欢迎感兴趣的观众去阅读
感谢收看本期视频，我们下期再见
”）
.
