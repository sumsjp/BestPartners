大家好，这里是最佳拍档，我是大飞
在过去的几年里
我们见证了大语言模型的惊人飞跃
从最初的简单聊天机器人
到如今能够编写复杂软件、操控浏览器
甚至进行科学研究的Agent
模型的推理能力和工具使用能力
已经达到了前所未有的高度
但是，有一个核心维度的能力
却始终像是一块短板
限制了AI真正成为我们的长期合作伙伴
那就是记忆的进化
目前的AI系统，大多依赖于RAG
也就是检索增强生成技术
它们能记住你说过的话
能从海量的文档库里找出你要的事实
但是，它们很难记住
我是如何解决这个问题的
很难从过去的成功或失败中
提炼出经验
并且将它应用到未来的新任务中
换句话说，它们记得事实
却学不会策略
为了解决这个痛点
Google DeepMind联合伊利诺伊大学厄巴纳-香槟分校（UIUC）
发布了一篇重磅论文
名为《Evo-Memory：
基于自我进化记忆的大语言模型Agent测试时学习基准》。
这篇文章不仅揭示了当前AI在经验复用上的无力
更提出了一个全新的框架，ReMem
试图让AI拥有像人类一样在实战中变强的能力
今天，我们就来拆解一下这篇论文
看看Google DeepMind是如何试图赋予AI进化的记忆的
以及这对我们通往AGI的道路
意味着什么
首先，我们把目光聚焦在一个概念上
那就是测试时学习
Test-time Learning
在传统的AI训练模式中
模型在实验室里被训练好
参数固定，然后打包发布
这就好比一个学生在学校里死记硬背了所有的课本
但是一旦毕业进入社会
也就是部署的阶段
他的大脑就停止了生长
无论他在工作中遇到多少新奇的挑战
他的知识库永远停留在毕业那天
但是这显然不符合真实世界的逻辑
在真实世界中
无论是作为交互式的助手
还是作为具身智能的机器人
AI都需要处理连续不断的任务流
它们需要从每一次的交互中吸取教训
积累洞察
如果不具备这种能力
模型就会陷入一种西西弗斯式的循环
不断地重复解决相似的问题
却没有任何长进
为了量化这种能力
研究团队提出了Evo-Memory基准测试
这不仅仅是一个数据集
更是一套全新的评估哲学
Evo-Memory的核心创新在于
它没有像传统测试那样把问题打乱
而是特意将数据集重构为顺序的任务流
这意味着
AI在第10次遇到问题的时候
它的表现应当优于第1次
因为它有前9次的记忆可以参考
这个基准测试涵盖了极其广泛的领域
目的就是为了全方位考验AI的记忆能力
在单轮推理任务方面
Evo-Memory包含了我们熟悉的MMLU-Pro和GPQA-Diamond
大家知道
MMLU-Pro是对经典MMLU基准的升级
去除了原本数据中的模糊性
增加了工程、哲学等领域的难度
而GPQA-Diamond更是被称为谷歌也搜不到答案的研究生级科学难题
需要极强的多步推理能力
不仅如此，为了测试数学和编程能力
基准中还加入了AIME-24和AIME-25
这是美国数学邀请赛的真题
主要测试符号推理和解题策略的迁移能力
通过这些任务
我们可以观察AI是否能从一道数学题的解法中
提炼出通用的公式或逻辑
应用到下一道类似的题目中
但是这些还不够
真正的挑战在于多轮交互任务
也就是所谓的具身智能场景
Evo-Memory引入了AgentBoard套件
这其中包括了著名的AlfWorld
一个模拟家庭环境的任务
AI需要像做家务一样，理解去厨房
拿起苹果，把它放进袋子这样的指令
还有BabyAI
用来测试AI的导航和组合推理能力
以及ScienceWorld
一个开放式的科学实验环境
在这些环境中，AI不仅要回答问题
还要执行一系列的动作
比如在AlfWorld中
如果AI第一次花了很多步骤才找到苹果
那么第二次当它需要找橘子时
它是否能复用先去餐桌看看这种高效的搜索策略？
这就是Evo-Memory想要考察的核心
不仅仅是知识的检索
更是策略的复用
为了在这个严苛的竞技场中一较高下
研究团队在两个当时最顶尖的模型上进行了实验
一个是Google自家的Gemini-2.5系列
包括Flash、Flash-Lite和Pro版本
另一个是Anthropic的Claude系列
包括3.5-Haiku和3.7-Sonnet
他们对比了四大类方法
第一类是无持久记忆的Agent
比如经典的ReAct框架
这类Agent就像一个金鱼般的助手一样
只能利用短期的上下文窗口
稍微长一点的任务就会让它们顾此失彼
第二类是自适应Agent的记忆方法
例如SelfRAG、MemOS和Mem0
这些方法引入了动态检索和持续更新机制
试图让AI拥有一个外挂硬盘
SelfRAG引入了反思机制
Mem0则试图构建一个结构化的用户记忆
虽然比第一类强
但是在面对极其复杂的推理任务时
依然显得力不从心
第三类是基于过程知识的记忆Agent
比如Dynamic Cheatsheet和Agent Workflow Memory
这些方法的思路很清奇
它们不再存储具体的事实
而是存储解决问题的流程或者攻略
这对于数学题这种结构化任务很有效
但是在开放世界中灵活性不足
而第四类
也就是本文提出的进化记忆框架
包括了ExpRAG和ReMem
这也是我们要重点分析的对象
先说ExpRAG
全称是经验检索与聚合（Experience Retrieval and Aggregation）
这是一个非常强大而且简洁的基线方法
它的逻辑很简单
每当AI完成一个任务
无论成功与否，都将这次的经历
包括输入、模型的预测、环境的反馈
打包成一条结构化的文本
存入记忆库
当下一次遇到新任务的时候
AI会先在记忆库中检索与当前任务最相似的K个过往案例
将它们作为上下文输入给模型
这听起来很像少样本学习（Few-Shot Learning）
但是关键的区别在于
这里的样本是AI自己在测试阶段实时生成的
而不是人类预先准备好的
ExpRAG证明了
哪怕只是简单地让AI看看自己过去是怎么做的
都能带来巨大的性能提升
但是，ExpRAG有一个明显的短板
那就是它是被动的
它只是一股脑地把过去的经历塞进提示词里
缺乏深度的思考和筛选
于是，ReMem应运而生
ReMem，即反思性记忆
Reflective Memory
是研究团队提出的终极解决方案
它引入了一个极其精妙的从行动
到思考
再到记忆的优化闭环
不同于传统的ReAct框架只在思考和行动之间循环
ReMem增加了一个关键的维度
记忆优化（Refine Memory）
在ReMem的每一轮决策中
AI都有三个选择
思考、行动，或者优化记忆
当AI选择思考时
它会生成内部的推理痕迹
分解任务，规划下一步
当AI选择行动时
它会真正与环境交互
比如在游戏里移动一步
或者输出最终答案
而当AI选择优化记忆时，奇迹发生了
这是一个元推理的过程
AI会审视自己当前的记忆库
主动思考
比如哪些过往的经验对当前任务是有用的？
哪些是误导性的噪音？
是否应该把这条新的经验
总结成一条通用的规则存下来等等？
这种机制让记忆不再是一个静态的仓库
而变成了一个有生命的有机体
AI在解题的过程中
不仅是在解决问题
也是在整理自己的大脑
这就像一个经验丰富的工程师
在写代码的同时
会不断重构自己的代码库
删除冗余的函数，提取通用的模块
让我们来看一个具体的例子
帮助大家理解ReMem的强大之处
假设在AlfWorld环境中
AI接到了一个任务
把一个洗干净的苹果放在餐桌上
在第一次尝试时
AI可能会像无头苍蝇一样乱撞
先去了厕所，又去了卧室
最后才在厨房找到苹果
然后去水槽洗干净，最后放到餐桌上
这个过程可能花费了20个步骤
如果是普通的RAG模型
它会把这20个步骤全部记下来
下次遇到类似任务的时候
它可能会检索到这段冗长的、包含大量无效探索的经历
反而干扰了当下的决策
但是ReMem不同
在任务完成后
ReMem会进入到优化阶段
它会反思这次经历
剔除掉去厕所和卧室的无效步骤
只保留去厨房，拿苹果，去水槽
洗苹果
去餐桌，放苹果这一条黄金路径
甚至
它会将这个路径抽象为一条规则
在寻找食物类物品时
应该优先搜索厨房
当下一个任务变成把一个热过的土豆放在盘子里时
ReMem就能迅速复用这个优先搜索厨房的策略
极大地提高了效率
接下来，让我们用数据说话
看看Evo-Memory基准测试的实验结果究竟如何
研究团队在Gemini 2.5 Flash和Claude 3.7 Sonnet这两个强大的模型上进行了详尽的测试
结果可以用全面碾压来形容
首先是RQ1，总体表现
在单轮推理任务中
ReMem在Gemini 2.5 Flash上实现了平均0.65的精确匹配率
而在API调用准确率上更是达到了0.85
相比之下
传统的ReAct方法只有0.30的精确匹配率
这说明
即使是对于逻辑性极强的数学题
拥有进化记忆的AI也能通过复用之前的解题思路
大幅提高准确率
在多轮交互任务中，优势则更加明显
在BabyAI这个导航任务中
ReMem的成功率达到了0.91
而没有记忆的基线模型只有0.61
在ScienceWorld这个极具挑战性的科学实验环境中
ReMem更是实现了0.88的成功率
远超基线的0.32
这再次印证了那个观点
任务越复杂、步骤越多
记忆的价值就越大
RQ2则探讨了一个非常有意思的问题
什么因素决定了记忆的有效性？
实验发现
ReMem的性能提升与任务的相似度（Task Similarity）呈强正相关
也就是说
如果测试流中的任务彼此之间有某种内在的联系
比如都是代数题
或者都是厨房场景的任务
ReMem就能发挥出惊人的威力
这其实符合我们的直觉
如果让你先学骑自行车
再学骑摩托车，你会学得很快
因为两者有相似的平衡技巧
但是如果让你先学做饭，再学开飞机
前者的经验对后者几乎没有帮助
RQ3关注的是任务序列的难度顺序
是先易后难好，还是先难后易好呢？
实验结果非常反直觉
对于普通模型来说，如果先做难题
往往会因为受挫而表现不佳
但是对于搭载了ReMem的AI来说
它在先难后易的序列中表现得异常稳健
这说明，ReMem具备极强的健壮性
通过反思机制
它即使在困难任务中失败了
也能从中提取出有价值的教训
从而在后续的简单任务中避坑
这就引出了RQ4，关于反馈机制的讨论
应该是只记住成功的经验
还是把失败的教训也记下来呢？
传统的观点认为，失败是成功之母
应该都要记
但是实验数据给了我们一记响亮的耳光
对于基线方法来说
如果不加筛选地引入失败经验
性能反而会下降
因为模型往往分不清哪些是错误路径
很容易被误导
但是，ReMem再次展现了它的优越性
得益于其内置的优化模块
ReMem能够识别出失败经验中的毒点
并将它转化为负面约束（Negative Constraints）
换句话说
普通的AI会被失败的记忆污染
而ReMem能把失败的记忆变成警示牌
即便如此，实验数据也显示
仅保留成功经验通常能获得最高的效率
这表明目前的模型在利用负面反馈方面还有很大的提升空间
最后是RQ5，时间维度的进化
这一点的结果最令人振奋
从累积成功率曲线来看
随着任务数量的增加
ReMem的性能曲线呈现出明显的上升趋势
而且收敛速度远快于其他方法
这证明了它真正实现了终身学习的雏形
用得越久
越懂你，越能干
当然，作为一篇严谨的学术论文
作者们也坦诚地讨论了Evo-Memory目前的局限性
首先是成本问题
ReMem引入了额外的思考和优化步骤
这无疑增加了推理时的计算开销
在追求极致响应速度的实时应用中
这可能是一个瓶颈
其次呢是依赖性
ReMem的表现
高度依赖于底层大模型的能力
如果底座模型本身连基本的反思指令都听不懂
那么再好的记忆机制也是空中楼阁
目前的实验是基于Gemini 2.5 Pro和Claude 3.7 Sonnet这样的顶尖模型进行的
对于参数量较小的开源模型
效果如何还有待验证
此外
目前的测试主要集中在文本和代码领域
对于更复杂的多模态环境
比如结合视觉、听觉的真实世界机器人
如何构建高效的进化记忆
依然是一片无人区
总结一下，《Evo-Memory》这篇论文
为我们揭示了AI进化的下一个关键方向
那就是从静态的知识库
走向动态的经验流
它告诉我们
真正的智能不仅仅是拥有海量的数据
更是拥有处理这些数据的智慧
ExpRAG向我们展示了经验复用的巨大潜力
而ReMem则通过模拟人类的行动—思考—反思闭环
为构建自我进化的AI Agent提供了一套可行的工程范式
随着Gemini 3.0、GPT5.2等更强力模型的发布
我们有理由相信
具备进化记忆的AI将在未来几年内
彻底改变我们的工作方式
也许很快
我们就能拥有一个真正意义上的养成系AI助手
它会陪着我们一起经历，一起成长
成为我们真正的，最佳拍档
感谢大家收看本期视频
我们下期再见
