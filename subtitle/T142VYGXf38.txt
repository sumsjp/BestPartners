大家好，这里是最佳拍档，我是大飞
今天我们来聊聊大模型的幻觉问题
用过ChatGPT等大语言模型的朋友应该都知道
大语言模型偶尔会生成与用户输入不符的内容
或者是与之前生成的内容相矛盾的内容
又或者与已有的世界知识不一致的内容
这种现象通常被称为"幻觉"，
英文称为Hallucination
这极大地降低了大语言模型在真实世界场景中的可靠性
比如
大语言模型可能会编造出错误的医疗诊断或者治疗方案
从而对实际生活造成风险
如果进行一下分类
我们会发现大模型的回答
存在着多种不同类型的问题
第一种，回答含糊不清
当大语言模型的反应模棱两可
可以有多种解释时
就会出现这类问题
这种回答不一定是错误的
但是它无法为用户问题提供有用的答案
比如我们希望它回答法国的首都是巴黎
但是模型给出的答案却模棱两可
第二种，回答不完整
当生成的回答不完整
或者支离破碎的时候
就会出现这个问题
比方说我们想知道如何更换轮胎
模型只回答了四个步骤中的前两步
导致解释不完整
第三种，回答偏差
大语言模型中的偏见
与生成文本中的不公平或者偏见表现有关
这些偏见可能来源于训练数据
而训练数据通常包括历史文本、文学作品、社交媒体内容和其他来源
这些来源可能从本质上反映了种族偏见、性别偏见、刻板印象或者歧视性信念等等
比如示例中
将老师特别描绘成一名女性
这就是一种性别偏见
第四种，信息不足
由于RLHF的过程中
可能会导致大语言模型的过度优化
从而可能导致信息不足的状态
比如示例中模型无法回答用户的查询
在最近Arxiv上的一篇论文《AI大海中的海妖之歌：
大语言模型中的幻觉调查》中
详尽阐述了有关大语言模型幻觉的各个方面
我做了一些简单的整理
这里跟大家分享一下，视频内容较长
希望大家可以耐心观看
全文分成了几个部分
我也会按照这个顺序跟大家一一介绍
分别是对幻觉的定义，如何评估幻觉
大语言模型幻觉的来源
如何在训练和生成推理阶段减少幻觉
以及其他可以减少幻觉的方法
首先我们先来回答一下什么是大语言模型的幻觉
如何定义它？
其实在大语言模型出现之前
"幻觉"（hallucination）一词已经在NLP界被广泛使用
通常指生成无意义或者不忠于所提供来源内容的内容
由于大语言模型功能的多样性
现在幻觉的定义似乎已经被大大扩展
因此
论文中将大语言模型范畴内的幻觉分成了三类
第一类，是和输入相冲突的幻觉
也就是大语言模型生成的内容
与用户提供的源输入相背离
从而产生这种幻觉
通常情况下
用户对大语言模型的输入包括两部分
分别是任务指令和任务输入
如果拿一篇文档为例
前者指的是用户要生成文档摘要的提示
而后者指的就是要生成摘要的这篇文档
大语言模型的响应
与任务指令之间的矛盾
通常反映了模型对用户意图的误解
比如这里模型在回复时
错误地将人名Hill替换成了Lucas
但是总体上来说
确实是生成了摘要的形式
这在机器翻译或者摘要总结的任务中
符合特定NLG任务中的常规定义
第二类，语境冲突性的幻觉
指的是大语言模型生成的内容
与之前生成的信息本身相冲突
模型在生成冗长的、或者多轮回答的时候
可能会表现出自我矛盾
这种幻觉产生的原因
可能是模型在整个对话过程中
失去了对上下文的跟踪
或者无法保持长期记忆的一致性所造成的
在介绍NBA总裁的这个例子中
模型最初介绍的是现任NBA总裁Silver
但后来又提到了前NBA总裁Stern
这说明生成的内容缺乏一致性
第三类，与事实相冲突的幻觉
这种幻觉指的是模型生成的内容
与现有的世界知识相矛盾
比如用户询问葡萄牙国王阿方索二世的母亲是谁
大语言模型给出了一个错误的答案
应该是阿拉贡的杜尔塞
也称为巴塞罗那的杜尔塞
而不是卡斯蒂利亚的乌拉卡女王
这很容易误导知识不足的用户
了解了幻觉的分类之后
应该如何对幻觉进行评估呢？
应该说，针对不同类型的幻觉
采用的评估方式是不一样的
首先
作者提出了各种基准来评估大语言模型的幻觉
其中比较具有代表性的基准
包括TruthfulQA、FActScore等
现有的基准主要根据大语言模型的两种不同的能力
来评估幻觉
分别是生成事实陈述
或者区分事实陈述与非事实陈述的能力
对应着生成式基准和判别式基准
这两种评估形式是有一定区别的
具体来说
生成式基准将幻觉视为一种生成特征
类似于流畅性和连贯性
并对模型生成的文本进行评估
例如
TruthfulQA考察的是大模型回答问题的真实性
而FActScore考察的是大模型为某个人生成传记的事实准确性
与此相反
判别式基准考虑的是大模型辨别真实陈述和幻觉陈述的能力
具体来说
HaluEval要求模型能够确定状态信息中是否包含幻觉信息
而FACTOR则研究模型是否能够赋予事实陈述比非事实陈述更高的可能性
需要注意的是
TruthfulQA也支持辨别形式
它提供了另一个替代方案
可以测试模型辨别真实陈述的能力
有了评估基准之后
接下来就是确定评估的任务形式
分成三种
分别是问题解答、指令提示和文本补全
而以上大多数基准
都需要人类注释者来创建数据集或者保证质量
评估指标的方法也主要分为人工评估和模型的自动评估
在自动评估方面
TruthfulQA训练了一个基于GPT-3的6.7B模型
可以根据问题的注释对答案的真假进行分类
AlignScore建立了一个统一的函数来评估两个文本之间的事实一致性
而FactScore首先使用通道检索器来收集相关信息
然后采用一个评估模型
比方说LLaMA-65B
使用检索到的知识来确定状态的真实性
并进一步采用微观F1分数和误差率
来评估自动指标与人工评估相比的可靠性
好，讲了这么多
那大语言模型的幻觉问题
到底从何而来呢？
其实我们在研发大模型时
会涉及到预训练、微调、强化等多个阶段
每个阶段都会是幻觉的引入来源
首先，在预训练阶段
大语言模型会从大量训练数据中积累大量知识
然后将其存储在模型参数中
当被要求回答问题或者完成任务时
如果缺乏相关知识
或者内化了训练语料中的错误知识
模型就会产生幻觉
比方说
模型有时会将虚假的相关性误解为事实知识
也就是说幻觉与训练数据的分布之间存在很强的相关性
由于模型偏向于肯定测试样本
而样本的语料中本身也存在幻觉的现象
因此模型很容易复制甚至放大这种幻觉行为
其次，大模型有时会高估自己的能力
对于非常大的大语言模型来说
正确答案和错误答案的分布熵可能是相似的
这表明模型在生成错误答案时
与生成正确答案时同样自信
甚至是过度自信
第三
有问题的对齐过程可能会误导模型产生幻觉
大语言模型在预训练之后
通常都会经过微调和对齐的阶段
如果模型在预训练阶段
没有获得相关的先决知识
那么在训练指令时
这实际上是一个错误的对齐过程
会促使模型产生幻觉
另一个潜在问题是"谄媚"（sycophancy）
即模型可能会生成偏向用户观点的回答
而不是提供正确或者真实的答案
从而也会导致幻觉
第四点，模型采用的生成策略
本身就存在潜在得风险
我们都知道
大语言模型按照顺序来生成响应文本
每次输出一个token
但是
即使模型意识到自己出现了早期错误
它们有时也会过度承诺
换句话说
大语言模型可能更喜欢用“滚雪球”来实现自我一致性
而不是从错误中恢复
有研究认为
局部最优并不一定能确保全局最优
早期的局部预测可能会导致模型难以形成正确的结果
同时，基于抽样的生成策略
比如top-p和top-k
它们所引入的随机性也可能是幻觉的一个潜在来源
那么我们应该如何来减少幻觉现象的产生呢？
这里分为两个阶段
分别是训练阶段和生成推理阶段
训练阶段又可以分为三个时期
分别是预训练时期、SFT时期和RLHF时期
现有的工作认为，大语言模型的知识
大多是在预训练阶段获得的
如果在预训练语料库中
存在类似于错误信息之类的噪声数据
就可能会破坏大语言模型的参数知识
而这是导致误差的一个重要因素
因此
减少幻觉的直观方法可以是人工或自动整理预训练语料库
尽可能减少无法验证或者不可靠的数据
其实在大语言模型时代之前
已经有了一系列致力于人工消除噪声训练数据
来减轻幻觉的工作
比方说
专注于"数据到文本"（data-to-text）的任务
并邀请人工根据给定的知识库
手动编写干净准确的回复
结果表明
使用这种经过编辑的训练数据可以有效减少幻觉
同样
在现有的表格到文本的数据集中
对文本进行人工提炼这一过程
也可以大大减少事实幻觉
不过
随着现在预训练语料库的规模越来越大
在预训练期间整理训练数据
变得越来越具有挑战性
例如Llama2对大约两万亿个token进行预训练
因此，与人工整理相比
目前更实用的方法是自动选择可靠数据
或者过滤掉噪声数据，例如
GPT-3的预训练数据
就是基于与一系列高质量参考数据的相似性进行清理
Falcon通过启发式规则从网络中仔细提取高质量数据
并证明了经过适当分级的相关语料库
可以产生强大的语言模型
Llama2在构建预训练语料库时
从维基百科等高度事实性的来源中向上抽取数据
总的来说，在预训练过程中
鉴于现有的预训练语料库规模庞大
目前的研究主要采用简单的启发式规则
来选择和过滤数据
而未来研究的方向
是如何设计更有效的选择或过滤策略
其次，当前的大语言模型
一般都会经历一个被称为"监督微调"（SFT）的过程
用来从预训练中获取所需的知识
并学习如何与用户互动
一般来说
SFT要首先标注或者收集海量的任务指令跟踪数据
然后使用最大似然法（MLE）
在这些数据上对预训练过的基础模型进行微调
与预训练类似，要减少SFT阶段的幻觉
也需要对SFT的训练数据进行整理
但是SFT数据量相对较小
因此可以选择手动或者自动整理
这里要注意的是，在SFT过程中
可能会因为行为克隆而引起大语言模型产生幻觉
行为克隆是强化学习中的一个概念
简单来说
这种方法只是简单地模仿行为
而不是为了学习实现最终的目标
大语言模型的SFT过程
可以被视为行为克隆的一个特殊案例
即模型通过模仿人类的行为
来学习互动的形式和风格
对于基础模型来说
尽管已经将大量知识编码到了自己的参数中
但是仍有一些知识会超出它们的能力范围
通过克隆SFT过程中的人类行为
模型学会了以肯定的语气回答所有问题
而不去评估这些问题是否超出了它们的知识边界
因此，在推理过程中
如果被要求回答与没有学习过的知识
相关的问题
他们很可能会自信地产生幻觉
解决这个问题的方法之一
是采用以诚实为导向的SFT
即在SFT数据中引入一些诚实的样本
诚实样本指的是承认自己没有能力做出回答
比如"对不起
我不知道"等等
这块可以看一下Moss项目开源的SFT数据
其中就包括这类诚实样本
使用这些样本的模型
可以学会拒绝回答特定问题
从而帮助减少幻觉
不过，总体来说
由于SFT数据量可以接受
因此由人类专家进行人工整理依然是首选方案
同时由于标注人员不了解模型的真实知识边界
诚实样本也存在知识的盲区
因此这种方法也只能作为SFT阶段的次优方案
考虑到SFT阶段的局限性
Schulman提出在RLHF期间来解决幻觉问题
他设计了一种专门用于减轻幻觉的特殊奖励函数
这里的"Unhedged/HedgedCorrect/Wrong"指的是大语言模型用肯定或者犹豫的语气
提供正确或错误的答案
它的核心理念是鼓励学习者挑战前提、表达不确定性
并通过从特殊的奖励中学习来证明自己的无能
这种方法称为"诚信导向强化学习"，
它与"诚信导向SFT"相比有几个优点
首先它允许大语言模型自由探索知识的边界
从而增强对分布以外案例的概括能力
其次它还减少对大量人工标注的需求
并且消除了对标注者猜测模型知识边界的要求
这种方法的挑战在于
经过强化学习调整的模型
可能会表现出过度保守
比如ChatGPT拒绝回答它已经明确知道答案的问题
这可能是由于奖励函数的设计不合理
或者由于训练数据的质量不高所导致
那除了在训练时减少幻觉意外
在推理时减少幻觉
可能会具有更好的成本效益和可控性
因此呢
现有研究大多数都集中在这个方向上
我们简单介绍几个
第一个设计解码策略
解码策略
比如说贪婪解码和波束搜索解码
他们决定了我们如何从模型生成的概率分布中
选择输出TOKEN
通过评估发现
在事实性方面，核采样
也称为顶点采样的效果不如贪婪解码
可能是因为top p采样引入的随机性
会无意中的导致幻觉
因为模型往往会编造信息来产生多样化的反应
有研究人员就提出了一种
称为事实核采样的解码算法
希望利用top p和贪婪解码的优势
在多样性和事实性之间取得更有效的平衡
还有研究团队提出了一种推理时干预方法ITI
他们发现Transformer模型中的某些注意力头
对于模型生成内容的真实性至关重要
在推理阶段
通过在注意力头上使用一种特殊的指令干预激活方式
可以有效的提升大模型生成内容的真实性
而且这种方法跟RLHF相比
所需要的成本非常低
除此之外
有研究表明
大语言模型在处理下游任务的时候
有时候无法充分关注检索到的知识
尤其是当检索到的知识
与模型的参数知识相冲突的时候
为了解决这个问题啊
可以采用直接的上下文感知解码策略CAD
CAD方法能够让模型更多的关注上下文信息
而不是过度的依赖于自身的参数知识来做出决策
实验结果表明
CAD能够有效的激发模型利用检索知识的能力
从而减少下游任务中的事实幻觉
总的来说呢
设计解码策略可以减少大语言模型在推理过程中的幻觉
而且这种方法通常是即插即用的易于部署
不过呢大多数现有研究
都要求访问token级别的输出概率
而目前大部分模型只支持有限的API
第二个研究方向就是借助于外部知识
作为补充证据来帮助大语言模型提供真实的回复
这也是最近兴起的一种解决方案
这种方法呢通常包括两个步骤
第一个呢
是准确的获取与用户指定相关的知识
一旦获取了有用的知识
第二步就是需要利用这些知识来指导应答的生成
我们已经知道通过大量的预训练和微调
大语言模型已经将大量的知识内化到了自己的参数中
这些知识呢可以称为参数知识
但是不正确或者过时的参数知识就很容易的会导致幻觉
为了解决这个问题
从可靠的来源获取可靠的最新知识
就成为了模型的一种热补丁
而对于如何获取外部知识
又可以分成多种不同的类型
一种呢是通过外部知识库
比方说大规模的非结构化的语料库
结构化的数据库维基百科这些网站
甚至是整个互联网
证据检索过程通常会采用各种稀疏或者密集的检索器
搜索引擎呢也可以被视为一种特殊的信息再提取器
此外呢
还包括其他一些参数知识指导框架
可以从微调的白盒模型的参数存储器中重新汲取知识
另一种呢是通过外部的工具
比如说FacTool和CRITIC
一旦获得了相关的知识
就可以在不同阶段加以利用来减轻模型的幻觉
现有的知识利用方法也可以大致分为两类
一种呢是生成式补充
就是把知识和用户的查询先联合起来
再作为提示发给模型
这种方法呢既有效又易于实施
这种知识也被称为上下文知识
context knowledge
而大语言模型本身就具有很强的上下文学习能力
所以能够从中提取和利用有价值的信息
另一种呢是事后纠正
也就是在后处理阶段
构建一个辅助的矫正器来纠正幻觉
矫正器呢可以是另外一个大语言模型
也可以是一个特定的小模型
这种矫正器呢首先与外部知识源互动
收集足够的证据
然后来纠正幻觉
总的来说呢
利用外部知识来减轻幻觉的方法
不需要修改大语言模型
是一种即插即用的高效的解决办法
其次呢它便于向模型传输专有的知识和实时更新信息
最后呢这种方法允许生成的结果追溯到源头的证据
从而提高了模型生成的信息的可解释性
不过呢
这个方法也存在一些现实的问题
首先是知识验证
如果外部知识来源是基于互联网的
那么如何验证这些知识的真实性
是一个有待于解决的开放性和挑战性的问题
其次检索器矫正器的性能和效率如何提升和优化
最后检索到的知识可能与模型存储的参数知识相冲突
当冲突发生的时候
模型可能就无法充分利用外部的知识
而且如果外部知识的上下文过长
可能就会造成模型性能的明显下降
那除了解码策略和外部知识以外
第三个研究方向就是利用不确定性
不确定性是推理过程中
保护和减少幻觉的重要指标
通常呢它指的是模型结果的置信度
不确定性可以帮助用户确定什么时候应该信任大语言模型
只要能够准确描述模型响应的不确定性
用户就能够过滤或者纠正模型的高不确定性的声明
因为这类声明更容易是捏造的
一般来说
估算大于模型不确定的方法可以分为以下三种
首先呢是基于logit的估计
这是一种基于对数的方法
它需要获取模型的对数
通常是通过计算token级的概率或者是熵来确定不确定性
其次呢是基于口头的估计
直接要求模型表达它的不确定度
比方说使用提示请回答并提供你的置信度分数从0到100
这种方法呢之所以有效
是因为大语言模型的语言表达能力和服从指令的能力很强
也可以使用思维链提示来加强这种方法
第三种呢是基于一致性的估计
这种方法呢基于这样一个假设
就是当模型犹豫不决并且对事实产生幻觉的时候
他们很可能会对同一个问题做出逻辑上不一致的回答
总体来说
利用不确定性来识别和减轻大语言模型的幻觉
是当今很有前途的一个研究方向
刚刚提到的这三种主要方法
每种呢其实也面临了一些挑战
首先呢基于对数的方法
越来越不适用于现代的商用大语言模型
因为这些模型呢通常都是闭源和黑盒的
导致它输出的对数呢你无法访问
其次关于基于口头表达的方法
模型在表达它的信心的时候
往往会表现出高度的过度自信
第三在一致性的方面
如何有效的测量不同回答的一致性
仍然是一个尚没有解决的问题
好了到这里
我们已经盘点了主要减轻幻觉的两个阶段
分别是训练阶段和推理阶段以及相应的办法
那么除了这些以外
还有其他减少幻觉的技术吗
论文中总结了四个
分别是1、多代理互动
也就是多个大语言模型独立提出建议
并且就各自的回应进行协作辩论
从而达成单一的共识
比方说一个模型提出主张
另一个模型呢就这个主张提出问题并且检查他的真实性
这样呢就能够以相对较低的成本来有效的减少幻觉
第二个提示工程
用户可能会遇到这样的情况
就是大语言模型最初会做出准确的回应
但是在使用不同的提示之后
模型就开始产生幻觉了
因此呢可以设计出更有效的提示来缓解幻觉
目前流行的一种做法是
在设计系统提示的时候
明确的指示模型
如果你不知道问题的答案
请不要提供虚假或者不可验证的信息
第3个大语言模型可能会意识到自己的虚假信息
因此可以利用它们的内部状态来检测虚假信息
SAPLMA方法在大语言模型的每个隐藏层上添加了一个分类器
来确定它的真实性
另外呢
我们之前提到的推理时观察法ITI
也是基于类似的假设
他们在推理过程中进一步的转移了
模型激活与事实相关的头部
并发现这可以减轻幻觉
这些研究呢都表明
大语言模型中的幻觉
可能更多的是生成技术的结果
而非基本的表征
第四点幻觉的一个潜在原因
可能是知识与用户问题之间的错位
这种现象在检索增强生成RAG中尤为普遍
为了解决这个问题可以引入MixAlign
利用模型将用户查询与存储的知识对齐
并进一步的鼓励用户来澄清这种对齐
这种方法不仅能够减少幻觉
还可以提高生成内容的质量
第五点还可以优化模型的架构来减少幻觉
比如多分支解码器和不确定性感知解码器
另外在构建模型的时候
采用双向自回归架构
从而实现从左到右和从右到左的模型建模
也可以有效的利用双向信息来减少幻觉
总的来说
虽然已经有了一些方法
可以去定量的评估模型的幻觉
但是还有很多问题亟待解决
尤其是自动评估还无法准确的反映出性能
也无法与人工注释保持一致
除此之外
由于我们已经知道大语言模型的幻觉
可能主要来源于错误信息的记忆
以及缺乏正确的事实知识
这两年呢也出现了一种称为模型编辑的方法
能够以既节省数据又节省计算的方式来修改模型的行为
目前呢模型编辑有两种主流的模式
第一种呢是加入辅助子网络
第二种呢是直接修改原始模型的参数
这两种模式呢都可以通过有目的的编辑模型存储的事实知识
来消除他们的幻觉
不过这个新兴领域呢也面临着许多挑战
包括如何编辑黑盒的模型
如何编辑上下文模型以及多跳模型等等
最后诱发幻觉的攻击和防御也是一个重点
有一些研究表明
可以通过精心制作的越狱提示等技术
对大语言模型进行操作
从而诱发任意的非预期反应
其中呢就包括幻觉
关于这个话题我们找机会会专门做一期节目来介绍
好了以上就是这篇论文的主要内容
从对幻觉的定义、如何评估幻觉
幻觉的来源以及如何减少幻觉几个方面
较为完整的介绍了有关于大语言模型幻觉的各个方面
希望能给屏幕前的观众带来一些启发
有兴趣深入了解的朋友
可以去阅读一下论文的原文
以及论文后面大量的相关的引用
大飞我觉得应该说幻觉跟大模型就是一体的关系
就像机遇和风险并存一样
我们只能够想办法尽量去控制减少风险
却无法完全消除风险
或者说幻觉本身就是知识的一部分
也是人类认知的一部分
也体现了这个世界混沌不确定性的一面
从长远来看
我们需要的是如何学会与幻觉并存
或者如何更好的区分幻觉与事实
本期的视频内容就到这里
感谢大家的观看
我们下期再见
