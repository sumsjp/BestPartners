大家好，这里是最佳拍档，我是大飞
如何让小模型在特定领域
也能达到大模型的专家水平呢？
这是当前AI行业中的一个核心痛点
而Thinking Machines Lab最新发布的技术文章
在线策略蒸馏
恰好就是解决这个问题的关键
简单来说
这是一种将强化学习的纠错相关性
与监督微调的奖励密度相结合的训练方法
Thinking Machines Lab发现
把它用在数学推理和内部聊天助手的时候
在线策略蒸馏可以极低的成本超越其他方法
CEO Mira Murati 也表示
这种方法可以让小模型具备强大的领域性能和持续学习的能力
文章作者主要是凯文·卢（Kevin Lu）
他之前曾经在 OpenAI 工作
领导了 4o-mini 的发布
并且参与过 GPT-5系列、GPT-oss
以及o1、o3和o4一系列模型的研发工作
今天我们就来给大家解读一下这篇论文
在解读之前，我们先思考一个问题
一个大语言模型
要想在特定领域达到专家级的水平
到底要经历哪些训练阶段呢？
只有明白这个
才能理解在线策略蒸馏的价值在哪里
首先，任何一个强大的模型
都不是一步训练成的
这个过程大致可以分成三个核心的阶段
而每个阶段的目标和任务都完全不同
第一个阶段是预训练（Pre-training）
这是模型的基础教育阶段
目标是让模型掌握一些通用能力
比如理解语言结构、基础逻辑推理、常识性世界知识
我们熟悉的GPT、Qwen系列模型
预训练阶段都会用海量的公开文本来训练
本质是让模型学会说话和学会思考的基本逻辑
这个阶段的成果是通用基础
但是还谈不上专家
因为它没有特定领域的深度知识
第二个阶段是中期训练（Mid-training）
这一步是给模型填充专业知识
让它从通才向专才转变
中期训练的核心是领域知识的注入
这个阶段结束后
模型虽然有了领域知识
但是还缺一个关键的能力
那就是把知识用在具体任务上的行为规范
第三个阶段就是后训练（Post-training）
这一步是塑造模型的行为
让模型知道在特定场景下该怎么做
比如，同样是医疗模型
有的需要它用通俗语言解释病情
有的需要它输出结构化的诊断报告
后训练就是解决这类行为适配的问题
而这也是今天我们要聚焦的重点
因为在线策略蒸馏
就是一种高效的后训练方法
那么
当前主流的后训练方法都有哪些呢？
它们又有什么问题呢？
这就不得不提到两种经典的思路
在线训练（On-policy training）和离线训练（Off-policy training）
这两种方法各有优劣
却都存在难以解决的痛点
我们先来说在线训练
最典型的就是强化学习
它的逻辑就是让学生模型自己试错
然后根据结果给予奖励
这种方法的优点很明显
学生模型是在自己的轨迹上学习
能够直接避免自己会经常犯的错
但是它的缺点也致命
那就是反馈太稀疏
文章里举了个很形象的例子
强化学习就像教孩子下棋
只告诉孩子这盘赢了或者这盘输了
但是不告诉他哪一步走得好
哪一步是昏招
这就导致每一轮训练
只能学到一个模糊的教训，效率极低
而且强化学习的计算成本很高
后面我们会看到
同样是提升数学成绩
强化学习需要的GPU小时
是在线策略蒸馏的10倍以上
我们再来说离线训练
最常见的是监督微调和传统的离线蒸馏
它的逻辑是让学生模仿教师
先找一个性能好的教师模型
让它生成大量正确的轨迹
然后让学生模型照着这些轨迹学
目标是和教师的输出一模一样
这种方法的优点是反馈密集
学生每一步都知道教师是怎么做的
哪里错了能直接对标
而且成本比强化学习低
但是它的问题更加隐蔽
会存在复合误差
也就是教师模型的轨迹是教师常遇到的场景
而学生模型实际使用的时候
可能会遇到教师没见过的场景
尤其是在处理长序列任务的时候
这种偏离会越来越严重
最后学生模型的表现会远不如教师
更麻烦的是
传统离线蒸馏还有一个隐患
学生可能只模仿到了教师的风格
却没学到准确性
比如教师模型回答问题的时候很自信
学生也学了这种自信的语气
但是内容还是错的
这就是古迪班德（Gudibande）等人在2023年的论文中指出的问题
离线蒸馏很容易陷入形似神不似的陷阱
说到这里
大家应该能感受到这种两难
在线训练能让学生在自己的场景里学习
但是反馈稀疏、成本高；
离线训练反馈密集、成本低
但是又容易偏离实际场景、学错重点
那有没有一种方法
能把两者的优点结合起来
既让学生在自己的轨迹上学习
又能得到每一步的密集反馈呢？
答案就是我们今天的主角
在线策略蒸馏
在线策略蒸馏的核心逻辑其实很简单
用一句话概括就是
让学生模型走自己的轨迹
然后让教师模型给每一步打分
具体到模型的训练
这个过程可以拆成两步
第一步，学生模型生成自己的轨迹；
第二步
教师模型对学生轨迹的每一个token进行评分
然后学生模型根据这些token的分数来调整参数
下次尽量生成教师认可的token
这样一来
在线策略蒸馏就把在线训练的场景相关性
和离线蒸馏的反馈密集性
结合在了一起，实现了两全其美
当然，这个方法也不是凭空而来的
它借鉴了两个经典技术的思路
第一个是DAGGER算法
由罗斯（Ross）等人在2010年的论文中提出
这种算法会让教师对学生访问过的状态进行评估
核心是关注学生的实际场景；
第二个是过程奖励建模（Process Reward Modeling）
由莱特曼（Lightman）等人在2023年的论文中提出
核心是对学生的每一步都给奖励
在线策略蒸馏相当于把这两个思路融合
并且针对大语言模型的特点做了优化
介绍完核心逻辑
我们再来看一下它的实现细节
首先是损失函数
反向KL散度（Reverse KL Divergence）
大家应该都清楚
KL散度是用来衡量两个概率分布的差异
在在线策略蒸馏里
就是衡量学生模型的token分布和教师模型的token分布之间的差异
差异越小
说明学生越像教师，表现越好
在反向KL散度的公式里
π_θ是学生模型的分布
π_teacher是教师模型的分布
x_{t+1}是下一个token，x_{1..
t}是前面的上下文
通俗来讲
这个公式计算的是在学生生成的上下文下
学生预测的下一个token
和教师预测的下一个token
差距有多大
我们的目标就是最小化这个差距
也就是让学生在自己的上下文里
尽量预测出教师认可的token
为什么用反向KL散度，而不是正向呢？
这里有两个关键原因
第一个是不可欺骗性
正向KL可能会让学生钻空子
比如只生成教师概率高的几个token
却忽略逻辑
而反向KL确保低差距一定对应教师认可的行为
因为它是从学生的分布出发
强制学生贴近教师
第二个是模式寻找（Mode Seeking）
反向KL会让学生专注于学习教师的核心行为
而不是分散在多个次优的行为上
比如拿解数学题来说
教师的核心行为是正确的推导步骤
反向KL会让学生专注于学这个
而不是学教师偶尔的笔误
另外，研究团队还做了个选择
把折扣因子设为0
这意味着学生模型只优化当前下一个token
不考虑未来的token
他们解释说
虽然理论上折扣因子大于0会更加严谨
但是他们在实际实验中发现
设为0不仅不会影响性能
还能够简化计算
为了方便大家理解这个过程
研究团队还做了个可视化的案例
可以直观看到在线策略蒸馏的作用
团队用了一道来自SimpleBench基准的数学题
题目涉及煎锅里的冰块
正确答案是B，0
因为冰块会在煎锅里融化
不能用纯数学来计算
而学生模型犯了一个典型错误
把它当成了纯数学题
忽略了物理常识，给出了错误的结果
随后
教师模型对学生的轨迹进行了逐个token的评分
用颜色深浅来表示惩罚程度
深红色是高惩罚，浅红色是低惩罚
结果显示
学生开始写根据数学公式计算时
教师给了深红色的高惩罚
因为这一步忽略了物理背景；
而学生写最终答案是XX时
惩罚反而低
因为这个答案是基于前面错误推导的合理结果
错不在答案本身，而在前面的步骤
这个案例说明
在线策略蒸馏能精准定位错误的根源
而不是只看最终的结果
这正是它比强化学习好的地方
最后
研究团队还给出了基于Tinker API的伪代码步骤
核心只有四步，非常简洁
第一步，初始化教师客户端
因为不需要教师模型的梯度传播
所以用采样客户端就行
Tinker API的好处是能够自动管理模型的引擎
不用我们操心资源的分配
第二步，采样学生轨迹
这一步和强化学习完全一样
让学生模型生成自己的轨迹
同时记录学生对每个token的对数概率
以便用来计算KL散度
第三步，计算奖励
把学生的轨迹传给教师客户端
调用compute_logprobs接口
得到教师对每个token的logprobs
然后用学生的logprobs减去教师的logprobs
得到反向KL散度
再把这个散度取负，就是学生的奖励
散度越小，奖励越高
第四步，用强化学习进行训练
将每个轨迹的advantages设置为反向KL散度取负
然后调用强化学习的importance sampling损失函数
来更新学生模型的参数
在实验环节中
研究团队用了两个核心场景
来验证了在线策略蒸馏的效果
第一个是数学推理
这是大语言模型的经典难点
也是最能体现反馈密集性价值的场景
实验设置是
学生模型采用Qwen3-8B-Base
教师模型采用Qwen3-32B
基准测试是AIME24
团队对比了三种后训练方法
分别是离线蒸馏、强化学习、在线策略蒸馏
先看离线蒸馏 的表现
团队先给学生模型用OpenThoughts-3数据集做SFT
400k样本后
学生在AIME24上的得分是60%。
要想达到70%，根据对数曲线外推
需要200万的样本
这意味着要收集或者生成大量教师轨迹
成本很高
而且，即使到200万样本
也不能保证性不会停滞
因为在很多SFT实验中
样本量超过一定阈值后
性能就不再涨了
再看强化学习的表现
参考Qwen3技术报告中的实验
在类似的初始化基础上
用强化学习训练达到67.6分
需要17920个GPU小时
即使我们按单卡每小时1美元算
也就是1.7万美元
还不算数据标注、人力成本
最后看在线策略蒸馏的表现
同样从400k SFT初始化开始
只用了150个训练步骤
就达到了70分，如果继续训练
能达到74.4分
而GPU小时数只用了1800
只有强化学习的1/10
为了更精准地对比成本
团队还计算了FLOPs
结果显示，200万样本离线蒸馏
学生FLOPs为1.5×10²¹
教师FLOPs为3.4×10²¹；
而7.7万样本的在线策略蒸馏
学生FLOPs为8.2×10¹⁹
教师FLOPs为8.4×10¹⁹
总的来说，计算效率提升了9到30倍
团队在实验中还发现
在线策略蒸馏对LoRA模型特别友好
LoRA的缺点是在大规模SFT中
性能不如全量微调
但是在用了在线策略蒸馏后
LoRA的差距从13分缩小到了6分
这意味着
我们可以用LoRA+在线策略蒸馏的组合
在轻量微调的前提下
达到接近全量微调的性能
进一步降低部署的成本
除了推理任务以外
在线策略蒸馏还有一个重要的应用场景
那就是个性化
比如企业内部的AI助手
既需要懂公司的内部知识
又需要会按照指令做事
但是这里有个经典的矛盾
那就是训练内部知识的时候
很容易让指令遵循能力退化
这就是研究团队要解决的第二个问题
团队以Qwen3-8B模型作为实验对象
这个模型已经经过强化学习的后训练
IF-eval得分85%，
但是内部QA得分只有18%。
团队的第一步是中期训练
给模型混合公司内部文档和聊天数据
微调模型
结果出现了预期的矛盾
当文档数据比例越高
内部QA的得分越高
但是IF-eval的得分越低
即使混合了30%的聊天数据
IF-eval得分也从85%降到了79%，
而且无论怎么调整比例
都无法恢复到原始的85%。
更麻烦的是，训练时间越长
IF-eval退化越严重
即使后期学习率衰减
也只能减缓退化，不能恢复
团队还试了LoRA微调
因为LoRA参数少
理论上能减少灾难性遗忘
但是结果还是不理想
LoRA确实让IF-eval退化少了一点
但是内部QA得分也低了
相当于为了不忘记旧的能力
牺牲了新能力
这显然不是我们想要的
这时候，在线策略蒸馏登场了
团队的思路是
用早期的Qwen3-8B模型作为教师
对中期训练后的模型做在线策略蒸馏
目标是恢复指令能力的同时
不丢失知识
蒸馏的过程很简单
让中期训练后的模型生成指令遵循类的轨迹
然后让早期Qwen3-8B教师逐个token的评分
最后更新模型参数
结果非常惊喜
内部QA得分从36%提升到41%，
IF-eval得分从79%恢复到了83%，
几乎完全恢复了指令能力
这个实验证明了在线策略蒸馏是一个持续学习的有效工具
在实际应用中
模型需要不断学习新的知识
但是又不能忘记旧的能力
传统方法要么学新忘旧
要么守旧不学新
而在线策略蒸馏通过固定教师模型
让学生在新的场景下学习
完美解决了这个矛盾
团队还提到，这种阶段交替
也就是先学新知识
再用在线策略蒸馏恢复旧能力的思路
还可以扩展到更复杂的持续学习场景
比如每个月给模型更新一次行业数据
然后用1-2天做在线策略蒸馏
就能让模型既懂新知识
又不丢旧能力
这对于企业级AI的落地至关重要
通过前面的实验
我们已经看到了在线策略蒸馏的优势
但是研究团队还揭示了4个更深层次的洞察
这些洞察能帮我们更好地理解大模型训练的本质
第一个洞察是
密集监督是提升计算效率的关键
强化学习和在线策略蒸馏都使用了反向KL散度学习
但是效率差了50-100倍
核心原因是奖励的密度
团队做了一个对比实验
用强化学习训练Qwen3-8B-Base模型解数学题
需要70个梯度步骤才能达到目标性能
而使用在线策略蒸馏
只需要7-10个梯度步骤，快了7-10倍
为什么呢？
因为强化学习每轮训练
只能传递1个bit的信息
而在线策略蒸馏每轮能传递N个bit的信息
这也印证了莱特曼（Lightman）等人2023年的结论
密集的过程监督要比稀疏的结果监督更加高效
第二个洞察是
在线策略蒸馏能够高效的复用训练数据
对于很多团队来说
收集大量训练数据是个难题
比如在医疗领域
高质量的病例推导数据很少
而在线策略蒸馏解决了这个问题
它能够反复使用同一条数据训练
而不会像强化学习那样死记硬背的记答案
团队做了一个极端实验
只用一道数学题
让模型反复训练了20个步骤
结果显示，即使只练这一道题
模型的AIME24得分也接近了教师模型的水平
这意味着
在线策略蒸馏不需要海量数据
只需要高质量的数据+反复训练
就能让模型学到通用能力
为什么能做到呢？
因为在线策略蒸馏是让模型去学过程
而过程是通用的
所以能迁移到其他的题目上
这对于数据稀缺的领域来说
是个巨大的突破
第三个洞察是
强化学习的本质是语义策略搜索
而蒸馏是直接学习最终的策略
很多人以为强化学习的核心是梯度更新
但是研究表明
强化学习的核心是搜索
它通过大量试错
寻找能够解决问题的语义策略
而在线策略蒸馏
相当于跳过搜索过程
直接学习强化学习找到的最终策略
打个比方
强化学习就像是科学家做研究一样
需要反复实验、试错
才能找到正确的理论；
而在线策略蒸馏就像老师教学生一样
直接把科学家找到的理论教给学生
不用学生再重复试错
这就是为什么在线策略蒸馏比强化学习快的原因
它省去了搜索策略的时间
直接学习最终的成果
当然，这不是说强化学习没有用
强化学习的价值在于找到新的策略
而在线策略蒸馏的价值在于高效传递已有的好策略
在实际应用中
我们可以用强化学习先给大模型找到好策略
再用在线策略蒸馏传递给小模型
实现大模型找策略
小模型高效落地的分工
第四个洞察是
在线策略蒸馏是持续学习的理想工具
前面我们提到
SFT自我蒸馏会导致性能退化
这是因为在实际训练中
每个批次的样本分布都会有微小差异
训练次数多了
模型就会偏离原始策略，导致退化
而在线策略蒸馏不会有这个问题
因为教师模型是固定的
无论学生训练多少轮
都是以同一个教师为目标，不会偏离
这比传统的Replay Buffer方法更加高效
因为不需要存储大量旧的数据
只需要保留教师模型就行
讲到这里
我们可以用三句话总结在线策略蒸馏的核心价值
一、性能上
结合了在线训练的场景相关性和离线蒸馏的反馈密集性
能让小模型在特定领域达到大模型级的性能
二、成本上
计算效率是强化学习的10倍以上
FLOPs节省9到30倍
还能复用数据、支持LoRA
大幅降低开发和部署成本
三、落地性上，基于现有工具和框架
步骤清晰，能够快速复用代码
还能解决持续学习、个性化等实际落地难题
对普通开发者和企业来说
在线策略蒸馏的意味着
它让我们不用再依赖百亿参数的大模型
也不用承担强化学习的高额成本
只用中小模型+在线策略蒸馏的方式
就能在自己的领域做出高性能的AI应用
当然，在线策略蒸馏也不是完美的
还有很多值得探索的方向
比如如何让教师模型的评分更精准
如何在多任务场景下应用
以及如何进一步降低教师模型的依赖
这些都是未来研究的重点方向
在文章最后
Thinking Machines Labs提到
他们的使命是用AI模型来赋能人
结合前沿性能、适配性和个性化
而在线策略蒸馏
可以说正是这个使命的体现
好了，以上就是本期视频的内容
欢迎大家在评论区留言
感谢观看，我们下期再见
