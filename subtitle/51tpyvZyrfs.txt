大家好，这里是最佳拍档，我是大飞
8月12日
Google 的开发负责人洛根·基尔帕特里克（Logan Kilpatrick）
和 DeepMind的CEO德米斯·哈萨比斯（Demis Hassabis）进行了一场对谈
整个访谈信息量很大
不仅聊到了 AGI 的进展
AI 在未来需要具备的能力
也重点提到了DeepMind刚刚发布的 Genie 3
这是一个 100% 可控的、实时的 AI 世界引擎
相信很多人已经看过它的视频 demo了
也许有的人会觉得
这不过就是一个高端的视频生成器罢了
但是实际上，Genie 3 背后代表的
是一个其实最值得被大家认真讨论的概念
那就是世界模型，World Model
今天，我们就来借着这次访谈的内容
聊一个可能之前很少想过的问题
如果一个 AI 不理解这个世界
它还能算是智能的吗？
在回答这个问题之前
我们需要先来真正的认识一下 Genie 系列模型
早在 2024 年
DeepMind 就发布了第一代的Genie模型
它当时的主打口号是
用视频训练 AI 来理解世界
那时候它的能力还很有限
只能根据用户输入的图像或者语义
生成十几秒钟的视频片段
质量也比较粗糙
不仅帧率低、画面模糊
有时候人物的动作还会扭曲成一团
和我们最早用 Midjourney 做图、用 Runway 做视频的那种
差不了多少
但是DeepMind把自己的野心藏得很深
实际上
他们是想拿这些视频当教材
让 AI 从中学会物理规律、空间动态和因果关系
就像小孩通过看动画片
就能学会“水是会流的”、“人摔倒了要站起身”这些物理概念和常识
于是，到了 Genie 2的时候
它已经能够生成更为连贯的 3D 环境了
比如一个人在屋子里走路、滑雪、翻滚等等
但是当时的互动还很有限
大多数时候你只能看着 AI去表演一小段短片
你既不能插手干预
也不能下命令去控制它
相当于它就是个自动播放的窗口而已
而且，Genie 2的记忆是断裂的
如果你刚刚看到了一个红色的滑雪板
下一帧它可能就直接变成了绿色
又或者一个物体刚刚出现在你左边
过了两秒它可能就凭空消失了
所以从实用性上讲
Genie 1 和 Genie 2 更像是一种概念证明
类似于我们能够用视频来教会 AI去梦见一个世界
但是它还无法维持住梦的连贯性
但是到了这次的Genie 3
情况被彻底改变了
它不但将画面的清晰度提升到了 720p
还能够稳定地以 24FPS 的速率
实时生成画面
这是什么概念呢？
就是说你在 AI 生成的世界里
每走一步
它都能够立刻刷新你眼前的场景
没有任何卡顿
就像是你在玩一款开放世界游戏一样
但是关键在于
这完全是由 AI 实时创造的
不仅如此
Genie 3 还首次引入了“提示式世界事件”（Promptable World Events）的机制
你不光是可以走、可以看
你还可以实时地给它下剧情指令
从控制的方式来看
Genie 3 也不再只是看看视频这么简单
而是真正支持了第一人称视角的导航和实时互动
让你可以体验在虚拟世界中的生活
比如，你想把一条山林小径
变成 AI 的世界吗？
只需要给它一句提示词，比如
在冰川湖畔奔跑
穿行于林中分岔小径
跨越流淌的山间溪流
坐落在美丽的白雪皑皑的群山和松林之间
丰富的野生动物使旅程充满乐趣
Genie 3 会立刻为你生成这样的一个环境
而且你可以进到这个场景里
看到水流是怎么绕着石头走的、鸟是怎么飞的、天上的阳光是怎么洒下来的
再比如说
我们想生成一个飓风的现场
Genie 3 就会创建出一个可交互的三维环境
你可以置身其中，用第一人称视角
看着海浪一波波的打到公路上
棕榈树在风中剧烈的摇摆
所有的一切都是如此的真实
而且所有细节都保持了逻辑和物理上的一致性
这也解释了为什么 哈萨比斯在访谈里会多次的强调
Genie 是我们在模拟世界理解这件事情上
迈出的最重要的一步
也是 DeepMind 从 AlphaGo 一路走来
最想实现的梦想之一
换句话说，能够生成一个世界
这本身就是对 AI 是否理解世界的最好测试
也许，看到这里
你可能会有个疑问
为什么像GPT 的大语言模型
更新一代比一代快
各家公司的产品也越来越多
而世界模型似乎从来没有走进大众的视野
始终只有几家公司在孤独地推进呢？
要理解这个问题
我们不妨先问问自己
训练一个 AI 学会说话
和让它理解世界，哪个更难呢？
应该说，大语言模型这些年突飞猛进
很大一部分，是因为它自带外挂
一是数据多，二是成本低
互联网上几十TB 的文本语料
你可以随便爬
像公众号文章、小说、维基百科、知乎、Reddit等等
人类已经用语言把世界描绘得密密麻麻
更核心的点在于
语言本质上其实就是一维序列
一句话接着一句话
所以训练方式也很直接
预测下一个词就行
可以说是成本低、效率高
但是对于世界模型来说
它要做是预测下一个世界
首先面对的就是数据问题
模型需要训练的是视频 + 物理 + 因果的数据
哪有那么多现成的呢？
而想要训练 AI理解世界
单纯只依靠文本基本没戏
它需要的是图像、视频、动作轨迹、物理动态、空间结构、因果链条
等等等等，这些信息不但数据量大
而且复杂度高
比如视频的一帧高清图像
就相当于几万个 token
一段视频可能就是上百万个token
而且它还涉及到时序、空间一致性、甚至角色之间的交互与反馈
所以得把世界一帧一帧的生成清楚
关键是，这些数据从哪来呢？
它不像大语言模型那样能爬网页
基本上都得靠自己来造数据
比如
DeepMind选择用游戏Minecraft来自己合成环境
Meta用机器人来采集第一人称视频
而英伟达的 Cosmos 模型背后
是千万个小时的车载视频+LiDAR+深度图+边缘图+多模态标签等等数据
哪怕我们真搞到了数据
还得经历拆分、去噪、标注、去重、分词、空间结构、跨模态对齐、token 压缩等一系列的过程
英伟达就曾经提到
哪怕只训练一个能生成5秒钟720p分辨率的视频模型
也要 PB 级的视频 + 百万美元级别 的GPU资源才行
这个门槛对于初创公司来说
基本上都很难够得到
其次
世界模型还面临着算法方面的挑战
大语言模型的任务是生成所谓合理的句子
哪怕它胡说八道点，只要读起来通顺
你可能都很难发现
但是世界模型不一样
它必须做到因果成立、物理合理、空间连续
比如，一个杯子从桌上掉下去
不能下一秒就消失了，一辆车拐了弯
必须保持同一个方向
不能突然就漂移上天了
一个角色说我要出门
不能下一秒就直接出现在山顶了
也就是说，世界模型不仅要生成内容
还得维持这个世界的逻辑闭环
而要做到这点
模型内部得构建一个完整的“世界模拟器”，
能够预测结果、想象未来、评估路径、对未知场景做出合理的回应
这背后的计算复杂度
相比大语言模型来说几乎是指数级的上升
以Dreamer V3算法为例
为了让 AI 在脑海中模拟 Minecraft 的场景
它不得不在每一帧里
预测图像、奖励、是否终止、行为反馈
每一项都连着下一步
一步出错就会满盘皆错
另外，大语言模型的进展
很大程度上得益于 Transformer 架构和算力的加持
上下文窗口越来越大
模型的参数规模也是越来越大
但是世界模型呢
很难纯粹靠堆算力就能解决问题
因为它面临的是一些更复杂的问题
比如它既要看图像，又要预测运动
既要记住过去
还要能够推演未来，既要生成细节
又要逻辑连贯
甚至还得考虑从动作到反馈再到后果的因果链条
所以
不同公司都在尝试自己的混合架构
比如DeepMind 的 DreamerV3 用的是循环状态空间模型RSSM
英伟达Cosmos-Reason1 用的是 Mamba + MLP + Transformer 的混合体
Meta 的 NWM 用的是 CDiT
一种能够减少 FLOPs 的条件扩散网络
如果说GPT 的成功来自于
把全人类写下来的语言压缩成一个预测器
那么世界模型想要成功，就得靠构建
从视觉、动作、因果等等要素中
构建出一个有逻辑可遵循的小宇宙
如果前者可以看做是抄书
那么后者就是要自己去原创一本小说
哪个更难，应该一目了然
那么，既然世界模型这么难做
为什么还要去做呢？
因为它是我们离 AGI 最近的一条路
甚至可以说，如果没有世界模型
就不可能诞生真正意义上的通用人工智能
我们人类认知的根基，从来不是语言
而是经验
语言只是我们记录世界的方式
而不是感知世界的方式
德国计算机科学家于尔根·施密德胡伯（Jürgen Schmidhuber ）很早就指出
一个具身智能体如果想进行有效的学习
就必须在脑海中构建出环境的“内部模型”，
也就是所谓的 world model
借助它
智能体才可以在没有真实交互成本的情况下
在想象中进行“行动、反馈、更新”的闭环
从而像人类一样
在梦中学习、在梦中试错、在梦中总结出通用的策略
这个观点
在 2018 年的《World Models》论文中首次被系统性的验证
研究人员训练了一个生成式 RNN网络去模拟游戏场景
然后在这个模拟出来的世界中训练控制策略
最后，这个只在“梦中”练习过的策略
居然可以在真实的游戏环境中直接上场
完成任务
图灵奖得主
Meta的首席科学家杨立昆（Yann LeCun）也 同样把世界模型放在核心的地位
他曾经公开强调，没有对世界的建模
AI 就无法进行真正的推理
他提出的 JEPA（Joint Embedding Predictive Architecture）模型
尝试跳出像素层面的建模
转向预测隐藏状态的抽象表示
强调的是模型预测未来潜在表征的能力
而非逐个像素的生成
这种思路与人类的认知极为相似
因为我们就并不是通过逐帧还原画面的
而是基于抽象模型来推测世界会如何演化的
如今，Genie 3 也继承了这种观念
它可以预测某个动作将如何影响场景、可以回忆之前帧的状态
来确保逻辑上的一致性
而这些能力正是世界模型的核心
大语言模型虽然能够生成条理清晰的文字
但是终究只能在“语言的世界”里活动
它们对重力、摩擦、遮挡、空间关系等方面的知识
都是靠语言的语料“猜”出来的
而世界模型的目标
则是希望让 AI 在脑海中
建立一个物理上可信的现实模型
比如
GPT可以告诉你“骑车要掌握平衡”，
“拐弯要减速”，
但是它自己从来没有骑过
而在沙盒世界里骑了上千个小时虚拟自行车的 Genie 3
即使它写不出一句“优雅”的文本
也能够精准地避开障碍、掌握重心
并且实时的调整策略
显然，我们期待的AGI更应该是后者
如果说
大语言模型是大脑的“逻辑中枢”，
那么世界模型就是 AI 的“运动皮层”与“感觉神经”。
没有这些组成部分
AGI 只能是停留在我们的嘴皮子上
而且
世界模型也不只是为了建立起一个“看起来像是世界”的模拟器
而是为了给智能体提供行为试错的空间
甚至于，它是智能体意识的投影空间
是规划与预演的底座
更是让智能体能够脱离人类提示、自主做出策略选择的重要前提
即使大语言模型能够制定计划
但是它无法验证计划是否可行
但是世界模型可以在脑海中尝试运行每个计划的分支
从中挑出最优的路径
这也就是为什么所有追求AGI的研究团队
最终都会走到世界模型这条路上来的原因
所以呢
如果我们回到那个最开始的问题
一个 AI 如果不理解这个世界
它还能算是智能的吗？
Genie 3 给出的答案是，不能
至少，不能算是真正的智能
当然
我们可以让大语言模型去模仿一个“聪明人”的样子
让它能考满分、能讲道理、能把文章写得头头是道
但是它的“聪明”，
就像是一个从来没有出过门、靠听别人的描述来理解世界的孩子一样
你和它说“地震来了”，
它想象的是文字
而不是晃动的地面；
你说“风吹过树叶”，
它浮现的是一组词语
而不是沙沙的响声
回想人类自己
我们并不是因为能够说话
才变得足够聪明
而是因为从小就摔过跤、踩过坑、吹过风
淋过雨
在这些一次次的身体记忆中
我们才逐渐理解了这个世界
也才有后来的思考与表达
所以，真正的智能
必须先从感知这个世界开始
如果再往更长远的未来看
也许就像哈萨比斯在访谈中说的
Genie、Veo、Gemini 这些目前相对独立的模型
注定将逐渐的走向融合
形成所谓的“全能模型”（Omni Model）
它既能处理语言、多媒体
又能进行物理推理和内容生成
这才是通往AGI的终极之路
好了，今天的内容就到这里
大家是如何看待Genie 3的发布和世界模型的发展呢
欢迎在评论区留言
感谢收看本期视频，我们下期再见
