大家好，这里是最佳拍档，我是大飞
昨天，也就是北京时间8月8日的晚上
黄仁勋又一次身着标志性的黑皮衣
再度站在了SIGGRAPH的舞台发表演讲
扔下多枚重磅核弹
我们先对这次发布的内容做个总结
在大约1小时20分钟的演讲中
英伟达发布新一代GH200 Grace Hopper平台
这个平台依托于搭载了全球首款HBM3e处理器的新型Grace Hopper超级芯片
专门为加速计算和生成式AI时代而打造
老黄生动地将GH200称为“世界上最快的内存”。
这是因为数据中心想要满足生成式AI不断增长的需求
需要有针对特殊需求的加速计算平台
而新的GH200 Grace Hopper超级芯片平台
提供了卓越的内存技术和带宽
不仅提高吞吐量
而且提升无损耗连接GPU聚合性能的能力
以及拥有可以在整个数据中心轻松部署的服务器设计
此外，英伟达表示
这款名为GH200的超级芯片将于2024年第二季度投产
毫无疑问
GH200超级芯片的发布有望进一步提升英伟达在AI芯片领域的霸主地位
自从2022年底ChatGPT上市以来
全球AI大模型开发热情暴涨
进而让英伟达的芯片变得供不应求
有数据证实
全球95%以上的大模型都使用英伟达的GPU芯片
另外我们前两天也做了一期视频
各大公司对GPU的需求约43万张H100
比上一代模型飙涨了近10倍
因此
老黄也将英伟达自比为全球AI发动机
并称正在努力为所有客户提供服务
同时，他还提到
在AI时代
英伟达的技术可以替代传统数据中心
投资800万美元的新技术
可以取代用旧设备建造的1亿美元设施
而且用电量可以减少20倍
老黄说
这就是数据中心在向加速计算转变的原因
你买得越多，越省钱
此外，在昨晚的活动上
英伟达还发布了新的统一工具包AI Workbench
对英伟达Omniverse软件服务的重大升级
以及一款新的工作站产品
最多可以塞进去四张NVIDIA RTX 6000显卡
对应的参数为5
828 TFLOPS AI算力和192GB显存
在这里，老黄直言像这样的工作站
用来开发大模型
几个月就能把本金赚回来了
总而言之
英伟达拿出一系列令人惊叹的技术和产品
让全世界在一起见识
到AI芯片霸主是如何引领世界AI和图形计算的新浪潮
好了
接下来我们就来详细拆解一下昨天演讲的内容
首先，老黄甩出最强生成式AI处理器
正式推出面向加速计算和生成式AI的新一代NVIDIA GH200 Grace Hopper超级芯片
简称GH200
GH200由72核Grace CPU和4PFLOPS Hopper GPU组成
在全球最快内存HBM3e的“助攻”下
内存容量高达141GB
可以提供每秒5TB的带宽
它每个GPU的容量达到NVIDIA H100 GPU的1.7倍
带宽达到H100的1.55倍
这个超级芯片可以用于任何大型语言模型
降低推理成本
与当前一代产品相比
新的双GH200系统
共有144个Grace CPU核心、8PFLOPS计算性能的GPU、282GB HBM3e内存
内存容量达到3.5倍，带宽达3倍
如果将连接到CPU的LPDDR内存包括在内
那么总共集成了1.2TB超快内存
GH200将在新的服务器设计中提供
黄仁勋还放出了一段动画视频
展示了组装面向生成式AI时代的Grace Hopper AI超级计算机的完整过程
首先是一块Grace Hopper
用高速互连的CPU-GPU Link将CPU和GPU“粘”在一起
通信速率比PCIe Gen5快7倍
一个Grace Hopper机架装上NVIDIA BlueField-3和ConnectX-7网卡、8通道4.6TB高速内存
用NVLink Switch实现GPU之间的高速通信
再加上NVLink Cable Cartridge
组成了NVIDA DGX GH200
NVIDA DGX GH200由16个Grace Hopper机架
通过NVLink Switch系统连成集群
能让256块GPU组成的系统像一块巨型GPU一样工作
由256块GH200组成的NVIDIA DGX GH200 SuperPod
拥有高达1EFLOPS的算力和144TB高速内存
NVIDIA Quantum-2 InfiniBand Switch可以使用高速、低延时的网络
连接多个DGXSuperPod
进而搭建出面向生成式AI时代的Grace Hopper AI超级计算机
这带来的主要优势是
实现同等算力的情况下
用更少卡、省更多电、花更少钱
过去
1亿美元能买8800块x86 CPU组成的数据中心
功耗是5MW
如今
1亿美元能买2500块GH200组成的Iso-Budget数据中心
功耗是3MW
AI推理性能达到之前CPU系统的12倍
能效达20倍
如果达到跟x86 CPU数据中心相同的AI推理性能
Iso-Troughput数据中心只需用到210块GH200
功耗是0.26MW
成本只有CPU数据中心的1/12
仅800万美元
此外，老黄还表示
新的GH200 Grace Hopper超级芯片平台
提供了卓越的内存技术和带宽
来提高吞吐量
能够连接GPU并毫无妥协地聚合性能
以及可以轻松部署在整个数据中心的服务器设计
新的GH200将于明年第二季度投产
其次
桌面AI工作站GPU系列一口气推出了4款新品
分别是RTX 6000、RTX 5000、RTX 4500和RTX 4000
如果H100以及配套的产品线展示的是英伟达GPU性能天花板的话
针对桌面和数据中心推出的这几款产品
则是老黄对成本敏感客户秀出的绝佳刀法
作为旗舰级专业卡
RTX 6000的性能参数毫无疑问是4款新品中最强的
凭借着48GB的显存，18176个CUDA核心
568个Tensor核心，142个RT核心
和高达960GB/s的带宽
它可谓是一骑绝尘
RTX 5000配备了32GB显存
12800个CUDA核心
400个Tensor核心，100个RT核心
RTX 4500配备了24GB显存
7680个CUDA核心
240个Tensor核心，60个RT核心
RTX 4000配备了20GB显存
6144个CUDA核心
192个Tensor核心，48个RT核心
基于新发布的4张新的GPU
针对企业客户
英伟达还准备一套一站式解决方案
RTX Workstation
它支持最多4张RTX 6000 GPU
可以在15小时内完成8.6亿token的GPT3-40B的微调
还能让Stable Diffusion XL每分钟生成40张图片
比4090快5倍
接下来是搭载了L40S的OVX服务器
基于Ada Lovelace架构的L40S
配备有48GB的GDDR6显存和846GB/s的带宽
在第四代Tensor核心和FP8 Transformer引擎的加持下
可以提供超过1.45 petaflops的张量处理能力
对于算力要求较高的任务，L40S的18
176个CUDA核心
可以提供近5倍于A100的单精度浮点FP32的性能
从而加速复杂计算和数据密集型分析
此外
为了支持如实时渲染、产品设计和3D内容创建等专业视觉处理工作
英伟达还为L40S配备了142个第三代RT核心
可以提供212 teraflops的光线追踪性能
对于具有数十亿参数和多种模态的生成式AI工作负载
L40S相较于老前辈A100
可实现高达1.2倍的推理性能提升
以及高达1.7倍的训练性能提升
针对数据中心市场
英伟达推出了最多可搭载8张L40S的OVX服务器
以拥有8.6亿token的GPT3-40B模型为例
OVX服务器只需7个小时就能完成微调
而对于Stable Diffusion XL模型
则可实现每分钟80张的图像生成
除了上述硬件产品外
老黄还分享了3个关于优化生成式AI流程的新发布
这些将有助于加速行业采用基于大型语言模型的生成式AI
一是NVIDIA和全球最大AI开源社区Hugging Face建立合作
二是推出NVIDIA AI Enterprise 4.0
把DGX Cloud中的所有功能放到NVIDIA AI Enterprise软件中
三是推出NVIDIA AI Workbench
将生成式AI工作所需的一切打包在一起
只用点击一下就能将这个项目移动到任何终端设备或云端
接下来我们详细说一下这三点
首先第一点与Hugging Face的合作
老黄宣布
NVIDIA和Hugging Face已经建立了合作伙伴关系
将为构建大型语言模型和其他高级AI应用程序的开发人员
提供生成式AI超级计算
开发人员可以访问Hugging Face平台内的NVIDIA DGX Cloud AI超级计算
来训练和调优先进的AI模型
他们将有一个非常简单的界面来推进工作
无需担心训练的复杂性
因为这些都会由DGX Cloud处理
DGX Cloud的每个实例有8个NVIDIA H100或A100 80GB Tensor Core GPU
每个节点的GPU内存总计640GB
DGX Cloud包含来自NVIDIA专家的支持
可以帮助客户优化其模型并快速解决开发挑战
作为合作的一部分
Hugging Face将推出一个新的服务
名为“训练集群即服务（Training Cluster as a Service）”，
来简化为企业创建新的和自定义的生成式AI模型
这个服务由NVIDIA DGX Cloud提供支持
将在未来几个月内推出
第二点
推出的另一款新品NVIDIA AI Workbench
是一个统一、易用的工作空间
能让开发人员随处构建或运行自己的生成式AI模型
开发者可以很方便地将所有必要的企业级模型、框架、SDK和库
从开源代码库和NVIDIA AI平台打包到这个统一的开发者工作空间中
然后只需点击几下鼠标
就能将自己的AI项目从一个位置移动到另一个位置
这样就能在个人电脑、笔记本电脑或工作站上
快速创建、测试和定制预训练的生成式AI模型
并在需要时将其扩展到数据中心、公有云或NVIDIA DGX Cloud
总的来说
AI Workbench为跨组织团队创建基于AI的应用程序提供了简化的途径
通过在本地系统上运行的简化的界面访问
让开发人员能使用自定义数据
从主流的代码库
比如Hugging Face、GitHub和NVIDIA NGC中
定制模型
并且能够轻松跨多平台共享
戴尔、惠普、Lambda、联想、超微等AI基础设施供应商正采用AI Workbench
来增强自己最新一代多GPU桌面工作站、高端移动工作站和虚拟工作站的能力
第三点
最新版的企业软件平台NVIDIA AI enterprise 4.0
可提供生产就绪型生成式AI工具
并提供了可靠的生产部署所需的安全性和API稳定性
NVIDIA AI Enterprise 4.0新支持的软件和工具有助于简化生成式AI部署
其中一大亮点是引入用于构建、定制和部署大型语言模型的云原生框架NVIDIA NeMo
其他工具还包括NVIDIA Triton管理服务
它可以通过模型编排实现可扩展AI高效运行、NVIDIA Base Command Manager Essentials集群管理软件
可以帮助企业在数据中心、多云和混合云环境中
最大限度提高AI服务器性能和利用率等等
NVIDIA AI Enterprise软件支持用户跨云、数据中心和边缘构建和运行支持NVIDIA AI的解决方案
经认证可在主流的NVIDIA认证系统、NVIDIA DGX系统、所有主要云平台和新发布的NVIDIA RTX上运行工作站
最新版本的企业软件平台
也将会集成到谷歌云、微软Azure、Oracle云基础设施等NVIDIA的合作伙伴市场中
最后一个大的更新
就是Omniverse平台的升级了
Omniverse是Nvidia创建的实时3D图形协作平台
主要的功能就是创建“数字孪生”，
在虚拟世界中去模拟现实世界
在演讲中
老黄也拿出了世界最大的广告公司WPP
和比亚迪腾势汽车
作为Omniverse云和生成式AI的使用案例
WPP通过Omniverse为腾势N7
打造了一个实时的
囊括各种外观配置的汽车数字孪生
并且能够通过Adobe等提供的AIGC功能
修改宣传材料的图像背景
而这次推出的升级版omniverse平台
是一个基于高性能3D场景描述技术
openUSD的原软件平台
用于跨3D工具和应用的连接
描述和模拟
能够加快创建
虚拟世界
和工业数字化的高级工作流程
Cesium、Convai
Move AI、SideFX Houdini 和 Wonder Dynamics
现在都已经通过openUSD
连接到了Omniverse
新平台的亮点还包括
更新了开发原生openUSD应用
以及扩展的引擎Omniverse kit
以及更新了英伟达Omniverse Audio2Face
基础应用和空间计算功能等等
更新后的omniverse让开发人员可以通过
openUSD利用生成式AI来强化他们的工具
并且让工业企业能够构建更大
更复杂的世界级模拟
作为其工业应用的数字测试厂
好了
以上就是本次老黄在SIGGRAPH大会上
分享的主要内容
通过发布了一系列对开发者
以及企业极具吸引力的软硬件新品
再度验证了NVIDIA对自己在新时代的定义
NVIDIA是一家平台公司
由于时间关系
没有办法介绍这次分享的所有细节
建议大家有时间可以去看一下原视频
相信会感受到更多的核弹冲击
感谢大家观看本期节目
我们下期再见
