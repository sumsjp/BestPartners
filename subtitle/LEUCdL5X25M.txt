大家好，这里是最佳拍档，我是大飞
上周发布的llama3.1余温尚存
Meta抓住机会
来了一波趁热打铁
30日凌晨，Meta在SAM的基础之上
对一代的架构、功能以及准确率等方面
进行了大量的更新
并且正式开源了SAM-2
这次在Sam一代分割图形能力的基础上
SAM 2 的能力进一步得到升级
无论是视频也好图像也好
SAM2全都通通能够实时的分割
可以说朝着分割一切Segment Anything的目标更近了一步
今天大飞就来带大家了解一下
这款最新的图形分割模型
对于不太熟悉SAM模型的朋友
听到分割这个词
可能会有点蒙
大模型发展那么快
已经可以拿刀切东西了？
实际上
分割是一个图形视觉专有名词
对象分割（Object segmentation）的简称
指的是计算机去识别图像中与感兴趣的物体相对应的像素
用大白话说就是抠图
大伙要是用PS扣过图，就应该知道
在不用辅助工具的情况下
把图片扣好那还是挺麻烦的
你得一个像素一个像素地把需要的图案从图片里切割出来
在那个PS还不成熟的2000年
抠图当真是件苦差事
但是在二十三年后的2023年
Meta的SAM就可以做到
就可以一次性识别任何类型图片中的所有对象
然后轻松切割每个像素
而今天的SAM-2在性能上更进一步
不只是图像，视频也可以分割了
而且不仅是任何视频或图像中的任何对象
甚至是它以前没有见过的对象和视觉域
从而可以支持各种不同的使用场景
无需去自定义适配
在SAM 2的官方介绍中
Meta 多次强调了SAM 2 是首个可以用于实时、可提示的、图像和视频对象分割的统一模型
它让视频的分割体验发生了重大变化
并且可以在图像和视频应用程序中无缝使用
SAM 2 在图像分割准确率方面
超越了之前的版本
并且实现了比现有更好的视频分割性能
所需要的交互时间仅为原来的1/3
与此同时，SAM 2模型的架构
采用了创新的流式内存（streaming memory）设计
使得模型能够按顺序来处理视频帧
用大白话说的话，那就是新的SAM2
剪视频是又快又准
只要你有个视频
能够正常地按照时间轴播放
SAM2就能在很短时间
精准地抠出你在视频里需要的对象图像
并且保证这个图像还和以前一样流畅的播放
不会像现在市面上各种剪辑软件中的自动抠图
要么抠不干净
要么识别的物体不对
SAM2甚至还可以实时追踪所有镜头
在SAM 2的演示预览中
AI可以轻松将一个滑板的人和背景图像分割
然后再跟踪视频中被选择的对象
添加用户需要的特效
在视频中可以看到
baseline完全没法准确地分割并且跟踪用户选定的内容
而相比之下
SAM 2 能够在整个视频中
准确的跟踪用户选定的对象部分
除了最直观的视频分割演示
SAM 2 论文也展示了模型在数据上的多项提升
SAM 2 在 17 个零样本视频数据集的交互式视频分割方面
表现明显优于以前的方法
并且所需要的人机交互
减少了大约三倍
相较于自家的SAM一代
SAM 2 在 23 个数据集零样本基准测试套件上的表现
都更加优秀
视频处理速度快了整整六倍
与之前的最先进模型相比
SAM 2 在现有的视频对象分割基准
包括DAVIS、MOSE、LVOS、YouTube-VOS上
也都表现出色
在各项数据上都甩开了前辈们十到二十分的差距
当然
更强大的性能也带来了更高的设备需求
就如同一般用户跑不动的llama 3.1405b 一样
SAM2对算力的要求也是非常之高
普通人就别想着本地部署了
只有像 Meta 这样能提供强大硬件资源的巨头才能运行
但是这种进步还是说明了一些问题
一年前，这种快速、灵活的分割
几乎是不可能的
但是现在
SAM 2 可以在不借助数据中心的情况下运行
证明了整个行业在计算效率方面的进步很大
在短短一年的时间内
模型就从图像分割迈进了视频分割
性能的进步也令人惊讶
那么如此短的时间内
Meta究竟对SAM做了什么呢？
其实，打一开始Meta 就认为
通用的分割模型应该可以同时适用于图像和视频
第一代SAM只能算是个半成品
在Meta的研究人员看来
图像可以被视为具有单帧的、非常短的视频
它与处理视频的唯一区别是
模型需要依靠内存
来调用这个视频之前处理过的信息
以便在当前时间
进一步准确地分割对象
SAM 虽然能够了解图像中对象的一般概念
然而
图像只是动态现实世界的静态快照
许多重要的现实场景
都需要在视频数据中进行准确的对象分割
比方说混合现实、机器人、自动驾驶和视频编辑
而视频中对象的成功分割
需要了解实体在空间和时间上的位置
与图像分割相比
视频提出了重大的新挑战
对象运动、变形、遮挡、光照变化和其他因素
可能会因帧而异
由于摄像机运动、模糊和分辨率较低
视频的质量通常会低于图像
这就更加增加了难度
因此，现有的视频分割模型和数据集
在为视频提供分割任何内容的功能方面
存在不足
而为了解决这个问题
Meta的研究团队首先开发了可提示的（promptable）视觉分割任务
将图像分割任务推广到视频领域
并且设计了一个能够执行这个任务的模型
也就是SAM 2
SAM一代是以图像中的输入点、框或者掩码
来定义目标对象
并预测分割掩码的
而SAM 2 则可以在视频的任何帧中
根据输入提示
来预测当前帧的时空掩码
也就是「masklet」，
一旦预测出初始的
就可以在任何帧中
通过提供附加提示的方式来进行迭代完善
这个过程可以根据需要重复多次
直到获得所需的掩码
通过这个简单的循环
SAM 2 可以做到通过点击边界框或者给出提示
来定义给定帧中对象的范围
轻量级掩码解码器会采用当前帧的图像嵌入和编码提示
来输出当前帧的分割掩码
而在视频设置中
SAM 2 会将这个掩码预测
传播到所有视频帧来生成掩码
然后在任何后续帧上
迭代式的来添加提示
从而细化掩码预测
为了能够准确预测所有视频帧的掩码
研究团队还引入了一种由记忆编码器、记忆库（memory bank）和记忆注意力模块组成的记忆机制
当SAM 2应用于图像的时候
内存组件为空
模型的行为类似于 SAM
而对于视频
记忆组件能够存储会话中的对象和先前用户交互的信息
从而允许 SAM 2 在整个视频中生成掩码预测
如果在其他帧上提供了额外的提示
SAM 2 还可以根据对象存储的记忆上下文
有效地纠正掩码预测
在SAM 2中，Meta 采用了流式架构
一次处理一个视频帧
并将有关分割对象的信息存储在记忆中
而视频帧的记忆
是由记忆编码器根据当前掩码预测所创建的
并放置在记忆库中用于分割后续的帧
记忆库由先前帧和提示帧的记忆组成
记忆注意力会从图像编码器获取每帧的嵌入信息
并且根据记忆库进行调整来产生嵌入信息
然后将它传递给掩码解码器
来生成这个帧的掩码预测
对于所有后续的视频帧会重复这个操作
这种设计允许SAM2可以实时处理任意时长的视频
在SAM一代中
当图像被分割的对象存在模糊性的时候
SAM 会输出多个有效掩码
比方说
当一个人点击自行车轮胎的时候
模型可以将这次点击
解释为仅指轮胎
或者是指整个自行车
并且输出多个预测
而在视频中
这种模糊性可能会被扩展到视频帧中
比方说，如果在一帧中只有轮胎可见
那么轮胎上的点击可能就只与轮胎相关
或者随着自行车的更大部分在后续帧中变得可见
这种点击可能又会变为针对整个自行车的
为了处理这种模糊性
SAM 2 会为视频的每个帧创建多个掩码
如果进一步的提示不能解决歧义
模型会选择置信度最高的掩码
以便在视频中进一步的进行传播
当然了，光有基础框架还不够
想要做出史无前例的视频分割模型
Meta还需要更好地训练数据集
现有的视频分割数据集
通常都存在着一些限制
比如
注释对象主要集中在人、车辆和动物等特定的类别上
并且往往只覆盖整个对象
而忽略了对象的部分和子部分
此外，这些数据集的规模相对较小
无法满足训练强大的视频分割模型的需求
为了解决这些难题
Meta开发了SA–V数据集
并且在数据集中使用了所谓的三大阶段
在第一阶段
Meta使用SAM模型来辅助人类标注
标注者的任务是在视频的每帧中
以每秒6帧的速度用SAM和像素精准的手动编辑工具
来标注目标对象的掩码
由于这是一种逐帧的方法
所有帧都需要从头开始标注掩码
因此流程非常缓慢
平均标注时间为每帧37.8秒
但是这种方法能够产生高质量的空间标注
在这个阶段
Meta一共共收集了16000个掩码片段
涵盖了1400个视频
第二阶段，引入了SAM 2 Mask
它只接受掩码作为提示
标注者首先使用SAM和其他工具
在第一帧中生成空间掩码
然后使用SAM 2 Mask
将标注的掩码在时间上传播到其他帧
从而获得完整的时空掩码片段
通过这个阶段的工作，Meta收集了635
000个掩码片段
标注时间下降到每帧7.4秒
相比第一阶段有了显著的提高
速度提升了约5.1倍
第三阶段，使用了完全功能的SAM-2
它能够接受各种类型的提示
包括点和掩码
与前两个阶段不同
SAM-2受益于对象在时间维度上的记忆
来生成掩码预测
这意味着标注者只需要偶尔对SAM 2提供的预测掩码
进行细化点击
就能够在中间帧中编辑预测的掩码片段
而不需要第一阶段那样从头开始标注
通过多次重新训练和更新SAM-2
标注时间进一步下降到了每帧4.5秒
相比第一阶段速度提升了约8.4倍
所以
SA–V数据集在开发SAM-2过程中发挥了重要作用
也是目前最大的视觉分割训练数据集之一
以上，就是有关SAM2的基本介绍了
模型现在已经被发布到了Meta自家官网和GitHub上
除了模型以外
Meta 也根据 CC BY 4.0 许可发布了 SA-V数据集
包括大约 51 000 个真实世界的视频和超过 600 000 个 掩码标准
比现在最大的视频分割数据集
视频数量多 4.5 倍
标注多 53 倍
在 SAM 2 的论文中
还提到了另一个包含了超过 100 000 个训练用的视频数据集
但是没有公开
与 SAM 一样
SAM 2 也会开源并且免费使用
以及在 Amazon SageMaker 等平台上的托管
为了履行对开源 AI 的承诺
Meta 采用了宽松的 Apache 2.0 协议
共享了SAM 2的代码和模型权重
并根据 BSD-3 许可共享了 SAM 2 的评估代码
估计不少朋友已经想要去尝尝鲜了
但是这里大飞也要提醒一下
SAM2的功能虽然强悍
但是依然有缺点
根据Meta自己的官网介绍
SAM 2 可能会在摄像机视角发生剧烈变化、长时间遮挡、拥挤的场景
或者较长的视频中失去对象的追踪
与此同时
当目标对象只在一帧中指定的时候
SAM 2 有时会混淆对象
无法正确的分割目标
如同视频中的马匹所示
在许多情况下
通过在未来帧中进行额外的细化提示
这个问题可以完全解决
并且在整个视频中获得正确的掩码
除了这两点以外
对于复杂的快速运动对象
SAM 2 有时也会漏掉一些细节
而且预测结果在帧和帧之间可能会不稳定
如同骑自行车的人的视频所示
而这个问题
只能通过在同一帧或者其他帧中
添加进一步的提示来优化预测
部分的缓解
大家尝试模型的时候可以留意一下这些问题
也欢迎在评论区里留下自己的使用体验
感谢大家的观看，我们下期再见
