大家好，这里是最佳拍档，我是大飞
在AI的发展过程中
芯片是个始终绕不过去的环节
也奠定了如今AI飞速发展的基石
两周前
知名播客20VC的主持人哈里·斯特宾斯（Harry Stebbings）
采访了Cerebras公司的联合创始人兼CEO安德鲁·费尔德曼（Andrew Feldman）
聊到了他对AI芯片领域的许多前沿信息和深刻见解
今天大飞就来给大家分享一下
谈话从回顾Cerebras的创立开始
时间回到2015年
当时AI的发展还处于相对早期的阶段
但是Cerebras的几名联合创始人
却敏锐地察觉到了其中蕴含的巨大机遇
他们意识到
AI软件对底层芯片处理器的需求
与传统计算场景截然不同
需要为AI打造一个更适配的硬件系统
事实证明
他们的判断是极具前瞻性的
尽管他们最开始的时候也低估了AI市场的规模
但是还是深刻认识到了内存带宽和通信架构
将成为制约AI发展的关键因素
正是基于这些判断
Cerebras公司毅然投身到了AI芯片的研发浪潮之中
芯片的核心功能，简单来说
其实就是执行运算和传输数据
偶尔也涉及一些数据存储
但是AI运算有着自身的特点
那就是它的基础运算其实并不复杂
主要是矩阵乘法
对于一名大二电子工程系的学生来说
设计一个浮点数乘积累加运算单元并非什么难事
但是真正的挑战在于
AI运算需要进行海量级的简单计算
这意味着数据量极其庞大
并且
运算结果和中间结果需要频繁地在内存间转移
还常常要被拆分后
跨GPU传输，严重影响整个系统的效率
因此，Cerebras在芯片设计理念上
充分考虑了AI运算的这些特点
他们明白
要想打造出更快更节能的AI计算系统
就必须要攻克数据频繁转移这个核心难题
在技术路线的选择上
Cerebras没有局限于模型微调、训练或者推理中的某一个环节
而是选择三者兼顾
从计算层面来看
训练和微调的需求基本上是一致的
虽然目的有所差异
但本质上都是对数据进行处理和优化
但是推理与训练却存在明显的差异
尤其是生成式推理
对内存带宽提出了极高的要求
以一个700亿参数的常规模型为例
每个参数16位
生成一个单词就需要移动140GB的数据
如此巨大的数据搬运量
难度可想而知
而GPU采用的HBM存储器
虽然在图形计算时代表现出色
但是在AI推理场景中却存在速度局限
因为在图形计算中
内存交互的频率相对较低
而AI推理需要频繁地读取和写入数据
HBM就显得力不从心了
与之形成对比的是SRAM
它虽然速度极快
但是传统芯片尺寸的SRAM容量有限
根本无法承载完整的模型参数
如果用传统SRAM芯片运行4000亿参数的模型
可能就需要4000枚芯片协同工作才行
而处理DeepSeek这类模型
甚至需要6000到8000枚芯片
这不仅会带来巨大的成本
管理复杂度也将达到灾难性的程度
为了解决这个问题
Cerebras通过晶圆级集成技术
实现了海量SRAM的布局
成功地兼顾了高速与大容量的优势
英伟达作为行业巨头
显然他们也清楚HBM的局限
但是由于英伟达并不生产存储器
所以需要从SK海力士、三星或美光等少数几家存储巨头采购产品
这就涉及到了复杂的架构权衡
虽然英伟达靠着HBM取得了巨大的成功
但是费尔德曼认为
Cerebras的晶圆级集成方案在推理效率上具有更加显著的优势
而且根据多个第三方测试的数据显示
Cerebras的方案在多个模型上都保持着最快的推理速度记录
在成本方面
Cerebras也有着独特的优势
虽然采用晶圆级方案也面临了许多的挑战
比如良率的问题
但是通过独创的工艺技术
Cerebras成功地解决了这个难题
在传统的芯片制造中
晶圆就像一块12英寸的圆形硅片
芯片则是从这块硅片上切割出来的
但是这个过程中不可避免地会存在一些天然缺陷
从而影响芯片的质量
一般来说，芯片面积越大
碰到缺陷的概率也就越高
废片率也就越大
行业中通常会采用binning
也就是将芯片按照性能和质量进行分类的方法
来尽量减少损失
但是这也只是一种无奈之举
而Cerebras的突破在于
将处理器设计成了由数十万个相同功能单元组成的阵列
并且配合了冗余行列的设计
当某个单元出现缺陷的时候
只需要关闭这个单元并调用备用单元即可
这种在内存制造领域成熟的技术
以前从来没有在处理器领域实现过
就连行业之父吉恩·阿姆达尔（Gene Amdahl）创立的Trilogy公司
都曾经在这个难题上遭遇挫折
而Cerebras凭借着这一创新
成为了70年来首个实现完整晶圆交付的企业
另外，客户在选择芯片的时候
最看重的因素往往是因场景而异的
在癌症诊断等对准确率要求极高的场景中
93%和94%的准确率
可能就有着本质的区别
在一些对时间要求不那么严格的场景
多等几天也不会对结果产生太大影响
而在像搜索引擎或聊天机器人这样的即时交互场景中
速度则是至关重要的
Google的乌尔斯·霍尔茨（Urs Holz）早已证明
毫秒级的延迟就会导致用户的注意力流失
因此
Cerebras采用了一种分化的策略
比如对批处理任务优先考虑成本控制
对交互场景则必须追求极致速度
面对如今AI训练与推理的两个不同阶段
费尔德曼认为
未来五年这两个阶段之间的资源分配
将发生显著的变化
简单来说，训练是创造AI的过程
而推理则是消费AI的环节
要想理解推理市场的规模
就需要考虑三个关键变量
分别是使用AI的人数、使用频率
以及每次调用消耗的算力
当前
我们正处于一个罕见的“三增长”时期
也就是这三个变量都在同步爆发式增长
这也正是推理市场呈现指数级增长的原因
虽然目前大部分资源都集中在训练阶段
但是当AI逐渐更多的融入到人们的日常工作中
完成从“有趣的工具”到“日常工作必备”的转变
那时就意味着推理将在AI产业中占有更重要的地位了
到了那个时候
推理的需求可能会超过如今的100倍
在这样的变化下
AI的基础设施也将面对更大的挑战
大家都知道
AI产业是一个高耗能的领域
消耗着巨量的电力和水资源
在美国，能源分布极不合理
尼亚加拉瀑布地区电力充沛
但是适合建设数据中心的地区却电力短缺
此外
地方监管壁垒也严重阻碍了国家级电力设施和数据中心的建设
比如在硅谷建设数据中心
就需要应对地方政府和既得利益集团的各种限制
这种分散决策模式使得数据中心的建设变得困难重重
相比之下
德州等放宽监管的地区正在吸引大量的数据中心投资
展现出了更强的发展活力
同时
数据中心的建设也存在一些问题
主持人提到
当前大量新建的数据中心存在“游客式建设”问题
许多设施根本不符合标准
资源配置严重不足
费尔德曼认为
数据中心的本质是电力接入+建筑工程+设计优化的综合体
虽然他承认行业中存在着投机者
但是他也认为
也要看到一些专业的团队
比如早期布局的比特币矿企
像Terawolf、Crusoe等
已经在欧洲等地积累了廉价电力的建设经验
如今正在主导多个吉瓦级的大型项目
另一方面
推理需求的增加也会带来成本的下降
推理成本由多个要素构成
包括数据中心的运营成本、硬件本身的成本
以及算法优化空间
目前
GPU执行推理时的利用率仅为5%到7%，
这意味着93%到95%的算力被浪费了
在这其中，算法的优化至关重要
费尔德曼认为当前流行的“Scaling laws 已达极限”的论调
与实际情况存在一定的矛盾
资深的机器学习研究者普遍都认为
算法仍然存在巨大的改进空间
OpenAI的o1项目也证明
至少在推理领域
增加算力仍然能够持续提升效果
当前的主流模型也已转向MOE 架构
不再对所有token呈现全部参数
稀疏化等技术也有望进一步提高算法的效率
可以预见的是，在未来的3到5年
大家对Transformer的依赖程度将显著降低
新的架构可能会应运而生
推动AI的技术水平发展
合成数据在AI训练中的作用也备受关注
以飞行员训练为例
模拟器可以生成大量数据
但是像直线飞行这类简单场景的数据
对训练的价值相对有限
真正对飞行员提升技能有帮助的是起降环节
特别是模拟发动机故障等极端情况的数据
在自动驾驶等AI领域也是如此
我们需要的不是高速公路的常规行驶这类容易获取的数据
而是像暴雪天气无保护左转这种高危场景的大量变体数据
合成数据能够填补这些难以获取但是至关重要的训练空白
并且随着技术的发展
合成数据的质量还将大幅提升
而当算力、算法和数据这三个维度都取得了突破时
AI的体验将发生质的变化
首先，AI将变得更快更便宜
这将使得更多的人能够轻松使用AI技术
推动AI在各个领域的普及
其次
技术的进步总会催生出新的应用场景
就像计算机的发展历程一样
当算力成本下降后
计算机先是装进汽车
然后又被塞进口袋
现在就连洗碗机和电视都内置了强大的芯片
这种扩散效应正在AI领域重演
所以未来会有更多令人惊喜的AI应用出现
从投资的角度来看
费尔德曼对AI行业的价值和壁垒也有着自己的看法
在硬件领域，投资难度较大
需要像埃里克·维什里亚（Eric Vishria）这样的顶尖投资人才有勇气涉足
而在模型领域
虽然竞争者众多而且性能相近
但是仍然存在着投资价值
费尔德曼认为
AI公司要想证明自己的长期价值
不仅需要立竿见影的成效
更需要持续进化的轨迹
在软件领域，竞争非常激烈
短暂的领先可能很快就会被超越
但是如果一家企业能够常年保持头部地位
即便不是第一名，只要稳居前10%，
这种持续力就极具价值
硅谷的许多巨头企业
起初的技术也并不是最顶尖的
但是他们率先达到了“足够好用”的临界点
从而占据了成熟市场
目前，AI行业仍处于早期阶段
数据、算力、算法都还有巨大的提升空间
这也为投资者提供了更多的机会
谈到AI芯片领域，费尔德曼指出
虽然英伟达目前占据着近乎垄断的地位
但是未来五年
英伟达的市场占有率可能会降至50%到60%。
费尔德曼首先肯定了英伟达的市场地位
在过去十年创造了企业史上最辉煌的增长
市值从2014年的100亿美元
飙升至现今的3万亿规模
它在训练领域的实力无可匹敌
CUDA生态也确实构成了一定的护城河
但是在推理场景中
费尔德曼认为并不存在CUDA的锁定效应
用户可以很方便地从英伟达的GPU切换到其他平台
比如Cerebras、Firework或者TogetherAI等等
而且谷歌通过TensorFlow
Meta通过PyTorch
一直在试图瓦解CUDA生态
如今大多数的AI代码都是用PyTorch编写的
理论上可以编译后在任何硬件上运行
费尔德曼指出，英伟达真正的护城河
其实还是在于它的市场主导地位和默认解决方案
当整个行业都习惯用它的架构来思考AI的时候
这种优势就成为了强大的壁垒
而英伟达的致命软肋
就在于GPU外接内存的基础架构
并不适合推理计算
他们自己心里也很明白
那费尔德曼又是怎么看待自己的Cerebras呢？
在技术方面
费尔德曼认为Cerebras有着一定的技术优势
传统上
毛利率反映了技术差异化的程度
如果毛利率为负
说明企业在销售大宗商品
市场对它创造的价值认可度不高
而Cerebras凭借自己的技术
维持了较高的毛利率
体现了产品的高附加值
在是否上市的问题上
Cerebras 有着自己的考量
上市对于企业来说
其实就像一把双刃剑
一方面
上市可以为企业带来巨额的资金
有助于企业扩大规模、进行研发投入以及拓展市场
另一方面，上市也伴随着许多弊端
企业需要满足严格的监管要求
投入大量的时间和资源在合规事务上
同时
公开市场的投资者往往更关注短期业绩
这可能会与企业的长期战略规划产生冲突
不过，在权衡利弊后
Cerebras 还是选择了积极地筹备上市
这说明他们希望通过上市进一步提升企业的竞争力
在 AI 芯片市场中占据更有利的地位
不过，虽然Cerebras在G42的大单中
完成了对供应链的淬炼
但是费尔德曼也承认在中东市场的开拓上交了不少学费
在谈到对中国 AI 发展的看法时
费尔德曼认为需要从多个角度进行分析
从硬件层面来看
美国政府试图通过技术封锁政策
限制中国获取先进的芯片技术
因为硬件具有可追踪性
所以这种封锁政策在一定程度上
会对中国的 AI 硬件发展造成短期的阻碍
但是从软件层面来看
情况则有所不同
软件是无形的
难以通过类似的封锁手段进行管控
中国在软件研发方面也有着庞大的人才储备和创新能力
这是中国 AI 发展的重要优势
而且，中国拥有丰富的数据资源
也为 AI 的训练提供了充足的 “燃料”。
在费尔德曼看来
美国对中国 AI 发展能力的低估
可能会在未来付出代价
好了
以上就是Cerebras CEO安德鲁·费尔德曼这次访谈的主要内容了
可以看到他对AI芯片领域有着自己深刻的见解与洞察
建议有时间的观众可以去看一下原视频
相信会有更多收获
感谢大家收看本期视频
我们下期再见
