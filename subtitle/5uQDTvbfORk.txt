大家好，这里是最佳拍档，我是大飞
如今我们会有一个基本的认识
那就是随着模型规模的不断扩大
它基本上会展现出越来越强大的能力
但是与此同时
推理成本和访问显存的效率
却成了限制模型大规模应用的“拦路虎”。
就好比是一辆动力强劲的超级跑车
却因为油箱太小、加油的速度太慢
没办法在赛道上尽情的驰骋
为了解决这些问题
研究人员们绞尽脑汁
提出了不少方案
比如说混合专家模型MoE和乘积键记忆PKM架构
不过它们都存在各自的局限性
今天
我们就来介绍国内的豆包大模型团队
提出的全新稀疏模型架构，UltraMem
它到底有什么神奇之处
能够让大模型突破这些瓶颈
提前声明一下
这期视频纯粹是技术分享
大飞我没拿豆包的红包
更不是什么业配
所以大家不要花精力往这个方面去想了
我也不希望在评论区看到一些乱七八糟的声音
咱们都纯粹一点
就来了解一下最新的技术进展
不过在探讨 UltraMem 之前
我们先来了解一下之前的方案为什么不够完美
我们都知道
大语言模型的性能和它的参数数量、计算复杂度之间
呈现出的是对数的关系
这意味着，想要提升模型性能
就需要增加大量的参数和计算量
推理成本也会随之急剧的增加
推理速度也会变得越来越慢
我们可以想象一下
一个原本能快速回答你问题的智能助手
随着知识储备的增加
反而开始变得慢吞吞的
这显然不是我们想要的结果
为了应对这个问题
混合专家模型MoE应运而生
MoE 的核心思路是通过稀疏激活专家
来解耦计算和参数
简单来说
就是把模型的不同功能模块
看作是不同的“专家”，
在处理任务的时候
只让相关的“专家”工作
这样就可以减少不必要的计算
但是在推理场景中
MoE 却遇到了麻烦
因为大模型在推理时
通常是一个字一个字地生成内容
这时候批大小和序列长度都很小
在这种情况下
MoE 的所有专家往往会被全部访问到
这就像在一个小超市里
所有人同时去拿东西
一下子就把通道堵得水泄不通
所以就非常容易遇到访问瓶颈
导致推理的延迟大幅增加
那我们刚才提到的乘积键记忆PKM架构呢？
应该说，PKM 最早提出了大记忆层
也就是 large memory layer
这个大记忆层里包含了数量庞大的稀疏参数值
value，每个 value 就是一个向量
在推理时
每个 token会根据一个“行路由”和一个“列路由”，
定位到得分最高的几个 value
然后激活这些 value 并做加权求和池化
然后得到的结果再作为记忆层的输出
这种方法的好处是
每个 token 在推理的时候
只会激活极少数的 value
所以不会遇到访问瓶颈
但是它也有自己的缺点
那就是模型的效果很差
而且 scaling 的能力也比较差
这就好比一个人虽然做事很轻松
不会累着
但是他总是做不好
而且也很难通过增加工作量
来提高整体的成果质量一样
既然 MoE 和 PKM 都存在不足
那么有没有更好的办法呢？
豆包大模型 Foundation 团队就提出了 UltraMem 这个全新的稀疏模型架构
它参考了 PKM 的设计
但是针对 PKM 的缺陷进行了补充
在保证模型效果的前提下
成功解决了推理的瓶颈问题
同时还降低了显存和部署成本
那么UltraMem 是如何做到的呢？
首先，它在模型结构方面进行了优化
在PKM 的设计中，记忆层只有 1 层
而且插在整个 Transformer 的中间层
这种设计对于大规模的训练不太友好
因为这么庞大的稀疏参数
应该尽可能多地参与到每次的残差连接中
这样才能更好地发挥作用
就像搭积木
每一块积木都应该充分利用起来
才能搭出更坚固、更复杂的结构
于是UltraMem 团队想到了一个巧妙的办法
他们拆分出多个小的记忆层
然后以固定的间隔分布在 transformer 层中
这样一来，模型在运行的时候
就可以并行地执行记忆层的访问显存操作和 transformer层的计算
大大提高了效率
不仅如此
他们还增加了 skip-layer 的操作
也就是当前记忆层的输出会加到后面某个transformer层的输出
这就像是给模型内部搭建了一条“快速通道”，
让信息的传递更加高效
其次
UltraMem 优化了 value 的检索方式
在检索时
只有分数最高的 m 个 value 会被激活
PKM 的分数是通过“行分数”加上“列分数”得到的
而 UltraMem 团队探索了一种更为复杂的乘法方法
Tucker 分解查询-键检索，简称TDQKR
这种方法的灵感来源于 Tucker 分解
具体来说，给定一组 values
它的形状是（n
n，h），其中 h 为 隐藏层维度
那么 values 的分数S_grid 可以进行如下分解
在这个结构下
每个 value 的分数是由 r 个行分数和 r 个列分数的乘积和相加组合而成的
复杂度更高
也就意味着检索更加精准
打个比方
以前找东西可能只能是大概看一下
而现在有了更为精确的方法
能够更快、更准地找到需要的信息
最后
UltraMem 还提出了隐式扩展稀疏参数的方法
通常来说
更多的稀疏参数通常会带来更好的效果
但是过多的参数又会给显存和部署带来麻烦
这就像是你想在房间里放更多的东西
但是空间有限
你还得考虑能不能放得下
于是UltraMem 团队提出了隐式值扩展
简称IVE的方法
同时引入了虚拟内存和物理内存的概念
以 4 倍扩展为例
虚拟内存的数量是物理内存的 4 倍
具体来说，给定多个（分数
索引）对之后
会首先根据虚拟内存地址表进行查找
4 个 虚拟块会查询同一个物理内存表
之后各自做加权求和池化
然后经过不同的线性层
最后再求和输出
由于最后的线性层和取 value 之间
没有任何的非线性操作
所以每个线性层都可以和物理内存表做融合
生成一个全新的内存表
这样一来
实际上就隐式扩展了 4 倍的 value 数量
既增加了参数
又不会给显存和部署带来太大压力
那么
UltraMem 的实际效果到底怎么样呢？
研究团队进行了一系列的实验
他们在 151M、680M、1.6B 三个尺寸的激活参数上进行了实验
并且保证 MoE、PKM 和 UltraMem 的总稀疏参数
保持在激活参数的 12 倍
这样对比起来更加公平
从实验结果来看
UltraMem 在 680M、1.6B 规模的模型上展现出了显著的效果优势
在多个性能指标评估中
UltraMem 的表现都十分出色
比如说，在 TriviaQA 问答任务上
UltraMem - 680M - x12 模型的得分达到了 55.17
而 MoE - 680M - 2in33 模型只有 34.19
PKM - 680M - x12 模型为 46.31；
在 HellaSwag 常识推理任务中
UltraMem - 1.6B - x12 模型得分 71.52
MoE - 1.6B - 2in34 模型是 67.34
PKM - 1.6B - x12 模型为 65.45
这些数据充分表明
UltraMem 在模型性能上超越了 MoE 和 PKM
研究人员还关注了稀疏参数对 UltraMem 效果和推理速度的影响
从实验数据来看
随着稀疏参数的增加
UltraMem 的效果提升和损失值loss的下降呈现出对数关系
也就是说，稀疏参数增加得越多
loss 下降得越快
但是下降的幅度会逐渐变小
这说明稀疏度持续降低所带来的收益在逐渐饱和
在推理速度方面
当持续增加稀疏参数的时候
UltraMem 的推理时间几乎不变
而 MoE 的推理时间却有了显著增长的趋势
这就好比两辆车在不同的道路上行驶
UltraMem 走的是一条平坦宽阔的大道
速度稳定；
而 MoE 却遇到了越来越多的阻碍
速度越来越慢
为了进一步验证 UltraMem 架构改进的有效性
研究团队还进行了消融实验
他们在 151M 激活、1.5B 总参数的稀疏模型上
从最原始的 PKM 开始
逐渐增加各种改进措施
比如增加 rm softmax、share query 等操作
以及我们之前提到的拆分大记忆层和skip-layer、IVE、TDQKR 等关键改进
通过一系列的实验对比
最终得到了 C4 验证损失值为 -0.092 的显著收益
同时稀疏参数和计算量几乎不变
相比 MoE来说
UltraMem可以实现最高达 6 倍的速度提升
推理成本最高可以降低 83%，
而且在相同的参数和计算量情况下
UltraMem 比 MoE展现出了更强的扩展能力
不过，豆包团队也指出
UltraMem 还有很大的提升空间
比如
如何更高效地优化和激活稀疏参数
以及如何进一步提升稀疏模型的推理能力等等
总的来说
UltraMem 架构为解决大模型的推理效率方面
提供了一个新的方案和思路
可以提升模型在一些对延迟要求较高的推理场景
比如代码补全和实时交互等场景下的应用
好了，以上就是本期视频的内容了
大家如果有对于论文内容的理解和想法
欢迎在评论区留言
感谢观看，我们下期再见
