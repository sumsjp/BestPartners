大家好，这里是最佳拍档，我是大飞
距离每四年一度的美国总统大选
只剩下不到五个月的时间了
驴象两党都在抓紧最后的窗口期
炒作一切能炒作的社会议题
争取潜在的选票
各种骚操作也是层出不穷
前有参议员在听证会上质疑TikTalk是否需要联网
后有德州州长因为难民问题要求独立
可谓你方唱罢我登场
让吃瓜群众看了个爽快
近期火热的AI话题自然也难逃两党看得见的大手
被加州政府一纸草案送上了热搜
5月21日
加州参议院以32比1的压倒性投票
通过了1047号草案
又称《前沿人工智能模型安全可靠创新法案》。
这个草案由州参议员斯科特-维纳（Scott Wiener）提出
目的是要建立一系列措施来规范大语言模型以及相同规模的AI开发及运营
这个草案要求开发人员对用户使用或修改他们开发的模型
承担民事甚至刑事责任
换而言之
开发人员可能因为自己训练的大模型被人滥用
而支付天价的罚金
甚至吃上几年牢饭
这个草案还将设立一个新的监管机构
全权负责制定AI的安全标准
以及提供相关法律的建议
现在
草案正在排队等待8月的议会投票
如果通过，它将正式成为加州的法律
尽管该草案暂时还不具备任何效力
但是其中针对开发者的种种条款
已经足以在行业内引发一场小地震
知名投资机构a16z 邀请了合伙人安吉尼·美狄亚Anjney Medea和风险编辑德里克·哈里斯Derrick Harris一起分析了该草案可能造成的影响
他们认为
如果1047号草案在参议院顺利通过
可能会对初创公司和开源项目产生重大的负面影响
甚至可能减缓AI行业的创新速度
今天大飞就来带大家一起看看本次议案的详情
1047号草案中最引人瞩目的一点
莫过于要求开发者为大模型造成的损失
承担直接责任
这项议案规定
"覆盖模型 "（Coverred Models）
也就是使用大于 10^26 次浮点运算 (FLOP) 的计算能力进行训练
并且训练成本超过了1亿美元的人工智能模型的开发者
必须先对模型进行安全评估
确定模型不具备任何 "危险能力"(hazardous capabilities)。
所需的安全措施包括
在人工智能系统出现不安全行为时关闭系统的能力
以及随时向新成立的监管机构
报告任何安全事故的机制
当一个模型按照预期运行的时候
开发者必须定期通过 "积极安全判定 "（positive safety）证明使用的技术是安全的
这种判定必须每年进行一次
对首次违反法案规定的行为
政府将处以人工智能模型训练费用 10% 的罚款
在此之后
每次违反草案规定的行为会被处以 30% 的罚款
与此同时
如果政府机构的负责人认为
开发者在人工智能的安全性方面误导了他们
该法案的安全认证和合规机制
还给予了他们以刑事伪证指控开发者的权力
从而导致最高四年的有期徒刑
德里克将1047号草案称为
对模型开发者强加的责任
首先
该草案要求开发人员证明他们的模型不具备危险能力
但是关于什么样的能力才算是“危险能力”，
法案却没有给出清晰的解答
更糟糕的是，随着大模型技术的发展
和“危险能力”相关的标准还有可能随时变动
换言之
开发者需要随时做好调整模型的准备
这对于大公司还好说
但是对于初创公司和个人开发者而言则是巨大负担
此外
"积极安全判定 "（positive safety）的认证非常繁琐
开发者需要在开始训练模型之前
保证自己的模型必须遵守由美国国家标准与技术研究院、加利福尼亚州、学术界、非营利部门专家和标准制定组织确定的所有现行标准和法规
虽然对于已经上市的产品来说
这可能是一个合理的措施
但是对于尚未经过训练的模型来说
合规的高成本和复杂性
可能会阻碍小型公司进行人工智能创新
从而进一步巩固少数大公司的权力
其次
没有任何一个理性的初创公司创始人、或者学术研究人员
会为了推进AI的最新技术而冒险坐牢或陷入财务困境
他们只会将放弃需要接受监管的大模型项目
业务转移到监管环境更合理的司法管辖区
说到初创企业
就不得不提到AI的开源项目了
它们在过去几十年里一直是AI技术创新和发展的重要驱动力
也是眼下业内创业者的重要依仗
安吉尼认为
1047号草案几乎没有对大模型的恶意使用者形成任何有效的限制
反而沉重地打击了现有的开源社区
他解释道
1047 号草案完全没有减少坏人故意滥用AI的风险
那些试图恶意使用人工智能的人
可能压根就不在加州政府的管辖范围内
这些互联网上的”流寇根本不会在乎一个州政府的法律
但是遵纪守法
有车有房的开发者反而被草案圈住
法案要求这些被圈住的开发人员对任何滥用他们模型的行为负责
即使这种滥用来自其他经过微调或修改的模型
a16z 认为
这就像要求汽车制造商对每一辆改装汽车造成的事故负责一样离谱
“按照这个法案的逻辑
如果有个军火商把我做的一辆车变成坦克并用它射击
作为汽车制造者的我就该被关进监狱
” 此外，目前的草案中
对于微调模型和衍生模型的定义也非常模糊
他提到
草案的提案人正在考虑一项修正案
将25%的计算差距视为衍生模型的标准
a16z认为这个标准是相当武断的
因为现在大模型之间的差异
并不局限于计算量的差异
例如，在伯克利的一些研究表明
只需要极少量的计算就可以微调像Vicuna这样的模型
其中只有70000个共享的GPT对话
他们还对LLaMA进行了微调
让它成为当时最好的开源模型之一
也就是说
一辆车变成坦克真的不需要太多的计算或数据
无论如何
在高额的罚金甚至于牢狱之灾面前
德里克表示
闭源几乎成了唯一的选项
没有任何开发者可以保证
自己的开源模型不会被社区中的恶意分子拿去干坏事
作为开源开发者
你今天还为自己实现的某项新功能沾沾自喜
吃着火锅唱着歌
突然就有可能因为世界上某个犄角旮旯的人
恶意使用你的开源模型而被罚款
a16z认为
承担这种风险实在是太过荒谬
与此同时
市面上一些坚持开源的AI公司也会受到这个草案的牵连
Derrick指出
开源的下游贡献在今天仍然是巨大的
当像Mistral或者Facebook这样的公司
开源模型并且发布权重之后
其他初创公司就可以在这些开源模型上开发自己的衍生模型
开发第一款产品
从而获得生存所需的第一桶金
就像将Linux连接到闭源Windows操作系统一样
没有这些
就没有机会让当下AI革命的速度维持下去
而1047号草案很可能会导致市面上的开源公司
出于对风险的考虑终止开源
德里克强调
如果1047号草案在八月得到国会的通过
将会大大延缓美国AI行业的开发速度
除了开源社区
闭源工程的开发也受到了1047号草案的限制
安吉尼还指出
草案中使用 10^26 次运算作为监管分水岭的做法
十分不合时宜
在计算成本下降和算法效率上升的当下
新的初创公司也可以达到这一标准
然后不得不承受针对大公司的罚金和刑事风险
根据他的经验
GPU的每FLOPS成本大约每2年到2.5年
就会降低一半
这表明
按照目前的硬件发展趋势依然遵循着摩尔定律
一个现在需要大约1亿美元来训练的模型
大约在五年后
它的训练成本就会降到约2500万美元
而在十年后
成本将会减少到不足600万美元
与此同时
算法的进步也使得开发者可以使用更少的计算量来实现相同的基准性能
安吉尼推断
算法的推理或者能力基准所需要的计算能力
每14个月或更短时间会减少一半
如果今天需要1亿次Flops才能达到某个给定的基准
那么在五年内
只需要大约600万次Flops就可以达到同样的结果
当安吉尼把这两个趋势结合在一起时
一幅非常关于未来科技的惊人想象就摆在了我们面前
那就是达到任何给定的推理能力基准的成本
每五年会下降大约50倍
这意味着如果一个模型在2024年需要花费1亿美元来训练到某个基准
到2029年
它的成本可能会不到200万美元
这完全在初创公司的预算之内
而到了2034年，十年后
这个成本将降到40到50000美元之间
让数百万人触手可及，也就是说
10^26 次运算将在很短的时间将沦为白菜价
尽管有这些明显的趋势
但是该草案的倡导者似乎忽视或者低估了这个快速的进展
安吉尼认为
随着计算成本的大幅降低和效率的显著提升
小型公司和学术机构将在不久的将来
开始达到这些性能基准
他们将不得不努力应对合规和监管等问题
至于一亿美元的门槛
安吉尼不认为它会带来任何本质的改变
因为这个训练成本的定义非常模糊
法案甚至没有明确定义在计算这些训练成本时应该包括什么
数据集采集？
研究人员的工资？
开发者应该包括之前训练的成本
还是只包括最终训练的成本？
模型微调费用的人工反馈应该计算在内吗？
如果开发者对别人的模型进行了微调
是否应该包括基础模型的成本？
这些都是没有明确答案的开放性问题
安吉尼认为
这些模糊不清的规矩会迫使初创企业、创始人、学者在这个阶段
为这些各种成本组成部分浪费大量的精力
来确保自己不违法
从而这些小团队带来巨大的负担
其中许多人没有资源来应对这些超级复杂的监管要求
另外这几乎就像说
我们刚刚发明了印刷机
现在我们只会让那些负担得起1亿美元预算的人来决定
什么可以印刷，什么不可以印刷
安吉尼斥责道
这是摆在明面上的支持垄断
是他长期以来看到的最反对自由竞争的提议之一
他认为
政府应该关注的是监管特定的高风险应用程序和恶意最终用户
而不是把时间和精力浪费在模型上
实际上
安吉尼并不反对AI行业受到来自社会和政府的监管
在他看来
政府的监管目标应该更加务实
而不是沉迷于科幻小说式的想象
他认为，技术永远是中立的
这些模型与过去的数据库或者工具没有什么不同
它们只是给了人类更高效、更好的表达方式
当然
更加发达的技术可能会增加、或者允许不良行为者
加速和扩大攻击的规模
比如Fork式网络钓鱼、深度造假、错误信息等等
但是犯法的始终是使用技术的人而不是技术本身
安吉尼直言
监管部门应该关注如何处理人的部分
比如在加强在网络安全方面的执法
并在这些攻击的速度和规模不断增加的情况下
为国家提供更好的工具来执行这些法律
至于那些异想天开的AI安全性论题
比如模型自主生成大规模杀伤性武器
或者变成天网终结者
又或者像2001太空漫游那样学会了隐瞒了自己的真实意图
安吉尼表示
这些情况只在未来存在极小的可能性
一个人在未来被AI威胁的概率
比他明天被马路上汽车威胁的概率都要小
换句话说就是杞人忧天
因此
眼下应该被立法针对的是恶意的用户
而不是基础模型和基础设施
安吉尼再次强调
1047号草案的根本缺陷在于它试图规范模型和基础设施
而不是人
好了
以上就是三人在播客中谈论的主要内容
加州向来以高尖科技的桃源之乡而闻名于世
此次出台的草案以及政府内部如此高的支持率
属实叫人感到意外
这到底是大选年的炒作之一
还是加州政府真的下定决心监管这一新兴行业
观众朋友们是怎么看待的呢？
你支持1047号法案
还是支持播客中的观点呢？
欢迎在评论区留言
感谢大家的观看，我们下期再见
