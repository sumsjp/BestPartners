大家好，这里是最佳拍档，我是大飞
昨天
Lex Fridman与SemiAnalysis的创始人迪伦·帕特尔Dylan Patel和Allen AI的内森·兰伯特Nathan Lambert
进行了一场深度对话
时间长达疯狂的5个小时
对话呢涵盖了DeepSeek的技术突破
中国AI生态系统的崛起
以及全球AI竞赛的未来格局等等
信息量呢巨大
观点呢也非常的犀利
尤其是3米analysis
前两天发表的关于DeepSeek的文章
信息量巨大
今天大飞就来为大家解读一下这次对话的重点
提醒一下，本视频时间也很长
时间关系也没有做后期
建议大家准备好零食饮料
或者在干家务的时候顺便听一下
如果能有不错的催眠效果
那正是大飞我的本意
首先Lex大概介绍了一下DeepSeek V3和DeepSeek R1
这个我相信大家应该都已经很熟悉了
所以我尽量简单过这部分
2023年12月26日
DeepSeek 发布了V3模型
这是一个混合专家Transformer模型
用户可以在互联网上公开获取模型的权重参数
遵循MIT许可证
随后在2024年1月20日
DeepSeek 又发布了R1
这是一个推理模型
两个模型基于相同的预训练基础模型
但是在后续的训练步骤上有所不同
导致了它们在功能和应用场景上的差异
除了开放权重以外
DeepSeek还提供了详细的训练报告和代码示例
有助于其他研究团队进行复现和改进
接下来
对话重点围绕DeepSeek的这两个模型展开
首先是模型的训练阶段
分为预训练和后训练两个阶段
预训练阶段主要通过自动回归预测
来预测文本序列中的下一个Token
训练数据通常来自大规模的互联网文本
比如Common Crawl等公开数据集
预训练完成后，模型进入后训练阶段
通过不同的训练方法来优化模型的特定行为
常见的后训练方法包括指令调优、偏好调优和强化学习调优
其中，指令调优是一种监督学习方法
通过在训练数据中添加指令格式
来指导模型生成特定格式的回答
这种方法通常用来生成结构化的文本
比如问答对、代码示例等等
像DeepSeek V3就是用这种方法进行的后训练
而偏好调优是通过收集人类对不同回答的偏好
来优化模型的输出质量
这种方法通常涉及到收集人类对多个回答的偏好评分
然后使用这些评分来训练一个奖励模型
指导模型生成更符合人类偏好的回答
强化学习调优则是一种通过奖励机制来优化模型的方法
这种方法通过在数学、编程等特定领域中
设置奖励函数
让模型通过试错来学习生成正确的答案
DeepSeek R1就是用这种方法进行的后训练
在性能和应用场景上
DeepSeek V3和R1有所不同
DeepSeek V3是一个通用的聊天模型
能够生成高质量的、格式化的回答
适用于各种应用场景
比如问答系统、编程助手等
DeepSeek R1则是一个专注于推理能力的模型
能够生成详细的推理过程
适用于需要复杂推理的任务
比如数学问题求解、代码调试等
根据基准测试结果
DeepSeek V3的性能与OpenAI GPT-4和Llama 405B相当
而DeepSeek R1在推理任务上的表现优于其他模型
此外，DeepSeek V3和R1都开放了权重
用户可以自由地使用和修改模型
无需担心数据隐私和商业限制
接下来
三人就开放权重的数据隐私和安全性展开了讨论
虽然模型本身不会窃取用户的数据
但是用户在使用这些模型的时候
需要信任模型的托管方，当然
用户也可以选择在本地运行模型
从而完全控制自己的数据
不过，如果用户通过API访问模型服务
数据就会被托管方处理和存储
也就存在数据泄露和滥用的风险
因此
选择合适的模型托管方和使用方式
对于保护用户数据隐私至关重要
由于R1是一个推理模型
所以它在生成回复的时候
会先输出一个详细的思考过程
然后再给出最终的答案
这个思考过程通常会表现为一长串的Token
模型会逐步解释问题
并且分解成多个步骤
比如
模型会先说明用户的问题是什么
然后列出解决问题所需要的步骤
这些步骤会快速地生成并显示在屏幕上
最终，模型会切换到一个不同的语气
总结其思考过程并给出最终答案
这种两阶段的生成过程
就是DeepSeek R1的一个显著特点
在技术实现上
DeepSeek R1的模型被训练成能够自动进行这种两阶段的推理
具体来说，模型在生成思考过程后
会生成一个特殊的Token来标记答案的开始
这个Token通常对用户来说是不可见的
通过这种方式
模型能够独立地完成从推理到给出答案的整个过程
相比之下
像OpenAI这样的公司可能会通过用户界面
将这个过程分解成多个部分
比如“问题分解”、“计算”、“结果清理”等等
然后逐步展示给用户
内森以一个DeepSeek R1推理的例子来说明这个过程
假设用户问了一个关于人类独特性的哲学问题
模型会首先详细地分解这个问题
然后逐步推理出答案
例如
模型可能会提到人类具有独特的元情绪
也就是对于自身情绪的感受
这种递归的情绪层
使得人类的行为动机更加复杂
接着
模型会进一步探讨人类同时持有矛盾信念的能力
即认知失调
这种能力可能有助于灵活适应环境
最终，模型给出的答案是
人类通过集体假装抽象规则
比如金钱、法律和权利的存在
将自私的欲望转化为合作系统
从而将冲突转化为社会发展的动力
这个答案不仅深刻
而且具有一定的启发性
在训练和推理效率方面
DeepSeek R1也做出了显著的改进
主要的技术包括混合专家模型MoE和多层低秩注意力MLA
混合专家模型是一种将模型参数
分成多个子模型的技术
每个子模型，也称为专家模型
只会在特定的任务中被激活
这种方法大大减少了训练和推理时需要计算的参数数量
从而降低了计算成本
比方说
DeepSeek R1虽然有6000多亿个参数
但是在训练和推理的时候
每次只会激活大约370亿个参数
相比之下
Llama 405B模型需要激活全部4050亿个参数
计算成本显著更高
DeepSeek在混合专家模型中还引入了一种新的路由机制
传统方法中
混合专家模型可能会依赖辅助损失（auxiliary loss）
来确保所有专家在训练过程中都被使用
辅助损失的作用是在训练时平衡不同专家的使用频率
防止模型只依赖少数几个专家进行预测
不过
这种方法可能会引入额外的偏差
限制模型的学习能力
但是DeepSeek采取了一种不同的方法
在每个批次训练结束后
模型会更新一个额外的参数
从而确保后续批次中所有专家的使用频率更加均衡
这种方法避免了辅助损失可能引入的偏差
同时确保了所有专家的有效利用
另外
多层低秩注意力是一种优化注意力机制的技术
通过使用低秩近似
来减少内存使用和计算复杂度
这种方法在训练和推理过程中能带来显著的效率提升
此外
DeepSeek R1还对底层通信机制进行了优化
由于训练过程中涉及大量的GPU通信
DeepSeek R1通过自定义通信调度策略
进一步提高了效率
具体来说
DeepSeek R1直接在GPU的汇编语言PTX层面进行编程
优化了不同核心之间的通信
从而实现了更高的效率
这些技术的结合
使得DeepSeek R1在保持高性能的同时
大幅降低了训练和推理的成本
例如，与Llama 405B相比
DeepSeek R1在训练时可以节省大约30%的计算资源
这种高效的训练和推理能力
使得DeepSeek R1能够在资源有限的情况下
仍然保持较高的性能水平
内森提到
在The Bitter Lesson苦涩的教训中
就强调了在训练过程中
要避免引入过多的人类先验知识
让模型能够自主学习的重要性
以及通过引入简单的、可扩展的解决方案
而不是复杂的、特定于问题的技巧
模型能够在更大的问题上取得更好的表现
而DeepSeek的创新正是这一理念的体现
通过引入新的路由机制
DeepSeek避免了辅助损失可能引入的偏差
同时确保了所有专家的有效利用
这种简单而有效的解决方案
使得模型在保持高效的同时
取得了显著的性能提升
此外，训练大型模型
本身也是一个复杂且充满挑战的过程
在训练过程中
模型可能会遇到各种问题
包括损失函数的突然上升（loss spikes）
这些问题可能是由于数据质量问题、模型架构问题或者其他原因引起的
为了确保模型的稳定性和性能
训练团队需要密切监控训练过程中的各种指标
包括损失函数、令牌处理速度等等
当发现异常的时候
需要及时采取措施进行调整
比如，如果发现损失函数突然上升
可以暂停训练
检查数据质量，调整超参数等等
通过不断的调试和优化
训练团队能够找到最佳的超参数组合
提高模型的性能
迪伦接下来提到了YOLO Run的概念
这个概念来源于一种“一次性投入”的策略
在小规模实验的阶段
研究人员会进行各种实验
比如测试不同的专家数量（4个专家、128个专家）或不同的架构排列方式
这些实验通常会在少量GPU上进行
比如3个GPU、数十个GPU或数百个GPU
然而，当决定进行大规模训练的时候
所有资源都会被集中使用
不再进行过多的实验
而是直接选择认为可行的方案进行投入
这种策略带来的压力在于
某些在小规模实验中有效的方案
可能在大规模训练中会失效
反之亦然
因此
YOLO Run强调在大规模训练时要敢于冒险
尽管这可能会伴随着一定的风险
迪伦还指出，在研究领域
存在两种不同的方法论
一种是系统化的方法
通过全面搜索参数空间并进行大量实验
来找到最佳配置；
另一种是依靠直觉
根据数据和经验做出判断
一些研究人员能够系统地探索整个参数空间
找到最佳的模型架构；
而另一些研究人员则凭借直觉
在短时间内做出决策
比方说
选择在后训练阶段进行优化的原因之一是
训练阶段的GPU成本较低
可以进行更多的YOLO Run实验
虽然YOLO Run看起来像是运气
但是实际上更多的是技能的体现
在面对训练效果不佳的情况时
研究人员通常会遵循一套固定的改进策略
包括数据改进和其他的局部优化
这些改进最终会积累起来
让整个模型性能得到显著提升
尽管搜索空间几乎是无限的
但是计算资源有限
因此研究人员必须在短时间内做出最佳决策
例如，OpenAI在2022年
就投入大量资源进行GP4模型的训练
这种做法可以被视为YOLO Run的典型例子
接下来三人谈到了幻方量化以及DeepSeek算力的猜测
这部分内容大家可以去看semianalysis那期节目
内容基本上是一致的
这里就不再多说了
谈到未来的AGI
内森预计会有更大比例的计算资源
被用于推理和决策过程
设想一下，一个AGI进入一个房间
思考如何控制世界
并且在2.7小时内完成任务
这将需要极其强大的计算能力
他还认为，语言模型本身就是一种AGI
具备广泛的应用价值
然而
未来的重点会转向更具有自主性的AI
这些AI能够执行训练数据中没有包含的任务
Lex提到Anthropic的CEO Dario曾经使用“超级强大的AI”这个术语来描述这一目标
认为到2026年
将出现一种具有显著军事和地缘政治优势的超级强大的AI
他还在《充满爱意的机器（Machines of
Loving Grace）》一文中
认为AI有可能彻底改变生物学等领域
内森认为
尽管Dario没有足够的科学背景
来评估AI在生物学领域的具体影响
但是可以肯定的是
AI将在任何计算科学领域加速进步
DeepSeek R1的发布就是一个很好的例子
展示了AI在新范式下的巨大进步潜力
而且这种快速的进步趋势将延续下去
带来更多的突破
不过，对于具体的AGI时间线
内森业表示难以预测
他认为到2030年之后
可能会出现具有重大地缘政治影响的AGI
尽管如此，内森也指出
AI技术的发展已经对地缘政治产生了影响
比如，在印度和巴基斯坦的选举中
人们接收到的AI语音电话
让他们误以为是在与政治家对话
此外
美国最近通过的禁止AI扩散出口管制框架
限制了对某些国家的云计算和GPU销售
即使这些国家与地缘政治冲突无关
这种做法也表明美国对AI技术的担忧
另外就是英伟达最近大幅削减了今年的H20芯片生产计划
原本计划生产200万个
但是最终取消了所有订单
这个举动表明
Nvidia可能担心H20芯片会受到进一步的出口限制
随后三人又聊回了技术方面
主要集中在推理架构中的关键技术
首先，在Transformer架构中
注意力机制是核心的组件之一
通过计算每个token与其他token之间的相对连接性
它能够让模型理解上下文中各个单词之间的关系
而不仅仅是参数本身
在注意力机制中
有三个核心组成部分
分别是查询（Query）、键（Key）和值（Value）
通常简称为QKV
这些矩阵在计算过程中相乘
从而确定每个token与其他token之间的关系
查询是模型试图获取信息的目标
键和值则用来检索这些信息
在自回归模型中
模型会逐个生成token
并且在每次生成的时候更新KV缓存
KV缓存里存的是之前所有token的压缩表示
而模型在生成下一个token的时候
会参考这个缓存
应该说
KV缓存的使用极大地提高了推理效率
因为它避免了重复计算
不过
注意力机制也存在一个显著的缺点
那就是它的内存成本与上下文长度成正比
这意味着，随着上下文长度的增加
内存需求也会迅速增长
这对于大规模推理服务构成了挑战
对于长序列上下文
内森提到了一些新的注意力机制
可以通过优化内存使用
来提高模型处理长序列的能力
比方说Gemini就拥有业界最长的上下文长度
高达200万token
这主要得益于Google在TPU架构上的优化
对于输入和输出Token的价格为什么存在差异
内森指出
这主要是因为生成Token的过程不是并行的
具体来说，输入一个查询的时候
可以并行计算所有Token的KV缓存
而生成一个Token的时候
必须顺序地读取整个模型和KV缓存
计算下一个Token
并将新生成的Token及其KV缓存
追加到缓存中
因此，生成Token的计算复杂度
要远高于输入Token
通常，API提供商对输入Token的收费
大约为输出Token的四分之一
就是因为输入Token可以批量处理
而输出Token则需要逐个生成
而DeepSeek R1模型在推理成本方面表现出色
每百万输出Token的成本仅为2美元
而OpenAI的GPT-4则高达60美元
这种成本上的差异
主要源于DeepSeek在模型架构上的创新
包括通过MLA注意力机制
将内存使用节省了80%到90%，
以及使用局部-全局注意力和滑动窗口机制等其他优化技术
对于各个模型的表现
Lex做了一个简单的哲学问题测试
他自己认为
o1 Pro的表现最好，也最稳定
接下来是DeepSeek R1
Gemini Flash 2.0排在第三
而o3 mini则排在最后
尽管o3 mini在头脑风暴中的表现要优于R1
但是在开放性哲学问题上的表现较差
这其中
DeepSeek R1展示了完整的思考链
这种透明的思考过程
对于欣赏智能和推理过程的人来说
具有极大的吸引力
通过观察这种思考路径
可以看到智能系统的非线性思维过程
类似于詹姆斯·乔伊斯的《尤利塞斯》或《芬尼根的守灵夜》中的思维过程
夜中的思维过程
关于Nvidia股票的下跌
Lex提到主要是由于DeepSeek的发布
引发了市场对Nvidia GPU需求减少的担忧
不过，这种担忧可能被夸大了
Nvidia GPU的需求仍然很高
尤其是在数据中心领域
而且AI的进步
可能还会进一步推动对高性能计算的需求
这将有利于Nvidia等公司
接下来一大块的内容是讨论训练集训与数据中心的建设
迪伦指出
数据中心的电力消耗在过去几十年中逐渐增加
预计到2028年或2030年
这一比例可能达到10%。
这个数字对于AI公司来说显得尤为重要
Anthropic和OpenAI等公司认为
现有的电力消耗水平远远不够
未来需要更多的电力支持
集群建设主要分为两种类型
分别是分布式集群和集中式集群
分布式集群在全球范围内或者美国境内广泛分布
主要用于处理推理任务
这种模式在AI服务中非常常见
比如Word Copilot、Apple Intelligence等等
而集中式集群则主要用来训练大型模型
以GPT-3和GPT-4为例
GPT-4使用了20000块A100 GPU进行训练
耗电量达到了15到20兆瓦
另外，随着技术的进步
GPU的功耗也在不断增加
比如H100 GPU的功耗从400瓦提升到了700瓦
加上其他硬件设备
每块GPU的总功耗大约为1200到1400瓦
因此
大规模集群的建设不仅需要大量的电力支持
还需要高效的冷却系统和强大的网络连接
在数据中心的设计和扩展方面
迪伦举了几个例子
比如Meta最初的数据中心设计是呈H型的
通过连接多个这样的模块来实现扩展
起初，Meta部署了16000块GPU
最终扩展到24000块GPU
不过，由于GPU的高故障率
只有大约16000块GPU用于实际的训练
其余作为备用
随着时间的推移
Meta的数据中心规模不断扩大
目前LLaMA 4的训练使用了大约100000块GPU
计划扩展到128000块GPU
考虑到每块GPU大约消耗1400瓦的电力
这意味着数据中心的总电力消耗
从2022年的大约15兆瓦
增加到2024年的大约150兆瓦
实现了近10倍的增长
Elon Musk的XAI
在数据中心建设方面也展现了极高的热情和决心
XAI在2022年开始建设数据中心
并且迅速成为全球最大的GPU集群
规模达到200000块GPU
为了支持如此庞大的计算需求
XAI在田纳西州孟菲斯市
购买了一座废弃的电器工厂
并且进行了大规模的基础设施改造
其中包括升级变电站、部署移动电源生成系统、连接天然气管道
以及建设天然气发电厂
此外
XAI还引入了特斯拉的Megapack电池储能系统
来确保电力供应的稳定性
并且使用工业级冷水机来冷却服务器
而相比之下
OpenAI在亚利桑那州和德克萨斯州阿本纳建设的数据中心计划
更是令人震惊
据OpenAI官方宣布
这个数据中心的总电力消耗将达到2200兆瓦
其中大约1800兆瓦会直接用于芯片运算
这个规模相当于一个小城市的电力消耗
足以支持大规模的模型预训练和后训练任务
OpenAI的Stargate项目
正是想通过多吉瓦级的数据中心
来加速AI模型的发展
特别是在强化学习、计算机视觉等前沿领域
迪伦还特别提到了一个值得大家注意的点
那就是被称为幕后英雄的冷却和电气系统
他举了一个例子
那就是在训练过程中
计算和权重交换之间的电力消耗差异极大
在模型训练的每一步中
计算任务会消耗大量的电力
如果计算和通信不能完美重叠
GPU可能会进入空闲状态
导致电力消耗出现尖峰
这种尖峰可能会导致数据中心的电力设施过载
甚至引发故障
为了解决这个问题
Meta在PyTorch中添加了一个名为`PowerPlant no blowup`的操作符
这个操作符能在权重交换期间
让GPU计算一些虚拟数据
从而避免电力消耗的剧烈波动
而特斯拉则采用了一种不同的方法
即使用大量的Tesla Mega Packs来解决电力的管理问题
虽然每家公司都有自己的解决方案
但是Meta的做法是公开且透明的
而且可以通过简单的操作符调整
来优化电力使用
此外，迪伦指出
传统的数据中心冷却系统主要依赖于空气冷却
包括金属散热器、热管和风扇等组件
不过，随着计算能力的提升
传统的空气冷却系统已经无法满足需求
谷歌的TPU已经使用了多年的水冷系统
但是对于GPU
大规模的水冷系统还没有普及
Nvidia已经在最新一代的高端GPU中
强制要求使用水冷系统
特斯拉则在现有的GPU中采用了大规模水冷系统
比如在Memphis数据中心
就有90个大型水冷机
这种冷却系统不仅能提高冷却效率
还能提高数据中心的整体性能
在集群规模竞赛中
特斯拉目前处于领先地位
Memphis数据中心拥有200000个GPU
其中包括100000个H100和100000个H20
Meta和OpenAI紧随其后
分别拥有128000和100000个GPU
虽然其他公司拥有更多的GPU
但是这些GPU通常分散在不同的地区
因此，特斯拉的单体集群规模
在当前竞赛中占据优势
预计到今年年底
Anthropic和Amazon将建设一个包含400
000个Trainium 2芯片的集群
Meta和OpenAI也有计划在未来几年内
将GPU集群规模扩大到500000到700000个
这些大规模的GPU集群主要用来训练预训练任务
不过，随着现有数据集的趋于饱和
预训练阶段的扩展空间有限
相比之下
后训练阶段将消耗更多的计算资源
这些任务包括模型的自我训练、模拟环境中的任务执行
以及复杂的推理任务等等
传统的FLOPS指标可能已经不再完全适用于这些任务
因此未来可能会出现新的性能评估指标
来更好地反映这些复杂任务的计算需求
对于目前的几家云计算大厂
迪伦指出
Google Cloud虽然在某些方面表现强劲
但是在整体市场份额上
Google Cloud排名第三
微软排名第二，亚马逊则遥遥领先
微软看似市场份额较大
但是它主要是在企业级许可证（比如Microsoft Office 365）中占有很大比例
实际上差距更大
亚马逊之所以领先
是因为使用AWS更为便捷
而且在许多情况下更为经济实惠
此外，AWS是最早进入市场的
一旦用户开始使用，切换成本极高
而且存在高额的转换费用
AWS为亚马逊贡献了超过80%的利润
甚至可能超过90%，
盈利能力惊人
尽管AWS的用户界面仍然显得有些笨拙
亚马逊的服务质量更优
自主研发的硬件更是降低了成本结构
包括存储、CPU和网络等传统的云服务
在数据库领域
亚马逊的五大收入产品中
有四个与数据库相关，比如Redshift
这进一步巩固了它的市场地位
谷歌的硬件团队虽然拥有TPU等优秀产品
但是这些硬件主要用在内部服务
而非面向外部客户
相比之下，Nvidia从成立之初
就专注在为外部客户提供高性能的计算解决方案
在《英伟达之道》一书中
就阐述了Nvidia的整个企业文化
其实都是围绕这个目标构建的
Nvidia通过优化CUDA软件库
迅速适应高性能计算的新需求
这与谷歌的服务模式截然不同
在硬件领域
Nvidia的优势难以被Intel和AMD等竞争对手超越
尽管AMD和Intel的硬件在某些方面优于Nvidia
但是软件支持相对不足
尤其是对于开源库的支持
Intel目前面临严峻的挑战
市场份额不断下滑
尤其是在服务器和PC市场
苹果的M1芯片、Nvidia和Qualcomm的PC芯片
以及各个超大规模数据中心自研的ARM服务器芯片
都在侵蚀Intel的市场份额
另外，Intel在AI芯片领域进展缓慢
并且在移动市场错失良机
导致它失去了技术领先地位
尽管Intel正在努力追赶
但是前景仍不明朗
迪伦认为，在未来的AI竞赛中
单一公司独占鳌头的可能性比较小
许多公司将会在AI的不同领域受益
不仅局限于训练最佳的模型
像Meta就可以通过其庞大的用户基础和多样化的产品线
从AI中获得巨大的收益
对于OpenAI来说
尽管在大语言模型领域占据优势
但是在商业模式面临很大挑战
ChatGPT虽然价值巨大
未来
OpenAI还需探索其他的应用领域
比如推理、代码生成和机器人等等
来实现可持续发展
总的来说
像谷歌和Meta这些公司拥有更广泛的业务组合
可以从AI中获得多重收益
而像OpenAI和Anthropic这些专注于先进模型的公司
必须不断创新
才能保持竞争力
访谈接近尾声，在有关Agent的话题上
迪伦认为，目前刚刚进入推理阶段
可能还需要一两年，然后才是Agent
虽然人们现在可以尝试Agent的能力
让代理持续几分钟甚至几小时
自主地执行任务
但是最大的问题是
就像制造业中的六西格玛一样
每增加一个步骤
即使是最先进的系统也会降低整体的性能
即使最好的语言模型在基准测试中表现良好
但是它们也并不是100%准确的
因为存在噪声
因此
如何达到足够的可靠性仍然是一个挑战
这与自动驾驶类似
而足够的可靠性
在一个开放、混乱的网络环境中是不可能实现的
就像在互联网历史上
航空公司和酒店虽然有很强的动力让自己的网站工作良好
但是预订机票的界面通常非常糟糕
想象一下
AI Agent能否处理这些网站
就连人类用户都经常在预订机票时感到困惑
如果航空公司能够把网站优化的更易于AI处理
那么这将会带来显著的经济利益
在编程方面
AI Agent已经取得了显著的成果
比如代码补全、函数生成和代码审查等功能
已经得到了广泛应用
软件工程Agent不仅可以进行单元测试或者编译
还可以检查整个代码库
这是普通工程师无法做到的
因此，软件工程的成本将大幅下降
这将导致不同的市场，比方在中国
由于软件工程师的成本较低
企业更倾向于构建自己的技术栈
而不是使用平台SaaS
因此
编程的大语言模型在中国的采用程度较低
因为工程师成本较低
但是
当每个公司都能以低成本和快速的方式
构建自己的业务逻辑时
将不会再选择使用平台SaaS
而是会选择构建定制化的解决方案
从而提高效率
所以说，软件工程领域的进步
将会导致软件工程师的成本急剧下降
不过
这并不意味着软件工程师会突然失业
而是工作性质会发生变化
人类将在AI系统中扮演更重要的角色
人类需要监督和修正代码，进行调试
并且设计最佳的解决方案
AI可以来提供多种选项
但是人类需要判断哪个更好
因此
软件工程师需要具备高水平的编程技能
并且成为某个领域的专家
最后
内森还介绍了一下自己Ai2实验室的Tulu开源模型
并且跟DeepSeek V3做了一些对比
在平均基准测试上略高一分
具体内容我们就不多展开说了
对这个模型有兴趣的观众
可以去看一下他们的官网
好了
以上就是Lex Fridman这次5小时播客的主要内容了
其实很多内容在我们频道的很多节目中
都陆陆续续覆盖到了
这次三个人相当于做了一个通盘的回顾和总结
不得不说
Lex的节目真是令人看得发狂
希望大飞这期节目能帮大家节省下几个小时的时间
其间总结的比较仓促，难免会有错误
欢迎大家指出，我及时改正
感谢大家的观看，我们下期再见
