大家好，这里是最佳拍档，我是大飞
在人工智能飞速发展的今天
Transformer架构已经成为了深度学习领域的中流砥柱
无论是在自然语言处理
还是计算机视觉
Transformer都有着举足轻重的地位
然而，就在刚刚
何恺明、杨立昆（Yann LeCun）两位行业巨头携手合作
通过仅仅9行代码
就砍掉了Transformer架构中被视为 “标配” 的归一化层
而且令人惊讶的是
模型的性能不仅没有下降
反而得到了提升
那么这个突破到底是如何实现的？
又会给未来的深度学习发展带来哪些影响呢？
今天
大飞就来给大家简单解读一下这篇论文
在深度学习的世界里
归一化层可以说就像是神经网络大厦的基石
无处不在
长期以来
它一直被认为是现代神经网络中不可或缺的一部分
几乎所有的现代神经网络架构都会采用归一化层
尤其是在Transformer架构中
层归一化LN（Layer Normalization）更是备受青睐
这是因为归一化层在优化神经网络的训练过程中
展现出了显著的实证优势
被普遍认为是深度网络高效训练的关键因素
在过去的十年间
随着神经网络技术的不断发展
许多研究都在致力于探索新的架构和方法
新的神经网络架构也层出不穷
有些尝试替换注意力层
有些则在卷积层上做文章
但是归一化层却始终稳坐钓鱼台
很少有人质疑它的必要性
也很少有人尝试去替换它
直到何恺明、杨立昆等人的这项研究出现
才彻底打破了人们对归一化层的固有认知
研究人员发现
即便去掉Transformer中的归一化层
通过一种巧妙的方法
依然能够让模型达到相同甚至更好的性能
这种方法就是动态Tanh（Dynamic Tanh）
简称DyT
它的原理其实源于一个看似简单却又极具洞察力的观察
那就是研究人员发现
层归一化（LN）在将输入转换为输出的过程中
呈现出了类似tanh函数的S形曲线特征
这种曲线能够有效地压缩输入中的极端值
同时在中心区域保持较好的线性形态
基于这一发现，研究人员提出了DyT
用它来替代传统的归一化层
DyT的定义是这样的
这里的α是一个可学习参数
它的作用是学习合适的缩放因子；
而tanh函数则利用自身的有界性来抑制极端值
这也是为什么将这个操作命名为 “动态” Tanh的原因
γ和β同样是可学习的、逐通道的向量参数
它们的存在允许输出缩放到任意的尺度
因此在新的设计中
它们被视为DyT层的一部分
就如同在归一化层中也包含类似的参数一样
DyT的实现代码非常简洁
仅仅9行代码就完成了从构思到实践的跨越
具体代码如图所示
从代码中可以看出
DyT在结构上并不复杂
但是却蕴含着创新的思维
它无需像传统归一化层那样计算激活统计信息
就能同时实现对输入的缩放和极值的抑制
这无疑是对传统归一化方式的一种大胆革新
为了验证DyT的有效性
研究团队还进行了一系列广泛而深入的实验
涵盖了多个不同的领域和任务
涉及多种Transformer结构和现代架构
在视觉领域的监督学习任务中
研究人员选择了ImageNet - 1K分类任务
并且对 “Base” 和 “Large” 规模的ViT和ConvNeXt模型进行训练
ViT以其独特的注意力机制
在图像分类等任务中表现出色；
而ConvNeXt则基于卷积操作
同样在视觉领域占据重要地位
实验的结果令人惊喜
DyT在这两种架构和不同模型规模上
都展现出了优异的性能
在ViT - B模型上
DyT的Top - 1分类准确率达到了82.5%，
相比使用LN的82.3%，提升了0.2%；
在ViT - L模型上，提升更为明显
从83.1%提升到了83.6%，
涨幅达到0.5%。
在ConvNeXt系列模型中
虽然部分模型的准确率提升幅度较小
但是DyT也保持了与LN相当的性能
例如ConvNeXt - B模型中两者均为83.7%，
ConvNeXt - L模型中DyT的准确率为84.4%，
略高于LN的84.3%。
如果进一步观察训练的损失曲线
会发现DyT和基于LN的模型收敛行为高度一致
这表明它们在学习动态上具有相似性
也从侧面证明了DyT的有效性
在视觉的自监督学习方面
研究人员采用了掩码自编码器MAE和DINO这两种流行的方法
进行了基准测试
MAE通过重建损失进行训练
DINO则使用联合嵌入损失
两者均以ViT作为骨干网络
在ImageNet - 1K上进行预训练后
通过附加分类层
并使用标签进行微调来测试模型的性能
实验结果显示
DyT在自监督学习任务中表现与LN相当
在MAE ViT - B模型中
两者的准确率均为83.2%；
在DINO ViT - B（patch size 16）模型上
DyT的准确率为83.4%，
略高于LN的83.2%；
在DINO ViT - B（patch size 8）模型中
DyT的优势更为明显
准确率达到84.5%，
比LN的84.1%提升了0.4%。
由于在图像生成领域
扩散模型有着重要的应用
所以研究人员在ImageNet - 1K上
训练了三种规模的DiT模型
分别为B、L和XL
其中patch大小各不相同
而在实验中
研究人员保留了归一化层中的affine parameters
仅将归一化变换替换为tanh(αx)函数
训练完成后
通过评估FID分数来衡量图像的生成质量
分数越低表示生成图像质量越好
实验结果表明
DyT在FID上取得了与LN相当或更好的性能
例如在DiT - B模型中
DyT的FID分数为63.9
低于LN的64.9；
DiT - L模型中，DyT的FID分数为45.7
同样低于LN的45.9；
DiT - XL模型中
虽然DyT的FID分数为20.8
略高于LN的19.9
但是整体性能也在可以接受的范围内
在大语言模型方面
研究人员对LLaMA 7B、13B、34B和70B模型进行了预训练
评估DyT相对于RMSNorm的性能
实验结果显示
DyT在所有四种模型规模上的表现
均与RMSNorm相当
从训练损失和平均性能来看
两者差异极小
例如在LLaMA 7B模型中
RMSNorm的训练损失为1.59
DyT为1.60，性能分数都为0.513；
LLaMA 13B模型中
RMSNorm的训练损失为1.53
DyT为1.54，性能分数都为0.529
观察损失曲线可以发现
所有模型规模的训练趋势都很相似
训练损失在整个训练过程中高度一致
这进一步证明了DyT在大语言模型中的有效性
在DNA序列建模任务中
研究人员预训练了HyenaDNA模型和Caduceus模型
使用来自人类参考基因组的数据进行训练
并在GenomicBenchmarks上进行评估
结果显示
DyT在任务中保持了与LN相当的性能
HyenaDNA模型中两者的准确率都为85.2%，
Caduceus模型中都为86.9%。
在语音领域的自监督学习方面
研究人员在LibriSpeech数据集上预训练了两个wav2vec 2.0 Transformer模型
从最终的验证损失来看
DyT在两种模型规模上的表现都与LN相当
wav2vec 2.0 Base模型中
两者的验证损失都为1.95；
wav2vec 2.0 Large模型中
DyT的验证损失为1.91
略低于LN的1.92
为了更全面地了解DyT的性能
研究团队还进行了额外的实验
评估超参数调优的影响
特别是针对所有非大语言模型的学习率和α初始化
在学习率调优实验中
研究人员对比了使用原始学习率与调优后学习率训练的模型性能
结果发现
调优学习率对DyT模型的性能提升较为有限
这意味着最初为LN模型优化的原始超参数
已经非常适合DyT模型
进一步体现了DyT与LN模型之间的内在相似性
而在对α初始化的研究中
研究人员发现，在大多数非LLM模型上
调整α的初始值，也就是α₀
只能带来轻微的性能提升
这表明默认得初始值（α₀ = 0.5）
通常已经能够实现接近最优的性能
例如在ViT - B模型中
默认α₀ = 0.5时
DyT模型的准确率为82.5%，
调整α₀到1.0后
准确率提升到82.6%，提升并不明显
不过，在大语言模型的训练中
对α₀进行精细调整却可以带来明显的性能提升
研究人员对LLaMA系列模型进行α₀调优后发现
较大的模型需要较小的α₀
例如LLaMA 7B模型的最优α₀为0.6和0.15
而LLaMA 70B模型的最优α₀则为0.2和0.05
此外
提高注意力层中的α₀值也有助于提升性能
在注意力块中的DyT层设定较高的α₀
而在其他位置
比如FFN块或最终线性投影之前
设定较低的α₀
能够有效降低训练损失
提升模型的性能
研究还发现
模型宽度在确定最优α₀方面至关重要
更宽的网络通常需要更小的α₀来实现最佳性能
而模型深度对α₀的选择影响则较小
除此以外
研究人员还对DyT的计算效率进行了评估
比如在LLaMA 7B模型上
对RMSNorm和DyT进行基准测试
测量100次前向传播和100次前向-反向传播的总时间
实验采用了4096个token的单一序列
在英伟达H100（BF16精度）上运行
结果显示
DyT层的计算时间显著低于RMSNorm层
在推理阶段
RMSNorm层的计算时间为2.1s
而DyT层仅为1.0s，时间减少了52.4%；
在训练阶段
RMSNorm层的计算时间为8.3s
DyT层为4.8s，时间减少了42.2%。
在FP32精度下也观察到了类似的趋势
这表明DyT在面向效率优化的网络设计中具有很大的潜力
以上就是对这篇论文主要内容的介绍了
它的意义不仅仅在于发现了一种可以替代归一化层的方法
更在于它打破了人们长期以来对归一化层不可或缺的固有观念
在过去
归一化层被视为深度网络高效训练的关键甚至不可或缺的组成部分
而现在DyT的出现
证明了在不使用传统归一化层的情况下
模型依然可以取得优异的性能
从实际应用角度来看
考虑到模型训练和推理可能需要进行数千万次的计算
DyT的高效性能够极大的帮助降低成本
从学术研究的角度
这项研究也为后续研究如何进一步优化神经网络架构
提供了新的思路和方法
最后
我们来介绍一下这篇论文的作者
朱家晨（Jiachen Zhu）是纽约大学柯朗数学研究所的四年级计算机科学博士生
导师是杨立昆
他对图像或视频的自监督学习
以及神经网络架构设计方面有着浓厚的兴趣
陈鑫磊（Xinlei Chen）目前是Meta FAIR的研究科学家
毕业于卡内基梅隆大学语言技术研究所
本科毕业于中国浙江大学计算机科学专业
主要研究视觉表示的预训练与理解
何恺明目前是麻省理工学院MIT
电气工程与计算机科学系的副教授
致力于构建能够从复杂世界中学习表示
并且发展智能的计算机模型
目标是用更强大的AI来增强人类智能
杨立昆不用我们多说了
图灵奖三巨头之一
Meta的首席科学家
纽约大学终身教授
在深度学习领域有着卓越的贡献
刘壮是项目的负责人
在Meta FAIR实验室担任研究科学家
毕业于加州大学伯克利分校电子工程与计算机科学系
本科毕业于清华姚班
他主导开发了CVPR最佳论文的DenseNet
以及ConvNeXt
主要研究领域是深度学习和计算机视觉
关注深度学习模型架构、训练方法、计算效率及模型理解
好了，本期视频内容就到这里
感谢大家的观看，我们下期再见
