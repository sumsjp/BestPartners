大家好，这里是最佳拍档，我是大飞
相信大家最近打开科技新闻
几乎满屏都是AI算力、英伟达市值、大模型竞赛这些关键词
但是很少有人能说清楚
AI领域的万亿美元竞争
到底是在争什么？
是模型参数的大小？
还是数据的多少呢？
应该说
SemiAnalysis的创始人兼CEO迪伦·帕特尔（Dylan Patel）
最近在《Invest Like The Best》的访谈里
把这个问题回答的相当清楚
他说 算力才是AI世界的货币
而英伟达就是这个世界的中央银行
在这次访谈里
他从产业链、能源网络、资本流动三个维度
撕开了AI竞争的表层技术战
揭露出了背后算力-资金-地缘的深层博弈
今天咱们就来顺着他的分析
拆解一下AI竞争的核心逻辑、技术路线和产业格局
以及最容易被忽略的人才和能源问题
聊到AI竞争
首先得看最核心的资本绑定
现在行业里最火的三角交易（Triangle Deal）
就是OpenAI、甲骨文（Oracle）和英伟达这三家的合作
这已经不只是一个简单的商业合作了
而是在把算力变成具体的货币
我们先简单来梳理一下这个交易的时间线和资金流
2025年6月
甲骨文第一次披露和OpenAI的重大合约
OpenAI要在未来五年内
向甲骨文购买总额大约3000亿美元的算力服务
根据监管文件，从2027财年开始
这笔合约就能给甲骨文带来每年超过300亿美元的收入
那甲骨文拿到这笔钱要做什么？
它得建数据中心、买硬件
而核心硬件就是英伟达的GPU芯片
所以OpenAI给甲骨文的钱
很大一部分会流回英伟达
更关键的是2025年9月
英伟达宣布要向OpenAI投资最高1000亿美元
双方要合作建至少10吉瓦的AI数据中心
专门用来训练下一代的大模型
10吉瓦是什么概念呢？
相当于800万户美国家庭的总耗电量
消息出来当天
英伟达股价直接涨了4%，
市值接近4.5万亿美元
迪伦·帕特尔在访谈里算了一笔更细的账
1吉瓦（GW）数据中心的建设成本
每年大概是100到150亿美元
合同期一般是5年
所以一座1吉瓦数据中心的总投入
就是500到750亿美元
如果OpenAI要建10吉瓦的集群
总资金需求就是几千亿美元
这显然不是单一公司能扛得住的
所以必须靠三方绑定
更有意思的是英伟达的利润回流逻辑
以1吉瓦数据中心为例
英伟达给OpenAI的股权投资大概是100亿美元
但是这座数据中心建设的500亿美元总成本里
有350亿美元会直接流向英伟达
而且这部分收入的毛利率高达75%。
换句话说
英伟达相当于把一半的毛利
又转化成了OpenAI的股权
通过这种结构性让利
把OpenAI、甲骨文和自己牢牢绑在一条船上
迪伦·帕特尔说
这不仅是一种应对算力稀缺的办法
更标志着AI行业进入了资本化的新阶段
以前是技术驱动资本
现在是资本放大技术速度
算力成了一种硬通货
谁能控制算力的生产和分配
谁就能主导价值分配
那除了资本绑定
AI行业的话语权到底在谁手里呢？
很多人以为是谁的模型强
谁就说了算
但是迪伦·帕特尔的观点是
真正的话语权藏在三个东西里
数据、接口，还有切换成本
我们举两个实际的案例
第一个是Cursor和Anthropic
Cursor是个AI代码编辑器
用的是Anthropic的模型
表面看，Anthropic作为模型提供方
能拿到大部分的毛利，但是实际上
Cursor手里握着用户数据和代码库
而且它可以随时切换到其他模型
所以Anthropic没法完全掌控定价权
反而Cursor有议价空间
第二个案例更典型
就是OpenAI和微软的博弈
2023年的时候
微软几乎垄断了OpenAI的算力和资本支持
OpenAI对微软的依赖度非常高；
但是到了2024年下半年
微软不愿意承担3000亿美元的长期算力支出
暂停了部分数据中心建设
也放弃了独家算力供应的身份
这就给了OpenAI转向甲骨文的机会
现在双方还在谈判利润分配和知识产权共享
微软之前的封顶利润49%条款也成了争议点
控制权从微软主导慢慢转向了双方平衡
硬件层面也是一样
英伟达没法靠并购扩大影响力
只能靠资产负债表兜底
比如给合作伙伴做需求担保、签回购协议
甚至提前分配未来的算力
通过这些方式来巩固自己的生态
迪伦·帕特尔把这个过程叫GPU货币化
指的是GPU不再只是硬件
而是AI行业的通用货币
英伟达就像中央银行一样
通过制定融资规则和供给规则
控制着这笔货币的流通
这种提前绑定的模式现在已经很普遍了
甲骨文、CoreWeave这些公司
都会给合作伙伴开首年免付算力的窗口
也就是你先拿去用
免费训练模型、拿推理来补贴和扩大用户规模
后面四年再用现金流把成本还上
这么一来
AI行业就形成了一条横跨硬件、资本、云服务和应用的权力链
算力、资金、信用在里面循环
这才是AI世界的金融基础设施
在这条链上
还有一个新角色叫Neo Clouds
新型云服务商
它们是夹在芯片厂商和应用和模型厂商之间的中间商
做的是算力租赁、模型托管和推理服务的生意
但是这个角色并不好当
因为它们要承担所有的风险
Neo Clouds主要有两种模式
第一种是短期合同
通过购买GPU建数据中心
然后高溢价租出去
比如英伟达的Blackwell芯片
按六年摊销算
每小时算力成本大概2美元
短期合同能卖到3.5到4美元
利润很高
但是风险也很大
万一新一代的芯片出来
性能提升10倍，价格只涨3倍
手里的旧芯片就会快速贬值
之前赚的钱可能全亏回去
第二种是长期绑定
通过签多年期的合同来锁定客户
这种模式毛利稳定
但是完全依赖客户的信用
比如Nebius和微软签了190亿美元的长期合约
市场甚至觉得这个信用等级比美国国债还高
Nebius的现金流就很稳；
但是CoreWeave就没这么幸运了
它早期主要靠微软的订单
后来微软减少采购
它只能转去跟Google和OpenAI合作
虽然短期利润没掉
但是OpenAI的偿付能力一直是个隐患
这些长期合同的稳定性其实很脆弱
还有一类公司是推理服务商
它们也是Neo Clouds生态里的重要角色
帮Roblox、Shopify这些企业做模型托管和高效推理
这些企业虽然能自建模型
但是推理部署太复杂了
尤其是大模型和多任务场景
很容易出问题
所以推理服务商就通过提供开源模型的微调和稳定算力
成了系统里的关键节点
但是推理服务商的风险
其实比Neo Clouds还大
它们的客户大多是初创公司或中小型SaaS开发者
项目周期短、资金链脆弱
一旦客户倒闭，合同就成了废纸
风险会一层一层传导下来
而整个产业链里
只有英伟达是稳赚不赔的
它靠着卖GPU拿稳定收入
不管市场怎么波动
都跟它没关系
所有的不确定性和信用风险
最终都落到了这些中间角色的身上
我们再来看AI的军备竞赛
这显然已经不是企业之间的竞争了
而是国家层面的体系对抗
迪伦·帕特尔认为，对美国来说
AI是维持全球主导地位的最后关键点
如果没有AI
美国可能在本世纪末失去世界霸权
为什么这么说呢？
因为美国现在的增长动力快枯竭了
债务高企、制造业回流慢、基础设施老化
只有AI能让经济重新进入增长通道
帮助它控制债务风险、延续制度秩序
所以美国的模式是市场驱动+创新驱动
靠着开放生态和私营部门的巨额投入
通过建设超大规模算力集群来保持领先
而中国走的是完全不同的长期战略
靠以亏损换份额的模式推动产业崛起
像钢铁、稀土、太阳能、电动车、PCB这些行业
中国都是靠国家资本和政策扶持
慢慢拿到全球市场份额的
现在这个模式复制到了半导体和AI领域：
通过五年规划、国家大基金还有地方财政
累计投入已经达到5000亿美元
这个投入的目标不是性能领先
而是全链路的自给自足
从材料到封装
每个环节都能自己掌控
确保地缘政治紧张的时候
产业不会断供
而且中国的执行速度很有优势
如果要建一个2-5吉瓦的数据中心
中国可能只要几年
而美国通常需要两到三倍的时间
这种体系化能力和速度优势
让中美AI竞争成了两种模式的较量
聊完资本和话语权
咱们再深入到技术层面
很多人都有个疑问
AI模型是不是参数越大、算力投入越多
就越聪明？
Scaling Law会不会遇到边际效益的递减呢？
迪伦·帕特尔在访谈里明确说
Scaling Law不会有边际递减
但是模型大不等于聪明
他用人类智能的成长做了个类比
人类从6岁到16岁的变化
不是每天加一点知识的连续加法
而是从小学到高中的阶段性跨越
也就是说，这是一种质变
而不是量变
AI模型也是一样
虽然要实现下一个阶段的能力
可能需要投入10倍的算力
但是带来的经济回报足以支撑这种投入
只不过，这里有个隐藏的风险
如果模型规模和算力投入已经到了数千亿美元级别
突然出现算法创新的停滞
或者推理效率的下降
投资回报率就会瞬间塌陷
所以迪伦·帕特尔说
AI行业的赢家不是敢砸钱的
而是能在巨额投入和时间窗口之间精准押注的
如果Scaling Law的回报消失
整个行业可能会面临系统性过度建设的危机
更重要的是
Scaling Law不只是技术问题
还是资本结构问题
它逼着企业在模型更大和模型更可用之间做选择
模型太大，推理成本会飙升
延迟降不下来，用户体验会很差；
模型太小，性能又不够
怎么平衡这两者
是现在AI工程最核心的挑战
说到推理，就绕不开一个关键的矛盾
推理延迟和容量的平衡
几乎所有的AI工程问题
本质上都是在这两个点之间找最优解
比如在硬件层面，你想让GPU降低延迟
成本会急剧上升；
反过来，想要提高吞吐量
响应速度就会变慢
企业必须在用户能接受的延迟和自己能承担的成本之间
找到一个平衡点
迪伦·帕特尔说
如果有个魔法按钮能够同时解决延迟和容量的问题
AI模型的潜能会被彻底释放
但是在现实里
成本是一个绕不开的约束
现在的推理速度其实已经够快了
真正缺的是算力
就算我们能训练出比GPT-5大10倍的模型
也没法稳定的部署
因为没有足够的算力支撑
所以OpenAI才会在GPT-5上选择不盲目求大
而是保持和GPT-4相近的规模
把重点放在提升思考模式下的推理效率
让更多的用户能用得起
而不是做一个只有少数人能用的超级模型
这里还要避免一个误区
那就是模型大不等于智能高
单纯扩大规模会掉进过度参数化的陷阱
如果模型规模一直在涨
但是训练数据没有同步增加
模型其实只是在死记硬背数据
而不是理解内容
真正的智能提升
还是来自于顿悟grokking
就像人类从死记公式到理解公式背后逻辑的瞬间
模型也需要从记忆数据到理解数据关联的突破
那么怎么让模型实现顿悟？
迪伦·帕特尔认为
强化学习的新方向是关键
也就是给模型构建一个可交互的环境
让它在环境里试错、学习
而不是只从互联网数据里去读世界
为什么说互联网的数据不够呢？
因为互联网上没有教人如何用键盘操作Excel的细节教程
也没有企业数据清洗的具体流程
这类真实的素材
这些能力
模型只能在实际的环境里去学
现在湾区已经有几十家初创公司在做这件事
有的建虚拟电商平台
让模型学浏览、比较、下单；
有的做数据清洗任务
让模型在不同格式的数据里反复尝试；
还有的做游戏、医学案例场景
让模型通过试错、验证、反馈
慢慢形成真正的理解
迪伦·帕特尔打了个很形象的比方
现在的AI还处于婴儿的阶段
需要通过不断尝试和失败来校准自己的感知
就像人类成长中会忘记没用的信息
只留关键经验一样
AI模型训练时也会生成大量数据
但是最后只保留少数的核心内容
所以要让模型理解世界
必须让它走进环境里学
有些研究者认为，AGI的关键是具身化
让模型能和物理世界互动
这种理解没法通过视频或文本获得
只能靠行动和反馈
所以强化学习的重要性会越来越高
而且AI的发展阶段也会从预训练转向后训练
当然，预训练还是基础
但是后训练会成为算力消耗的主要来源
随着强化学习的成熟
AI会从回答问题变成直接行动
比如未来，AI可能不只是帮你选商品
而是直接在电商平台给你下单；
不只是帮你规划路线
而是直接帮你订车票
其实我们已经在把决策外包给AI了
比如Uber Eats的首页推荐、Google Maps的路线规划
都是AI在替我们做选择
只不过现在这个趋势还在加速
Etsy有超过10%的流量来自GPT
如果亚马逊没屏蔽GPT
这个比例会更高；
OpenAI的应用团队也在推进购物Agent
这会是它商业化的核心路径
这种Agent还会催生出新的商业模式
也就是从AI的执行中抽成
未来的平台可能像信用卡体系一样
从AI的执行环节里收取0.1%或者1%的费用
积少成多，从而形成巨大的利润空间
聊完了学习方式
我们再来说说模型的记忆问题
这是现在大模型的一个核心瓶颈
在算力有限的情况下
推理时间其实是提升能力的另一条路
也就是模型的规模不变
只要延长思考步数和推理深度
性能就能显著提升
Transformer架构的注意力机制
能让模型在有限的上下文中回溯和关联信息
但是遇到长上下文和稀疏记忆的场景
就会力不从心
比如上下文窗口扩大后
内存和带宽的压力会急剧上升
这时候HBM就成了关键
没有足够的HBM
大模型根本没法高效推理
但是迪伦·帕特尔不认为
模型的记忆系统必须模仿人脑
像外部的写作空间、数据库、甚至文档系统
都可以成为模型的外部记忆
关键是让模型学会怎么写、怎么提、怎么复用这些信息
在任务中建立长期语义关联
OpenAI的Deep Research就是个很好的例子
它让模型在较长的时间里
持续生成中间文本，不断压缩、回看
完成复杂的分析和创作
迪伦·帕特尔把这叫人工推理的显微镜
它展示了未来agent的工作方式
也就是在长时记忆和短时计算之间循环
真正实现思考
这种新的推理和记忆框架
会彻底改变算力需求结构
未来的百万GPU集群
不只是用来训练更大的模型
更要支撑这些长思考的agent
对于硬件创新
很多人会觉得硬件创新要靠初创公司
但是迪伦·帕特尔的观点是
真正能够推动硬件创新的
还是巨头公司
先看半导体制造
现在的半导体制造难度已经到了太空时代级别
是人类工程里最复杂的系统之一
但是背后的软件体系却很落后
哪怕是数亿美元的制造设备
还用着过时的控制系统和低效的开发工具
就算英伟达这么领先
它的供应链里还有很多老古董部件
比如变压器
基本原理几十年没怎么变
但是现在有个新的突破
那就是固态变压器
它能把超高压交流电
一步步转成芯片能用的低压直流电
能源效率提升很多
这些看似传统的工业环节
正在成为AI基建的新利润和创新源头
不过迪伦·帕特尔并不看好加速器公司
就是那些想和英伟达、AMD、谷歌TPU、亚马逊Trainium竞争的新兴芯片公司
因为这个行业资本密集、风险极高
技术的创新空间又有限
除非出现颠覆性的硬件跃迁
否则新玩家很难撼动现有的格局
在芯片互联方面
随着模型的上下文变长
内存需求激增
而DRAM行业的创新又慢
所以网络层面反而更容易出突破
帕特尔指出，现在主要有三个方向
第一是通过紧密互连
实现芯片间的内存共享
比如英伟达Blackwell架构里的NVL72模块
把大规模网络互连能力集成到单一系统
数据交换效率提升很多；
第二是光学互连
怎么让电信号和光信号之间转换、传输更高效
直接决定下一代数据中心的性能上限；
第三是极端性能下的可靠性
比如Blackwell的超高带宽互连
让同一个机架里的每颗芯片
能够以每秒1.8TB的速度通信
这已经逼近散热和可靠性的物理极限了
但是芯片互联的创新也面临产业内部的阻力
比如英特尔内部缺乏数据的共享文化
光刻团队和刻蚀团队不愿意共享实验数据
这些数据也没法上传到AWS云端做关联分析
导致迭代的效率很低；
台积电虽然制造能力强
但是也不允许数据外发
实验和分析流程很慢
要想解决这个问题
就必须变更企业文化
通过推动制造企业能更开放地用数据建仿真器
更精准地来模拟现实
最后是世界模型（World Model）
核心是让AI具备模拟世界的能力
这不只是软件层面
还在向物理层扩展
比如谷歌的Genie 3
模型能在虚拟环境里自由移动、观察、和物体互动；
更先进的模型能模拟分子反应、流体动力、火焰燃烧
用AI的方式学物理规律
现在有很多公司在做这个方向
有的专注机器人训练
有的聚焦化学和材料模拟
这些公司让AI从语言处理系统变成了理解现实的计算框架
也让机器人和物理仿真成了投资热点
但是迪伦·帕特尔强调
大部分最前沿的创新
还是发生在大公司里的
因为世界模型需要的算力、数据和供应链
只有台积电、英伟达、安费诺（Amphenol）这类巨头能支撑
聊完技术
咱们再来看支撑AI运行的三大支柱
算力、人才和能源
这三个东西看起来不性感
但却是决定AI竞争成败的关键
首先是算力的经济逻辑
英伟达提出的AI工厂的概念
其实是AI行业的经济学基石
这个概念是2020年马可·扬西蒂（Marco Iansiti）和卡里姆·拉哈尼（Karim R
Lakhani）在《Competing in the Age of AI》里提出来的
指的是一种专用的计算基础设施
能够覆盖AI的全生命周期
从数据获取、训练、微调
到大规模推理，最终把数据变成价值
AI工厂的产品不是硬件，也不是软件
而是智能，用token来衡量产出
迪伦·帕特尔打了个比方
一座AI工厂输出的不是电
而是智能本身
每个token都包含了计算能力、算法效率和能源投入
AI企业的核心问题
不再是造最强的模型
而是在相同功率下
怎么配置token产能最划算
比如是用多次调用弱模型
还是少次调用强模型呢？
算力分配、模型规模、用户数量、推理延迟
这些因素加起来
就决定了AI工厂的经济效率
最近两年
算法优化让模型的服务成本降了很多
比如GPT-3级别的推理
成本比两年前降了大约2000倍
但是成本下降并没有让AI的普及速度变快
因为算力还是稀缺的
所以OpenAI在GPT-5上才会克制
不盲目的扩大模型规模
而是保持和GPT-4相近的推理成本
提升思考模式下的效率
让更多用户能用得起
这也印证了迪伦·帕特尔的观点
未来的竞争
不在于谁训练了最大的模型
而在于谁能以最低的token成本
提供稳定、可规模化的智能服务
其次是人才，比GPU更稀缺的
是能够高效使用GPU的人
很多人不知道
AI研究里有大量的无效实验
一次实验没设计好
就会浪费大量的算力
所以
能把算力利用率提高5%的研究者
他的边际贡献在训练和推理环节
都会产生长期的复利，这种效率提升
足以抵消数亿美元的设备投资
这也是为什么现在AI领域的顶级研究者
能够拿到天价年薪和股权激励的原因
这不是泡沫
而是对效率价值的理性回报
当然，人才扩张也有相应的副作
巨头公司在全球收购式挖人
导致很多研究机构出现组织迟滞
团队被拆散
新进来的人不熟悉原有流程
算力浪费反而更加严重
迪伦·帕特尔直言
现在行业里太多人只会说不会做
规模化挖人往往导致结构性的失配
招来的人不适合现有项目
现有项目又因为核心人员的流失而停滞
他甚至建议
美国应该在高端制造、物理化学这些领域反向挖人
把其他国家的关键工艺和实验知识迁回本土
解决产业落地的人才缺口
最后是能源
这才是美国AI发展的最大短板
很多人担心AI数据中心耗电太多
但是实际的情况是
整个美国的数据中心用电
只占全国总电力的3%-4%，
其中一半左右来自AI数据中心
根据美国能源信息管理局（EIA）的数据
2024年美国的电力消耗是40860亿千瓦时
2025年预计到41650亿千瓦时
按照1.5%-2%估算
2025年美国AI数据中心的耗电量
大概是624-833亿千瓦时
这个比例其实不算高
真正的问题不是用了多少电
而是怎么新增发电能力
过去40年
美国能源体系转向天然气后
新增的发电能力几乎停滞了
现在要重启发电项目
面临着三大障碍
第一是监管复杂，审批流程长；
第二是劳动力短缺
尤其是电工这类技术工种；
第三是供应链紧张
燃气轮机、变压器这些关键设备
生产周期长、产能有限
而且AI数据中心的规模还在暴涨
比如OpenAI正在建一座耗电2吉瓦的设施
相当于整个费城的电力消耗
现在在业内
不到1吉瓦的项目几乎没人关注
500兆瓦的项目就意味着250亿美元的资本支出
这个规模已经远超以往任何数据中心建设记录
更麻烦的是波动负载
模型训练的时候
功率会突然变化
这会扰乱电网频率，造成设备损耗
就算不至于断电
也会让附近的家用电器提前老化
为了应对这个问题
美国电网出了强制减供的规定
比如得克萨斯州的ERCOT电网、东北部的PJM电网
在电力紧张时
会提前24-72小时通知大型用电企业
要求它们把用电量削减一半
所以企业们只能自己想办法
启用自备发电机
比如柴油机、燃气机
甚至氢能发电机
但是这又带来新问题
如果这些发电机连续运行超过法规规定的时间
就会违反排放许可，触碰环保红线
现在美国整个能源行业都在紧急补课
燃气机组、双循环机组重新开始建设
有的企业甚至用并联柴油卡车发动机来临时发电；
供应链也在恢复
马斯克从波兰进口发电设备
GE和三菱宣布扩大燃机产量
变压器市场已经全面售罄
能参与数据中心建设的电工
工资几乎翻倍
迪伦·帕特尔说
如果美国有足够多的电工
这些数据中心能建得更快
但是Google、Amazon、EdgeConneX这些公司的供应链差异太大
资源分散，效率不一
他认为，这场能源危机虽然混乱
但是也令人振奋
AI逼着美国重新启动发电建设
重塑供应链
甚至让整个国家重新理解电力的意义
如果解决不了能源问题
再强的算力和模型
也只是无米之炊
聊完了AI的核心支撑
咱们再看AI对传统行业的冲击
首当其冲的就是软件行业
尤其是SaaS行业
5-10年前
SaaS被认为是软件行业最完美的商业模式
研发成本相对稳定，毛利率极高
公司的主要支出集中在获客成本上
只要用户规模达到临界点
获客成本就会被充分摊薄
新增用户几乎直接转化为利润
公司就会变成一台现金流机器
但是这个逻辑成立的前提是
软件开发本身的成本极高
企业自己开发的代价
远高于向SaaS厂商订阅服务
所以才会选择租而不是买
而现在AI彻底打破了这个前提
AI工具让软件开发成本急剧下降
拿中国为例
中国企业本地化开发的成本
远低于在美国购买SaaS服务
所以很多中国企业宁愿自建系统
或者选择本地部署
也不愿意签订长期订阅合同
这也是为什么中国的SaaS和云服务渗透率
一直显著低于美国的核心原因
现在这个趋势正在全球扩散
AI让自建软件的门槛越来越低
传统SaaS租比买划算的逻辑
正在像中国市场那样逐渐失效
更糟的是
SaaS行业还面临双重成本压力
一方面，获客成本一直居高不下
尤其是AI时代
企业要和更多竞争对手抢用户；
另一方面，AI又抬高了服务成本
任何集成了AI功能的SaaS
都要承担token的计算成本
而且这个成本会随着用户的使用量增加而上升
同时
市场上还出现了大量的AI自建竞争者
以前只有大公司能够自建软件
现在初创公司用AI工具
几周就能做出一个类似的产品
导致SaaS市场越来越碎片化
用户很容易用脚投票
转向更便宜的自建方案
或者被客户的内部研发团队分流
这么一来，SaaS企业就陷入了困境
用户规模难以扩大
达不到摊薄获客成本和研发支出的逃逸速度
服务成本又在上升
净利润拐点被不断延后
盈利情况越来越差
不过
也有公司能在这场变革中找到优势
比如Google
得益于自研的TPU和垂直一体化的基础设施
Google在每个token的边际服务成本上
显著低于同行
这让它在AI时代的SaaS竞争中
有更大的定价空间
迪伦·帕特尔认为
未来纯软件公司的日子会越来越难
具备规模、生态、平台势能的企业会继续占据优势
随着内容生产和生成成本的持续下降
真正掌控平台的公司会成为最大赢家
比如YouTube这样的超级平台
可能会迎来新一轮高光时刻
因为它能够借助AI降低内容创作成本
同时通过生态留住用户
总的来说
AI带来的不只是软件技术的变化
更是软件行业商业逻辑的重构
旧的规则已经失效
新的规则还在形成
能适应这种变化的企业
才能在AI时代活下来
最后，咱们结合迪伦·帕特尔的观点
聊聊现在主流的AI玩家
他们的优势、劣势，还有未来的潜力
第一个是OpenAI
迪伦·帕特尔直接评价它是顶级优秀的公司
无论是技术的领先性
还是资本的绑定能力
OpenAI都是当前AI行业的标杆
不过它也面临着挑战
比如和微软的利润分配谈判
还有如何平衡技术突破和商业化落地
避免陷入烧钱无底洞的困境
第二个是Anthropic
迪伦·帕特尔对Anthropic的看法
甚至比OpenAI更加乐观
核心原因是Anthropic的聚焦
它专注于价值大约2万亿美元的软件市场
目标很明确
收入增长速度也比OpenAI快
而OpenAI的布局相对分散
同时推进企业软件、AI科研、消费者应用、平台抽成等多条路线
虽然潜力巨大
但是执行的难度也更高
Anthropic的稳健聚焦
在当前的市场环境下
反而更有优势
第三个是AMD
迪伦·帕特尔对AMD有着个人好感
一方面
AMD长期和英特尔、英伟达抗衡
扮演友善的挑战者角色
这种弱者逆袭的气质很吸引人；
另一方面
AMD是迪伦·帕特尔人生中的第一个十倍股
作为一名从小热爱组装电脑的人
他对AMD这种技术驱动的逆袭者一直有好感
不过
AMD要在AI GPU市场真正挑战英伟达
还需要在生态建设和软件优化上多下功夫
毕竟英伟达的优势不只是硬件
还有CUDA生态的护城河
第四个是xAI
迪伦·帕特尔认为xAI存在无法持续融资的风险
虽然马斯克个人很能吸引资金
xAI也在建设Colossus 2超级数据中心
建成后会是全球最大的单体数据中心
能部署30万到50万颗Blackwell GPU
这个规模很震撼
但是xAI的商业化一直没找到方向
目前唯一的产品Grok
变现情况并不理想
没有形成稳定的收入来源
迪伦·帕特尔建议
xAI完全可以把Grok的商业化做得更好
比如通过个性化的虚拟人物订阅服务增加收入
甚至可以和OnlyFans合作
把创作者的数字化形象整合到X的生态里
形成社交+AI+内容的超级应用
但是如果xAI一直找不到可持续的商业模式
就算有马斯克的资金支持
也很难负担3吉瓦级数据中心的长期投入
毕竟就算是世界首富
也没法一直靠个人财富
补贴一个没有收入的公司
第五个是Meta
迪伦·帕特尔认为Meta手中握有可能统治一切的牌
核心原因是Meta新推出的智能眼镜
这标志着人机交互方式的又一次革命
回顾历史
人机交互从打孔卡到命令行
从图形界面到触控屏
下一阶段会是无接触交互
用户不用动手
只要对AI说出需求，它就能直接执行
比如发邮件、下单购物
而且Meta是目前唯一拥有完整体系的公司
既有硬件
又有强大的模型能力、充足的算力供应
还有业界领先的推荐算法系统
这四者叠加，再加上Meta的资金实力
让它有潜力成为下一代人机接口的主导者
如果Meta能把硬件体验和AI功能做好
未来的增长空间会非常大
第六个是Google
迪伦·帕特尔说
他两年前还对Google相当悲观
今天则非常看好
核心原因是Google的彻底觉醒
以前Google在AI商业化上比较保守
现在开始主动对外销售TPU
积极推进AI模型的商业化
在训练和基础设施投资上也更有进取心
虽然Google内部还有低效和官僚等问题
但是它的硬件基础是独特的
自研TPU+垂直一体化基建
能让它在AI成本控制上占据优势
而且Google还有Android、YouTube、搜索这些庞大的生态资产
一旦人机交互进入无接触时代
这些资产就能被重新整合
形成强大的竞争壁垒
迪伦·帕特尔认为
Meta可能在消费市场领先
但是在专业和企业级应用领域
Google的潜力更大
好了
以上就是迪伦·帕特尔这次访谈的主要内容了
其实AI竞争的本质
早已经不再是技术单点突破
而是系统能力的比拼
谁能整合算力、资本、人才、能源
谁能在技术路线和商业化之间找到平衡
谁能适应行业逻辑的重构
谁就能在这场万亿美元的竞赛中
占据有利位置
大飞相信，未来几年
AI行业还会有很多变化
可能会出现新的技术突破
也可能会有企业倒下
但是核心逻辑是不会变的
算力是基础
创新是动力，生态是护城河
希望今天的内容
能够帮助大家更清晰地看懂AI行业的趋势
也欢迎大家在评论区分享你的看法
感谢收看本期视频，我们下期再见
