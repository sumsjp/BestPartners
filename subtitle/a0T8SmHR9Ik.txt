大家好这里是最佳拍档我是大飞
6月12号 是著名的Transformer论文
提交的六周年的日子
应该说
人工智能领域啊最近的这波突破
或许都应该感谢一下Transformer
6年前这篇名字有一点浮夸的论文
被上传到了预印版的论文平台arXiv
《Attention is all you need》
这句话甚至成为了
论文标题的一个潮流
当然了
Transformer也不再是变形金刚的意思了
它开始代表着AI领域最先进的技术
6年后回看当年的这篇论文
我们可以发现很多有趣
或者鲜为人知的地方
虽然论文的名字是attension is all you need
我们也因为它而不断的去推崇注意力机制
但是并不是Transformer的研究者们
发现了注意力
而是他们把这种机制推向了极致
实际上
注意力机制是由深度学习的先驱约书亚本杰奥
带领的团队在2014年提出来的
在这篇ICLR 2015的论文中
本杰奥等人提出了一种RNN+上下文向量
也就是注意力的组合
虽然他是NLP领域最伟大的里程碑之一
但是相比Transformer
它的知名度就要低多了
本杰奥团队的这篇论文
到现在被引用了2.9万次
而Transformer的论文
到现在已经被引用了7.7万次
Transformer的这篇论文
虽然现在的影响力很大
但是在当年的全球顶级的AI会议
NeurIPS 2017 2017上
连个Oral都没有拿到
更不用说拿到会议的奖项了
当年的这场大会共收到了3,240篇论文投稿
其中678篇被选为大会的论文
Transformer论文就是被接收的论文之一
在这些论文中40篇为Oral论文
112篇为sportlight论文
3篇最佳论文
1篇时间验证奖项论文
但是这里边都没有Transformer
不过事到如今
应该也已经没有人会再去否认
Transformer的影响力了
在Transformer诞生之前
AI圈的人在自然语言处理中
大多会采用基于RNN循环神经网络的
编码器解码器这个架构来完成序列的翻译
然而RNN及其衍生的网络
最致命的缺点就是慢
关键的问题就在于
前后隐藏状态的依赖性无法实现并性化
而Transformer它完全摒弃了递归的结构
依赖注意力机制
挖掘输入跟输出之间的关系
进而实现了并行计算
谷歌当年发的一篇博客
就阐述了Transformer是一种
语言理解的新型神经网络架构
具体来讲
Transformer由四部分组成
输入 编码器 解码器 以及输出
输入字符首先通过Embedding转为向量
再加入位置编码来添加位置信息
然后通过使用多头自注意力
和前馈神经网络的编码器和解码器
来提取出特征
最后再输出结果
谷歌就曾经以如何在机器翻译中
使用Transformer来举例
机器翻译的神经网络
通常会包含一个编码器
在读取完句子之后会生成一个表征
空心圆代表着Transformer
为每一个单词生成的初始表征
然后利用自注意力
从所有其他的词中聚合信息
在整个上下文中
为每一个词产生一个新的表征
由实心圆来表示
接着将这个步骤对所有的单词并行重复多次
依次生成新的表征
同样解码器的过程与这个也类似
但是每次从左到右生成一个词
它不仅关注其他先前生成的单词
还关注编码器生成的最终的标准
注意力机制其实是仿照人类的视觉注意力而来的
人类大脑呢有一种天生的能力
当我们看一幅图的时候
先是快速扫过图片
然后锁定需要重点关注的目标区域
如果人类不放过任何的局部信息
那必然就会做很多的无用功
不利于人类的生存
同样在深度学习网络中的注意力机制
也是为了简化模型加速计算
从本质上说
attention就是从大量的信息中
筛选出来少量重要的信息
并且聚焦到这些重要的信息上
忽略大多数其他不重要的信息
2019年谷歌还专门为它申请了专利
从此以后在自然语言处理中
Transformer便开始了它的逆袭之路
归根到底
现在各种层出不穷的GBT
都起源于这篇17年的论文
别忘了GBT里的T就是Transformer
然而Transformer点燃的不仅仅是NLP的学术圈
2017年在谷歌的一篇博客中
研究人员就曾经对Transformer
未来应用的潜力进行了畅想
不仅仅涉及自然语言
还涉及到了非常不同的输入和输出
例如图像和视频
自2012年以来
CNN逐渐就成为了视觉任务的首选架构
但是随着越来越高效的结构出现
使用Transformer来完成CV的任务
就成了一个新的研究方向
它能够降低结构的复杂性
探索可扩展性和提高训练的效率
2020年的10月
谷歌提出了Vision Transformer ViT模型
不用卷积神经网络CNN
就可以直接用Transformer对图像进行分类
值得一提的是
ViT的性能表现非常出色
在计算资源减少4倍的情况下
仍然超过了当时最先进的CNN模型
紧接着2021年
OpenAI连扔了两颗炸弹
发布了基于Transformer打造的DALL-E还有CLIP模型
这两个模型借助了Transformer实现了很好的效果
DALL-E能够根据文字输出稳定的图像
而CLIP能够实现图像与文本的分类
再到后来DALL-E的进化版DALL-E 2
还有stable diffusion同样都是基于Transformer架构
再次颠覆了AI绘画
这就是基于Transformer所诞生的模型的整条时间线
由此可见Transformer是有多么的能打
那除了论文本身之外呢
当年发表这篇论文的作者们现在都怎么样了呢
Transformer论文的作者一共有8位
他们分别来自谷歌和多伦多大学
5（6）年过去了
大部分的论文作者都已经离开了原来的机构
阿希什·瓦斯瓦尼（Ashish Vaswani）
他是在南加州大学拿到的博士学位
师从华人学者蒋伟和黄亮
主要研究现代深度学习在语言建模中的早期应用
2016年他加入了谷歌大脑
并领导了Transformer的研究
2021年他离开了谷歌
尼基·帕尔玛（Niki Parmar）是八位作者中唯一的女性
她硕士毕业于南加州大学
2016年加入了谷歌
在工作期间
她为谷歌搜索和广告研发了一些成功的
问答和文本相似度的模型
还领导了扩展Transformer模型的早期工作
将它扩展到了图像生成计算机视觉等等领域
2021年她也离开了谷歌
2022年4月26日
两人参与创立了AI公司Adapt
共同创始人一共有9位
其中瓦斯瓦尼担任首席科学家
帕尔玛担任首席技术官
Adapt的愿景是创建一个被称为人工智能队友的AI
这个AI经过训练可以使用各种不同的软件工具和API
2023年3月 Adapt宣布完成了3.5亿美元的B轮融资
公司估值超过10亿美元晋升独角兽
不过在adapt公开融资的时候
这两个人其实已经离开了Adapt
并且创立了自己的新的公司EssentialAl
今年5月
EssentialAl宣布了由Thrive capital
领投的800万元美元融资
目前该公司还处于隐身的模式
还没有推出任何的产品
另一位论文作者诺姆沙泽尔（Noam Shazeer）
是谷歌最重要的早期员工之一
他在2,000年底就加入了谷歌
直到2021年最终离职
之后成为了一家初创公司企业的CEO
公司的名字叫做Character.AI
Character.AI的创始人除了诺姆沙泽尔
还有一位是丹尼尔·德·弗雷塔斯
他们都来自于谷歌的LaMDA团队
在创办公司之前
他们在谷歌构建了支持对话程序的语言模型LaMDA
今年3月Character.AI宣布完成了1.5亿美元融资
估值也达到了10亿美元
是为数不多有潜力和OpenAI竞争的初创公司之一
也是罕见的仅仅用16个月的时间
就成长为独角兽的公司
他的应用程序Character.AI
是一个神经语言模型的聊天机器人
可以生成接近于人类的文本响应
以及上下文对话
Character.AI于2023年5月23日在Apple
APP store和Google的play store中发布
第一周的下载量就超过了170万次
后来又增加了每月9.99美元的付费订阅
允许用户优先的获得更快的响应
以及早期访问新功能的特权
艾丹·N·戈麦斯（Aidan N. Gomez）
他早在2019年就已经离开了谷歌
之后担任了FOR.ai的研究员
现在是Cohere公司的联合创始人兼CEO
Cohere是一家生成式AI的初创公司
于2019年成立
它的核心业务包括提供NLP的模型
以及帮助企业来改进人机交互
三位创始人中有两位是谷歌大脑团队的前成员
2021年11月谷歌cloud宣布与Cohere合作
用谷歌的强大的技术设施
为Cohere平台提供动力
而Cohere则使用Google cloud的TPU
来开发和部署他们的产品
Cohere之所以备受业界的瞩目
除了创始团队的背景之外
还有一部分原因在于他的投资者中
还有图灵奖得主杰弗里辛顿
知名人工智能研究员李飞飞
UC伯克利大牛彼得阿贝尔
这三位在当今的人工智能领域都是执牛耳的人物
Cohere刚刚也是获得了2.27亿美元的c轮融资
成为了市值22亿美元的独角兽
卢卡斯凯撒（Łukasz Kaiser）
他在2021年离开了谷歌
在谷歌一共工作了7年零9个月
现在是OpenAI的一名研究员
在谷歌担任研究科学家期间
他参与了机器翻译和解析的工作
以及生成式任务的SOTA神经模型设计
是Tensorflow、Tensor2Tensor的共同作者
雅各布·乌什科里特（Jakob Uszkoreit）
他被认为是发明了Transformer架构的幕后推手
在谷歌工作时间长达13年于2021年离开谷歌
在谷歌工作期间
雅各布·乌什科里特参与组建了
谷歌助理的语言理解团队
早期还从事过谷歌翻译的工作
之后他加入了Inceptive成为联合创始人
Inceptive呢是一家AI制药公司
致力于运用深度学习去设计RNA药物
伊利亚·波洛苏欣（Illia Polosukhin）
于2017年离开谷歌
现在是NEAR.AI的联合创始人兼CTO
这家公司主要是做区块链的底层技术
可以托管去中心化应用程序和智能合约
被誉为以太访杀手
目前估值约为20亿美元
那唯一还留在谷歌的就是利昂·琼斯（Llion Jones）
今年是他在谷歌工作的第九年
只不过他的工作地点换到了日本
他曾经打趣的说道
自己对论文做出了最有意义的贡献
就是写下了那个大逆不道的标题
虽然如今距离《Attention Is All You Need》
这篇论文发表已经过去了6年
原作者们也有的选择离开
有的选择继续留在谷歌
但是不管怎么样
Transformer还在继续影响着整个AI技术的发展
好了本期的分享就到这里
欢迎大家订阅我们的频道
我们下期再见
