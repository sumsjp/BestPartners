大家好，这里是最佳拍档，我是大飞
最近
OpenAI的 CTO 米拉-穆拉提 Mira Murati
在达特茅斯学院参加的一场采访中语出惊人
她声称，只需要一年半的时间
AI就可以在某些领域达到博士级别的智能
想想现在无数学子为了读个博士
卷的是昏天黑地
米拉的这番言论不禁让人沉默
难道人类真的要卷不过AI了吗？
在高智能AI问世的前夕
人类究竟该何去何从呢？
至少在米拉看来
人类马上就要经历一场前所未有的变革了
作为ChatGPT和DALL•E等革命性AI技术的参与者之一
米拉的视角无疑具有重要的价值
在这次访谈中
她不仅深入探讨了这些AI技术背后的原理
还就AI发展过程中持续存在的安全和伦理问题
以及AI对未来可能产生的深远影响
提出了自己的见解
今天大飞就来分享一下这次访谈的核心内容
希望能对大家有所启发
先来谈谈当下AI的性能问题
毕竟，抢人类的工作也好
读博士卷死人类也罢
想要和人类同台较量
AI必须先要拥有足够的智能
那么如今的大模型在智能方面的表现究竟如何呢？
米拉相信，只需要一年半的时间
大模型的智能水平就能追上人类的博士水平
她在采访中表示
随着大模型拥有更多的训练数据、更多的计算资源
它的智能会稳定的变得更加聪明
而且这个扩展过程是相当线性的
如果用人类的学历来比喻AI升级的过程
那么像GPT-3这样的系统
我们也许可以说它有着幼儿级别的智能
而GPT-4这样的系统
更像是聪明的高中生
在接下来的大约一年半里，米拉觉得
未来大模型在特定任务上的智能水平将能够达到人类博士的水平
一年半的时间
有些观众可能觉得是夸大其词
但是实际上
市面上的一些大模型已经摸到了博士的门槛
前几天我们做了一期介绍Claude 3.5 sonnet的视频
根据Anthropic自己公布的数据
在研究生级推理（GPQA）、本科级知识（MMLU）和编码能力（HumanEval）上
Claude 3.5 Sonnet都出人意料地刷新了过去的记录
在MMLU 5-shot COT上
它的得分为90.4%，
在GPQA 5-shot COT上，得分为67.2%。
这也是首次
大语言模型突破了GPQA 65%的分数
达到了最聪明的人类博士的水平
要知道
普通博士在GPQA上的得分仅为34%，
而领域专业博士的得分也就是65%，
显然
Claude 3.5 Sonnet已经超越了专业博士的得分
虽然我们还不能就此来断言
大模型已经打败了人类博士
但是这组数据足以证明
米拉给出的时限也绝对不是夸大其词
甚至还略显保守了
那么如果米拉的预言成真
OpenAI的大模型、或者其他模型
在一年半后真的达到博士级别水平
它们会用自己超强的智能做什么呢？
它会不会像科幻小说中想象的那样
在见证了人类世界中的诸多丑恶后
决定毁灭人类呢？
在达特茅斯工程学院的采访中
主持人问了米拉这样一个问题
如果三年后GPT变得异常智能
它会不会自己决定连接互联网并开始行动呢？
对于这个问题
米拉给予了肯定的答复
她表示
Open AI的团队已经对于这种情况进行了很多思考
他们觉得，只要AI继续发展下去
具备高智能体能力的系统肯定会出现
这些AI甚至会结成社群
连接到互联网上
相互进行交流
并且共同完成某些任务
或者与人类无缝地合作
所以，未来人类与AI的合作
就像是今天我们之间彼此合作一样
至于AI在学会上网之后
会不会萌生出毁灭人类的想法
米拉表示
他们的安全团队会掐死任何AI威胁人类的可能
她认为
AI安全问题不是事后诸葛亮式的小聪明
可以草草了事，必须给予足够的重视
你不能只是开发技术
然后再想办法来处理这些问题
研究者必须在AI技术开发的过程中
就开始同步地制定AI安全方案
原因很简单，AI太聪明了
它聪明到足以理解人类设下的限制是什么
米拉用了一个有趣的比喻来说明
她说AI安全有点像训练一只聪明的狗
因为它很聪明
所以它更理解人类设下的护栏是什么
也就是说，在她看来
大模型的安全性和性能是相辅相成的
AI越聪明
也就越容易理解安全的重要性
米拉认为
关于模型性能更重要、还是安全能力更重要的辩论
其实是有误导性的
因为开发安全性和提升模型性能
其实是一回事
大模型越是聪明，它就是越是安全
当然，米拉也承认
目前的研究还不能百分之百的掌控AI
因为大模型的黑盒子特性
它们有时会展现出开发人员意料之外的能力
也就是所谓的“涌现能力”。
她也说不准大模型涌现出的能力是好是坏
会对人类的未来造成什么样的影响
因此，更进一步地确保AI安全
开发一套用于预测”涌现现象”的技术
对于未来的高智能AI是十分必要的
与此同时
Open AI也在努力为大模型塑造一套安全的价值观
米拉提到
大模型拥有一个默认的价值观系统
其中很多价值观是通过训练数据输入的
这些数据来自互联网、授权访问的数据、以及由人类标记的某些问题
每一个输入都有特定的价值观
而大模型则是这些价值观的一个集合
当AI作为产品被投入到使用中时
大量的用户会通过不断得使用
重塑模型的价值观集合
现在
ChatGPT的免费版本已经被全球超过1亿人使用
这些人中的每一个
都可以为ChatGPT提供反馈
如果人们允许OpenAI使用这些数据
米拉认为可以用它们来创建一个价值观的集合
让系统变得更好
更加符合人们的期望
当然
除了大模型本身的默认价值观系统之外
米拉还希望能够在未来
制作一个自定义的价值观系统
用户可以根据需要
在默认的价值观上面
建立一个自定义层
这样每个社区都可以有自己的价值观
比如学校、教堂和国家
用户可以在这个具有基本人类价值观的默认系统之上
提供更加具体和精确的价值观
米拉提到OpenAI开发了一个Spec功能
它提供了系统价值观的透明度
为了实现这个目标
米拉和她的团队正在建立一种反馈机制
收集如何改进用户的输入数据
来帮助大模型更加精准的认知
价值观随着时间产生的变化
不过，即便有了更好的安全技术
米拉依旧认为
创造零风险的AI是一件不可能的事情
在她看来
AI安全的关键是如何将风险降到最低
并且为人们提供工具来实现这一点
以政府和监管机构为例
眼下的AI安全开发
最重要的就是让政府和监管机构参与进来
企业需要为第三方访问者
提供早期足够的知识和权限
从而确保这些手握大权的机构
了解AI的现状
在出了问题之后，可以快速的响应
米拉表示
Open AI一直在倡导对前沿模型进行更多的监管
这些模型有着惊人的能力
也会因为误用而存在潜在的负面影响
米拉提到，白宫和联合国委员会
已经在一定程度上介入了Open AI的工作
除了第三方机构
公众也是维系AI安全的重要因素
米拉认为，ChatGPT最重要的贡献之一
是将AI引入到公众意识中
让人们对技术的能力和风险有一个直观的理解
从新闻或者短视频中了解到AI
与亲身体验过大模型的功能
是完全不同的两种概念
当一个人在自己的业务中尝试使用AI的时候
他就会看到AI的能力与局限性
从而更好的理解AI对于劳动力意味着什么
从某种意义上来说
米拉的说法并没有完全抹消AI毁灭人类的可能
也许
AI真的会变成一柄悬挂在所有人头顶的达摩克利斯之剑
这不仅仅是因为AI能像科幻小说里那样毁灭人类
就算是AI真的融入了人类社会
它对于当下人类社会的各种成文、或者不成文的规定
都会造成巨大的冲击
版权就是其中之一
无论是专有模型还是开源模型
它们训练所用的数据都是从互联网上获取的
通过这些数据训练出的绘画以及语音合成能力
很可能会存在版权争议
事实上，自从GPT问世以来
各种和AI相关的语音和肖像权问题
就一直闹的沸沸扬扬
米拉表示，语音确实是一个大麻烦
Open AI在语音技术方面做了大量的研究
即使直到最近才发布它们
结果还是因为它们带来了很多风险和问题
毕竟
声音的相似度是一个非常主观的问题
哪怕研究人员给出了两种声音的波长对比
证明它们不是一种声音
人们也可以主观的找到它们之间的相似度
米拉承认
这个问题目前没有什么特别好的解决办法
他们只能是一个声音、一个声音的去测试
如果某个音频被认为
非常接近一个知名人物的声音
那么就需要把它从产品里踢出去
而至于肖像侵权之类的问题
Open AI会让一些鉴别专家提前使用大模型
通过人工的方式来鉴别生成的图片
米拉表示
这种做法帮助可以他们很好地理解
部分内容可能会带来的风险
然后去制定一些应急措施
当公司认为万无一失之后
再把模型逐步向更多人开放访问
当OpenAI将种子用户数量扩展到一千人的时候
他们将与这一千名用户密切合作
收集反馈
并且理解之前没有被注意到的例外情况
为继续扩展到十万用户使用做好准备
然后是百万用户、上亿用户，等等
如果公司认为某些用例可能会带来侵权风险
那么他们就会选择雪藏这些有问题的用例
除了版权以外
虚假信息也是AI进入日常生活之后
可能会造成的负面影响之一
自从OpenAI成立以来
米拉和团队一直在研究虚假信息
并且创建了很多用来解决问题的工具
比方说水印、内容政策等等
让人们可以从工具层面来管理虚假信息的产出
米拉也承认
这是一个极具挑战的领域
毕竟她们只能从工具的层面
来限制虚假信息的产出
但是无法真正避免让用户接触到大模型生产的虚假信息
为了防范虚假信息
米拉和团队也在与民间社会、媒体和内容创作者合作
试图解决这些问题
比方说
当OpenAI在开发像音频或者Sora等技术的时候
他们就会主动选择与内容创作者合作
实际去了解技术是如何帮助他们的
以及如何去构建安全、有用而且对社会有益的产品
不过
尽管大模型会带来这样那样的麻烦
米拉依然愿意保持一个乐观的态度
来迎接AI时代的到来
在采访中
米拉谈到了不少AI在未来生活中的使用案例
比如说，草稿制造机
米拉认为，拥有足够高智能的AI
可以为任何事情先制定一套草稿
再交给人类打理真正需要创意的成品
这样就可以将很多繁琐的工作外包给AI来做
除了单纯的体力活
AI也可以当你的秘书
帮你选择最适合当前工作的工具
米拉提到
已经有很多工具连接到了核心模型上
让模型来处理繁琐的图像分析、文书工作或者代码编写
在人类通过AI使用这些工具的同时
AI也在一步一步地
将这些功能集成到自己身上
再添加到核心模型中
最终，模型将有能力来决定
何时使用分析工具、何时搜索未知的信息
等等等等
在米拉看来
AI象征着人类在下一个生产力阶段所使用的工具
她预计AI与人类的合作
会极大地扩展人类的创造力
因为在当代
创造力还是一种“奢侈品”，
无论是绘画还是音乐
又或者是拍电影
都还只是少部分人参与的事业
以及只有少部分人才能够享受到创作的快乐
米拉认为
AI将会大幅度降低创作的门槛
让每个人都参与到创作中来
好了
以上就是米拉-穆拉提在达特茅斯学院中分享的主要内容
她对于AI威胁论的乐观态度基本与CEO 萨姆奥特曼一致
二人都觉得AI不会威胁到人类
并且会和人类展开有益的合作
也许这就是Open AI的企业文化吧
大家赞同米拉对于未来AI的看法吗？
欢迎在评论区留言
感谢收看，我们下期节目再见
