大家好，这里是最佳拍档，我是大飞
上个月2月19日
被称为人工智能教父的Geoffrey Hinton教授
在牛津大学的年度Romanes讲座上
做了一个公开的演讲
题目为数字智能会取代生物智能吗
他从哲学的角度对AI的未来走向
提出了自己的一些严肃而且重要的思考
不仅讨论了人工智能的危险性
还有如何确保它不会控制人类并灭绝人类
他说
如果数字超级智能真的想要控制世界
我们不太可能阻止它
他还谈到了人工智能可能如何在劳动力市场中取代人类
以及如何被用来传播错误的信息
他之前认为人工智能系统可能需要长达100年的时间
才能变成“超级智能”，
但是他现在认为这可能比他预期的要早得多
整个演讲内容非常精彩
为了方便大家理解
大飞我总结了一些核心的观点
其实原视频时间也不长，总共37分钟
希望大家如果有时间
尽可能去看一下Hinton的原视频
首先，Hinton介绍了1950年以来
人工智能的两种研究范式
分别是逻辑启发式方法和生物启发式方法
逻辑启发式方法认为智能的本质是推理
是通过使用符号规则来操作符号表达式完成的
他们认为人工智能不要急着去“学习”，
当理解了如何表示事物之后
学习就很简单了
而生物启发式方法认为智能的本质
是在神经网络中学习连接的强度
不要着急去“推理”，
在学习完成后，推理自然就来了
接下来
Hinton介绍了一下神经网络和反向传播
并举了图像识别作为例子
以前神经网络经常被用来识别图像中的对象
现在
神经网络可以针对一个图片
产生一个对于图片的描述作为输出
这是很多年以来符号方法都没有做到的一件事
2012年
Hinton的两位学生伊利亚·苏茨克维尔Ilya Sutskever和亚历克斯·克里热夫斯基Alex Krizhevsky
在他的一点帮助下
展示了可以通过反向传播来制作一个非常好的神经网络
在有一百万张训练图片时
可以识别一千种不同类型的对象
伊利亚很有远见
他知道这个神经网络会在ImageNet竞赛中获胜
他是对的，而且他们赢得相当炸裂
他们的神经网络只有16%的错误率
而当时最好的视觉系统错误率也超过了25%。
然后
科学界发生了一件非常奇怪的事情
通常在科学界中
如果有两个竞争的学派
当其中一个学派取得一点进展时
另一个学派会说你的成果是垃圾
但是在这个案例中
由于差距足够的大
使得符号学派最好的研究者吉滕德拉·马利克Jitendra Malik和安德鲁·齐瑟曼Andrew Zisserman
直接转换了研究方向来做这个
其中安德鲁·齐瑟曼还给Hinton发送邮件说
这太神奇了
Hinton认为，在AI方面
生物方法显然占了优势
其次在语言处理方面
符号学派认为他们应该很出色
神经网络是无法处理语言问题的
很多语言学家也是这么想的
但是，事实表明
大型神经网络仅仅通过学习大量的文本
就能无师自通掌握了语言的语法和语义
统计学家和认知科学家认为这完全是疯狂的
曾经有统计学家说大模型有100个参数就可以了
训练一百万个参数的想法是愚蠢的
但是现在我们正在训练的参数是一万亿个
Hinton说
乔姆斯基（Noam Chomsky）曾说语言是天赋而非习得的
这很荒谬
虽然乔姆斯基曾经做出了惊人的贡献
但是他的时代已经过去了
接着
Hinton准备讲一下他在1985年第一个用反向传播训练的语言模型
可以说它是现在这些大模型的祖先
在这之前，关于人是怎么理解事物的
有两种理论
结构主义理论认为
一个词的意义在于它与其他词的关系
符号学派非常相信这种方法
你必须要通过一个关系图
其中有单词的节点和关系的弧线
才能捕捉他们之间的意义
而心理学理论认为
一个词的意义是一大堆特征组成的
比如“狗”有一大堆特征
它有生命、它是哺乳动物、它会捕食等等
但是他们没有说特征从哪里来
或者特征到底是什么
而Hinton在1985年的这个模型中
将两种理论统一了起来
有超过一千个权重
它不仅可以学习每个单词的特征
还可以学习单词的特征之间如何相互作用
从而预测下一个单词的特征
这就跟现在的大语言模型微调时所做的一样
最重要的是这些特征的交互
并不会有任何显式的关系图
如果非要弄个关系图
那么可以根据特征来生成
所以说，它是一个生成模型
知识存在于你赋予符号的特征中
以及这些特征的交互中
Hinton举了一个意大利家族的例子
来说明模型的工作原理
在符号方法中
对于回答“詹姆斯的妻子是谁”这种问题
符号人工智能是这么做的
它看到科林有父亲詹姆斯
科林有母亲维多利亚
它用规则推断出
詹姆斯有妻子维多利亚
而Hinton用一个神经网络
让它能学习到相同的信息
但是都是通过特征交互的方式
神经网络是怎么做的呢？
在神经网络中，有一个代表人的符号
一个代表关系的符号
符号通过一些连接变成了一个特征向量
随后这个网络学习到“詹姆斯”有一堆特征
“妻子”有一堆特征，推理的时候
它让这些特征相互作用
得出了“詹姆斯”的“妻子”应该有的特征
然后发现“维多利亚“最匹配这些特征
那么“维多利亚”就是”詹姆斯“的”妻子”了
虽然这个网络只有六个特征神经元
还是在一台每次浮点乘法需要12.5微秒的机器上运行
但是它学习了这个领域中所有显而易见的特征
比如说国籍特征和代际特征
也学会了如何让这些特征相互作用
就类似于匹配特征向量
如果一个向量与另一个向量相似
就让它更多影响
如果不相似则减少影响
最终可以生成输出
或者说，它以符号串作为输入
创建出了相应的特征
并且让这些特征之间进行交互
最终生成了符号串
但是它没有存储最后的符号串
就像GPT-4一样
它没有在长期知识中存储任何的词语序列
不过它将知识全部转化为权重
从中可以再生成序列
应该说
我们今天拥有的大型语言模型
都可以算是这个微小的语言模型的后代
他们遵循了基本的框架
拥有同样的基本理念
将简单的字符串转化为单词片段的特征
以及这些特征向量之间的交互作用
乔姆斯基学派认为这并不是真正的智能
它们只是一种被美化的自动完成功能
使用统计规律将人创造的文本片段拼贴在一起
但是大模型的工作原理显然不是这样
Hinton坚定地认为
在大模型中
数百万个特征以及特征之间数十亿次的交互
就是理解
大模型当然也是一种模型
只不过是一种人类以前从没见过的巨大的模型
它如此巨大
大到能够包容人类拥有的所有知识
我们人类总以为
我们知道自己是如何理解的
但是其实
大模型才是人类认识自己的最佳模型
大脑其实就是在给单词分配特征
并且让特征之间进行交互
因此
神经网络模型就是为了模拟人类理解而设计的一个模型
大模型正如大脑那样工作
也正如大脑那样理解
随后
Hinton聊了下对于大模型幻觉的看法
人们总说大模型有幻觉问题
只是在编造东西
但是心理学家从来不说大模型有幻觉
因为心理学家知道人类也经常编造东西
任何研究过记忆的人
都知道20世纪30年代的巴特利特的研究
都知道人们实际上就像这些大型语言模型一样
只是在虚构东西，对我们来说
真实记忆与虚假记忆之间并没有明确的界限
如果某件事是最近发生的
并且它与你理解的事情相符
你可能会大致正确地记住它
如果某件事是很久以前发生的
或者是比较奇怪的事
你就会记得不正确
而且你经常会非常自信地认为你记得正确
但是实际上你错了
Hinton举了约翰·迪恩的例子
约翰·迪恩在水门事件中宣誓作证
事后看来很清楚，他试图说出真相
但是其实他说的很多事情是完全错误的
他弄混了谁参与了哪次会议中
他弄错了某人说过的话
他关于会议的记忆完全是一团糟
但是他正确地把握了白宫当时正在发生的事情的要点
在乌尔里希·奈瑟讨论约翰·迪恩的记忆的一篇文章里
说他就像一个聊天机器人
只是在编造东西
但是他的话听起来是有道理的
他只是制造了一些对他而言不错的东西
如果有朋友看过我们之前的一期视频
就知道Hinton其实最担心的是人工智能的风险
这次他也谈了下
他担心人工智能会破坏民主
以及造成大规模的失业
但是这也要分情况
像医疗行业这种可以无限扩展工作量的行业
不会有失业，但是在其他领域
可能会有相当大的失业
人工智能还会产生致命的自主武器
它们将非常可怕
而且真的会自主运行，据他所知
美国计划到2030年将有一半的士兵是机器人
人工智能还将导致网络犯罪和蓄意制造流行病
但是Hinton真正担心的威胁
是长期存在的威胁
也就是说人类的灭绝
而且他不觉得这是科幻小说
人工智能有若干种方式将我们消灭
它们，尤其是智能代理
将会通过获得更多的权力
来实现更多对我们有益的事情
然后它们会更容易获得更多的权力
长此以往将能够操纵人们
任何打算关闭它们的人
都会被超级智能说服
再如果超级智能之间发生了竞争
有了自我保护意识，就会出现进化
具有更强的自我保护意识、能够获取最多资源的、更具攻击性的那个将获胜
然后就会遇到人类进化所面临的所有问题
也就是当人类从小的族群进化的时候
会与其他族群存在大量的侵略和竞争
最后
Hinton说了一个他在2023年初的顿悟
那就是他原来一直以为离超级智能还很远
大模型离人类的大脑也还差的很远
但是，由于他前两年所从事的工作
他突然开始相信
现在的大模型已经非常接近于大脑的水平
并且将变得比大脑更好
为什么呢
因为我们可以在不同的计算机上运行相同的程序
在不同的硬件上运行相同的神经网络
我们所需要做的就是保存权重
这意味着一旦你有了一些不会消失的权重
它们就是永生的
即便硬件损坏，只要你有权重
你可以制造更多的硬件并运行相同的神经网络
不过
数字计算的缺点是需要大量能量
但是我们可以通过进化硬件
让大模型的能量消耗降低
所以Hinton开始思考“有限计算”（mortal computation）的概念
也就是利用非常低功耗的模拟计算
来消除硬件和软件之间的差别
如果我们将神经活动表示为电压
将权重表示为电导
电压乘以电导就是单位时间内的电荷
然后电荷会自然相加
这样就可以通过将一些电压送给一些电导
来完成向量矩阵乘法运算
而下一层中每个神经元接收到的输入
将是这个向量与这些权重的乘积
这样计算的能效就要高得多
而且现在已经可以买到执行这种操作的芯片了
另一个相信的理由就是
GPT-4的权重只有人类的2%左右
但是却拥有比人类多上千倍的知识
为什么
因为它不是由一个模型实现的
而是由不同硬件上运行的大量相同模型的副本实现的
一千个副本都去互联网上查看不同的内容并学习东西
然后，通过平均梯度或平均权重
每个代理都掌握了其他代理学到的东西
这种沟通比人类要强太多了
虽然生物计算在进化方面非常出色
它只需要很少的能量
但是显然数字计算更优秀
因此Hinton认为，在未来的20年内
有50%的概率
数字计算会变得比我们更聪明
很可能在未来的一百年内
它会比我们聪明得多
很少有例子表明
更聪明的事物被不太聪明的事物所控制
如果人工智能变得非常聪明
并且有了自我保护的意识
它们可能会认为自己比我们人类更重要
好了
以上就是Hinton这次演讲的主要内容
虽然时间不长
但是非常清晰地表明了
他之前为何如此担忧人工智能的未来
甚至要离开谷歌
关于人工智能与人类的未来
是我们每个人都要面对和思考的事情
大家对Hinton的观点有什么看法呢
欢迎在评论区留言
感谢大家的观看，我们下期再见
