大家好，这里是最佳拍档，我是大飞
我们都知道如今AI发展的一个基石就是Scaling Laws
但是由于它是一个经验观察
所以近几年受到了很多质疑
而最近，谷歌旗下三大研究团队
谷歌Research、谷歌Search和谷歌DeepMind共同发现
之前谷歌提出的DiLoCo（Distributed Low - Communication Optimization）
也就是分布式低通信优化的训练方法
在训练大规模模型时具有更稳定的Scaling Laws
不仅带宽比数据并行训练少了几个数量级
甚至在小模型训练上也可能比数据并行要好
很有可能将改变大模型的训练方式
今天大飞就来给大家解读一下谷歌的这项研究
看看它到底会给AI领域带来哪些影响
在探讨谷歌的新研究之前
我们先来介绍一下Scaling Laws
它最早是由OpenAI团队在2020年的论文中详细阐述的
揭示了模型性能与模型规模、数据量和计算资源之间的数学关系
简单来说，就是在一定范围内
增加模型的规模、数据量和计算资源
模型的性能会相应提升
这个发现为大模型的发展提供了重要的理论依据
推动了人工智能领域的快速发展
许多大语言模型，比如GPT-3等
都是在这个理论的指导下不断地去扩大规模
从而展现出强大的能力
不过在实际应用中
Scaling Laws也面临着一些挑战
随着模型规模的不断增大
数据并行的训练方法作为大模型训练的基础技术之一
缺点也逐渐暴露出来
数据并行训练是让多个计算节点同时处理不同的数据批次
然后将计算结果进行汇总
再更新模型的参数
但是，当模型规模变得非常大的时候
这种方法就会带来巨大的通信开销和内存限制问题
想象一下
大量的数据在不同节点之间频繁传输
不仅需要消耗大量的时间
还对网络带宽提出了极高的要求
而且
每个节点都需要存储完整的模型参数
对于内存的需求也呈指数级的增长
这些问题严重限制了大模型的进一步扩展和训练效率的提升
为了解决这些问题
谷歌旗下的DeepMind全年研发出了DiLoCo方法
目的就是为了减少通信的开销并且提高扩展性
DiLoCo的核心思路是让每个模型副本独立训练一定数量的内部优化步骤
然后再通过外部优化步骤进行同步
并且在外部优化步骤之间引入动量机制
打个比方
就像是一群人各自在自己的小空间里先进行一些准备工作
然后再一起协调
这样就可以减少不必要的沟通成本
提高整体的效率
这种方法在理论上看起来非常有前景
最近更是通过对DiLoCo的进一步研究
发现了它的更多优势
在固定计算预算的情况下，首先
DiLoCo的超参数在不同模型规模下
表现得非常稳健而且可预测
这意味着无论模型规模如何变化
我们都能较为准确地设置超参数
从而保证模型的稳定训练
相比之下
传统的数据并行训练在模型规模变化的时候
超参数的调整往往比较复杂
难以把握
其次，随着模型规模的增大
DiLoCo相较于数据并行训练的优势更加明显
研究表明
DiLoCo的评估损失随着模型规模的增加
相对于数据并行训练有着显著的改善
根据Scaling Laws的预测
当使用2个模型副本、也就是M=2的时候
DiLoCo在模型参数达到几十亿以上的时候
损失会比数据并行更低
研究人员通过训练了一系列不同规模的模型
从3500万参数到100亿参数不等
验证了这一预测
在实验中
他们发现随着模型规模从2^25增长到2^31
使用1、2、4、8个模型副本的DiLoCo和数据并行的评估损失都在下降
但是DiLoCo的损失下降得更为明显
尤其是在M值较大的时候
这表明DiLoCo在处理大规模模型时
能够更好地优化模型性能
第三
DiLoCo在带宽需求方面也具有巨大优势
它所需要的带宽比数据并行训练少几个数量级
这对于在多个数据中心进行大规模模型训练来说
是一个至关重要的优势
在实际的分布式训练场景中
数据中心之间的网络带宽往往是有限的
DiLoCo能够大大降低对带宽的依赖
使得在多个数据中心训练超大规模模型成为可能
此外
DiLoCo能够容忍比数据并行训练大得多的批大小
在训练过程中
批大小的选择对模型性能有着重要影响
较大的批大小可以提高训练效率
但是传统的数据并行训练在批大小增大的时候
容易出现性能下降的问题
而DiLoCo则不同
它不仅能够支持更大的批大小
而且在批大小变化的时候表现更加稳定
实验结果显示
DiLoCo提高了最佳批大小
并且最佳全局批大小可以随着副本数M的增加而变大
例如，在5500万参数的模型训练中
数据并行训练在批大小较小时的评估损失最低
而DiLoCo在批大小更大的时候表现更优
在HellaSwag数据集上的零样本准确率测试中
即使在较小的模型规模下
DiLoCo在M = 2的时候
也能在更大的全局批大小下实现更高的准确率
这说明DiLoCo在横向扩展能力上比数据并行训练更加具有优势
为了验证这些发现
研究人员还进行了一系列严谨的实验
在实验中
他们采用了类似Chinchilla的纯解码器Transformer架构
并加入了QK - LayerNorm技术
QK - LayerNorm是一种改进的层归一化技术
主要用于Transformer架构中的自注意力机制
它可以降低模型对于学习率的敏感性
使得训练更加稳定
同时
研究人员还使用了z - loss正则化来进一步提高训练的稳定性
实验使用的词汇量为32768个
其中包括32000个词汇表内的单词
以及一些额外的标记
用来表示句子开头和词汇表外的内容
研究人员将多个序列打包到每个批次中
最大序列长度全程固定为2048
接着他们训练了一系列不同规模的模型
这些模型的Transformer层数、注意力头的数量、QKV维度和前馈层的隐藏维度各不相同
例如
3500万参数的模型有6层Transformer层、8个注意力头、QKV维度为512、前馈层隐藏维度为2048；
而100亿参数的模型则有48层Transformer层、64个注意力头、QKV维度为4096、前馈层隐藏维度为16384
在训练过程中
研究人员使用了C4数据集的训练集来训练模型
并且用C4的验证集作为评估指标
此外
他们还在三个下游任务HellaSwag、Piqa和Arc-Easy上进行了零样本评估
从而来更为全面地评估模型的性能
在优化器的选择上
数据并行训练和DiLoCo的内层优化都使用了AdamW优化器
β1设为0.9，β2设为0.99
训练开始时会有1000步的预热
然后采用余弦学习率衰减
权重衰减参数λ设为T⁻¹
其中T是总训练步数，到训练结束时
学习率衰减到峰值的5%。
为了保证训练稳定
他们将内层梯度的全局范数剪裁到1
外层梯度不剪裁
对于DiLoCo的外层优化
研究人员使用了带Nesterov动量的SGD
动量设为0.9，外层学习率保持不变
在实验环境方面
研究人员在谷歌张量处理单元TPUv5e和TPUv6e上进行了大部分实验
并且在TPUv-5上进行了最大规模的实验
他们假设模型在跨多个数据中心进行训练
在数据中心的内部使用高带宽网络
而跨数据中心则分别设置了高带宽、中带宽和低带宽的网络环境
以此来模拟不同的实际场景
通过这些实验
研究人员得到了许多关键的发现
在评估损失方面，当副本数M = 1时
DiLoCo在不同模型规模下获得的评估损失都比数据并行训练低
例如
在3500万参数、5500万参数、13亿参数和24亿参数的模型实验中
DiLoCo的评估损失
始终低于数据并行训练
并且随着批大小的增加
两者之间的差距进一步扩大
在HellaSwag零样本准确率的测试中
DiLoCo同样优于数据并行训练
而且DiLoCo（M = 1）的表现对批大小的稳定性更强
即使批大小翻倍甚至翻四倍
对数据并行的性能影响很大
但是对DiLoCo几乎没有影响
在批大小对性能的影响方面
DiLoCo显著提高了最佳批大小
并且最佳全局批大小随着副本数M的增加而增大
这意味着DiLoCo在横向扩展能力上有了很大的提升
研究人员通过对不同规模模型在不同批大小下的评估损失和零样本准确率进行测试
验证了这一结论
在几乎所有情况下
DiLoCo在较大批大小下的表现都优于数据并行训练
尤其是在模型规模较大时
这种优势更加明显
关于外部学习率
研究发现最佳外部学习率基本上与模型的规模N无关
但是会随着副本数M的变化而变化
对于足够大的模型
也就是N≥3.35亿参数
每个M的最佳外部学习率是固定的
而且M越大
最佳外部学习率似乎也越大
这个结果与之前联邦学习的研究是一致的
表明外层学习率应该随着客户端的数量
相当于DiLoCo中模型的副本数量的增加而增加
这使得DiLoCo在水平扩展上更加自然
缩短总训练时间
此外
DiLoCo不仅在正常训练中表现出色
在处理过度训练的问题上也有独特的优势
过度训练可能会导致模型过拟合
并且消耗大量的计算资源
然而
DiLoCo通过增加批大小并且减少通信量
使得在相同的时间内可以进行更多的过度训练
研究表明
使用DiLoCo通常可以在相同时间内
进行4倍于数据并行训练的过度训练
这为研究模型的性能边界提供了更有力的工具
好了
以上就是对DiLoCo所展现出来的Scaling Laws的介绍了
从训练方式来看
DiLoCo为大规模模型训练提供了一种更为高效、更具扩展性的方法
它解决了数据并行训练在通信和内存方面的瓶颈问题
使得在多个数据中心训练超大规模模型成为可能
不过
这个研究也应该引起我们对于AI未来发展的更多思考
目前
AI模型的发展在很大程度上依赖于Chinchilla模式
也就是通过投入大量的计算资源和数据来提升模型的性能
但是
这种模式如今也面临着许多挑战
一方面，前期投入巨大
科技大厂需要疯狂建造数据中心
购买大量的英伟达GPU，耗费巨额资金
另一方面，随着投入成本的激增
性能增益却可能越来越小
性价比正在下降
而且，训练数据可能正在枯竭
高质量数据的供应有限
而AI对数据的需求却越来越大
与此同时，新型“推理模型”正在兴起
像OpenAI的o1、o3，DeepSeek R1
谷歌Gemini 2.0 Flash Thinking等新模型
都采用了Test Time Compute
不再依赖长时间的预训练
另外
混合专家模型MoE也逐渐成为一种被广泛采用的技术
它通过训练多个小型“专家”模型
让它们与大模型协同工作
只在需要时调用部分算力
从而降低了基础设施需求
在这样的背景下
Chinchilla模式的未来充满了不确定性
巴克莱资本的顶级分析师罗斯·桑德勒就指出
未来AI行业可能面临两种截然不同的情景
一是“Chinchilla”继续主导
巨额算力和数据投入持续攀升
二是增长“停滞”，
新型技术和模型可以以更少的资源来实现更强的性能
这两种路径的资本支出差距高达3万亿美元以上
足以影响整个行业的走向
如果推理模型、MoE技术等不断成熟
AI可能走向轻量化、高效率的未来
数万亿美金的基础设施投资或许不再必要
但是如果“合成数据”技术取得突破
让Chinchilla模式重焕生机
那么算力竞赛可能会再次卷土重来
大飞我觉得，以目前的形式来看
这两种趋势可能会同时并存
一方面资本巨鳄会继续推动算力和基础设施的投入
另一方面也会有很多小公司借助算法和工程创新来降低成本
那大家是如何看待这个趋势的呢？
欢迎在评论区留下自己的看法
感谢大家观看本期视频
我们下期再见
