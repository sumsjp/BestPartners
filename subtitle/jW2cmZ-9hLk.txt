大家好，这里是最佳拍档，我是大飞
如今大语言模型的参数规模是越来越大
像GPT-3这样拥有1750亿参数的巨无霸
虽然能够展现出惊人的智能水平
但是对硬件资源的要求也是高得离谱
即便在FP16精度下也需要325GB的存储空间
推理时至少得用五张80GB内存的A100 GPU
想要在移动嵌入式设备等资源有限的场景下
运行这样的大模型
更是天方夜谭
为了在保持模型能力的同时
给这些巨无霸瘦身
于是就产生了模型压缩技术
今天咱们就来简单聊聊模型压缩的四大核心技术
量化、剪枝、蒸馏和二值化
提前声明
本期视频只是普及基础性的知识
不涉及复杂的论文解读或者公式推导
专业人士或者已经非常了解的观众
可以直接跳过本视频
我们刚才已经说过
其实模型压缩的目标很明确
就是在保证性能不怎么下降的前提下
大幅减少大型预训练模型的存储空间和计算量
一方面
我们要考虑把模型的存储空间从GB甚至TB
压缩到MB甚至更小
另一方面，要降低模型的计算复杂度
减少浮点运算次数FLOPS
让模型的推理速度更快
同时，还要优化模型的结构
让它能更好地适配GPU、NPU等硬件设备
提高资源利用率，降低能耗
最重要的是
压缩后的模型在实际应用中
得和原来的大模型性能差不多
不能“减肥”之后就变得“肌无力”了
那有了目标之后
接下来我们就来了解一下具体的技术
首先说量化
量化在模型压缩里可以说是关键的一环
它的核心原理是减少表示每个权重所需的比特数
在传统的深度学习模型里
权重通常是用数字
比如32位浮点数来存储的
但是这就好比用一个大箱子来装一个小物件
既占地方，计算起来也麻烦
而量化就像是把这些“大箱子”换成合适的“小盒子”，
把浮点数权重转换成8位、4位甚至1位的整数
这样一来
模型的存储空间和计算量都能大幅的减少
根据范霍克（Vanhoucke）等人的研究
8位参数量化在损失最小精度的情况下
可以把模型的存储空间
压缩到原来的四分之一
计算复杂度也大大降低
因为低精度的整数运算
比浮点运算要高效得多
量化的方法主要有三类
分别是训练后量化、量化感知训练和量化感知微调
训练后量化操作简单
模型训练完之后
直接对权重进行量化
像谷歌的TensorFlow Lite就提供了这样的工具
可以把32位浮点的权重
量化成8位整数
轻松减少模型的存储空间
但是它有个缺点
就是由于量化时没考虑训练中的动态变化
所以可能会让模型的精度下降
量化感知训练则是在模型训练过程中就引入量化操作
让模型提前适应低精度的表示形式
英伟达的TensorRT就支持这种方法
可以在训练时模拟量化
从而让模型量化后还能保持较好的性能
不过，这种方法的训练过程更为复杂
需要更多的计算资源
而量化感知微调则是基于预训练模型进行微调
同时加入量化操作
这种方法结合了预训练模型的优势和量化技术的高效性
在一些自然语言处理任务中
研究人员经常会在预训练的BERT模型上进行量化感知微调
它能够快速适应特定的任务
减少存储和计算开销，但是性能上
可能比不上从头开始训练的量化感知训练模型
应该说，量化技术的优势很明显
它能显著减少存储空间
像1位量化甚至能减少到原来的三十二分之一
计算效率也能大大提高
比如8位量化能够在不明显降低精度的情况下
把推理速度提升2 - 3倍
相应的，能耗也会跟着降低
从而让移动和嵌入式设备续航更久
而且量化后的模型在GPU、NPU等硬件上也能运行得更好
充分发挥硬件的并行计算能力
但是量化也有它的局限性
首先精度损失是个大问题
尤其是使用1位或2位这种低精度量化时
精度会下降很多
其次
不同模型对量化的敏感度不一样
有些模型量化后性能依旧出色
有些却会大幅下降
另外像量化感知训练和量化感知微调
还得修改训练过程
从而增加了训练难度和计算资源需求
而且从实际情况来看
虽然现在硬件对低精度运算的支持好很多了
但是还是有部分的硬件平台
对量化模型的支持不够完善
接着我们来讲讲剪枝
剪枝的原理是去除神经网络里不重要的连接或者神经元
神经网络在训练的时候
就像一个人学习知识
难免会记住一些冗余信息
这些对最终结果影响不大的部分
就可以去掉
从而达到压缩模型的目的
像韩松（Han）等人就提出了基于权重重要性的剪枝方法
通过分析权重对模型输出的影响
把那些影响小的权重“剪掉”，
在不怎么降低模型性能的同时
大幅减少模型参数数量
剪枝方法主要可以分为非结构化剪枝和结构化剪枝
非结构化剪枝指的是随机移除单个权重或者连接
就像在一片果林里
随机摘掉一些果子
它的好处是能实现很高的压缩比
可以精准去除对模型输出影响最小的权重
但是问题也很明显
那就是产生的稀疏结构在硬件上很难高效实现
因为硬件更喜欢规整的矩阵操作
在一些实验中
非结构化剪枝能够把模型参数减少50%以上
但是推理速度的提升并不明显
结构化剪枝则是按照一定规则
比如移除整个神经元、滤波器或者层
就像把果林里的一整排果树砍掉
这种方法产生的稀疏结构更适合硬件加速
像滤波器级剪枝就可以去掉卷积层中不重要的滤波器
降低卷积操作的计算量
虽然结构化剪枝的压缩比
可能比不上非结构化剪枝
但是在提升推理速度方面表现更好
在某些卷积神经网络中
能把推理速度提高2到3倍
剪枝技术的优点也不少
它不仅能减少模型的大小
比如在一些实验里
模型参数能减少50% 到80%。
像结构化剪枝还能提高推理速度
降低能耗
这对于移动和嵌入式设备非常友好
而且，由于剪枝去掉了冗余信息
所以能够降低模型的过拟合风险
提高泛化能力
让模型在没见过的数据上也能表现得更好
剪枝的缺点在于
首先还是绕不开的精度损失问题
尤其是剪枝比例较高的时候
另外一些剪枝方法还需要修改训练过程
增加了训练的复杂性和计算资源需求
虽然结构化剪枝更适合硬件加速
但是不同硬件平台对剪枝后模型的支持程度不一样
可能还需要额外的优化
同样
不同模型对剪枝的敏感度也不同
有些模型剪枝后性能依旧稳定
有些则会出现较大的波动
我们再来看看蒸馏技术
这也是最近引起DeepSeek风波的主要原因
蒸馏严谨来说应该叫知识蒸馏
简单来说
知识蒸馏就是把大型的复杂模型
也就是教师模型的知识
迁移到小型的简单模型
也就是学生模型
教师模型的参数多、结构复杂
能够学到丰富的特征和模式
但是难以部署在资源受限的设备上
而学生模型通过学习教师模型的输出
比如软标签、中间特征等等
能够在保持小巧身材的同时
尽可能接近教师模型的性能
举个例子，在图像分类任务中
教师模型可能是深度为50层的ResNet
它输出的软标签包含了对每个类别的置信度信息
学生模型可以是一个较浅的网络
通过学习这些软标签
就能学到更丰富的类别区分信息
从而在分类任务中表现得更好
知识蒸馏的流程一般会包括几个步骤
首先要训练一个大型复杂而且性能优异的教师模型
让它在大规模数据集上学习
积累丰富的知识
然后选择一个较小的模型作为学生模型
并且进行初始化
不过这个学生模型虽然结构简单
但是要有一定的学习能力
接着进入蒸馏训练阶段
把教师模型的输出当作额外的监督信息
和学生模型的输出对比
通过优化损失函数来训练学生模型
损失函数通常会包含两部分
一部分是学生模型的原始损失
比如交叉熵损失
另一部分是学生模型输出和教师模型输出之间的差异
经常会用KL散度来衡量
最后在蒸馏训练结束后
可以对学生模型进行微调
进一步提升它的性能
其中
温度参数相当于知识蒸馏的“难度调节旋钮”，
在高温模式下
可以学习到复杂的关联关系
而在低温模式
答案会接近原始分布，适合简单任务
当然也可以考虑动态策略
也就是初期用高温广泛吸收知识
后期降温再聚焦关键的特征
像是在自然语言处理领域
研究人员经常会把预训练的BERT模型当作教师模型
把轻量级的LSTM模型作为学生模型
经过蒸馏训练
学生模型能够学到教师模型的语言表示能力
在文本分类、情感分析等任务中取得不错的成绩
蒸馏技术的优势很突出
它的模型压缩效果十分显著
学生模型的参数量能减少到教师模型的十分之一甚至更少
推理速度还能提高好几倍
因为教师模型的软标签中包含丰富的类别区分信息
所以学生模型在学习后
泛化能力更强
能够更好地应对新的数据
而且知识蒸馏的应用范围很广
不管是图像分类、目标检测
还是自然语言处理任务
都能通过它实现模型压缩
但是蒸馏技术也有一些不足
那就是学生模型的性能
很大程度上依赖教师模型的质量
如果教师模型表现不好
学生模型也很难出彩
蒸馏训练还要同时考虑教师模型和学生模型的训练过程
此外
虽然蒸馏能够在一定程度上保持模型性能
但是在一些复杂任务中
学生模型的精度还是会略低于教师模型
更何况
选择合适的教师模型和学生模型也是个挑战
不同的模型组合
蒸馏效果可能相差很大
最后，我们来了解一下二值化技术
二值化是一种极端的量化技术
它把神经网络中的权重和激活值限制在了两个值上
通常是+1和-1
这就好比把所有的选择都简化成了“是”和“否”，
极大地减少了模型的存储空间和计算复杂度
因为二值化的权重和激活值用1个比特就能表示
而不是传统的32位浮点数
所以存储空间一下就减少了32倍
二值化网络的计算过程也简单很多
由于权重和激活值只有两个可能的值
所以乘法运算可以用简单的加法和位移操作替代
大幅提升计算效率
二值化网络的卷积运算可以通过同或XNOR和位操作实现
而这些操作在硬件上的执行效率非常高
二值化方法非常适合用在资源受限的设备上
像库尔巴里奥（Courbariaux）等人提出的BinaryConnect方法
不仅能够大幅减少模型的存储空间和计算复杂度
在一些简单的图像分类任务中还能保持较高的精度
而拉斯泰加里（Rastegari）等人进一步提出了XNOR-Net
不仅将权重二值化
还把输入激活值也二值化了
计算效率更高
在实际应用中
二值化网络能在一些低功耗的物联网设备里
实现高效的图像识别和语音识别功能
而且不需要复杂的硬件支持
在自然语言处理任务里
研究人员也能通过二值化技术来压缩预训练的Transformer模型
显著减少模型的存储空间和推理延迟
二值化技术的优点很明显
首先是它的压缩率极高
同时也带来推理速度的提升
在一些实验中
二值化网络的推理速度能够提高10倍以上
而且二值化网络很适合硬件优化
比如使用专用的二值化硬件加速器
当然，二值化的缺点也很明显
那就是精度损失比较严重
尤其是在复杂任务中
此外
二值化训练需要特殊的技巧和方法
比如用直通估计器来处理不可导的二值化操作
不同模型对二值化的敏感度也不同
有些模型二值化后性能还不错
有些则会大幅下降
好了
以上就是对这四种技术的基本介绍了
各有各的优缺点
在实际应用中
我们需要根据不同的场景需求
来选择合适的模型压缩技术
比方说，如果是在资源受限的场景
二值化和量化是优先考虑的对象
二值化更适合精度要求不高的任务
而量化则可以通过调整量化精度
在压缩效果和模型性能之间找到平衡
如果更注重计算效率
希望显著提高模型的推理速度
同时对模型精度还有一定要求
那么量化和结构化剪枝是不错的选择
如果目标是在保持较高模型性能的前提下进行压缩
那么知识蒸馏就是理想之选
展望未来
模型压缩技术还有很大的发展空间
一方面
综合使用多种模型压缩技术可能会成为主流方向
比如把量化和剪枝结合起来
或者在蒸馏过程中引入量化感知训练
另一方面，随着硬件技术的进步
模型压缩技术和硬件设计会结合得更紧密
另外，现在的模型压缩过程
还需要研究人员有较高的专业知识和丰富的经验
手动来选择压缩策略和参数配置
但是未来
如果有更加智能和自动化的模型压缩工具
能够根据用户的需求和硬件环境
自动选择最优的方案
也将大大降低模型压缩的门槛
希望今天的分享
能让大家对模型压缩的四种技术
量化、剪枝、蒸馏和二值化有一些基本的了解
后续大飞我会再介绍一些更为深入的细节
感谢大家的观看
我们下期再见
