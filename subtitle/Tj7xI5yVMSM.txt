大家好这里是最佳拍档
我是大飞
过去这十年对于人工智能
可以算是一段激动人心
但是又荆棘丛生的坎坷之路
从对深度学习潜力的浅浅探索
到最终引发了AI全领域的爆发式增长
于是有了电子商务中的推荐系统
有了自动驾驶汽车上的对象检测功能
也有了从创建逼真的图像
到输出连续文本的强大生成式模型
在这个视频中
我们将沿着记忆的轨迹回到过去
重新审视AI一路走来的关键性突破
无论你是经验丰富的AI从业者
还是单纯对现在
铺天盖地的AI宣传感兴趣的小白
这都将是一段值得体验的回忆旅程
2013年如今被广泛认为是深度学习的成熟元年
推动这股浪潮的
是计算机视觉的重大进步
杰弗里辛顿在最近一次采访中也坦言道
到2013年 几乎所有的计算机视觉研究
都开始转向神经网络
而这一波繁荣期的开端源自于一年前
图像识别领域的一项令人惊讶的突破
2012年9月
基于深度卷积神经网络CNN的AlexNet
在ImageNet大规模视觉识别挑战赛
ILSVRC上创下了新的历史记录
用真凭实据证明了
深度学习在图像识别任务中的潜力
其TOP5错误率达到了15.3%
比位列第二的竞争对手低出10.9%
这个大获成功的底层技术
不仅基本划定了AI的未来发展方向
同时也极大地扭转了
人们对深度学习的认知方式
首先作者使用了一个由5个卷积层
加3个完全连接的线性层
组成的深度CNN
在当时呢很多人觉得
这样的架构设计完全是不知所谓
此外由于网络深度会产生大量的参数
所以训练选择在两个图形处理单元
也就是GPU上并行进行
这也是GPU与大规模训练数据集
成为黄金搭档的起点
通过将传统的激活函数sigmoid等等
替换成为效率更高的整流线性单元ReLU
训练时间得到了进一步的缩短
这些进步共同促成了AlexNet的成功
也标志着AI历史上的一个转折点
成功激起了学术和技术界
对深度学习的浓厚兴趣
正因为如此
许多人将2013年
视为深度学习真正开始腾飞的转折点
2013年的另一件大事
则是变分自动编码器简称VAE的出现
尽管在风头上它无法与AlexNet相媲美
但是作为一种能够学会表示
和创作图像和声音等数据的生成式模型
VAE同样具有非凡的意义
他们会在低位空间
即所谓的潜在空间中
学习输入数据的压缩
来对潜在空间进行采样
借此学习并生成新的数据
VAE的诞生
为生成式建模和数据生成
开辟了新的路径
也开始在艺术设计和游戏等领域得到应用
2014年6月
随着伊恩·古德费洛（Ian Goodfellow）
及其同事推出的生成式对抗网络GAN
深度学习领域再次取得了重大的进展
GAN是一种神经网络
能够生成与训练及内容类似的
新的数据样本
从本质上讲
GAN会同时训练两个网络
一个是生成器网络
负责生成伪造或者说是合成的样本
一个是鉴别器网络
负责评估生成器输出的真实性
整个训练过程类似于游戏的设置
生成器努力生成更逼真的样本
来骗过鉴别器
而鉴别器则努力识别出那些虚假的样本
在当时GAN代表着一种强大
并且相当新颖的数据生成工具
不仅可以用于生成图像和视频
也可以应用在音乐和艺术创作中
GAN甚至让不依赖于显式的标注
就生成高质量的数据样本成为了可能
为无监督学习的发展做出了贡献
为这片有待开发极具挑战的蓝海
开辟了新的航道
时间来到2015年
AI领域在计算机视觉和
自然语言处理NLP方面
继续迎来长足的进步
何恺明及其同事发表了一篇题为
《图像识别中的深度残差学习》的论文
介绍了残差神经网络即ResNet的概念
简单来说呢
这是一种通过添加快捷方式
来让信息更容易地通过网络流动的架构
跟常规神经网络中
每一层都将前一层的输出作为输入不同
Resnet中添加了额外的残差连接
这些连接会跳过一个或者多个层
并直接接入到网络中的更深的层
如此一来
Resnet成功解决了梯度消失的问题
让训练更深层的神经网络成为了可能
这是一项打破当时固有认知的巨大成就
以此为基础
AI在图像分类和对象识别任务中的表现
迈上了新的台阶
大约在同一时间
研究人员在循环神经网络RNN
和长短期记忆LSTM模型的开发方面
也取得了长足的进步
虽然这些概念自从1990年代起就已经存在了
但是相关模型
直到2015年左右才真正的引起轰动
这主要是因为几点原因
第一点到这个时候
规模更大多样性更强的训练数据集才开始出现
第二点
算力水平和硬件配置的持续改进
能够训练深度更大复杂度更高的模型
第三点在此期间技术本身也有了改进
例如引入了更复杂的门控机制
这些架构能够让语言模型
更好的理解文本的上下文和含义
极大的改进了在语言翻译
文本生成和情感分析等任务中的表现
当时RNN和LSTM带来的成功
也为我们如今熟悉的大语言模型的发展
铺平了道路
继卡斯帕罗夫1997年负于IBM深蓝之后
又一场举世瞩目的人机大战
在2016年拉开了帷幕
这次的双方选手
分别是谷歌的AlphaGo
与围棋世界的冠军李世石
李世石的失败也标志着
AI发展史上的又一个重要里程碑
它证明了在曾经被认为过于复杂
而无法由计算机处理的项目中
机器智能也已经能够击败
哪怕最强大的人类选手
AlphaGo采用的是深度强化学习
加蒙特卡洛树搜索的设计逻辑
分析了围棋领域的数百万场对局
并最终评估出每一步的
潜在的最佳的走法
这种级别的布局谋篇能力
已经远远超越了人类的决策极限
2017年可以说是AI发展的关键一年
为我们如今耳熟能详的生成式AI
这一历史性突破奠定了基础
2017年12月
瓦斯瓦尼（Vaswani）及其同事发表了
基础研究论文
《Attention Is All You Need》
其中介绍了利用自注意力概念
处理顺序输入数据的Transformer架构
这种架构能够更有效的处理
距离较远的依赖关系
解决这一长期束缚传统RNN架构的挑战
Transformer由两个基本组件构成
编码器与解码器
其中编码器负责对输入的数据进行编码
例如对单词序列做编码
之后他会获取输入序列
并运用多层自注意力和前馈神经网络
借此捕捉句子中的关系特征
并学习有意义的表示
本质上自注意力机制
使得模型能够理解语句中
不同单词间的关系
而且跟以往按照
固定顺序处理单词的传统模型不同
Transformer其实是同时检查所有单词
并且根据每个词
跟句子中其他词的相关性
为各个词分配所谓的“注意力得分”指标
另一方面解码器则从编码器
获取编码之后产生输出序列
在机器翻译和文本生成等任务中
解码器会根据从编码器处接收到的输入
生成经过翻译的序列
跟编码器类似
解码器同样由多层自注意力
加前馈神经网络组成
但是解码器还包含额外的注意力机制
用于专注的处理编码器的输出
保证解码器在生成输出时
可以考虑到来自输入序列的相关信息
从此开始
Transformer架构开始成为
大语言模型开发中的关键组成部分
也标志着自然语言处理领域
全面飞跃的起点
机器翻译、语言建模
和聊天问答由此翻开了新的篇章
在瓦斯瓦尼等人发表论文的几个月后
OpenAI于2018年6月
发表了新的基础研究论文
《Generative Pretrained Transformer》
也就是大名鼎鼎的GPT-1
研究利用Transformer架构
成功捕捉到了
文本中相距较远的依赖关系
在对特定于自然语言处理任务
进行微调之后
GPT-1成为首批能够证明无监督
预训练有效性的模型之一
谷歌自然不会错过
当时还相当新颖的Transformer架构
于2018年底发布
并开源了自己的预训练方法全称为
Bidirectional Encoder Representations from Transformers
简称BERT
与以往的单向文本处理模型
包括GPT-1不同
BERT会同时在两个方向上
考虑各个单词的上下文
为了更好的理解这个重要概念
论文作者提供了一个直观的示例
在“我访问了银行账户”这句话中
单向的上下文模型
会将我访问的对象表示为银行而非账户
但是BERT却能够结合前后的上下文
正确将句子表示成我访问了某个账户
具体来讲
BERT从深度神经网络的最底层开始
实现了深度的双向解释
这里双向的概念非常强大
也让BERT在各种基准测试中
成功碾压了其他当时最先进的自然语言处理系统
除了GPT-1和BERT之外
图神经网络GNN在当年
也引发了不小的轰动
在设计上
这是一类专门用于处理
图形数据的神经网络
GNN利用消息传递算法
在图的顶点和边之间传递信息
使得网络能够更直观的学习
数据的结构和关系
这项工作让AI模型能够从数据中
提取更为深入的见解
扩大了深度学习所适应的问题范围
借助于GNN
AI在社交网络分析
推荐系统和药物发现等领域
再次取得了重大的进展
2019年的生成模型继续一路过关斩将
尤其是GPT-2的亮相更加令人眼前一亮
通过在诸多的自然语言处理
任务中的先进性能
这套模型取得了令同行望尘莫及的成绩
此外它还能够生成极为顺畅的文本
事后看来
如今的GPT-3和GPT-4所取得的辉煌成就
在当时就早有预兆了
领域内的其他改进包括Deep Minde的Big GAN
它生成的高质量图像
几乎与真实的图像没有任何区别
还有英伟达的Style GAN
它能够更好的控制生成图像的外观效果
总的来说
这些开启了生成式AI时代的成果
继续推动着AI时代的边界
而颠覆一切的大爆发
也在悄悄的积蓄着力量
此后不久
AI模型界的新宠儿呱呱坠地
甚至在科技行业之外
也引发了极强的破圈效应
它就是GPT-3
这个模型标志着大语言模型
在规模和能力上的重大飞跃
家族的老大哥GPT-1只有区区1.17亿参数
GPT-2的参数量上升至15亿
而GPT-3这个数字疯狂增长至1,750亿
更大的参数空间
让GPT-3能够根据不同提示和任务
生成极为连续的文本
也在各种自然语言的处理场景下
表现出令人印象深刻的性能
包括文本补全聊天问答
甚至是创意写作等等
此外GPT-3再次强调了
自监督学习技术的潜能
他允许模型利用大量未标注的数据进行训练
从而获得对语言的广泛理解
由于无需针对特定的任务进行大量的训练
所以模型的经济性也有了改善
从蛋白质折叠
到图像生成和自动编码辅助
随着AlphaFold 2、DALL-E
和GitHub Copilot的相继问世
2021年注定是AI领域不平静的一年
AlphaFold 2被誉为攻克
困扰人类数十年之久的
蛋白质折叠问题的破局利器
DeepMind的研究人员
扩展了Transformer架构
并打造出了evoformer的模块
即一个可以利用进化策略
进行模型优化的架构
最终构建起了一套
能够根据蛋白质的一维氨基酸序列
预测其3D结构的模型
这一突破具有巨大的潜力
有望彻底改变药物发现、生物工程
甚至是人类对于生物系统的理解方式
这一年OpenAI还发布了DALL-E
并再次成为了新闻的焦点
从本质上讲
这套模型将GPT风格的语言模型
和图像生成概念结合了起来
能够利用文本描述
创造出高质量的图像
这套模型到底有多强
大家不妨参考一下这个图
这就是当时DALL-E根据提示词
用油画风格描绘科幻世界中的飞行汽车
所生成的结果
最后呢GitHub发布了如今
每位开发人员
都耳熟能详的编程好伙伴Copilot
这项工作呢由GitHub与OpenAI合作实现
由OpenAI提供底层的语言模型Codex
Codex模型利用了大量公开可用的代码语料库
进行训练
借此掌握了理解
并且生成各种编程语言代码的能力
开发人员只需简单通过代码注释
来说明自己想要通过
Copilot的解决的问题
Codex模型就会返回相应的代码建议
Copilot的其他功能还包括
按照自然语言描述生成代码
以及在不同编程语言间翻译代码内容
过去十年间
快速发展的AI终于踢出临门一脚
OpenAI的ChatGPT是一款聊天机器人
于2022年11月正式发布
这款工具代表着
自然语言处理领域的前沿成就
能够生成连续
而且与上下文相关的响应结果
同时具备极强的查询和提示适应能力
此外它还能够与用户对话、提供解释
输出创意建议、协助解决问题
编写和解释代码
甚至模拟不同的个性或者是写作风格
凭借简单直观的聊天机器人交互界面
ChatGPT在世界各地的不同群体中
都激发了一波体验浪潮
以往最新的AI发明
大多都只是技术社区内部的小玩具
但是如今AI工具已经渗透到了
几乎所有的专业群体
包括软件工程师、作家、音乐家和广告商
不少企业还利用这套模型
推动服务的自动化改造
例如客户支持、语言翻译
或者常见问题的解答
事实上
这波自动化冲击来的太快太猛
已经引发了人们的担忧
以及哪些岗位可能会被AI自动化
所淘汰的全民大讨论
虽然ChatGPT在2022年堪称绝对的主角
但是图像生成领域
也一刻没有停止过发展的脚步
Stability AI在这一年发布了Stable Diffusion
这是一种基于潜在空间的文本到图像扩散模型
能够按照文本描述生成逼真的图像
Stable Diffusion是对传统扩散模型的进一步扩展
它的工作原理是向图像中迭代添加噪声
再反转整个过程以恢复数据
不同于直接对输入图像进行操作
stable diffusion会对图像的低维表示
或者潜在空间进行操作
借此来加快整个过程
此外
模型还会将来自用户的嵌入文本提示词
添加到网络中来修改扩散过程
使其能够在每次迭代中
都指导图像的生成过程
总体而言
2022年亮相的ChatGPT和Stable Diffusion
已经凸显出多模态生成式AI的巨大潜力
也让AI领域获得了进一步发展
和吸引投资的驱动力
毫无疑问
今年已经成为大语言模型
和聊天机器人遍地开花的一年
更多的模型正在以越来越快的速度
发展壮大进入公众的视野
例如今年2月24日
Meta AI发布了LLaMA
尽管参数规模远低于GPT-3
但是这套语言模型
在大部分基准测试中
仍然成功实现了反超
到一个月后的3月14日
OpenAI发布了GPT-4
比GPT-3体量更大更强的多模态版本
虽然目前还不清楚GPT-4的参数数量
但推测可能已经达到了万亿级别
3月15日
斯坦福大学的研究人员发布了Alpaca
这是一种轻量级的语言模型
是通过指令跟随演示
对LLaMA做微调后的产物
几天后的3月21日
谷歌也推出了与ChatGPT
正面对垒的产品Bard
谷歌还在5月10日
发布了另一套最新的大语言模型PaLM-2
随着语言模型领域的快速发展
越来越多的企业
也开始将语言模型融入到自家的产品
例如
Dolingo也公布了基于GPT-4模型的Dolingo Max
这项新的订阅服务将为每位用户
提供量身定制的语言课程
Slack也推出了AI助手Slack GPT
能够草拟回复内容
或者对之前的话题做出概括
除此之外
Shopify还为公司的shop应用
引入了基于ChatGPT的智能助手
可以帮助用户通过各种提示词
提炼出相应的产品
有趣的是
如今的AI聊天机器人甚至已经
开始推动心理治疗师的大众化普及
美国聊天机器人应用Replika
就在为用户提供关心他人的AI伴侣
它永远在线倾听你 回应你 相伴左右
公司创始人尤金尼亚库伊达（Eugenia Kuyda）表示
这款应用面向的是不同类型的客户群体
包括自闭症的儿童
想交新朋友的孤独的成年人等等
在总结之前
我想强调一下
可能这十年来AI技术变革的收官之作
那就是Bing
今年早些时候
微软基于GPT4
将Bing打造成了Web版的Copilot
这款产品针对于搜索进行了定制
而且第一次给搜索业务的长期霸主谷歌
带来了一点小小的AI震撼
回顾过去十年间的AI发展
我们明显身在其中
并且亲眼见证了这场意义深远
甚至彻底改变了我们工作
经营和交互方式的技术革命
生成模型
特别是大语言模型
近期取得的大部分重要进展
似乎都秉承了越大越强的普遍观念
也就是说模型的参数空间越大
实际性能越好
这一点在GPT家族中体现的尤为明显
从1.17亿参数的GPT-1开始
每个后续模型
都将参数提升到了新的数量级
并最终迎来了参数
可能达到万亿之距的顶峰
GPT-4
是根据最近的一次采访
OpenAI公司CEO萨姆奥特曼认为
越大越强这条道路
可能已经走到尽头了
展望未来
他虽然承认参数数量将继续的上升
但是后续模型的主要改进重点
将放在提高模型的能力实用性
和安全性等层面上
这里呢安全性尤为重要
毕竟这些强大的AI工具
都已经被掌握在了公众的手里
不再是过去那种只供实验室研究的学术玩具了
所以我们比以往任何时候
都更加应该谨慎行事
确保这些工具安全可靠
而且符合人类的利益和福祉
希望AI安全
也能够获得与其他探索方向
相匹配的发展和投入
好了今天的分享就到这里
欢迎小伙伴们订阅我们的频道
我们下期再见
