大家好，这里是最佳拍档，我是大飞
上周刚刚做完Claude 3.5 sonnet的节目
这周google又搞了个大新闻
这AI圈的热点啊，真是永远也追不完
早在一个多月前的 Google I/O 上
Google 便官宣将会在未来几周内
推出开源模型 Gemma 的第二代 Gemma 2
Google DeepMind的研究VP克莱门特·法拉贝特 Clement Farabet、主管特里斯·沃肯廷Tris Warkentin 联袂发文
宣告 Gemma 2 正式向所有研究人员和开发者开放
接下来
就让我们一起来看看 Gemma 2 究竟带来了什么新的突破
先来简单介绍一下
Gemma 2是 Google 最新的开放大语言模型
它有两种规模，9B参数和 27B参数
分别有预训练基础和指令调优两个版本
所以一共是4个版本
与上一代的Gemma相比
这次Gemma2的性能大幅度提升
但是部署要求却大幅度下降
只需要一块NVIDIA H100 Tensor Core GPU
或者是TPU主机就能使用
相当的平民
即使你的硬件水平不达标
现在也可以在Google AI Studio 中使用Gemma 2
因此
用户也可以在没有满足硬件要求的情况下
测试27B参数版本的全部功能
此外
Kaggle和Hugging Face Models也提供了Gemma 2模型的下载渠道
Google强调
为了方便用户使用Gemma2进行研究和开发
Gemma 2也提供了免费的使用方式
用户可以通过 Kaggle 或者 Colab 笔记本
来免费使用该模型
至于学术研究人员
则有机会申请 Gemma 2 的学术研究计划
从而获得Google Cloud的信用额度
加速他们使用 Gemma 2 进行研究
计划申请现在已经开放
将会持续到 8 月 9 日
Gemma 2 沿用了与第一代相同的许可证
几乎没有给用户留下什么限制
不管你是想拿去分发、微调、还是把模型用作商业用途和创作衍生作品
都是完全没有问题的
大飞会把4个模型的地址都放在视频简介中
感兴趣的朋友可以去下载试试了
相较于第一代的Gemma
Gemma 2在架构和训练数据量方面
都在上一代的基础上进行了全方位的改良
在架构方面
新的模型使用了局部滑动窗口注意力和全局注意力
所谓的局部滑动窗口注意力
是一种用来减少Transformer模型中
注意力计算的内存和时间的方法
之前Mistral等模型就已经有使用过
Gemma 2 的新颖之处在于
每隔一层会应用一个4096 token的滑动窗口
而中间层仍然使用8192 Token的全局二次注意力
经过这一改良
Gemma 2就可以在保持长上下文长度的前提下
提高输出的质量
即使token数量过半
模型仍然有余力关注所有的Token
这就是滑动注意力的优势所在
与此同时
Gemma 2 还对最终层和每个注意力层都使用了软上限
软上限是一种防止 logits 过度增长而无法截断的技术
它通过将 logits 除以最大值阈值 (soft_cap)，
然后通过 tanh 层
确保它们在 (-1
1) 范围内） ，最后再乘以阈值
这样就确保了最终值在 (-soft_cap
+soft_cap) 区间内
不会丢失太多信息的同时稳定了训练
需要注意的是，在发布时
虽然软上限与 Flash Attention / SDPA 不兼容
但是它们仍然可用于推理
并不会干扰模型以最高的效率执行推理任务
Gemma 2 团队还观察到
就算在推理过程中不使用软上限机制
输出结果和使用了软上限得出的结果之间
差异非常小
除了架构方面的更新
Gemma2在训练数据的总量和方法上
也迎来了一次新的迭代
训练数据的总量方面
Gemma 2 模型的训练数据量
大约是第一代的两倍
27B的版本吃掉了13 万亿的Token
9B参数的版本也消费了8万亿Token 的网页数据
这些训练数据 主要是由英语语料、代码和数学数据构成
在预训练阶段
Gemma 2在文本合成、英语合成和人类生成的提示、响应对上
应用了监督式微调
然后在这个基础上
应用了基于偏好数据训练的奖励模型
以及基于相同提示的RLHF强化训练
由于google还没有公布训练数据的详细情况
我们不知道训练数据混合的具体细节
但是可以猜测
更大和更仔细的数据整理
是Gemma2性能提高的重要因素之一
为了让模型能够顺利地消化掉这些数据
Google这次也改变了训练策略
在Gemma 2用上了知识蒸馏的方法
知识蒸馏是一种常用的模型训练策略
通常被用来训练较小的学生模型
来模仿较大、但是表现更好的教师模型的行为
开发者可以将大语言模型的下一个Token预测任务
与教师模型提供的Token概率分布结合起来
从而为学生模型提供更丰富的学习信号
目前市面上能承担教师责任的模型有很多
例如 GPT-4、Claude 或者 Gemini
根据 Gemma 2 技术报告
知识蒸馏技术被用来预训练 9B参数的模型
而 27B参数模型则是从头开始预训练的
在后期的训练中
Gemma 2 团队生成了来自教师模型的多样化补全集
然后使用这些合成数据
通过SFT来训练学生模型
这种训练手法也是许多开源模型的基本做法
比如Zephyr和OpenHermes
它们完全基于较大语言模型的合成数据进行训练
不过
报告中并没有提及Gemma 2 具体是从哪位教师模型那里顺利毕业的
所以还需要等待Google官方的进一步报告
知识蒸馏这个办法听起来很讨巧
老师带学生
学生毕业了还可以带更小的学生
子子孙孙无穷尽
岂不是可以批发生产AI模型了？
不过，理想很丰满，现实很骨感
尽管知识蒸馏有一定效果
但是这种方法也存在着缺点
因为学生和教师之间的模型容量不一致
比如说老师模型有个20亿的参数
而学生只有8亿
就算20亿的老师把自己的token分布教给了学生
学生也可能压根学不来
这其实不难理解
人类就有同样的情况
名师不一定出高徒
就算是名校出来的辅导老师
也不一定能把学生都送进名校
这种学生脑容量不够导致老师教不来的情况
在学生模型上
会最终体现为训练和推理过程中文本的不匹配
也就是学生模型在推理期间生成的文本
与训练期间看到的文本不一样
颇有差生上辅导班
老师一问全是懂了懂了
写起题目来却一字不会的既视感
为了解决这个问题
Gemma 2 团队采用了“在线蒸馏”的方式
其中学生模型从SFT提示中生成补全
这些补全用于计算教师和学生logits之间的 KL散度
这就相当于一个课堂笔记
通过在整个训练过程中最小化KL散度
学生能够准确地模拟教师的行为
同时最小化训练和推理之间、文本不匹配的可能性
这种方法的有趣之处在于
在部分解决了训练推理不一致的情况后
在线蒸馏就可以尽情发挥自己更加廉价、便捷的优势
只需要一个教师的 logits
开发者就可以开班带学生了
无需再依赖奖励模型、或者大语言模型作为评审员来改进模型
这对于开源社区中的开发者而言
可以说是一个重大的利好
在技术方面的更新讲得差不多了
相信大家也很好奇
Gemma2的实际使用效果究竟如何呢？
我们可以来看一下Google自己发布的技术测试结果
在google自己的评估中
开发者评估了在13万亿token上训练、但是没有使用蒸馏的27B模型的性能
并且将这个模型与规模相似的模型Qwen1.5 34B
以及规模大2.5倍的模型LLaMA-3 70B
在HuggingFace的评估套件上进行了比较
总体而言
Gemma2模型在同规模类型中的表现是最好的
甚至与训练时间更长的大型模型相比
也具有一定的竞争力，与此同时
研究人员在MBPP、MMLU、ARC-C、GSM8K、BBQ Disambig等知名基准测试上
也进行了综合评测
测试结果显示
Gemma 2在多轮基准测试中都表现得非常出色
例如在MMLU 5-shot、ARC-C 25-shot、GSM8K 5-shot等测试中
27B模型相比之前的版本和其他标准模型
比如Mistral和LLaMA-3
均有明显提高
在同类小参数模型中
Gemma 2超过了Llama-3 8B等知名模型
性能逼近Qwen1.5
成为同类中的最佳模型之一
在MMLU 5-shot测试中
27B的得分达到了75.2%，
相较于Gemma-1的42.3%有显著的增长
不过
这些测试基本是Google自己给出的结果
目前其他平台依然在测试Gemma2的性能中
著名社区Hugging Face 目前正在新的开源大语言模型排行榜基准上
单独评估 Gemma 2
并且将在今天晚些时候更新结果
到时候大飞会把他们的测试结果
同步到视频的评论区里
在提升模型性能的同时
Google也没有落下安全保障
根据他们自己的声明
在训练 Gemma 2 的时候
研究人员遵循了严格的内部安全程序
对预训练数据进行了筛选
并且对一系列综合指标进行了严格的测试和评估
从而识别和缓解潜在的偏见和风险
与此同时
开发团队致力于为开发者和研究人员
提供构建和部署安全AI所需的资源
包括负责任的生成式 AI 工具包
最近开源的 LLM Comparator
可以帮助开发者和研究人员深入地评估语言模型
而且从今天开始
任何人都可以使用配套的 Python 库
进行模型和数据的比较评估
并且在应用中可视化查看结果
此外
团队还开源了基于Gemma 模型开发的文本水印技术SynthID
好了
以上就是Gemma2目前已知的情况
内容基本源自于Google自己的技术测试报告
其他机构的测试还需一些时间
也欢迎大家去亲自动手测试Gemma2
把自己的使用心得和体验分享在评论区里
感谢大家的观看，我们下期再见
