大家好，这里是最佳拍档，我是大飞
OpenAI的创始成员、前研究科学家安德烈·卡帕西Andrej Karpathy最近又开始整活了
他尝试在 llm
c中重现了GPT-2
这里的GPT-2 是 15.58B 参数的完整版本
最初亮相于OpenAI于2019 年 2 月 14 日发布的博文中
按照Karpathy的说法，2019 年的时候
GPT-2的训练工作还是一个要涉及到整个团队、需要规模化投入的项目
但是如今 5 年过去
随着计算、软件和数据等层面的改进
如今已经能够在24个小时之内
凭借着八个H100的单节点
成功复刻GPT-2模型
而且总成本仅为672美元
而当年训练GPT-2的成本
卡帕西估计要10万美元左右
安德烈·卡帕西大家应该很熟悉了
他在 2017 年从OpenAI离职后
加入特斯拉担任 AI 高级总监
2023 年再次回到 OpenAI 组建团队
并推出了 ChatGPT
一年后，Karpathy 离开了 OpenAI
并且出于教育的目的，开发了llm
c
llm
c 是一个简单、纯 C/CUDA 的大语言模型
总计约 5000 行代码
无需使用Python 解释器或者复杂的深度学习库
比如PyTorch/JAX、huggingface transformers等常见的训练技术栈
至于这次GPT-2复刻的具体效果
卡帕西特意与 19 年的 GPT-2 版本做了对比
按照当时博文介绍里的提示词
新模型的输出结果也是相当连贯
质量大致与 GPT-2 相当
好了
接下来大飞就来带大家回顾一下卡帕西的复刻过程
首先，Karpathy 强调，使用 llm
c 训练 GPT-2 非常简单
因为它是用 C/CUDA 编写的
所以全程不涉及 minconda、Python、PyTorch 等
只需要一台八个H100 GPU 的设备24小时即可
哪怕只有一张GPU
训练出自己的GPT-2也只不过需要8 天时间
而如果拥有16张 GPU
比如使用新的 Lambda 1 Click Clusters
那么还可以开展多节点训练
前后只需要等待 12 个小时
在节点启动之后
下面来看看 GPT-2 训练的完整说明
不用担心，Karpathy 表示
保证一分钟以内就能开始执行
脚本的内容启示非常简单
还配了详细的注释
基本就是下载安装CUDA和MPI相关的包
然后下载llm
c代码仓库
下载模型权重和训练数据集
编译一下，就可以直接开始训练了
这里我们重点介绍一下执行命令的参数
主要参考的是GPT-3论文中的超参数解释
因为GPT-2的论文中几乎没有相关内容
首先
-i -j 用来训练和验证分割标记文件
需要提前使用 edu_fineweb
sh 进行下载
-o 是指定写入日志和检查点的输出目录
-v 250，表示要求每 250 步执行评估
并且记录验证 loss
-s 300000
表示要求每 30 万步采样部分 token
因为总步数不足 30 万
所以这其实是一种关闭采样的灵活方式
实际只会在最后采样一次
-g 384
表示将最后需要采样的 token 数设置为 384
-h 1
表示要求评估 HellaSwag 准确性
-b 16，表示将微批次大小设置为 16
如果内存不足，就降低这个值
按照8、4、2、1的顺序依次尝试
-t 1024
表示将最大序列长度设置为 1024
与原版的GPT-2 保持一致
-d 1048576
表示要求总批次大小为 2 的 20 次方
与 GPT-3 论文中的超参数设置相同
代码会来确保满足所需要的总批次大小
并且计算优化所需要的梯度累积“内循环”步骤
比方说
我们之前提到 Karpathy 拥有 8 张 GPU
每张 GPU 执行 16 x 1024 个 token
因此每个微步骤
也就是一次的向前向后计算
对应 8 x 16 x 1024 = 131072 个 token
因此代码计算梯度累积步数应该为 8
才能满足每步所需的 1M 的批次大小
也就是每向前 + 向后 8 次
然后进行一次更新
-r 0 ，表示将重新计算设置为 0
重新计算是一种在计算与内存之间获取平衡的方法
如果设为 -r 1，则代表在反向过程中
重新计算前向传递的一部分
也就是说 Karpathy 不需要通过缓存来节约内存
但是需要付出更高的算力成本
因此如果内存不足
可以尝试设置 -r 1 或者 -r 2
-z 1
表示在多个 GPU 上启用 ZeRO-1
也就是优化器状态分片
这么设置主要是针对于使用1 张以上的 GPU 进行训练
在单张GPU 上这样设置是没有实际效果的
-c 0.1，表示将权重衰减设置为 0.1
-k "cosine"，
表示 设置余弦学习率计划
这里暂且直接使用默认值
-l 0.0006
将最大学习率设置为 6e-4
根据 GPT-3 论文的解释
Karpathy 这个大小的模型应当使用 2e-4
但是这里 Karpathy 将它增加了三倍
似乎训练速度更快也没有引发任何问题
-q 0.1 ，表示在训练过程中
将学习率衰减到最大 LR 的 10%。
-u 700 ，表示在前 700 次迭代中
将学习率从 0 提升到最大值
当总批次大小为 0.5M 的时候
即对应 3.5 亿个 token
-n 2000
表示每 2000 步要保存一次模型检查点
-x 32000 ，要求总共 32K 步
之所以选择这个数字
是因为它好读好记
而且正好对应 24 个小时
-ge 1
为 CublasLt 设置最近合并的 gelu 重新计算
-y 1，开启“恢复”标记，这个值表示
如果训练过程因为任何原因崩溃或者挂起
那么你可按下 CTRL+C 并重新运行命令
系统将会尝试恢复优化
-e "d48" ，
要求从头开始初始化深度为 48 的 GPT-2 模型
好了，介绍完参数
我们来看看执行的结果
可以看到
每个步骤大约需要 2.75 秒
而其中总共涉及到3.2 万个步骤
所以现在需要等待大约24 个小时
在每一步中
训练作业的运行都会占用大约 100 万个 FineWeb-EDU token
并且对模型的 15.58 亿个权重进行更新
到最后总共要处理 3.2 万 x 1048576 = 33.6B 个 token
随着预测下一个token的能力越来越强
loss 也会随之下降
接下来的工作是归一化
也就是将数值范围控制在 0.1到1之间
学习率也在前几个步骤中逐渐升温
从结果来看
这套模型的 flops 利用率
也就是MFU
大约为 50%，效率可以说是相当高了
现在唯一要做的
就是等待 24 小时完成训练
之后可以使用 vislog jupyter notebook 对 main
log 日志文件进行可视化
注意
这里大家需要提前安装 Mython 和 matplotlib
对于评估结果，我们可以看到
左边是跟踪 FineWeb-EDU 验证数据的 loss
如果大家以OpenAI 发布的 GPT-2来评估loss
得到的就是红色水平线
loss 2.83
而 Karpathy 模型的运行结果快速将其超越
步长约为 5000
当然，这样的比较并不是很公平
因为两者的训练数据集不同
Karpathy 也认为loss 验证
更多只是一种健全性的检查
模型性能的真正比较还得借助第三方评估
那么右侧就是基于HellaSwag 的评估
在大约 25K 步左右
与 GPT-2 模型的性能发生了交叉
绿线为同等参数规模的 GPT-3 模型
它的模型架构与 GPT-2 几乎相同、仅仅是上下文长度从 1024 增长到了2048
同时是针对 3000 亿 token 进行训练
不过HellaSwag基准缺少对于多语言、数学或者代码内容的评估
所以暂时也无法提现这部分的能力比较
由于很多人的训练GPT内存可能达不到80GB
Karpathy 表示
没关系，只要有耐心
这些任务也都能顺利运行完成
只是速度会稍慢一些
如果模型实在太大
那么可以调整微批次大小
也就是 -b参数
尝试把它缩小到一个合适的水平
比如改成8、4、2或者1
另外就是可以调整重新计算设置 -r
0表示最快
但是占用的内存最大、1表示最慢速度
但是可以节约大量内存
2则表示速度稍慢
但是节约内存节也较少
最后，Karpathy又介绍了一下llm
c这个项目
它是 C/CUDA 中最直接、最小且可读的实现
只需要基本 CUDA 依赖即可运行
总计约有 5000 行 C/CUDA 代码
它的编译和运行速度极快
一般也就是几秒钟
在运行开始时
它会一次性分配所有的GPU 内存
并且在之后的训练期间
保持内存占用量的恒定
因此你只要执行启动步骤
基本上就能保证接下来的运行状态始终良好、不会发生OOM
此外，llm
c的运行效率还很高，MFU略低于50%。
如果你是手握大量 GPU 的“土豪”，
llm
c 也支持多节点训练
Karpathy见过的 llm
c 训练
最多能支持到大约 500 张 GPU
随后
Karpathy还给出了一个并行 PyTorch 的代码实现
运行的结果是这样，可以看到
PyTorch 占用的内存量更大
大约为 80 GB，而 llm
c 仅占用57GB，节省约29%。
其次，PyTorch的实现
每次迭代大约是3386 毫秒，而 llm
c 的迭代是2750 毫秒
速度快大约 19%。
好了
以上就是对这次GPT-2复刻过程的回顾
应该说
Karpathy 让我们看到了更多的可能
也证明了AI领域的快速发展
四五年的时间
已经把之前十万美元的训练成本干到了几百美元
但是
这并不意味着未来整个训练成本会大幅下降
前几天
AI 初创公司 Anthropic 的CEO达里奥·阿莫代伊 Dario Amodei 就在采访中表示
目前 GPT-4o 这样的模型训练成本
大约是 1 亿美元
而目前正在开发的 AI 大模型
训练成本可能高达10亿美元
他还预计，未来三年内
AI大模型的训练成本
将上升至100亿美元甚至1000亿美元
总的来说
AI行业还是一个极度烧钱的行业
从算力到人才再到数据，上个月20号
红杉资本合伙人兼首席运营官大卫·卡恩David Cahn
提出了AI行业的6000亿美元问题
说的是AI公司的投入和产出之间
存在着6000亿美元的差额
而这个数字在去年9月还只有2000亿美元
究竟AI是否正处于泡沫破裂的边缘
我们有时间会再做一期视频来讲讲
感谢大家观看本期视频
我们下期再见
