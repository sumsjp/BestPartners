大家好，这里是最佳拍档，我是大飞
这期节目我们来聊聊微软的AI之路
2023年，它还是AI赛道的绝对领导者
为OpenAI砸下千亿投资
建起全球最大的AI数据中心；
2024年，它突然踩下急刹车
冻结数据中心租赁、暂停自建项目
眼睁睁看着OpenAI转向甲骨文、CoreWeave等竞争对手；
2025年，它又高调回归
通过新的合作协议、多元算力布局和Token即服务的模式
试图重新夺回主导权
这家市值万亿的科技巨头
为何会在AI巅峰时期选择暂停？
错失的1500亿美元毛利能否挽回？
自研模型与定制芯片的困境背后
又藏着怎样的行业博弈呢？
今天
我们就基于SemiAnalysis刚刚发布的万字报告
给大家拆解一下微软AI战略的底层逻辑、失误与反攻计划
要理解微软的AI战略
必须从2022年11月ChatGPT的发布说起
作为最早押注OpenAI的科技巨头
微软早在2019年就投入了10亿美元
但是ChatGPT的爆火让它看到了AI的真正潜力
随即在2023年1月将投资规模扩大了10倍
同时启动了史上最激进的数据中心建设计划
当时的微软
在AI基础设施领域的投入堪称疯狂
从2023年第一季度到2024年第二季度
微软数据中心的预租赁规模
远远超过了亚马逊、谷歌、Meta等其他超级云服务商的总和
仅2023年第三季度
它单季度的租赁量就几乎等同于2022年全年北美市场的总租赁量
除了租赁，微软的自建项目同样惊人
2024年和2025年
微软自建数据中心的兆瓦数增长
达到了前所未有的水平
同时还与CoreWeave和甲骨文签订了数十亿美元的合同
只为抢占更多的算力资源
这个时期最具标志性的
是微软为OpenAI打造的Fairwater计划
包括一系列全球最大的AI数据中心集群
第一个大型训练集群位于爱荷华州
正是GPT-3.5的训练地
配备了大约2.5万块英伟达A100芯片
占用了巴拉德（Ballard）建筑中的两个数据大厅
功率大约为19兆瓦
随后在亚利桑那州
微软又逐步扩展出第二个集群
2023年完成首栋H100建筑
2024年建成H200设施
2025年计划再添加两座配备GB200芯片的数据中心
总计在四栋建筑中部署大约13万块GPU
而真正的巨无霸
是威斯康星州和乔治亚州的Fairwater设施
每一个Fairwater由两栋建筑组成
一栋是48兆瓦的标准CPU及存储设施
另一栋是超密集GPU建筑，为两层结构
总面积大约80万平方英尺
功率高达300兆瓦
相当于超过20万个美国家庭的用电量
配备超过15万块GB200 GPU
乔治亚州的设施虽然冷却系统不同
但是GPU建筑功率同样达到300兆瓦
风冷机组和现场变电站的规模
在全球都无出其右
更令人震撼的是微软的长期规划
在亚特兰大
第二个Fairwater园区已经在紧锣密鼓的建设
威斯康星州的第二个Fairwater也即将开工
而第三阶段
微软计划建造两栋超过600兆瓦的单体建筑
每个设施的CPU/存储和柴油发电机数量
是标准300兆瓦Fairwater的两倍
完全建成后
这个园区将拥有超过2吉瓦的IT容量
成为全球最大的AI数据中心园区之一
为了连接这些分布在各地的超大规模集群
微软还计划打造超高速的AI广域网
速度超过300Tb/秒
未来可扩展到10Pb/秒以上
SemiAnalysis估计
这个分布式集群的网络设计
能够支持5吉瓦级的算力协同
让不同区域的GPU集群实现高效联动
这在当时被视为OpenAI击败谷歌基础设施的关键布局
然而
就在这一系列宏伟计划推进的关键时刻
微软突然按下了暂停键
2024年第二季度之后
微软的新数据中心租赁活动完全冻结
而其他大型云服务商则在显著增加租赁规模
此前微软在高峰期
独占了超过60%的租赁合同
而到2024年底
它的预租赁容量占比已经降到了25%以下
除了冻结租赁
微软还放弃了多个吉瓦级的非约束性意向书
包括美国的菲尼克斯、芝加哥
欧洲的英国、北欧
以及澳大利亚、日本、印度、拉丁美洲等全球多个地区
这些被放弃的站点
很快被甲骨文、Meta、CoreWeave、谷歌、亚马逊等竞争对手接手
同时，微软的自建项目也大幅放缓
仅公开披露的冻结IT容量
就达到大约950兆瓦
这还不包括弗吉尼亚、乔治亚、亚利桑那州
以及国际上的多个其他数据中心项目
根据统计
微软总共暂停了超过3.5吉瓦的容量建设
这些容量原计划要在2028年前建成
这波大暂停直接也导致OpenAI开始多元化合作
2025年
OpenAI直接与甲骨文、CoreWeave、Nscale、SB能源公司、亚马逊和谷歌
签订了大量的计算合同
这对微软来说无疑是巨大的打击
那么
微软为什么会在巅峰时期突然自断臂膀呢？
SemiAnalysis指出了三个核心原因
首先是对未来AI需求的误判
微软严重低估了GPU的云需求规模
导致它的战略规划与市场实际需求脱节
其次是自建项目执行缓慢
以威斯康星州的Fairwater项目为例
动工两年多后
第一阶段仍然没有投入运营
而甲骨文2024年5月在德克萨斯州阿比林动工的项目
9月就开始运营，两者效率差距悬殊
更关键的是
微软对1.5吉瓦扩容的电力传输规划不佳
满负荷容量至少要到2027年中期才能交付
比甲骨文的阿比林集群突破1吉瓦的时间
晚了一年
无法满足OpenAI尽快扩展的需求
第三个原因
是对利润率和资本回报率的重新权衡
当时有传言称
微软为OpenAI准备了1000亿美元的星际之门项目
但是这个合同最终落到了甲骨文的手中
从微软的角度来看
拿下所有OpenAI的合同
会恶化自家Azure业务的质量
因为OpenAI将在几年内占据Azure近50%的收入
但是它的利润率和资本回报率
远不及Azure云业务在历史上的表现
数据显示
甲骨文在AI方面的投资回报率为20%，
而微软整体业务的投资回报率高达35%到40%，
即便剔除掉OpenAI的收入分成
微软自身的AI投资回报率
也并不比甲骨文高多少
但是微软显然忽略了一个关键
那就是AI业务的收入结构
正在从裸机服务器的工作负载
转向更多API和Token工厂业务模式
这种转型本来就会带来投资回报率的持续提升
而这波大暂停
也让微软错失了巨额利润
甲骨文与OpenAI签订的合同价值超过4200亿美元
转化为大约1500亿美元的毛利润
如果按照五年期限计算
每年300亿美元的毛利润
能让微软2025财年1940亿美元的年毛利润
提升超过18%。
更严重的是
微软的暂停导致它的剩余履约义务
也就是RPO的份额大幅流失，数据显示
暂停前微软的12个月RPO新增量为500亿美元
占比为38%，
而暂停后新增量为1320亿美元
占比仅为18%，
而甲骨文的新增量则飙升到4250亿美元
成为最大的赢家
不过
要想真正看懂微软的AI战略失误
还不能只看数据中心的起伏
更要深入它的AI产品组合的各个层级
SemiAnalysis提出了一个AI Token工厂经济栈的框架
从芯片、IaaS、PaaS、模型层到应用层
每个层级的利润率和竞争格局
都直接影响着微软的战略决策
先看最底层的IaaS层
这是微软AI业务的基石
也是大暂停受到冲击最严重的领域
IaaS层的核心是裸机GPU集群的建设与租赁
成功的关键在于执行速度、对市场需求的理解、选址和融资能力
但是微软在这一领域的表现堪称令人失望
最典型的就是错失星际之门合同
前面提到
微软的威斯康星州数据中心项目
本来计划将容量提升到超过2吉瓦
但是由于执行缓慢和电力传输规划滞后
最终被甲骨文抢走了价值1000亿美元的星际之门合同
而且甲骨文的阿比林集群
从动工到运营仅用了4个月
而微软的项目动工两年多仍然没有投入运营
电力传输设施要到2027年才能全部完工
这种执行效率的差距
让微软在裸机服务器市场失去了核心竞争力
更糟糕的是
当微软2025年意识到失误、想要重返市场的时候
扩展近期容量的选项已经所剩无几
只能被迫选择从CoreWeave这样的NeoCloud租用GPU
再转售给第三方
这种转租模式导致Azure的利润率显著低于平常
因为NeoCloud的毛利率已经达到35%，
微软再次转手
很难保持原有的利润水平
从数据来看
IaaS层的盈利模型有着明确的成本结构
以H200 GPU为例
每8块GPU的服务器总资本成本为246391美元
GPU的折旧年限为4年
每小时折旧费用为0.88美元；
托管成本为每千瓦每月110美元
电费为每千瓦时0.087美元
总计每块GPU的每小时可变成本为1.23美元
而每块GPU的每小时收入为1.90美元
这意味着
IaaS层的利润空间本就有限
微软通过转租模式进一步压缩了自己的利润
竞争力自然大打折扣
再看PaaS层
这一层的核心是将GPU算力转化为Token服务
通过API向企业客户销售
微软的Azure在这一层原本占据领先地位
凭借网络性能、安全性和最新的GPU
曾经被SemiAnalysis评为AI云服务的金牌层级
与CoreWeave、Nebius、甲骨文等并列
但是到2025年11月
Azure的地位开始动摇
甚至面临被降级为银牌的风险
问题的核心出在产品体验上
通过与140多位来自OpenAI、Meta、Snowflake等大型AI公司
以及PeriodicLabs、AdaptiveML等初创企业的计算采购负责人交流
SemiAnalysis发现
Azure在托管集群或按需虚拟机领域
并非重要玩家
微软的大规模集群GPU容量
大多直接供应给OpenAI
剩余部分被财富500强的传统企业内部开发者抢占
而这些企业通常签有协议
专门从Azure采购所有的基础设施
对于真正积极寻求计算能力的AI创业公司来说
Azure的产品体验存在明显的短板
行业内典型的GPU计算买家
主要寻找规模在64到8000个GPU的H100、H200、B200或B300 HGX服务器
但是微软在AMD GPU以及针对OpenAI的GB200/GB300 NVL72机架规模系统上
投入了大量资源
与市场主流需求脱节
更关键的是
Azure的CycleCloud Slurm集群
在易用性、监控、可靠性和健康检查方面存在显著差距
它为OpenAI提供的整机裸机体验
与CoreWeave、Nebius等供应商
向终端用户提供的体验截然不同
开源社区的数据也印证了这一点
在HuggingFace上
与微软相关的每日模型下载量
比亚马逊少5倍，比谷歌少3倍
这意味着
微软不仅失去了OpenAI这样的大客户
也没能抢占企业和长尾市场
眼睁睁看着大量增长机会被竞争对手夺走
这些需求小到价值100万美元、为期1年的64个GPU的合同
大到价值超过5亿美元、为期3年的8000 个GPU的合同
而Azure正在错失这一切
为了扭转局面
微软推出了Azure Foundry
Token即服务的业务
它可以提供多种模型
既供给Microsoft 365 Copilot和GitHub Copilot等内部服务使用
也供给客户的模型推理使用
市场策略类似于OpenAI的API
面向个人和企业竞争
由于拥有OpenAI模型权重的知识产权
Azure还能够独立制定定价策略
根据SemiAnalysis的预测
目前大多数GPT API的token
仍然由OpenAI直接处理
但是Azure Foundry将成为微软未来的重要增长引擎
并且逐步夺回市场份额
更重要的是
无论通过OpenAI API还是Azure Foundry提供服务
Azure在2032年前
将拥有所有API推理计算的100%份额
这为微软的PaaS层业务提供了长期保障
不过，值得注意的是
向企业销售Token的业务仍然处于起步阶段
谷歌CEO桑达尔·皮查伊（Sundar Pichai）在2025财年第三季度财报电话会议中披露
过去12个月里
有近150个谷歌云客户用模型处理了大约1万亿个Token
但是这部分业务规模还不到谷歌云业务的0.5%。
这说明
将Token转化为大规模收入远比看起来要复杂
需要解决输入/输出比率、缓存Token、定价计算等一系列的问题
而微软在这个领域仍然需要时间积累
接下来是应用层
这是微软原本的优势领域
尤其是GitHub Copilot在代码辅助市场曾经占有绝对优势
微软拥有业内首个内联代码模型
并且由于独占的知识产权访问权限
很早就将GPT-4集成到了Copilot中
更重要的是
微软拥有VS Code和GitHub
以及庞大的企业客户基础
从外部看，这个护城河似乎固若金汤
但是微软低估了创业公司的挑战
一批创业公司通过对VS Code的Fork改造
构建了模型与代码库之间更紧密、更优的集成
再加上采用了Anthropic的模型
这些挑战者的产品体验整体超越了GitHub Copilot
数据显示
Anthropic的Claude Code、Cursor等产品的ARR增长迅猛
不断挤压GitHub Copilot的市场份额
面对竞争压力
微软在2025年初不得不勉强将Anthropic的模型
纳入GitHub Copilot产品
这个举措大幅压缩了自己的利润率
GitHub Copilot从几乎100%使用第一方Token
转变为必须购买大量Anthropic的Token
毛利率降到50-60%。
为了应对这个局面
微软加大了对模型超市生态系统的押注
推出了Agent HQ平台
接入了包括谷歌和xAI在内的多个Agent
试图通过生态多样性来挽回用户
但是微软面临的更大隐患是
它对OpenAI模型权重的访问期限
仅仅延长到了2032年
必须为当前利润最高的OpenAI模型产品制定备选方案
因此
自研模型就成为微软的必然选择
目前
微软已发布了涵盖文本、图像和语音的3款自研MAI模型系列
文本模型MAI-1目前在LMArena上的评分大约为38
还没有通过聊天界面或者API公开提供
这个模型是一个MoE模型
在15000个H100 GPU上训练
下一代模型将是更大规模的多模态大语言模型
图像和语音模型则已经位列LMArena前十
而且都已经集成到Copilot中
这两款已落地的模型代表了微软的策略
那就是低成本且质量尚可的模型应用场景
它们虽然远远没达到最先进模型的水平
但是微软正在悄然准备
投入更大规模的内部训练工作
根据SemiAnalysis的预测
未来几年内微软在自研模型上的年化计算支出
将接近160亿美元
显示出了在模型层实现自主可控的决心
除了GitHub Copilot
微软的Copilot生态还涵盖销售、财务、服务、安全等多个领域
Office 365 Copilot的月活跃用户已经超过1亿
成为推动AI普及的重要力量
最新的努力体现在Office Agent中
以Excel Agent为例
它是OpenAI某个推理模型的后训练版本
微软声称它的表现优于前沿实验室的成果
在SpreadsheetBench测试中
Excel Agent的准确率达到71.3%，
远超GPT-4o、Claude Opus 4.1等竞品
这背后离不开微软对OpenAI知识产权的充分利用
不仅能从原始的思维链中提炼知识
还能利用Office套件的细粒度数据进行微调
实现了模型与应用场景的深度融合
再来说说芯片方面
这也是利润的关键
对于微软这样的超级云厂商来说
定制ASIC芯片不仅能降低对英伟达的依赖
还能通过消除第三方的毛利空间
大幅提升自身的利润率
但是令人意外的是
在定制硅芯片开发方面
微软在超大规模云厂商中处于垫底位置
甚至没有试图迎头赶上
微软在2023年底
展示了首款AI ASIC芯片Maia 100
是四大云厂商中最后一个拥有AI ASIC的企业
而且如同第一代硅芯片的普遍情况
Maia 100并没有实现大规模生产
也没有用于实际的生产工作负载
更关键的是
这款芯片的架构设计早于生成式AI的爆发
因此在推理任务所需要的内存带宽方面
存在先天不足
Maia 100的内存带宽仅为1600GB/s
而谷歌的TPUv5p为2765GB/s
亚马逊的Trainium2E为2898GB/s
谷歌最新的TPUv7更是达到了7370GB/s
差距极为明显
不仅如此
微软的下一代芯片Maia 200的开发
也因为多项问题而停滞
导致设计进程延长、流片时间推迟到2024年底
量产要到2025年才开始
更糟糕的是
Maia 200芯片在微软内部被评估为失败
迫使微软重新制定AI ASIC路线图
甚至放弃了对Maia 200的软件开发
转而将精力投入到未来的Maia迭代版本
按照目前的进度
微软最早也要到2027年底
才能部署接近内部性能预期的2纳米迈Maia 300
但是在这期间
行业竞争门槛会被进一步拉高
微软需要与英伟达的Vera Rubin芯片竞争
而鉴于目前Maia团队的管理失误
SemiAnalysis对它在2027年的表现也不抱信心
从实际的出货量来看
Maia系列的差距更为显著
在CoWoS预订方面
微软AI硅芯片的出货量远低于谷歌、亚马逊和Meta
2023至2026年间
微软的ASIC芯片出货量几乎可以忽略不计
而谷歌、亚马逊和Meta的出货量呈快速增长态势
这意味着微软在定制芯片领域
已经被竞争对手远远甩在身后
反观其他超大规模云厂商
在ASIC芯片领域已经取得了实质性的进展
谷歌的芯片霸主地位无可匹敌
第七代TPU与英伟达的Blackwell芯片不相上下
不仅为Gemini模型家族提供算力
也让谷歌正在成为像英伟达一样的商用AI硬件公司
外部客户明年还将订购大量的TPU
比如Anthropic与谷歌联合宣布明年将至少采购100万个TPU
这标志着谷歌的ASIC芯片已经获得了外部市场的广泛认可
亚马逊也正在交付数百万个Trainium加速器
Anthropic是Trainium2的主力客户
几乎占据了整个Trainium2项目
这批集群为Anthropic带来的增长
正再推动AWS的收入大幅加速
而亚马逊几乎所有的Trainium需求都来自外部客户
值得注意的是
亚马逊在开发自研模型方面兴趣最小
更愿意做纯基础设施提供商
但是这反而让它通过出租自家AI硬件
获得了可观的毛利
Meta也正处于ASIC路线图的关键转折点
即将推出的MTIA Athena将成为首款接近现代GPU的Meta芯片
具备大规模计算引擎与HBM协同封装的特性
并且已经开始量产
明年Meta还将推出新一代的Iris
随后是升级版Arke
还制定了激进的路线图
目标是在硬件实现上超越英伟达
总结来看，所有的超大规模云厂商
都已经部署了支持实际工作负载的ASIC
唯独微软缺席
微软的芯片团队不仅要与其他超大规模厂商竞争
更要与英伟达竞争
而加速器芯片是部署AI基础设施时最大的成本项
如果以英伟达GPU为黄金标准
它的毛利高达75%以上
售价是成本的4倍
定制硅芯片的主要目标之一
就是消除这部分利润
让超大规模云厂商直接设计芯片
并且委托台积电制造
但是微软显然错失了这一机会
面对自研芯片的困境
微软不得不寻求其他的替代方案
一方面，微软通过早期投资OpenAI
获得了OpenAI定制芯片IP的访问权
而OpenAI的ASIC开发进展远远优于微软的Maia项目
因此微软最终可能会使用OpenAI的Titan芯片
来支持OpenAI的模型
但是这种依赖存在着风险
因为微软对OpenAI IP的使用权并非永久
无法实现定制ASIC项目普遍追求的硬件自给自足目标
另一方面
微软也开始多元化的芯片供应商
向初创企业寻求支持
微软的风险投资基金M12最近举办了一场活动
邀请了Modular、Neurophos等初创企业
Modular是一家致力于为英伟达之外的加速器
开发推理和编程框架的软件初创公司
它的Modular MAX
可以替代vLLM和SGLang等推理运行时框架
Neurophos是一家开发光处理单元OPU的芯片初创公司
宣称能将FLOPs提升1000倍
试图挑战英伟达的芯片霸权
但是这些初创项目
也只能是一场概率极低的下注
主线路径依然是失败的
SemiAnalysis认为
微软管理层被唯唯诺诺的观点所误导
导致在芯片战略上错失良机
如果微软完全依赖于租用英伟达的GPU
那就只能和甲骨文、CoreWeave、Nebius这类公司竞争
而谷歌和亚马逊则有机会
凭借自身差异化的技术栈
获得更高利润
尽管在芯片和部分产品层面临困境
但是微软在AI网络架构方面的创新
依然处于行业前沿
对于超大规模AI集群来说
网络是连接海量GPU的神经网络
直接决定了算力的协同效率和训练及推理速度
而微软在Fairwater 2数据中心部署的网络架构
堪称当前AI集群部署的标杆
传统的AI集群网络
通常会采用非阻塞Clos网络拓扑
使用k端口、L层交换机连接GPU
以英伟达的Spectrum-X SN5600交换机为例
每台交换机有64个800G逻辑端口
搭建两层网络时
最多可连接的GPU数量为2048块
如果增加到三层网络
最多可连接65536块GPU
但是层数越多
每台交换机分配到的GPU越少
导致每GPU的网络成本增加
四层网络虽然能连接2097152块GPU
但是由于每台交换机只能连接9.1块GPU
成本过高所以难以落地
微软的第一个创新
是将每块GPU的800G逻辑端口
拆分为了8个100G的端口
虽然CX-8网卡支持这种拆分
但是并不是所有51.2T的交换机
都支持512个100G逻辑端口
而微软选用了Spectrum-5 512端口交换机
成功实现了突破
通过这种拆分方案，搭建两层网络时
最多可连接的GPU数量
提升到了131072块
远远超过未拆分时的2048块
而且每台交换机平均可以连接21.3块GPU
比用800G端口搭建的4层网络更高效
不过微软并没有止步于此
而是推出了更具有创新性的纯轨道（rail-only）拓扑
在两层网络下可连接高达524288块GPU
在纯轨道的网络拓扑中
原本每块GPU的800G链路被拆分为8条
现在是每个计算托盘的3200G链路拆分为32条
分别连接32个平面
用24576台交换机连接524288块GPU
交换机与GPU的比例仍然保持在21.3
但是连接的GPU数量提升了4倍
不过，纯轨道拓扑也存在挑战
由于每个平面完全独立
导致同一计算托盘内的GPU
无法通过扩展网络互相通信
只能通过PXN和NVLink网络通信
这意味着难以在同一网络上重叠不同的通信流
但是微软的MRC协议
专门为了优化和调度这类流量而设计
成功解决了这个问题
让超大规模GPU集群的协同计算成为可能
除了数据中心内部的网络拓扑
微软还构建了专门的AI广域网（AI WAN）
分为园区级和远距离两个层面
园区级的AI WAN主要用于Fairwater 2园区内楼宇间的连接
比如A楼和B楼
每栋楼仅有大约300兆瓦的容量
对应大约160000块GPU
微软并没有追求在同一个多平面网络中连接更多的GPU
而是通过主干层的超额上行链路
连接到AI广域网
让训练任务能够充分利用广域网连接
最终实现亚特兰大和威斯康星的Fairwater园区、以及凤凰城、爱荷华和阿比林园区间的分布式训练
在园区级AI广域网中
微软使用光路交换机（OCS）来提供楼宇间的光链路重配置能力
无需复杂布线和深缓冲交换机
只需要使用专用协议
通过OCS发送数据包到不同的集群
谷歌已经在数据中心网络（DCN）中使用了阿波罗（Apollo）OCS
据称可以灵活扩展、技术升级和调整网络拓扑
而微软的应用
则进一步验证了OCS在超大规模AI集群中的价值
为了实现远距离、高带宽的连接
微软还采用了密集波分复用（DWDM）技术
虽然FR光模块能够连接Fairwater 2的两栋楼
但是跨越数千里的园区
需要更强功率和距离的转发器
并且用可重配置线路系统（RLS）来实现DWDM
DWDM可以将多路光波复用到同一个光纤对上
如果C波段和L波段各用32路
合计64×800G链路
一对光纤就可以承载高达51.2Pb/秒的带宽
数据显示，300Tb/秒的连接
如果使用FR光模块需要375对光纤
而用DWDM复用C波段32路
可以降到仅需12对光纤
大幅降低了部署的成本和难度
此外，ZR光模块也是微软的可选方案
可以在数百公里内传输信号
而且可以通过DWDM
将多路ZR信号复用到同一个光纤对上
ZR光模块部署简单
可以直接插在AI路由交换机上
Meta的跨区域扩展（scale-across）部署就正在用ZR光模块
而微软也在部分中短距离连接中
也采用了这一技术
微软的网络架构创新
不仅解决了超大规模AI集群的连接问题
还为算力协同提供了坚实基础
尽管在芯片和部分产品层存在短板
但是领先的网络技术
让微软在AI基础设施领域仍然保持着一定的竞争力
这也是微软2025年能够惊艳回归的重要支撑
梳理完微软AI战略的各个维度
我们可以清晰地看到
这家科技巨头正处于一个关键的十字路口
2023年的激进扩张让它抢占了AI赛道的先发优势
但是2024年的大暂停
导致它失去了大量的市场份额和利润机会
如今的微软正在通过多元算力布局、产品生态优化和技术创新
试图重新夺回主导权
但是面临的竞争格局已然十分严峻
从竞争对手来看
甲骨文凭借快速的执行效率和对OpenAI的支持
已经成为AI IaaS层的最大赢家
谷歌则通过TPU和Gemini模型的深度协同
实现了硬件与软件的垂直整合
不仅满足了内部需求
还向外部客户销售算力
利润率显著高于行业平均水平
亚马逊AWS则聚焦于基础设施提供商的角色
通过Trainium系列芯片吸引了Anthropic等核心客户
收入增长迅猛
Meta凭借着激进的ASIC路线图和AI应用场景
也正在构建差异化竞争力
而CoreWeave等NeoCloud则通过灵活的租赁模式
抢占了长尾市场
逐渐成为不可忽视的一股力量
对于微软来说
当前的优势主要集中在三个方面
一是与OpenAI的深度合作
拥有OpenAI模型和芯片的IP访问权
而且Azure在2032年前
将拥有所有API推理计算的100%份额
二是庞大的企业客户基础和Office、GitHub等生态系统
Office 365 Copilot的月活用户已超过1亿
为AI业务提供了广阔的落地场景；
三是领先的网络架构创新
能够支持超大规模GPU集群的高效协同
为算力提升提供了技术保障
但是微软的挑战同样不容忽视
自研芯片Maia系列进展缓慢
与其他超大规模云厂商差距明显
短期内难以摆脱对英伟达和OpenAI芯片的依赖
PaaS层的产品体验存在短板
CycleCloud和AKS功能开发停滞
应用层的GitHub Copilot遭遇Anthropic等竞争对手的冲击
利润率被压缩
而企业AI Token市场仍然处于起步阶段
将算力转化为大规模收入
还需要时间
展望未来
微软要想重新夺回AI主导权
需要在三个方面持续发力
首先是加速算力扩容
通过自建、租赁、合作等多元的路径
尽快弥补大暂停造成的容量缺口
减少对NeoCloud的依赖
提升IaaS层的利润率
其次是优化产品体验
尤其是PaaS层的CycleCloud和AKS产品
简化集群部署和监控流程
提升对AI创业公司的吸引力
抢占企业和长尾市场
最后是加快自研模型和芯片的迭代速度
Maia芯片需要尽快解决架构缺陷和执行问题
实现硬件与软件的垂直整合
才能在长期竞争中占据优势
SemiAnalysis预测
随着与OpenAI新的合作协议
Azure的增长将在未来几个季度加速
微软在AI Token经济栈的每一个环节都将经历加速增长
但是这个预测能否实现
关键在于微软能否解决执行的效率问题
以及能否在自研技术上取得突破性进展
AI行业的竞争本质上是算力、模型和生态的竞争
微软作为科技巨头
拥有雄厚的资金和技术储备
但是战略决策上的失误
让它错失了两年的黄金发展期
如今
AI赛道的竞争已经进入白热化阶段
每一个玩家都在加速奔跑
微软能否凭借自身优势实现逆袭
重新成为AI行业的领航者
我们还将拭目以待
感谢大家收看本期视频
我们下期再见
