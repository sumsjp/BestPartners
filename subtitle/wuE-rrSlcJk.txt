大家好，这里是最佳拍档，我是大飞
如果你关注人工智能和计算机科学
那么对杰夫·迪恩(Jeff Dean)这个名字一定不会陌生
他不仅是谷歌早期的核心工程师之一
更是一手缔造了谷歌大脑这个举世瞩目的AI研究团队
他的职业生涯
犹如一场精彩的连续创业
不断投身于新的挑战
推动着技术边界的拓展
今天
我们将通过The Moonshot Factory的播客采访
深入来了解一下杰夫·迪恩的成长经历、他在谷歌大脑的早期探索
以及他对人工智能未来发展的深刻洞察
杰夫·迪恩的童年充满了变动
在12年里搬了11次家
年幼时，乐高是他最好的伙伴
搭建各种模型是他最大的乐趣
这种对于构建的兴趣，在他9岁的时候
因为父亲的缘故，转向了计算机
当时，他的父亲是一位医生
对计算机如何改善公共卫生充满兴趣
但是大型机的使用门槛极高
有一天
他父亲在杂志上看到了一则广告
一个可以自己焊接组装的电脑套件
这款电脑甚至比苹果二代（Apple II）的发布
最初
它只是一个带有指示灯和拨动开关的盒子
输入指令全靠手动切换位
后来，他们添置了键盘
并且安装了BASIC解释器
正是这台简陋的电脑
为小杰夫打开了编程的大门
他手捧一本《101个BASIC语言电脑游戏》的纸质书
一个字符一个字符地敲入代码
然后修改它们，玩这些游戏
这种“创造然后使用和玩耍”的体验
让他深深着迷
也让他意识到软件可以被他人使用的巨大潜力
搬到明尼苏达州后
杰夫进入了一个技术氛围极其超前的环境
当地为所有初中和高中
提供了一个州范围的计算机系统
学生可以连接到在线聊天室
与全州的人互动
还能玩交互式的冒险游戏
那时的杰夫不过十三四岁
这简直就是“互联网诞生前的互联网”。
他沉浸在其中
不仅与其他的编程爱好者交流
还通过学习别人分享的开源软件
掌握了多用户软件的编写技巧
尽管他自嘲动手能力不强
对物理世界的构建不太擅长
但是软件的世界却让他如鱼得水
杰夫回忆起他曾经编写过一个多用户的在线游戏
当时，这款游戏的作者是一位博士生
因为即将毕业
杰夫偷偷用了一台激光打印机
把400页源代码全都打印了出来
将这个用Pascal语言为大型机编写的多用户软件
移植到自己家的UCSD Pascal系统上
这个过程让他学会了如何处理多用户、多端口中断
以及如何调度多个终端的输入
尽管他当时还是“摸着石头过河”，
但是这次经历让他对并发编程有了深刻的理解
当被问到他最习惯使用的编程语言时
杰夫毫不犹豫地提到了C++。
由于他主要从事分布式系统的工作
对底层性能有着很高要求
然而，他对C++的态度是“又爱又恨”，
因为它不安全
容易出现内存溢出等问题
而现代语言在这方面做得更好
在研究生期间，他的导师
一位编译器和编程语言专家
发明了一种名为Cecil的语言
这种语言在面向对象方法学和模块化设计方面表现出色
他们用Cecil编写了一个包含四种语言的编译器
代码量高达十万行
最终生成了三千万行的C代码
尽管Cecil语言的表达力和标准库设计都非常优秀
但是遗憾的是
全球使用它的人数可能不超过50个
杰夫·迪恩真正接触人工智能
还要追溯到他在明尼苏达大学（University of Minnesota）上大四的时候
当时
他选修了一门关于分布式和并行编程的课程
其中引入了神经网络
在20世纪90年代初
神经网络因为高度并行的计算特性和解决小型复杂问题的能力
引起了学界广泛的兴奋
当时
一个三层深的神经网络就已经被认为是“深度”的了
而如今我们用的都是上百层的神经网络
杰夫回忆说，神经网络的抽象方式
似乎与我们对人脑和动物大脑的理解
有着某种松散的联系
也就是通过人工神经元接收输入、判断兴趣、决定是否“激活”以及以什么样的强度激活
通过构建大量这样的神经元和更深的网络层级
就能形成更复杂的系统
神经网络在解决模式匹配任务上的表现
让他印象深刻
因为它能够自动学习和提取有用的特征
受到这个启发
杰夫向他的教授维潘·库马尔（Vipin Kumar）提出
希望以并行神经网络为题
进行自己的论文研究
他当时的想法是
利用系里那台32处理器的机器
训练一个比单处理器更大、更强大的神经网络
但是，他很快意识到
要实现这一目标
需要的计算能力不是32倍
而是百万倍
尽管如此
他还是实现了两种并行化神经网络训练的方法
一种是数据并行（data parallelism）
也就是将输入数据分成不同批次
每个处理器拥有网络的副本
但是只处理部分数据；
另一种是模型并行（model parallelism）
也就是将大型网络拆分成多个部分
让所有数据流经网络的各个部分
这些早期的探索
为他日后在谷歌大脑的工作奠定了基础
谈到神经网络在90年代末期的一度失宠
杰夫坦言，他从来没有“失去信仰”过
只是暂时将它给“搁置”起来了
他形容自己喜欢在不同的领域之间游走
从并行编程到公共卫生软件
再到编译器设计
他总是在寻找新的学习和探索机会
毕业后
他选择加入数字设备公司（Digital Equipment Corporation）在帕洛阿尔托（Palo Alto）市中心的研发实验室
那里汇聚了35位研究人员
从事着20多个项目
从多核处理器到早期的手持设备
再到用户界面研究
这种充满刺激性思想交流和跨领域学习的环境
正是他所向往的
后来，杰夫·迪恩加入了谷歌
在信息检索、大规模存储系统、机器学习的应用等多个领域
都留下了深刻的印记
而谷歌大脑项目的启动
正是他职业生涯中又一个重要的转折点
当时
杰夫正在从事Spanner大规模存储系统的工作
这款系统具备出色的数据一致性特性
目的是构建一个能够跨越全球数据中心的统一存储系统
随着Spanner逐渐稳定并且得到广泛应用
杰夫开始思考下一个工作方向
在一个偶然的机会
他在一个茶水间里偶遇了吴恩达（Andrew Ng）
吴恩达当时是斯坦福大学（Stanford University）的教员
每周会来谷歌X（Google X）工作一天
这次不期而遇的对话
成了谷歌大脑的“创世纪”。
吴恩达提到
他的学生们在将神经网络应用在语音和视觉方面
取得了一些有趣的进展
杰夫一听便来了兴趣，激动的说道
哦，真的吗？
我喜欢神经网络！
我们应该训练真正大的神经网络！
没想到，这句话
为谷歌大脑团队的成立埋下了种子
当时
吴恩达和他的学生们已经在使用GPU方面
取得了不错的成果
而杰夫则看到了谷歌数据中心庞大的计算资源的潜力
他提出
我们为什么不构建一个分布式的神经网络训练系统
来训练一个非常大的网络呢？
于是
他们开始用2000台计算机、16000个CPU核心
来训练大型神经网络
起初团队只有几个人
但是项目很快吸引了越来越多的人加入
逐渐训练了用于视觉的大规模无监督模型
以及用于语音的大量监督模型
并且与谷歌内部的搜索、广告等团队展开合作
最终
数以百计的团队开始使用他们最初搭建的神经网络框架
前几天我们在做吴恩达那期采访节目时
他对杰夫·迪恩的贡献给予了极高的评价
吴恩达表示
虽然斯坦福的学生们已经发现了“神经网络规模越大
性能越好”的秘密
拥有了所谓的“秘密数据”，
但是他们当时最缺乏的
正是像杰夫·迪恩这样
能够驾驭超大规模系统的思维
以及能够在单台计算机无法承载的情况下
将问题分解并且分布式处理的能力
这些是学术界里很少会教授的技能
在谷歌大脑的早期
人们认为MapReduce将是至关重要的
然而，随着项目深入
杰夫发现MapReduce的重要性略低于预期
他们观察到
当模型规模、训练数据量和计算资源的投入增加时
结果会持续改善
这催生了“更大的模型
更多的数据”这句口号
后来被量化为Scaling Laws
也就是计算量每翻一倍
结果就会有相应的提升
并且这种关系呈现出对数的特性
也正是Scaling Laws的发现
造就了今天人工智能领域的蓬勃发展
杰夫·迪恩随后回忆起谷歌大脑早期的“啊哈时刻”，
其中一个广为人知的里程碑
便是那个“发现猫咪”的无监督学习模型
它甚至登上了《纽约时报》的版面
成为了谷歌大脑的“亮相宣言”。
这个实验的原理是
他们向模型输入了来自YouTube视频的1000万帧随机图像
然后训练模型学习生成越来越高层次的特征
来描述原始的像素
本质上，模型试图找到一种压缩算法
能够从这些随机图片中提取关键信息
令人惊讶的是
模型竟然“发现”了猫的概念
通过分析最高层大约4万个神经元中
哪些神经元被特定的图像所激活
他们发现模型在优化算法中
为“猫”这个特征分配了一定容量
当他们找到了那些对猫的图像
反应最为强烈的神经元
并且试图通过这些神经元“逆向生成”图像的时候
一张“平均猫”的图像便跃然纸上
他们也用同样的方式
生成了一张诡异的人脸图像
这个实验的意义在于
它证明了神经网络在无监督学习中
能够自主地从海量数据中发现、并且抽象出高级概念
而无需人为预设这些概念
除了“猫的发现”，
谷歌大脑在语音识别和通用图像识别领域的突破也令人瞩目
他们使用经过无监督预训练的模型
在ImageNet的2万个类别数据集上进行了监督微调
结果将相对错误率降低了60%。
在语音识别方面
他们通过将原有的非神经网络声学模型
替换为神经网络模型
降低了30%的词错误率
这些成果
都是在仅仅使用800台机器训练五天的情况下实现的
这些惊人的进展
也促使谷歌开始思考定制化机器学习硬件的必要性
在谷歌大脑项目初期
曾经有人质疑是否需要专门的硬件
因为团队利用现有CPU
已经取得了如此巨大的成功
然而
杰夫·迪恩在2013年的一次实验中
清晰地看到了定制硬件的迫切性
他推测
如果一亿人每天对着手机说话三分钟
所需要的计算量将是天文数字
因此
他意识到必须寻找更好的解决方案
神经网络有两个关键的特性
一是它们主要由少数线性代数运算
比如矩阵乘法、向量点积组成；
二是它们对降低精度具有很强的容忍性
与高性能计算中需要64位、或者32位浮点数的数值模拟软件不同
神经网络可以使用非常低的精度
由此
张量处理单元TPU（Tensor Processing Unit）应运而生
第一代TPU专注于推理方面
甚至没有浮点运算
只使用8位整数运算
后来的TPU加入了BF16的降精度浮点格式
杰夫解释说
IEEE的16位浮点格式对机器学习并不理想
因为它同时损失了尾数位和指数位
而神经网络更关心的是表示更宽范围的值
对小数点后第五位的精度要求不高
因此
更好的做法是保留所有的指数位
牺牲所有的尾数位
将32位格式降到16位
从而在更宽的动态范围和可接受的精度之间取得平衡
除了这些硬件上的突破
谷歌大脑团队在自然语言处理领域方面
也取得了一些关键进展
其中最引人注目的莫过于“注意力机制”（Attention mechanism）
杰夫·迪恩解释了语言理解方面的三个主要突破
首先是对词的嵌入（word embedding）或者向量表示
比方说
用一个高维向量来表示“纽约市”或者“番茄”的内在含义和语境
这使得“king-man+woman = queen”这样的代数操作成为可能
另外，在高维空间中
方向就变得有意义了，例如
从阳性词到阴性词
从现在时到过去时
都对应着特定的方向
第二个突破是伊利亚·苏茨克维尔参与开发的Seq2Seq（Sequence to Sequence）模型
这个模型使用了长短期记忆网络LSTM（Long Short-Term Memory）
LSTM可以看作是一个具有向量状态的短时记忆系统
它能够处理一系列的词语或标记
每次更新它的状态
从而在扫描序列的时候
以向量的形式记住所有看到的信息
Seq2Seq模型能够读取一个输入序列
比如一句英语
然后用这个向量来初始化生成另一个输出序列
比如一句法语
这种模型不仅可以用来做机器翻译
还在医疗记录、单一语言理解
甚至基因组序列分析等领域
展现出了广泛的应用前景
第三个，也是最具里程碑意义的突破
就是在Transformer架构中引入的注意力机制
传统的序列模型在处理每个词的时候
都会去试图更新一个单一的向量状态
这就导致了信息传递的瓶颈
而Transformer的注意力机制则颠覆了这个思路
它不再只是记住单一的向量
而是记住所有的中间向量状态
虽然注意力机制的复杂度是O(N²)，
对空间要求较高
但是由于它能够与矩阵单元的高度并行性完美契合
使得Transformer模型在计算效率上远超之前的架构
对于人工智能的未来
杰夫·迪恩保持着自己乐观而且深刻的洞察
他认为
过去六年来模型性能的显著提升
得益于更大规模的训练、更多高质量的数据
以及Transformer等更强大的模型架构
更重要的是
模型已经从单纯的文本模式发展到了完全的多模态（multimodal）模式
能够处理人类的所有输入和输出模式
包括语音对话、视频理解、视频生成等等
这种模态转换的能力
正在催生像谷歌NotebookLM这样的新产品
用户可以上传大量PDF文件
然后要求模型生成一个由两个AI声音组成的播客
他预见到的是，未来人类的工作重心
将从“亲手制作”转向“更具体地指定想要什么”。
这或许不会让工作变得更简单
但是会释放巨大的创造力
他将这比喻为面对一个
相对不那么聪明、但是几乎无所不能的精灵一样
如果你无法明确自己想要什么
就不可能期待它能创造出什么特别的东西
所以他认为
提示词工程会成为我们未来工作和生活的重要手段
杰夫还分享了他个人使用Gemini的几个例子
他喜欢让Gemini列出某个观点的10个支持论点和10个反对论点
他发现Gemini在这方面表现得非常出色和公正
没有先入为主的立场
并且能够提供大量的切入点
帮助他思考和形成自己的观点
他将Gemini比作是一个“苏格拉底式的伙伴”。
在面对新的领域时
他会询问AI关于这个领域的问题
然后根据AI的回答提出更深入的问题
他认为
将这种世界知识与个人相结合的方式
也将是未来的一个重要趋势，比如
根据用户在东京喜欢的餐厅
来推荐亚利桑那州的类似餐厅
当然
杰夫·迪恩也一直高度关注人工智能会带来的安全、隐私和责任问题
他认为
这些都是技术人员和社会必须持续思考的问题
人工智能将深刻影响教育、医疗、经济等各个领域
他强调，作为社会中的一员
我们应该积极塑造AI的发展方向
最大化它的积极影响
同时警惕可能带来的负面影响
比如虚假信息
杰夫·迪恩还曾经与多位同事共同撰写了一篇名为《Shaping AI》的论文
探讨了AI发展中的许多社会问题
以及如何引导和塑造技术
来实现积极的成果并且最小化负面的影响
对于人工智能什么时候能够实现“自我突破”，
超越人类的创造速度
杰夫·迪恩认为在某些子领域
我们已经接近或者已经达到了这一水平
并且这个领域还会不断的扩大
他强调
这需要一个完全自动化的循环
从生成想法、进行尝试
到获取反馈、再在巨大的解决方案空间中进行探索
在具备这些特征的领域
强化学习算法和大规模的计算搜索
已经证明了这种方式的高效性
但是，在那些缺乏明确奖励信号
或者评估耗时过长的领域
想要实现自动化突破仍然面临着挑战
但是他相信，自动化的搜索和计算
将加速科学和工程等领域的进步
并且在未来的5到20年内
会极大提升人类的能力
最后，展望未来五年
杰夫·迪恩表示他希望专注在如何让强大的模型
变得更加具有成本效益
并且能够惠及数十亿人
他指出，目前最先进的模型
计算成本仍然相当高昂
所以他希望能够显著的改善这个状况
他也有一些正在酝酿中的想法
虽然不确定能否成功
但这也正是探索新方向的魅力所在
即使最终没能完全达到预期
在过程中也会产生许多有用的发现
好了
以上就是杰夫·迪恩这次访谈的主要内容了
应该说
作为谷歌大脑的奠基人、TensorFlow与TPU背后的关键推手
他亲身经历了这场神经网络革命的完整历程
在互联网时代，他创造了很多传奇
也希望在如今的AI时代
能够带给大家更多的惊喜
感谢大家收看本期视频
我们下期再见！
