大家好，这里是最佳拍档，我是大飞
9月9日的 AI 基础设施峰会上
英伟达宣布推出一款名为 Rubin CPX的新 GPU
号称专门为超过 100 万 token 的长上下文推理而设计
第二天
Semianalysis就发布了一篇专题报告
由迪伦·帕特尔（Dylan Patel）等7位行业分析师联合撰写
数据详实到甚至包含了机架的物料清单(BOM)和功率预算
今天我们就结合这篇报告
详细拆解一下Rubin CPX的技术价值和行业影响
看看为什么它被称为“AI推理基础设施的又一次巨大飞跃”。
首先，咱们得搞明白
英伟达为什么要推出 Rubin CPX 这款产品？
这就不得不提到大语言模型推理的两个关键阶段
分别是预填充阶段（Prefill Phase）和解码阶段（Decode Phase）
熟悉 AI 推理的朋友应该知道
大模型处理一个请求
并不是一步完成的
在预填充阶段
模型要根据用户输入的提示词生成第一个Token
这个阶段非常依赖计算资源
也就是我们常说的浮点运算能力（FLOPS）
但是对内存带宽的需求却相对较低；
而到了解码阶段
模型需要从 KV 缓存中加载之前生成的token
再生成新的token
这个阶段则恰恰相反
对内存带宽的需求极高
反而对计算资源的利用没那么充分
过去
行业里大多是用同一套硬件来处理这两个阶段的任务
比如用配备了高带宽内存HBM的 GPU
同时应对预填充和解码
但是问题也恰恰就出在这里
预填充阶段用不上高带宽的 HBM
导致昂贵的 HBM 资源被浪费；
而解码阶段又需要依赖 HBM 的高带宽
一旦 HBM 带宽不足
又会成为性能瓶颈
而且，随着 AI 模型越来越大
HBM 在 BOM中的占比也越来越高
像英伟达最新的 GB300 GPU
HBM 已经成为了BOM 中成本最高的单一组件
这种 “一个硬件包打天下” 的模式
不仅造成了资源的浪费
还推高了整体的总拥有成本TCO
正是看到了这个痛点
英伟达才针对性地推出了 Rubin CPX
这款加速器的核心设计思路
就是专门为预填充阶段优化
在保证足够计算能力的同时
降低对内存带宽的要求
从而大幅降低成本
接下来
咱们就来详细看看 Rubin CPX 的核心参数
也许从这些数据里
咱们能够更直观地感受到它的设计思路
Rubin CPX 是一款单片SoC
采用传统的倒装BGA封装
它的计算能力非常强劲
支持 FP4 精度的密集计算
能达到 20 PFLOPS，如果是稀疏计算
更是能达到 30 PFLOPS
这个计算能力是什么水平呢？
咱们可以和英伟达另一款用于推理的 GPU
R200 做个对比
R200 是双芯片设计
它的 FP4 密集计算能力是 33.3 PFLOPS
稀疏计算是 50 PFLOPS
也就是说，单芯片的 Rubin CPX
计算能力已经达到了双芯片 R200 的六成左右
这个表现对于一款专门针对预填充阶段的芯片来说
已经非常出色了
而在内存方面
Rubin CPX 并没有采用昂贵的 HBM
而是选择了 GDDR7 内存
容量为 128GB，内存带宽为 2TB/s
可能有朋友会觉得
2TB/s 的带宽好像比 R200 的 20.5TB/s 差远了
但这正是 Rubin CPX 的设计初衷
因为预填充阶段不需要那么高的带宽
而且，GDDR7 的成本比 HBM 低得多
按每 GB 计算
GDDR7 的成本还还不到 HBM 的一半
这一下子就把硬件的整体成本拉了下来
另外
英伟达在此次发布中还提到了一个细节
那就是R200 的 HBM4 内存速度也有了显著提升
从最初公布的 6.4Gbps 提升到了 10Gbps
这才使得 R200 的内存带宽能达到 20.5TB/s
相比之前的 13TB/s 有了不小的进步
除了计算和内存
网络连接也是 Rubin CPX 的一个重要特点
它没有采用英伟达传统的 NVLink 技术来实现横向扩展
而是选择了 PCIe Gen6 接口
通过 CX-9 网卡与其他 GPU 进行通信
NVLink 的带宽确实高
R200 的 NVLink 带宽能达到 14.4TB/s
但是对于预填充阶段的任务来说
PCIe Gen6 的带宽已经足够用了
而且
去掉 NVLink 还能进一步降低硬件成本和设计复杂度
毕竟 NVLink 相关的组件
包括 NVSwitch 和背板，成本可不低
估算下来
每块 GPU 对应的 NVLink 扩展成本
大约在 8000 美元左右
占每块 GPU 集群总成本的 10% 以上
所以
Rubin CPX 在网络连接上的选择
同样是出于成本和实用性的考量
讲完了 Rubin CPX 芯片本身
咱们再来看看它所搭载的机架系统
这次英伟达同步推出了基于 Rubin CPX 的 VR200 系列机架级服务器
一共有三种类型
分别是 VR200 NVL144、VR200 NVL144 CPX 和Vera Rubin CPX 双机架
这三种机架的设计
充分考虑了不同用户的需求
既有一体化的解决方案
也有灵活扩展的选项
首先是 VR200 NVL144
这种机架主要搭载的是 R200 GPU
一个机架包含 18 个计算托盘
每个托盘里有 4 个 R200 GPU 组件
总共是 72 个 GPU 组件
主要用于处理对内存带宽要求较高的解码阶段任务
然后是 VR200 NVL144 CPX
这种机架是 “混合配置”，
同样有 18 个计算托盘
但每个托盘里除了 4 个 R200 GPU 组件
还额外增加了 8 个 Rubin CPX GPU 组件
这样一来
一个机架里就有 72 个 R200 GPU 组件和 144 个 Rubin CPX GPU 组件
这种配置的好处是
一个机架就能同时处理预填充和解码任务
R200 负责解码
Rubin CPX 负责预填充
实现了 “一机两用”，
而且两者之间可以协同工作
减少了数据在不同机架之间传输的延迟
不过，由于组件数量增加
VR200 NVL144 CPX 的功率预算也比 VR200 NVL144 高不少
前者大约是 370kW，后者则是 190kW
所以在冷却方面
VR200 NVL144 CPX 也采用了液冷方案
以应对更高的散热需求
第三种是Vera Rubin CPX 双机架
它由两个独立的机架组成
一个是 VR200 NVL144 机架
专门用于解码；
另一个是 VR CPX 机架
专门用于预填充
这个 VR CPX 机架同样有 18 个计算托盘
每个托盘里有 8 个 Rubin CPX GPU 组件
总共 144 个
这种双机架的设计
最大的优势就是灵活性高
用户可以根据自己的业务需求
调整预填充和解码的比例
比如如果预填充任务多
就可以多部署几个 VR CPX 机架；
而且
VR CPX 机架不需要和 VR200 NVL144 机架物理上相邻
它可以通过 InfiniBand 或以太网连接到集群中
方便用户根据数据中心的布局灵活安排
另外，双机架设计还有一个好处
就是故障影响范围小
如果其中一个机架出了问题
另一个机架还能继续工作
不会导致整个服务中断
在这些机架的设计上
还有几个技术细节非常值得关注
体现了英伟达在硬件设计上的深厚积累
第一个是无电缆设计
过去的 GB200/GB300 机架中
使用了很多飞跨电缆来连接各个组件
但是这些电缆不仅在组装过程中容易损坏
而且在高密度设计的机架里
布线也非常困难
所以，在 VR200 系列机架中
英伟达去掉了这些电缆
转而采用安费诺（Amphenol）的 Paladin 板对板连接器和 PCB 中板来传输信号
信号从HPM，也叫 “Bianca” 板上
通过 Paladin 连接器传输到 PCB 中板
再从 PCB 中板传输到各个子卡
这样不仅减少了故障点
还节省了空间，让机架的密度更高
第二个是模块化设计
尤其是计算托盘里的子卡模块
VR200 NVL144 CPX 的计算托盘前端
采用了 7 个子卡模块的设计
这些子卡模块各司其职
有 4 个子卡模块是用来安装 Rubin CPX 和 CX-9 网卡的
每个这样的子卡模块里有两个 Rubin CPX、两个 800G CX-9 网卡、一个 1.6T OSFP 插槽和一个 E1
S SSD NVMe 模块；
中间有一个子卡模块安装的是 Bluefield-4 模块
里面包含一个 Grace CPU 和一个 CX-9 网卡；
还有一个子卡模块是电源分配板PDB
负责将从后部汇流条连接器进来的 48-54V 电压降到 12-13.5V
给其他组件供电；
最后一个小的子卡模块则是实用程序管理模块
里面有 BMC、HMC、DC-SCM 等管理组件
负责机架的日常管理和监控
这种模块化设计，不仅方便组装
而且后续维护也很简单
某个子卡出了问题
直接更换子卡就行
不用拆开整个托盘
第三个是冷却设计，前面咱们提到
VR200 NVL144 CPX 的功率很高
尤其是前端的 Rubin CPX 模块
每个 Rubin CPX 芯片的热设计功耗TDP大约是 800W
如果算上 GDDR7 内存
整个模块的功耗能达到 880W
18 个计算托盘前端的 Rubin CPX 模块总功耗就有 7040W
这么大的热量
用传统的风冷肯定是不够的
所以
英伟达借鉴了 2009 年 GTX 295 显卡的设计
将 Rubin CPX 和 CX-9 子卡采用夹层式设计
中间夹着一个共享的液冷冷板
而且，在 PCB 的外侧
还设计了热管和均热器
能把 GDDR7 内存模块背面的热量也传递到冷板上
这样就能确保所有发热组件都能得到有效的冷却
同时
这种夹层设计还充分利用了 1U 的托盘高度
把容纳这些GPU所需要的空间
减少了一半
进一步提高了机架的密度
了解了Rubin CPX的芯片和机架设计之后
咱们再来深入分析一下
它给AI推理领域带来的影响
尤其是在分布式服务方面的突破
之前咱们提到
传统的 AI 推理是用同一套硬件处理预填充和解码
效率很低，而 Rubin CPX 的出现
让 “硬件专用化的分布式服务” 成为了可能
这也是它被称为 “巨大飞跃” 的核心原因
首先，它解决了资源浪费的问题
用 Rubin CPX 处理预填充
不需要昂贵的 HBM
而是用成本更低的 GDDR7
而且去掉了不需要的 NVLink
硬件成本大幅降低
简单估算一下
同样是处理预填充任务
用 R200 的话
每小时因为内存带宽利用率低造成的 TCO 浪费大约是 0.9 美元
而用 Rubin CPX
虽然内存带宽利用率也不高
但因为 GDDR7 成本低
浪费的 TCO 就要少得多
而且，从整个系统来看
使用 Rubin CPX 的机架
HBM 在总系统成本中的占比也会下降
对于用户来说
同样的预算能买到更多的计算资源
性价比更高
其次，它提高了性能和稳定性
之前用同一套硬件处理预填充和解码
两个阶段的任务会相互干扰
比如预填充任务占用了太多计算资源
就会导致解码阶段的token生成延迟增加；
反之，如果优先保证解码
预填充的首Token生成时间（TTFT）又会变长
而有了 Rubin CPX 之后
预填充和解码用不同的硬件处理
相互之间没有干扰
用户可以根据SLA的要求
分别优化两个阶段的性能
比如，对于需要快速响应的场景
可以优化 Rubin CPX 的预填充速度
减少 TTFT；
对于需要高吞吐量的场景
可以优化 R200 的解码速度
提高每秒生成token的数量
这样一来
整个推理服务的性能更稳定
也更容易满足不同用户的需求
另外
Rubin CPX 在流水线并行（PP）方面也有独特的优势
对于一些大型的 MoE模型
比如 DeepSeek V3来说
它的模型权重很大
用 NVFP4 格式加载的话需要 335GB 的内存
而单个 Rubin CPX 只有 128GB 内存
根本装不下
这时候就需要用到流水线并行
把模型的不同层拆分到多个 GPU 上
每个 GPU 处理一部分层
然后把激活值沿着流水线传递下去
流水线并行的通信需求相对简单
只是简单的发送和接收操作
不需要像专家并行（EP）那样进行全对全的集体通信
所以 PCIe Gen6 的带宽就足够用了
而且
流水线并行的token吞吐量比专家并行更高
虽然首token的生成时间会稍长一些
但对于预填充阶段来说
这个权衡是值得的
不过
硬件专用化的分布式服务也有它的缺点
最主要的就是灵活性问题
不同的模型、不同的业务场景
对预填充和解码的比例
也就是 PD 比例的要求是不一样的
比如有的场景预填充任务多
有的场景解码任务多
而像 VR200 NVL144 CPX 这种机架
Rubin CPX 和 R200 的数量比例是固定的2:
1，如果用户的 PD 比例发生变化
比如预填充任务变少了
那么 Rubin CPX 就会出现闲置；
反之，如果解码任务变少了
R200 又会闲置
虽然双机架设计在这方面灵活一些
但是也需要用户提前规划好资源
否则还是会出现资源浪费的情况
从加速器的市场层面来看
这次Rubin CPX的发布
让AMD、谷歌、AWS、Meta等企业都面临着调整路线图的压力
接下来可能会采取一系列的行动
首先是AMD
AMD之前凭借MI400系列机架级系统
在内存带宽上已经接近英伟达的VR200 NVL144
而且MI400的FP4 FLOPS TCO比VR200 NVL144略低
原本已经看到了追赶的希望
但是现在
英伟达不仅把VR200的内存带宽提升到了20.5TB/s
还推出了Rubin CPX
AMD的优势一下子就没了
更关键的是
AMD缺乏强大的内部需求支撑
开发预填充专用芯片
需要大量的资金和人力投入
如果没有内部需求作为“试金石”，
芯片的迭代速度就会很慢
根据行业消息
AMD可能会推迟MI500系列的发布
优先启动预填充专用芯片的研发
预计最早要到2027年才能推出类似Rubin CPX的产品
这意味着在未来两年内
AMD在AI推理机架级市场上
会一直处于被动追赶的状态
然后是谷歌
谷歌的TPU有一个独特的优势
就是3D环形扩展网络
最大Pod规模能达到9216个TPU
而且每个加速器的扩展网络成本很低
支持的并行方案也更多
谷歌有大量的内部业务场景
比如搜索、YouTube、Gemini大模型等
这些都能为预填充专用芯片提供了稳定的需求
所以
谷歌很可能会在未来1-2年内推出一款预填充专用TPU
与现有的TPU配合使用
进一步降低内部AI推理的TCO
而且，谷歌的TPU拓扑结构特殊
在某些模型上的性能甚至能超过英伟达的系统
如果再加上预填充专用芯片
谷歌在内部AI基础设施上的优势会更加明显
甚至可能会将这种专用芯片开放给谷歌云的客户
与英伟达争夺企业客户市场
再看AWS和Meta
AWS的Trainium3 Max NVL72机架
在设计上模仿了英伟达的NVL72
有72个逻辑GPU
而且AWS有Anthropic这样的合作伙伴
能提供稳定的推理需求
不过，AWS面临一个问题
就是它的1U计算托盘已经非常紧凑了
搭载了4个大型Rubin GPU封装和8个CPX GPU封装后
就没有空间容纳AWS自己的EFA网卡了
所以
AWS很可能会采取“sidecar”的方案
把EFA网卡单独放在一个机架里
通过外部PCIe AEC线缆与VR 144 CPX机架连接
同时还需要使用Astera Labs的专用PCIe交换机
来连接各个组件
这种方案虽然能解决兼容性的问题
但是会增加一定的成本和延迟
不过AWS有足够的技术实力来优化这些问题
预计在2026年下半年能推出配套的预填充专用芯片
Meta的MTIAv4 SUE72机架
同样模仿了NVL72的设计
有72个逻辑GPU
而且Meta内部有大量的模型推理需求
比如WhatsApp、Facebook的AI助手
这些都能支撑预填充专用芯片的研发
Meta之前的MTIAv3因为只有16个GPU的规模
竞争力不足
所以这次Meta很可能会跳过MTIAv4的预填充优化
直接开发一款专门的预填充芯片
与MTIAv4 SUE72配合使用
预计在2026年就能推出相关产品
而且Meta在AI硬件上一直比较开放
未来甚至可能会将这种专用芯片对外授权
成为英伟达的另一个竞争对手
咱们再来看 Rubin CPX 对内存市场的影响
一方面，它增加了对 GDDR7 的需求
之前，GDDR7 主要用于消费级显卡
比如英伟达的 RTX Pro 6000
速度是 28Gbps
而 Rubin CPX 使用的 GDDR7 速度更高
达到了 32Gbps
英伟达已经向三星下了大量的 RTX Pro 系列订单
原本是计划用这些显卡替代 H20 进入中国市场
而 Rubin CPX 的推出
会进一步增加 GDDR7 的需求量
由于三星在 GDDR7 的供应上有一定的优势
它有足够的晶圆产能来满足英伟达的订单
而 SK 海力士和美光因为把更多的产能投入到了 HBM 生产上
可能暂时无法满足 GDDR7 的激增需求
所以三星可能会从 Rubin CPX 的发布中获得更多的收益
另一方面
虽然 Rubin CPX 减少了对 HBM 的需求比例
但这并不意味着 HBM 的整体需求会下降
因为 Rubin CPX 降低了预填充的成本
会让更多的企业愿意部署 AI 推理服务
从而增加对整个 AI 推理系统的需求
而解码阶段仍然需要大量的 HBM
就像很多技术创新一样
成本的降低会刺激需求的增长
最终 HBM 的整体需求量可能不仅不会减少
反而会因为 AI 推理市场的扩大而增加
除了这些以外
不知道大家是否还记得
英伟达的创始人兼 CEO 黄仁勋曾经提出过一个黄氏定律
大致意思是 AI 计算性能每三年会增长 10 倍
之前
推动黄氏定律的主要是两个因素
一是降低数据精度，从 FP32 到 FP16
再到 FP8、FP4
每一次精度的降低都能带来显著的性能提升；
二是稀疏性技术
通过忽略一部分不重要的数据
来提高计算效率
不过，现在数据精度已经降到了 FP4
再往下降的空间已经很小了；
而稀疏性技术虽然在营销中被频繁提及
但是实际带来的性能提升并没有达到预期的 2 倍
这也让黄氏定律的推进面临新的挑战
而Rubin CPX的出现
为黄氏定律提供了新的推动力
它不是通过提升单芯片的性能
而是通过硬件专用化和分布式服务
从系统层面提高了整体的AI推理效率
这种“系统级优化”的思路
可能会成为未来推动AI计算性能持续增长的重要方向
最后
不知道大家有没有想过一个很有意思的问题
英伟达为什么只推出了预填充专用芯片
而不推出解码专用芯片呢？
毕竟解码阶段对内存带宽的需求极高
专门设计一款芯片来优化解码
理论上也能进一步降低成本
其实
英伟达不是没有考虑过这个问题
只是目前推出解码专用芯片的时机还不成熟
首先
解码阶段的需求比预填充更复杂
不同模型、不同批次大小
对内存带宽的需求差异很大
而且解码阶段需要频繁访问KV缓存
对内存延迟也有一定要求
设计一款能适配所有场景的解码专用芯片
难度比预填充专用芯片高很多
其次
现有的R200在解码阶段的表现已经非常出色
20.5TB/s的内存带宽和288GB的HBM容量
能满足目前绝大多数模型的解码需求
短期内不需要专门的芯片来替代
不过，从长期来看
解码专用芯片是必然的趋势
一款解码专用芯片
应该会采用与Rubin CPX相反的设计思路
计算能力较弱，但内存带宽极高
因此
它可能会保留R200的I/O chiplet
以保证内存和封装外I/O的性能
但是会缩小主计算芯片的面积
同时在每个芯片边缘保留足够的HBM位点
这样既能保证高带宽
又能降低计算芯片的成本
而且
简化的计算单元能提高芯片的参数良率
降低TDP
进一步减少电源和冷却的成本
如果英伟达推出了解码专用芯片
那么整个AI推理系统就会形成“预填充芯片+解码芯片”的完全专用化架构
TCO还能再降低15%-20%，
而且性能会更稳定
不过，这个过程可能需要2-3年的时间
因为英伟达需要先观察Rubin CPX的市场反馈
再根据用户需求来优化解码专用芯片的设计
好了
以上就是对英伟达这次发布的Rubin CPX的介绍了
应该说
Rubin CPX不仅是一款硬件产品的更新
更是标志着AI推理硬件从“通用化”向“专用化”的转型
它通过针对性的设计
解决了预填充阶段HBM资源浪费的问题
大幅降低了TCO；
同时，它的机架架构设计
为分布式服务提供了灵活的解决方案
进一步提升了系统的效率和稳定性
对于竞争对手来说
Rubin CPX的出现打乱了他们的路线图
迫使他们投入更多资源来开发预填充专用芯片
这在短期内会拉大与英伟达的差距
而对于整个AI行业来说
Rubin CPX的创新思路
也为未来AI硬件的发展提供了新的方向
那就是从单芯片的性能提升
转向系统级的专业化优化
这可能会成为推动AI推理技术持续进步的关键动力
那么大家对于Rubin CPX的发布和未来专用芯片的发展是如何看的呢
欢迎在评论区留言
感谢观看本期视频，我们下期再见
