大家好，这里是最佳拍档
我是大飞
在上两节课里
我们完成了神经网络的基础搭建
从28×28像素的手写数字图片
如何映射为输入层784个神经元的激活值
到两层隐藏层如何通过权重、偏置和sigmoid函数传递信息
再到梯度下降如何像下山一样
调整13002个参数
最终最小化代价函数
我相信很多朋友看到这里
心里一定会有一个疑问
那就是经过梯度下降训练后的神经网络
真的像我们预设的那样聪明吗？
它的隐藏层真的在识别边缘、组合图案
最终看懂了数字吗？
还是说
它只是在训练数据中找到了一套应试技巧
看似能解决问题
却根本不理解背后的逻辑呢？
今天这期视频
我们就来一场黑箱探秘
不上新的数学公式
不推复杂的算法流程
专门聚焦训练后的神经网络
用直观的实验和分析
看看隐藏层里到底在发生什么
既有让人惊喜的表现
也有暴露本质的愚蠢时刻
这不仅能让我们对神经网络的真实能力
有更加客观的认知
也能为后续深入学习反向传播算法做好铺垫
更能让我们明白
为什么基础神经网络之后
会诞生卷积神经网络这样的现代技术
首先
我们先来看这个训练完毕的神经网络
到底交出了怎样的答卷
我们的训练数据和测试数据
都来自经典的MNIST数据库
这个数据库包含了数万张由不同人手写的数字图片
每张都标注了正确答案
而且所有图片都是28×28像素、居中显示、灰度值归一化到0.0-1.0之间的
经过梯度下降迭代训练后
我们用网络从未见过的测试数据进行验证
结果显示
它的正确识别率大约在96%左右
这个成绩该该如何评价呢？
如果单看数字，96%似乎不算顶尖
但是结合任务难度来看
这已经是一份相当出色的入门答卷
要知道
我们从来没给网络编写过任何一条识别数字3的规则
也没有告诉它边缘是什么
曲线是什么
它完全是通过观察数万张训练图片
自己调整13002个权重和偏置
硬生生摸索出了数字的特征规律
而且，这个成绩还有提升的空间
如果我们调整一下隐藏层的结构
比如把每个隐藏层的神经元数量
从16个增加到32个
或者微调学习率、增加训练迭代次数
它的准确率可以轻松提升到98%。
这意味着
在100个从来没见过的手写数字中
它只会犯两个左右的错误
这样的表现已经能满足很多简单场景的需求了
当然，我们也要客观看待这个成绩
它还远没达到能力的天花板
现代神经网络，比如卷积神经网络CNN
通过引入局部感受野、权值共享等设计
在MNIST数据集上的测试准确率
可以达到99.75%以上
这个数字有多惊人呢？
我们可以翻一翻MNIST数据库里的测试样本
有些手写数字的潦草程度
连人类都要犹豫半天
比如有的3和8几乎融为一体
有的7和1只有细微的差别
而顶尖神经网络能在这样的样本中
能做到99.75%的准确率
已经超过了大多数人类的表现
不过
当我们回头看这个基础神经网络的错误案例时
又会发现很多情有可原的情况
比如有一张手写数字
看起来既像3又像7
笔画的弧度和长度刚好卡在两者之间
还有一张数字6，尾部的弯钩太长
几乎触碰到了顶部的圆圈
网络误判成了0
如果把这些错误案例拿给人类判断
很多人也会犹豫甚至出错
所以对于这个结构简单的基础神经网络
我们确实可以手下留情
它能做到96%的准确率
已经超出了最初的预期
但是成绩背后
还隐藏着一个更值得我们探究的问题
这个网络到底是怎么做到的呢？
它的隐藏层真的像我们之前视频中设想的那样
在第一层识别边缘、第二层组合成曲线和圆圈吗？
为了找到答案
我们需要用到一个关键工具
权重可视化
还记得之前介绍过的层间信息的传递逻辑吗？
每个隐藏层神经元的激活值
都来自输入层神经元激活值与对应权重的加权和
再加上偏置后经过sigmoid函数压缩
这意味着
每个隐藏层神经元都对应着一套输入层的权重图案
如果我们把输入层到某个隐藏层神经元的784个权重
按照28×28像素的原始图片尺寸重新排列
就能得到一张权重地图
这张地图里，蓝色的像素代表正权重
表示输入层神经元激活时
会促进当前隐藏层神经元激活
红色的像素代表负权重
表示输入层神经元激活时
会抑制当前隐藏层神经元激活
像素的亮度则代表权重的绝对值大小
亮度越高
影响越强
按照我们最初的设想
第一层隐藏层的权重地图
应该会呈现出明显的边缘特征
比如有的权重地图是水平边缘探测器
也就是中间一排像素为正权重
上下为负权重
有的是垂直边缘探测器
也就是中间一列像素为正权重
左右为负权重
还有的是倾斜边缘探测器
但是当我们真正可视化了训练后网络的第一层隐藏层权重时
结果却让人大跌眼镜
这些权重地图根本没有清晰的边缘特征
反而呈现出松散、近似随机的模糊纹理
有的像一团不规则的墨渍
有的只是几个分散的亮斑
几乎找不到我们预期的边缘探测器图案
这到底是为什么呢？
答案其实藏在梯度下降的本质里
我们的神经网络在13002维的参数空间中
通过梯度下降寻找的是代价函数的局部最小值
只要找到一组权重和偏置
能让网络在训练数据上的平均代价足够小
梯度下降就会停止
但是这个局部最小值
并不一定对应着泛化性强的特征映射
它可能只是一组刚好能拟合训练数据的参数组合
简单来说
网络并没有主动学习边缘、图案这些通用特征
而是找到了一套专属的技巧
能把训练数据中的像素模式和数字标签对应起来
从而在测试数据上取得不错的成绩
但是这套技巧
并不依赖于我们预设的特征提取逻辑
为了更直观地理解这一点
我们可以做一个极具冲击力的实验
给网络输入一张完全随机的噪声图像
这张图像的每个像素值都是随机生成的
没有任何规律
更不是任何数字的形状
按照我们的直觉
网络应该表现出不确定性
要么所有输出层神经元的激活值都接近0.0
要么都比较平均
说明它不知道这是什么
但是实际结果却恰恰相反
网络会非常自信地输出一个数字
比如激活值最高的是对应5的神经元
激活值达到0.92
和它识别一张真实5的图片时的置信度几乎一样
这个实验暴露了神经网络的一个核心认知缺陷
它根本没有不确定性的感知能力
在它的世界里，所有的输入都是数字
因为它的训练环境太单一了
我们喂给它的所有训练数据都是规范的手写数字
代价函数只要求它尽可能准确地输出标签
却没有教会它
要判断输入是否是数字
从网络的角度来看
它的整个宇宙就是28×28像素的居中灰度图
里面只有0-9这10种事物
所以无论输入是什么
它都会从这10种事物中选一个最像的
并且给出高置信度的判断
哪怕输入的是完全无意义的噪声
这也引出了基础神经网络的另一个关键局限
那就是训练环境的强约束
会导致它缺乏泛化能力
我们的训练数据中
所有的数字都是居中、固定尺寸的
所以网络只学会了识别居中、固定尺寸的数字
如果我们稍微改变输入图像的形态
比如把数字画得太大
超出了原始训练数据的尺寸范围
或者把数字画得太小
集中在图片的角落
又或者把数字偏移一点
不在图片正中心，网络就很容易出错
为什么会这样？
因为基础神经网络没有利用图像的空间结构信息
在它的眼里
输入层的784个神经元是完全独立的
它不知道某个像素和它旁边的像素是相邻的
也不知道边缘是由相邻像素的亮度差异形成的
更重要的是，它没有知识的迁移能力
它在图片左上角学到的某个像素模式
无法应用到右下角的相同模式上
其实，这台神经网络的本质
就是20世纪80-90年代就已经成熟的多层感知机MLP
它是神经网络的入门款
也是我们理解核心原理的最佳载体
但是它的设计确实存在先天不足
我们之所以在一开始预设它会学习边缘、图案
是因为这是人类识别数字的逻辑
而神经网络的学习逻辑和人类完全不同
它不需要遵循人类的认知框架
只需要找到能拟合数据的参数组合即可
不过
虽然这台网络的隐藏层没有按照我们的预期工作
但是通过互动演示工具
我们依然能直观地观察它的决策过程
现在这个演示工具又有了升级
当你在画布上绘制数字的时候
工具会实时更新输入层、隐藏层和输出层所有神经元的激活值
你能清晰地看到，随着你笔画的增加
哪些神经元被激活了，激活值是多少
最终会如何影响输出层的判断
更酷的是
如果你点击第二层隐藏层的任何一个神经元
就能看到它对应的权重地图
也就是我们之前说的28×28像素的权重图案
建议大家亲自尝试一下
比如先在画布上画一个标准的数字
观察输出层对应的神经元激活值
再看看第二层隐藏层中
哪些神经元的激活值较高
然后，你慢慢的把它改成其他的数字
在这个过程中
你会发现输出层的激活值会逐渐变化
对应原来数字的神经元激活值下降
对应新数字的神经元激活值上升
而第二层隐藏层中
那些原本激活的神经元会被逐渐抑制
新的神经元会被激活
这个过程能让你直观地感受到
网络的决策不是一步到位的
而是通过隐藏层神经元的激活变化
逐步调整对输出的判断
而这些变化的背后
就是权重和输入像素的相互作用
看到这里，可能有朋友会觉得
神经网络也不过如此，甚至有些失望
但我想告诉大家的是
这正是学习的意义所在
只有看清基础技术的局限
我们才能理解后续技术迭代的必要性
这个基础神经网络虽然有很多不足
但是它的核心原理
比如神经元、权重偏置、梯度下降
是所有现代神经网络的基石
现代神经网络，比如卷积神经网络
正是为了解决这些局限而诞生的
它通过局部感受野让网络关注相邻的像素
通过权值共享
让在图片一个区域学到的特征
可以应用到其他区域
以及通过池化层增强对尺度变化的适应性
从而解决了基础神经网络的空间信息利用不足、泛化能力弱等问题
不过
我们现在还不需要着急去学习现代神经网络
因为还有一个核心问题没有解决
梯度下降中至关重要的梯度
到底是怎么高效计算的？
我们之前只讲了梯度下降的逻辑
沿着负梯度方向调整参数
但是对于有13002个参数的网络
如何快速计算每个参数的偏导数
也就是梯度向量的每个分量
其实才是训练网络的关键
如果我们逐个计算每个参数的偏导数
计算量会大到无法承受
而反向传播（Backpropagation）算法
就是解决这个问题的引擎
它利用链式法则
从输出层反向推导到输入层
一次性计算出所有参数的偏导数
把梯度计算的复杂度从指数级降到线性级
让大规模神经网络的训练成为可能
所以，下个视频
我们将聚焦反向传播算法
从数学原理到直观理解
一步步拆解它的计算过程
我们会详细讲解
如何从输出层的代价函数出发
反向计算每个隐藏层神经元的误差
再通过误差计算每个权重和偏置的偏导数
最终得到完整的梯度向量
理解了反向传播
你才算真正掌握了神经网络训练的核心
也才能为后续学习卷积神经网络、循环神经网络等现代技术
打下了坚实的基础
回到我们今天的主题
分析训练后的神经网络
到底有什么价值呢？
我想，最大的价值在于破除神秘感
很多人会把神经网络当成黑箱
觉得它的决策过程不可理解、充满魔力
但是通过今天的分析我们能发现
它的本质是一套基于数据的参数优化系统
它的表现好坏
取决于参数是否找到合适的局部最小值
而它的局限
往往源于结构设计的先天不足
这个基础的神经网络
虽然没有按照我们的预期学习边缘和图案
但是它依然能在手写数字识别任务上
取得不错的成绩
这恰恰证明了神经网络的强大
它不需要人类的认知指导
就能通过数据自主找到解决方案
而它的局限，也让我们明白
人工智能的进步
不仅需要算法的优化
更需要结构的创新
从多层感知机到卷积神经网络
从循环神经网络到Transformer
每一次技术突破
都是为了解决前一代技术的局限
让AI能更好地理解数据、利用数据
在结束今天的内容之前
我想给大家留一个思考问题
如果让你改进这个基础神经网络
让它能更好地利用图像的空间信息
你会从哪里入手呢？
是改变权重的连接方式
还是调整训练目标
或者是设计新的激活函数呢？
这个问题的答案
其实就藏在卷积神经网络的设计理念里
我们后续会详细探讨
感谢收看本期视频，我们下期再见
