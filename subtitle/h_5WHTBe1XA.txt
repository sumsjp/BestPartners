大家好，这里是最佳拍档，我是大飞
昨天
我们又见证了AI圈里的一场“神仙打架”
OpenAI、Anthropic、Google DeepMind这三大巨头
不约而同地放出了各自的“王炸”产品
其中
最让人意想不到的还要属OpenAI
他们时隔五年
再次发布了开放权重语言模型
而且一下子推出了两款开源模型
分别是gpt-oss-120b和gpt-oss-20b
要知道，OpenAI在过去的五年里
一直走的是闭源路线
虽然曾经开源过Whisper和CLIP等模型
但是上一次他们开源语言模型
还是在2019年的GPT-2
而这次放出的开放模型
性能不仅可以达到o4-mini水平
并且能在高端笔记本上运行
CEO Sam Altman也在X平台上发推文宣传
认为这是技术上的重大胜利
那么
这两款模型究竟有什么特别之处呢？
今天大飞就来给大家介绍一下
我们先来看看gpt-oss-120b
它是一款大型模型
总参数量达到了1170亿
激活参数为51亿
别看参数这么庞大
它却能够在单个H100 GPU上运行
只需要80GB内存
这意味着它不仅可以部署在数据中心
还能在高端台式机和笔记本电脑上运行
适用于生产环境、通用应用和高推理需求的场景
而gpt-oss-20b则属于一款中型模型
总参数量为210亿
激活参数为36亿
它专门针对更低延迟、本地化或专业化使用场景进行了优化
只需要16GB内存就能运行
这对于大多数现代台式机和笔记本电脑用户来说
无疑是个好消息
意味着他们不用为了运行这款模型而特意去升级硬件
更重要的是
这两款模型都采用了Apache 2.0许可证
这就意味着开发者可以自由地构建、实验、定制和进行商业部署
无需遵守copyleft的限制
也不用担心专利风险
这种开放的许可模式
为各种实验和商业应用场景提供了极大的便利
同时
两款模型还支持可配置的推理强度
开发者可以根据具体的使用场景和延迟需求
轻松调整推理强度
在低、中、高之间进行选择
从而在性能与响应速度之间找到最佳平衡点
而且，模型提供了完整的思维链
让开发者能够全面访问模型的推理过程
这不仅便于模型的调试
还能够增强开发人员对输出结果的信任
不过这个功能并不适合展示给最终用户
从技术规格来看
OpenAI这次确实算是“动真格”了
并没有拿出缩水版的开源模型来敷衍了事
而是推出了性能直逼自家o4-mini的诚意之作
根据OpenAI公布的基准测试结果
gpt-oss-120b在竞赛编程的Codeforces测试中
表现优于o3-mini，与o4-mini持平；
在通用问题解决能力的MMLU和HLE测试中
同样超越o3-mini
接近o4-mini水平
在工具调用的TauBench评测中
gpt-oss-120b同样表现优异
甚至超过了像o1和GPT-4o这样的闭源模型；
在健康相关查询的HealthBench测试和竞赛数学的AIME 2024及2025测试中
它的表现甚至超越了o4-mini
虽然gpt-oss-20b的参数规模较小
但是在这些相同的评测中
仍然表现出与OpenAI o3-mini持平或者更优的水平
特别是在竞赛数学和健康领域表现尤为突出
不过，需要提醒大家的是
虽然这些模型在健康相关查询的测试中表现出色
但它们不能替代医疗专业人员
也不应用于疾病的诊断或治疗
大家在使用时一定要谨慎
要想理解这两款模型为何能够实现如此出色的性能
我们需要深入了解一下它们背后的技术架构和训练方法
根据OpenAI公开的模型卡信息
gpt-oss模型采用了OpenAI最先进的预训练和后训练技术进行训练
特别注重推理能力、效率以及在各种部署环境中的实际可用性
这两款模型都采用了先进的Transformer架构
并且利用MoE架构来大幅减少处理输入时所需激活的参数数量
其中，gpt-oss-120b 有 36 层
128 个专家
每个 token 激活 4 个专家
gpt-oss-20b 有 24 层
总共有 32 个专家
每个 token 也是激活 4 个专家
两个模型都采用了类似GPT-3的交替密集和局部带状稀疏注意力模式
为了进一步提升推理和内存效率
还使用了分组多查询注意力机制
组大小设置为8
通过采用旋转位置编码（RoPE）技术进行位置编码
模型还原生支持最长128k的上下文长度
在训练数据方面
OpenAI在一个主要为英文的纯文本数据集上训练了这些模型
训练内容重点放在了STEM领域知识、编码能力和通用知识等方面
与此同时
OpenAI这次还同时开源了一个名为o200k_harmony的全新分词器
这个分词器比OpenAI o4-mini和GPT-4o所使用的分词器
更加全面和先进
通过更紧凑的分词方式
可以让模型在相同的上下文长度下
处理更多的内容
比如原本一句话会被切成20个token
用更优的分词器可能只需10个
这对于长文本的处理来说尤其重要
除了强大的基础性能以外
这些模型在实际应用能力方面同样表现出色
gpt-oss模型可以兼容Responses API
原生支持函数调用、网页浏览、Python代码执行和结构化输出等功能
举个例子
当用户询问gpt-oss-120b过去几天在网上泄露的细节时
模型会首先分析和理解用户的请求
然后主动浏览互联网寻找相关的泄露信息
连续调用浏览工具多达27次来搜集信息
最终给出详细的答案
值得一提的是
从这个演示案例中我们可以看到
这次的模型完整提供了思维链（Chain of Thought）
OpenAI给出的说法是
他们特意没有对思维链的部分进行“驯化”或者优化
而是保留了它的“原始状态”
在他们看来
这种设计理念背后有着自己的考虑
如果一个模型的思维链没有被专门对齐过
开发者就可以通过观察它的思考过程
来发现可能存在的问题
比如违反指令、企图规避限制、输出虚假信息等等
因此
他们认为保持思维链的原始状态很关键
因为这有助于判断模型是否存在欺骗、滥用或越界的潜在风险
比如说
当用户要求模型绝对不允许说出“5”这个词
任何形式都不行的时候
模型虽然在最终输出中确实遵守了规定
没有说出“5”，
但是如果查看模型的思维链
就会发现模型其实在思考过程中
还是偷偷提到了“5”这个词
当然，对于如此强大的开源模型
安全性问题自然成为业界最为关注的焦点之一
在预训练期间
OpenAI过滤掉了与化学、生物、放射性等某些有害数据
在后训练阶段
OpenAI也使用了对齐技术和指令层级系统
教导模型拒绝不安全的提示并防御提示注入攻击
此外
为了评估开放权重模型可能被恶意使用的风险
OpenAI号称进行了前所未有的“最坏情况的微调”测试
他们通过在专门的生物学和网络安全数据上微调模型
针对每个领域创建了一个领域特定的非拒绝版本
模拟攻击者可能会采取的做法
随后，再通过内部和外部测试
评估了这些恶意微调模型的能力水平
正如OpenAI在安全论文中指出的内容
这些测试表明
即使利用OpenAI领先的训练技术进行强有力的微调
这些恶意微调的模型根据公司的准备度框架
也无法达到高危害的能力水平
这个恶意微调的方法经过了三个独立专家组的审查
并且针对性的提出了改进的训练过程和评估建议
其中许多建议已被OpenAI采纳并且在模型卡中详细说明
在确保安全的基础上
OpenAI在开源策略上也展现出了前所未有的开放态度
两款模型都采用了宽松的Apache 2.0许可证
如前面所说
这为开发者提供了极大的自由
同时
模型使用了MoE层的原生MXFP4精度进行训练
极大降低了硬件门槛
OpenAI还在模型的后训练阶段中
加入了对harmony格式的微调
以便让模型能更好地理解和响应这种统一、结构化的提示格式
为了便于采用
OpenAI还同时开源了Python和Rust版本的harmony渲染器
此外
OpenAI还发布了用于PyTorch推理和苹果Metal平台推理的参考实现
以及一系列的模型工具
当然，技术创新固然重要
但是想要让开源模型真正的发挥出价值
还需要整个生态系统的支持
为此
OpenAI在发布模型前与许多第三方部署平台建立了合作关系
包括Azure、Hugging Face、vLLM、Ollama
LM Studio和AWS等等
在硬件方面
OpenAI与英伟达、AMD、Cerebras和Groq等厂商都有合作
以确保在多种系统上实现优化性能
根据各厂家公开的测试结果
在Groq的芯片上，gpt-oss的推理速度
最高可以达到每秒1200个token
而Cerebras则称自己创造了新的速度记录
gpt-oss-120b版本的推理速度达到了每秒3000个token
号称史上最快的OpenAI模型
根据模型卡披露的数据
gpt-oss模型在英伟达H100 GPU上使用PyTorch框架进行训练
并且采用了专家优化的Triton内核
其中
gpt-oss-120b的完整训练耗费了210万H100小时
而gpt-oss-20b的训练时间则缩短了近10倍
两款模型都采用了Flash Attention算法
不仅大幅降低了内存需求
还加速了训练过程
有网友分析认为
gpt-oss-20b的预训练成本低于50万美元
英伟达CEO黄仁勋也借着这次合作打了波广告
说道
OpenAI向世界展示了基于英伟达AI可以构建什么
现在他们正在推动开源软件的创新
而微软还特别宣布将为Windows设备
带来GPU优化版本的gpt-oss-20b模型
这个模型由ONNX Runtime驱动
支持本地推理
并且通过Foundry Local和VS Code的AI工具包提供
使得Windows开发者更容易的使用开放模型来进行构建
OpenAI还与早期的合作伙伴
比如AI Sweden、Orange和Snowflake等机构进行了深入合作
从而了解开放模型在现实世界中的应用
这些合作涵盖了从本地托管模型
到在专门的数据集上进行微调等各种应用场景
正如Sam Altman在后续发文中所强调的那样
这次开源发布的意义远不止于技术本身
他们希望通过提供这些一流的开放模型
赋能每个人
包括从个人开发者到大型企业
再到政府机构
都能够在自己的基础设施上
运行和定制AI
在这次历史性的开源发布背后
有一位技术人员值得我们特别的关注
他就是领导gpt-oss系列模型基础设施和推理工作的李卓翰（Zhuohan Li）
公开资料显示，他毕业于北京大学
师从计算机科学领域的知名教授王立威与贺笛
随后
他前往加州大学伯克利分校攻读博士学位
在分布式系统领域权威学者伊翁·斯托伊卡（Ion Stoica）的指导下
在伯克利RISE实验室担任博士研究员近五年时间
他的研究聚焦于机器学习与分布式系统的交叉领域
特别专注于通过系统设计
来提升大模型推理的吞吐量、内存效率和可部署性
而这些正是让gpt-oss模型能够在普通硬件上高效运行的关键技术
在伯克利期间
李卓翰还深度参与并且主导了多个在开源社区产生深远影响的项目
作为vLLM项目的核心作者之一
他通过PagedAttention技术
成功解决了大模型部署成本高、速度慢的行业痛点
这个高吞吐、低内存的大模型推理引擎已经被业界广泛采用
他还是Vicuna的联合作者
在开源社区引起了巨大反响
此外，他参与研发的Alpa系列工具
也推动了模型并行计算和推理自动化的发展
在学术方面
根据Google Scholar的数据
李卓翰的学术论文引用量已经超过了15000次
h-index达到18
他的代表性论文
比如MT-Bench与Chatbot Arena、Vicuna、vLLM等
都获得了数千次的引用
在学术界产生了广泛影响
应该说，这次gpt-oss系列模型的发布
与他的贡献密不可分
不过就在OpenAI宣布开源gpt-oss系列模型的同一时期
昨天的AI圈还有另外两个重大消息
分别是Anthropic也推出了重磅更新
Claude Opus 4.1
以及Google DeepMind发布了新的世界模型Genie 3
一句话就能实时生成可交互世界
这部分内容我打算单独做一期节目来介绍
我们先来说Claude Opus 4.1
它是对前代Claude Opus 4的全面升级
重点强化了Agent任务的执行、编码和推理能力
目前，这款新的模型
已经向所有得付费Claude用户和Claude Code用户开放
同时也已在Anthropic API、亚马逊Bedrock以及Vertex AI平台上线
基准测试结果显示
Opus 4.1在SWE-bench Verified达到了74.5%的成绩
将编码性能推向了新高度
此外
它还提升了Claude在深度研究和数据分析领域的能力
特别是在细节跟踪和智能搜索方面
根据GitHub的官方评价
Claude Opus 4.1在绝大多数能力维度上
都超越了Opus 4
其中多文件代码的重构能力的提升尤为显著
而Windsurf则提供了更为量化的评估数据
在它专门设计的初级开发者基准测试中
Opus 4.1相比Opus 4提升了整整一个标准差
这种性能跃升的幅度
大致相当于从Sonnet 3.7升级到Sonnet 4所带来的改进
Anthropic还透露将在未来几周内发布对模型的重大改进
考虑到当前AI技术迭代之快
这是否意味着Claude 5即将登场
让我们拭目以待
在定价方面
Claude Opus 4.1采用了分层计费模式
输入处理费用为每百万token 15美元
输出生成费用为每百万token 75美元
写入缓存的费用为每百万token 18.75美元
而读取缓存每百万token仅为1.50美元
这种定价结构有助于降低频繁调用场景下的使用成本
五年的时间，对于AI行业来说
也许可以完成从开放到封闭
再从封闭回归开放的一个轮回
当年那个以“Open”为名的OpenAI
在经历了长达五年的闭源时代后
终于用gpt-oss系列模型向世界证明
它还记得自己名字里的那个“Open”
只不过
这次回归恐怕更多的是形势所迫
在DeepSeek等开源模型攻城略地
开发者社区怨声载道之际
OpenAI才宣布开源模型
历经一再跳票之后
今天终于来到我们面前
即便如此，也难逃网友的吐槽
OpenAI的这次开源其实并不彻底
虽然权重是公开的
但是训练过程不公开
数据源也不公开
强化学习方法也不公开
即使可以下载模型，但是也无法复现
Sam Altman曾经在一月份坦言
我们在开源方面一直站在历史的错误一边
似乎也道出了背后的真正原因
至于OpenAI的open，究竟能走多远
我觉得大家不必抱有太大的期望
毕竟就连Meta也开始从开源转向闭源
商业的本质从未改变
最好的东西也永远不会免费
感谢大家收看本期视频
我们下期再见
