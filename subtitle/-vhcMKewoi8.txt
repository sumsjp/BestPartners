大家好，这里是最佳拍档，我是大飞
在人工智能飞速发展的今天
我们似乎已经习惯了各种模型不断刷新的能力上限
但是你是否想过
当下的人工智能与人类大脑相比
究竟还缺了点什么？
虽然深度学习推动着人工智能不断向前迈进
各种大模型在语言处理、图像识别等领域
也都取得了令人瞩目的成绩
但是即便如此
如今的AI在灵活性、效率
以及像人类一样举一反三、理解世界的能力方面
依旧存在不小的差距
这背后的原因究竟是什么呢？
来自东京的Sakana AI公司给出了他们的思考
这家公司由“Transformer八子”之一的利昂·琼斯（Llion Jones）联合创立
Sakana AI认为，问题的关键或许在于
我们在简化AI模型的时候
丢掉了生物大脑的一个核心要素
那就是时间
基于这个想法
他们推出了一种新型的AI模型
连续思维机器
Continuous Thought Machine
简称CTM
这个模型的出现
在AI领域引起了不小的轰动
今天我们就来简单介绍一下
要理解CTM的创新之处
我们得先回顾一下传统人工神经网络的发展历程
自从20世纪80年代以来
基于人工神经元的基础模型
在很大程度上没有发生过变化
研究人员主要使用神经元的单一输出
也就是代表神经元放电情况的信号
却忽略了神经元相对于其他神经元放电的精确时间
尽管深度学习在2012年带来了人工智能能力的重大飞跃
但是这个基础问题依旧存在
而在生物大脑中
有大量证据表明这种时序信息是至关重要的
比如在脉冲时序依赖可塑性
简称STDP的机制里
神经元之间放电的时间差
是生物大脑功能的基础
这就好比乐队演奏的时候
每个乐器手演奏音符的先后顺序和节奏
共同构成了美妙的音乐
但是一旦顺序或者节奏乱了
音乐就会变得杂乱无章
同样，在大脑中
神经元放电的时序信息如果被打乱
大脑的正常功能也会受到影响
CTM模型则试图改变这一现状
它的第一个创新点是引入了“内部思考维度”。
CTM可以在一个独立于外部数据输入节奏的内部时间维度上运行
想象一下
当我们看到一幅复杂的图像
或者面对一个需要解决的难题时
我们会在脑海里先“琢磨琢磨”，
思考从哪里入手、如何解决
CTM就具备类似人类的能力
无论面对静态数据
比如图像或者序列数据
它都能够在内部进行多步的“思考”和推理
迭代地构建和优化它对数据的理解
第二个创新点是神经元级别的时序处理NLMs
在CTM里
每个神经元不再像传统神经网络中那样
只是简单地应用像ReLU这样的激活函数
相反
每个神经元都有自己独立的、带参数的模型
比如一个小型的多层感知器MLP
这个模型会处理一小段的历史输入信号
也就是预激活
来计算当前的输出，也就是后激活
这意味着每个神经元都能根据近期的输入
来动态地调整自己的行为
从而产生极其丰富和复杂的神经活动模式
就像一个经验丰富的运动员
会根据比赛过程中的各种情况
不断调整自己的战术和动作
CTM中的神经元也能根据输入的变化
灵活地调整自己的输出
CTM最具颠覆性的创新点
还要属将神经同步作为核心表征（Neural Synchronization as Representation）
传统的神经网络通常会依赖神经元的激活值向量
作为信息载体
而CTM则另辟蹊径
它会追踪神经元在内部思考过程中的放电历史
计算不同神经元之间活动的同步程度
进而形成一个“同步矩阵”。
这个“同步信息”被直接用作模型的潜在表征（latent representation）
驱动模型进行注意力分配
也就是决定关注哪里
以及做出最终的预测输出
也就是做什么或者怎么做
简单来说
神经元之间如何协同工作的动态模式
成为了CTM理解世界和做出决策的核心依据
这就好比是一个团队
成员之间默契的配合协作
也就是行动的同步性
往往决定了团队任务的完成质量
CTM中的神经元也是如此
它们之间的同步程度也决定了模型的表现
为了展示CTM的强大功能和可解释性
Sakana AI团队还进行了一系列实验
其中最具代表性的是迷宫求解和图像识别任务
先来看迷宫求解任务
CTM会面对一个自上而下的二维迷宫
需要输出求解迷宫所需的步骤
这个任务并不容易
因为模型不能仅仅输出路径的视觉呈现
而是要真正理解迷宫的结构
并且规划出解决方案才行
在这个过程中
CTM内部连续的“思考步骤”就发挥了重要作用
因为它能够制定计划
我们可以直观地看到它在每个思考的步骤中
会关注迷宫的哪些部分
令人惊讶的是
CTM学会了一种非常类似人类的求解迷宫的方法
从它的注意力模式来看
它就像是在沿着迷宫的路径前进
而且
这种行为并非研究人员的刻意设计
而是在模型架构中自然产生的
研究团队发现
当允许CTM有更多的思考步骤时
它会继续沿着路径前进
甚至超过训练时设定的时间点
这表明它确实掌握了这个问题的通用解决方案
这就好比一个人在多次尝试后
真正理解了走迷宫的技巧
无论迷宫的规模如何变化
都能找到出口
我们再来看看图像识别任务
研究团队以ImageNet基准测试为例
展示了CTM在这方面的能力
传统的图像识别系统通常只需要一步就能做出分类决定
但是CTM却不同
它会采取多个步骤来检查图像的不同部分
然后再做出决策
这种循序渐进的方法带来了许多的好处
不仅让人工智能的行为更具有可解释性
还提高了准确性
简单来说，CTM“思考”的时间越长
答案就越准确
而且，它还能根据图像的复杂程度
自适应地调整思考时间
比如在识别大猩猩的时候
CTM的注意力会从眼睛转移到鼻子
再转移到嘴巴
这种模式与人类的视觉注意力其实非常相似
同时
CTM对自己预测的置信度非常“诚实”，
校准度优秀
甚至比人类标注者的平均水平还要高
另外，在处理图像的时候
CTM的神经元活动模式也更加多样和复杂
呈现出了类似生物神经信号的多尺度、甚至周期性振荡的行为
这与传统模型相对单调的动态
形成了更加鲜明的对比
除了迷宫求解和图像识别
CTM在其他任务中也有出色表现
在排序、奇偶校验（Parity）、Q&A MNIST
也就是看图回答计算题等任务里
CTM都展现出了良好的性能
尤其是在Q&A MNIST任务中
即使需要回忆的数字已经超出了神经元模型的直接“记忆窗口”，
CTM依然能通过神经同步机制
成功提取信息并且进行计算
这证明了同步表征在记忆和信息检索方面的潜力
在强化学习的相关任务中
CTM也能够在与环境的持续互动中学习策略
展现出了与传统模型相当的性能
但是CTM的神经动态更加丰富
不过，CTM也并非完美无缺
由于它内部的串行思考过程
无法像传统模型那样大规模并行
导致训练速度会比较慢
训练时间也会更长
另外
由于每个神经元都有自己独立的模型
这无疑也会带来额外的参数量
增加了模型的复杂度和计算成本
不过，总的来说
CTM的出现
还是为人工智能的发展带来了新的思路和方向
它在AI的计算效率与生物智能的动态复杂性之间
架起了一座桥梁
“神经同步”作为一种全新的信息表征方式
具有巨大的潜力和应用场景
尤其是在需要整合长期依赖、进行复杂推理和记忆的任务中
从更宏观的角度来看
尽管现代人工智能以大脑为基础
构建了所谓的“人工神经网络”，
但是其实人工智能研究与神经科学之间的重叠程度
仍然是比较低的
这一方面是由于人工智能的研究人员
习惯沿用20世纪80年代开发的简单模型
因为简单易用、训练高效
另一方面
神经科学研究的主要目的是理解大脑的工作原理
而非直接用来创建更智能的模型
实用性较差
而CTM则是在尝试弥合这两个领域之间的差距
不仅展现出了一些更接近大脑行为的初步迹象
同时仍然是一个能够解决重要问题的实用人工智能模型
未来
Sakana AI团队还计划将CTM应用在语言模型、视频等时序数据
探索在更自然的“连续世界”设定下进行训练
甚至结合生物的可塑性原理
比如赫布学习（Hebbian learning）
来进行梯度无关的优化
好了
以上就是对连续思维机器CTM的介绍了
它的诞生
也许标志着人工智能在向生物智能迈进的道路上
又迈出了重要一步
让我们看到了结合生物大脑特征
来提升人工智能能力的可能性
也为未来的研究开辟了新的方向
当然
要想实现人工智能与生物智能的真正融合
还有很长的路要走
但是CTM无疑是一个良好的开端
如果大家对CTM感兴趣
建议去Sakana AI的项目主页体验交互式Demo
以及深入阅读他们的技术报告和开源代码
相关链接我会放在视频简介里
方便大家查看
感谢大家收看本期视频
我们下期再见
