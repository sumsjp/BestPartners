好的，我將這篇文稿整理如下，目標是使其更易讀、更流暢，並保留原始信息和風格。我將專注於以下幾個方面：

*   **分段和結構調整:** 將內容組織成更清晰的段落，使其邏輯更流暢。
*   **語言潤飾:** 調整一些用語，使其更自然、更易懂。
*   **摘要和重點突出:** 在段落開頭提供一些關鍵信息，方便快速閱讀。
*   **去除重複或冗餘信息:** 在不影響完整性的前提下，簡化表達。

**整理後的文稿：**

大家好，這裡是最佳拍檔，我是大飛。今天要跟大家分享一個幾乎不可能完成的項目：在短短三個月內，從零開始，竟然有人成功打造出一塊能夠運行、既能推理又能訓練的TPU，而且還開源了！

這聽起來可能令人難以置信，畢竟TPU作為Google設計的專用AI芯片，從2015年部署至今，已經迭代到第七代，背後是頂尖的工程師團隊和先進的製程工藝。然而，來自加拿大西安大略大學的工程師們，利用一個暑假的時間，完成了這個看似不可能的挑戰，他們將這個項目命名為TinyTPU。

這個項目的發起者們並非芯片設計專業的學生。更令人驚訝的是，他們在開始時，甚至需要從頭學習多層感知機（MLP）等基本的神经网络概念。為了搞清楚網絡推理和訓練的數學原理，他們竟然手工計算了所有需要的運算，這種從基礎開始的鑽研精神，為整個項目奠定了獨特的基調。

他們為何要做這樣一件事呢？一方面，他們覺得構建一個機器學習的專用芯片聽起來就很酷；另一方面，當時還沒有一個能夠同時支持推理和訓練的、機器學習加速器的完整開源代碼庫。更重要的是，他們沒有硬體設計的相關經驗，這反而成為了一種動力，因為無法準確估計難度，也就少了很多心理負擔。

他們還定下了一個特別的設計理念，那就是始終嘗試那些“不靠譜的方法”（Hacky Way），意思是在諮詢外部資源之前，先試試自己想到的那些“愚蠢”的想法。在這種想法的驅動下，他們簡直不是在逆向工程Google的TPU，而是在重新發明它。其中的很多關鍵機制，都是他們自己推導出來的。

還有一個有趣的地方是，他們想把這個項目當作不依賴AI代寫代碼的練習。現在很多人遇到一些小問題，就會習慣性地求助AI工具，而他們希望培養一種能獨立解決難題的思維方式。在整個過程中，他們盡可能多地學習了深度學習、硬件設計和算法創建的基礎知識，並且發現把所有的知識畫出來是最好的學習方式，這一點在他們的技術文檔里體現得淋漓盡致。

在正式講解他們是如何做到的之前，有必要先明確一下，TinyTPU並不是Google TPU的1:1複製品，而是他們基於自己的理解重新發明的一個嘗試。所以我們接下來看到的，可以說是一條全新的探索路徑。

首先，我們需要了解什麼是TPU？ TPU是Google設計的專用集成電路，也就是ASIC芯片，專門用於提高機器學習模型的推理和訓練速度。與GPU既能渲染圖像又能運行機器學習任務不同，TPU完全專注於數學運算方面，這也是它效率如此之高的一個重要原因，因為在芯片領域，專注單一任務往往比兼顧多項任務更容易做得更好。

要理解硬件設計，我們需要了解兩個基礎概念：**時鐘周期**，這是硬件處理操作的時間單位，所有操作都在這個時鐘周期之間執行；**硬件描述語言Verilog**，它與軟件的編程語言不同，不是以程序形式執行，而是靠合成布爾邏輯門（包括與、或、非等等）。這些邏輯門組合起來，就能構建任何芯片的數字邏輯。

TPU之所以高效的另一個原因，在於它在執行矩陣乘法時的優勢。在Transformer模型中，矩陣乘法會佔到計算操作的80%-90%，在超大型模型中甚至可以高達95%，即便是在CNN中，也能佔70%-80%。每個矩陣乘法都代表了多層感知器MLP中單個層的計算，而深度學習模型往往有多層的MLP，這就使得TPU在處理大型模型時的效率倍增。

那麼，這群零基礎的工程師是如何打算從零開始構建TPU的呢？他們的起點非常基礎，只知道y = mx + b這個方程是構建神經網絡的基礎模塊。但是要在硬件中實現神經網絡，必須完全理解其背後的數學原理。所以在寫任何代碼之前，團隊的每個人都手工計算了一個簡單的、用來解決異或（XOR）問題的2→2→1的MLP網絡。

為何選擇異或問題呢？ 因為它被稱為神經網絡的“Hello World”，是最簡單的需要用神經網絡解決的問題之一。像與（AND）、或（OR）這些邏輯門，用一條線（也就是一個神經元）就能區分輸入對應的輸出。但是異或問題需要彎曲的決策邊界，這只有MLP才能實現。

假設我們要進行連續的推理，比如像自動駕駛汽車那樣，每秒會進行多個預測，這就需要同時處理多條數據。而這種數據通常是多維的，有很多的特徵，需要大維度的矩陣。但是異或問題簡化了維度，只有兩個特徵（0或者1），以及4種可能的輸入，從而形成一個4x2的矩陣（4是批處理大小，2是特徵大小），這就為他們的實驗提供了一個合適的簡化模型。

接下來，異或的輸入矩陣和目標輸出都很簡單，四行分別代表四種二進制組合（0, 0）、（0, 1）、（1, 0）、（1, 1），對應的目標輸出是0、1、1、0。矩陣乘法是神經網絡計算的核心，在數學上表示為 XW^T + b，其中X是輸入矩陣，W是權重矩陣，b是偏差向量。

為了在硬件中執行矩陣乘法，他們還用到了一個關鍵結構：**脈動陣列（systolic array）**。只不過他們對這個脈動陣列做了簡化，用2x2的陣列代替了TPUv1中的256x256陣列，但是數學運算是完全類似的，只是規模縮小了。實際上，TPU的核心就是脈動陣列，它由處理單元（Processing Elements，簡稱PE）組成，這些PE以網格狀的結構連接，每個PE都會執行乘法累加運算，也就是將傳入的輸入X與固定權重W相乘，再與傳入的累加和相加，所有操作在一個時鐘週期內完成。

當這些PE連接起來，就能以脈動的方式來執行矩陣乘法，每個時鐘周期可以計算輸出矩陣的多個元素。輸入從左側進入脈動陣列，每個時鐘周期向右移動到相鄰的PE。累加和從第一行PE的乘法輸出開始，向下移動，與每個連續PE的乘積相加，直到到達最後一行PE，成為輸出矩陣的一個元素。正因為TPU專注在這個核心單元上，而矩陣乘法又佔導了模型計算量的絕大部分，所以它能夠高效地處理各種模型的推理和訓練。

我們再來看具體異或問題的處理過程。他們的脈動陣列可以接受輸入矩陣和權重矩陣，對於這個異或網絡，初始化的權重和偏差有特定的值。要想將輸入批次傳入脈動陣列，需要兩個步驟：首先要將X矩陣旋轉90度，再錯開輸入，也就是每行要延遲1個時鐘周期，包括輸入權重矩陣的時候，也要進行類似的交錯排列，並且需要轉置。這裡要說明的是，旋轉和交錯沒有數學意義，只是為了讓脈動陣列正常工作，而轉置則是為了數學對齊，確保矩陣運算的正確。

為了實現交錯，他們在脈動陣列的上方和左側設計了幾乎相同的累加器。由於激活是逐個輸入的，所以適合採用先進先出隊列（FIFO）的數據存儲方案。但是他們的累加器有兩個輸入端口：一個用來手動寫入權重，另一個用來將上一層的輸出寫回，作為當前層的輸入。權重FIFO的邏輯類似，但是沒有第二個端口。

完成矩陣乘法之後，下一步就是添加偏差。他們在脈動陣列的每一列下創建了一個偏差模塊，當總和從最後一行流出時，直接會進入偏差模塊計算預激活的值，用變量Z表示。偏差向量會被添加到每個Z行，這一步就像我們學過的線性方程一樣，只是擴展到了多維形式，每一列都代表一個特徵。

之後需要應用激活函數，他們選擇了Leaky ReLU，這是一個逐個元素的操作，所以在每個偏差模塊下都有一個激活模塊。偏差模塊的輸出會直接流入到激活模塊，得到的结果用H表示。Leaky ReLU的計算規則是：輸入為正的時候輸出本身，為負的時候輸出輸入乘以0.5。

以異或網絡的第一層為例，我們可以看見脈動陣列會先計算矩陣乘法，然後添加偏差b1，再應用Leaky ReLU得到H1，負值會乘以0.5，正值保持不變。這裡有個關鍵的設計思路是**流水線技術**。

為何不把偏差和激活合併到一個時鐘周期裡呢？因為流水線允許不同階段的操作同時進行，就像裝配線一樣，當激活模塊處理一個數據的時候，偏差模塊已經在處理下一個了。這樣能讓所有模塊保持忙碌，避免閒置。如果某個模塊在一個周期內執行多個操作，就會成為瓶頸。所以將操作分解到多個時鐘周期是更高效的做法。

為了進一步提高效率，他們設計了一個稱為“travelling chip enable”的信號，用紫色的圓點表示。由於所有組件都是交錯排列的，只需在第一個累加器上斷言一個時鐘周期的啟動信號，就能夠準確傳播到相鄰的模塊，依次激活脈動陣列、偏差和激活模塊，確保每個模塊只在需要的時候工作，不浪費電量。

接下挑戰是如何處理多層網絡。當開始新的層時，需要使用新的權重矩陣，而脈動陣列是權重平穩的，那麼又該如何更換權重呢？他們借鑑了電子遊戲中的雙緩衝概念。

在遊戲中，經常會用雙緩衝來防止畫面撕裂，原理其實就是隱藏像素的加載時間，這和他們面臨的問題完全相同。於是，他們通過添加第二個“影子”緩衝區，在當前層計算的時候，去加載下一層的權重，從而讓總時鐘的周期減少了一半。

為了實現雙緩衝，他們還添加了兩個信號：一個是用藍點表示的“切換”信號，它會將影子緩衝區的權重複製到活動緩衝區，從左上角傳播到右下角；以及一個用綠點表示的“接受”標誌，表示權重要向下移動一行，新權重進入頂行，每行依次下移。通過這兩個信號的配合，就能讓脈動陣列執行持續的推理，通過不斷輸入新的權重和數據，計算任意層級的前向傳播，最大化PE的利用率。

比如，在第二層中，第一層的輸出H1成為輸入，經過同樣的矩陣乘法、加偏差、激活操作，得到最終的預測結果。對於異或這個問題來說，所有值都是正數，所以Leaky ReLU的輸出保持不變。

推理功能完成之後，他們面臨的下一個挑戰就是訓練。有趣的是，用於推理的架構可以直接用來訓練，因為訓練本質上也是矩陣乘法，只是多了一些步驟而已。

假設推理得到的預測結果是[0.8, 0.3, 0.1, 0.9]，而目標是[1, 0, 0, 1]，顯然模型表現不佳，需要通過訓練來改進。訓練的核心是**損失函數**，於是他們選擇了均方誤差（MSE），可以理解為預測與目標之間的“距離”，用L表示。但是訓練並不需要損失值本身，而是需要它的導數，這個導數指示了應該向哪個方向調整權重才能減小損失，就像一個指向“更好性能”的指南針一樣。

這裡還用到了微積分中的鏈式法則，它能將複雜的梯度計算分解成更小的部分，讓我們逐層計算梯度並且向後傳播。整個過程可以分為幾步：首先計算損失相對於最終激活的變化率，然後通過激活函數的導數（也就是Leaky ReLU）來計算損失相對於預激活的梯度，以此類推。

等繪製完完整的計算圖後，他們發現了一個驚人的**對稱性**，那就是反向傳播中的最長鏈與前向傳播非常相似。在前向傳播中，激活矩陣是與轉置權重矩陣相乘的；而在反向傳播中，梯度矩陣是與未轉置的權重矩陣相乘的，就像是照鏡子一樣。這種對稱性讓他們能夠復用很多前向傳播的設計。

比如將梯度傳播到隱藏層，再通過第一層的激活，只需要根據Z1中正負值的混合，應用相應的Leaky ReLU梯度即可，比如正值為1，負值為0.01。在計算激活導數的時候，還需要用到前向傳播中得到的激活值H，這就需要存儲每一層的輸出。

為此他們創建了一個**統一緩衝區（UB）**，在前向傳播計算H值後立即存儲。UB還設計了兩個讀寫端口，因為需要同時訪問兩個值，包括每行/列的2個輸入或者權重，這樣就最大限度地減少了數據爭用。讀取的時候，只需要提供起始地址和數量，UB就會在後台運行，每個時鐘周期讀取2個值，這樣就比每個周期用指令加載更加高效。

因為位於所有脈動陣列下方的模塊都需要處理逐個輸出的列向量，這促使他們將這些模塊統一為**向量處理單元（VPU）**。這不僅讓設計變得更加優雅，也更具有可擴展性，比如當脈動陣列超過2x2的時候，N個這樣的模塊就可以統一管理。

他們還對激活導數模塊做了一個小小的優化，由於H2的值只用來計算了一次梯度，所以他們在VPU內部創建了一個小型緩存（H-cache），而把其他的H值存儲在UB中，因為需要用來進行多個導數計算。

VPU通過整合控制信號（也就是VPU的通路位pathway bits），可以選擇性地啟用或者跳過特定的操作，同時支持推理和訓練。比如正向傳播時會應用偏差和激活，跳過損失和導數計算一樣；反向傳播時則啟用所有的模塊，但只在反向鏈中計算激活函數的導數。由於流水線技術，流經VPU的值會經過所有四個模塊，只不過沒有使用的模塊只會充當寄存器，不執行計算。

接下這幾個導數計算可以直接利用脈動陣列的矩陣乘法能力，因為有三個關鍵的恒等式：如果Z = XW^T，那麼對權重求導我們可以得到這個，對輸入X求導我們可以得到這個，而偏差的導數就是1。這意味著可以用前面得到的梯度與X、W^T和1相乘，得到權重和輸入的梯度，再乘以學習率來更新參數。

有了這些設計之後，反向傳播就能順暢運行：首先從UB獲取梯度和H矩陣，通過脈動陣列計算權重梯度，同時直接輸入到梯度下降模塊，再用當前權重和梯度更新參數。整個過程就會像流水一樣不間斷的執行。

隨著功能的完善，他們的指令集也從最初的24位擴展到了94位，但是確保了每一位都是必需的，從而在不影響速度和效率的情況下，實現了所有的控制功能。最終，通過不斷重複前向傳播、反向傳播、權重更新的循環，網絡性能逐步提升。在GTKWave中的波形模擬顯示，一個周期後內存中的權重和偏差已經得到了有效更新。

好了，以上就是這個項目的大概過程了。當然，這個項目只能說是一個簡單的原型，真正的TPU遠比這個要複雜得多。只不過，整個過程中最令人驚嘆的，不只是他們在三個月內從零開始構建出了能推理和訓練的TPU，更在於他們採用的“第一性原理”的方法，從最基礎的數學和硬件原理出發，重新發明而不是複製了現有的設計。這種方法讓他們在沒有專業背景的情況下，不僅完成了技術挑戰，也更為深入地理解了每一個環節背後的原理。

對於想要深入了解這個項目的觀眾，可以訪問他們的GitHub地址以及項目的官網。團隊成員蘇里亞·蘇雷（Surya Sure）也在X平台進行了分享。我會把這些鏈接都放到視頻簡介中。希望這個項目能給大家帶來一些有趣的想法。感謝觀看本期視頻，我們下期再見！

**額外說明：**

*   由於原文是演講稿，有些口語化的表達方式被保留了，以保持風格。
*   技術細節部分盡可能忠實原文，僅進行少量潤飾，以確保準確性。
*   重點概念（例如：脈動陣列、流水線技術、雙緩衝）被強調，方便理解。
*   部分重複或略顯冗餘的句子被合併或簡化。

希望這個版本對您有幫助！如果您還有其他需求，請隨時提出。

[model=gemini-2.0-flash,0]
