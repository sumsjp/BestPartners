好的，這是整理後的文稿，我主要進行了以下調整：

*   **語氣調整：**將部分口語化的表達轉換為更書面化的用語，使其更正式專業。
*   **分段與結構優化：**重新調整分段，使文章結構更清晰，邏輯更順暢。
*   **標點符號校正：**修正了部分標點符號的誤用，使其符合規範。
*   **術語一致性：** 確保專業術語使用前後一致。

**整理後的文稿：**

大家好，這裡是最佳拍檔，我是大飛。

現今我們已普遍認識到，隨著模型規模的擴大，其能力也會不斷增強。然而，推理成本和訪問顯存的效率，已成為限制模型大規模應用的「攔路虎」。這好比一輛動力強勁的超級跑車，卻因油箱太小、加油速度太慢，而無法在賽道上盡情馳騁。

為了解決這些問題，研究人員們絞盡腦汁，提出了不少方案，例如混合專家模型（MoE）和乘積鍵記憶（PKM）架構。但這些方案都存在各自的局限性。今天，我們將介紹國內豆包大模型團隊提出的全新稀疏模型架構——UltraMem。它到底有何神奇之處，能夠讓大模型突破這些瓶頸？

在此聲明，本期影片純粹為技術分享，我並未收取豆包團隊的任何費用，亦非業配。希望大家能以純粹的態度，一同了解最新的技術進展。

在探討 UltraMem 之前，我們先來了解之前的方案為何不夠完美。我們知道，大語言模型的性能與其參數數量、計算複雜度之間呈現對數關係。這意味著，想要提升模型性能，就需要增加大量的參數和計算量，推理成本也會隨之急遽增加，推理速度也會變得越來越慢。我們可以想像，一個原本能快速回答問題的智能助手，隨著知識儲備的增加，反而變得慢吞吞的，這顯然不是我們所樂見的。

為了解決這個問題，混合專家模型（MoE）應運而生。MoE 的核心思路是透過稀疏激活專家來解耦計算和參數。簡而言之，就是將模型不同的功能模塊視為不同的「專家」，在處理任務時，只讓相關的「專家」工作，從而減少不必要的計算。然而，在推理場景中，MoE 卻遇到了麻煩。由於大模型在推理時通常逐字生成內容，此時批大小和序列長度都很小。在這種情況下，MoE 的所有專家往往會被全部訪問到，這就像在一個小超市裡，所有人同時去拿東西，一下子就把通道堵得水泄不通，非常容易遇到訪問瓶頸，導致推理的延遲大幅增加。

那麼，我們剛才提到的乘積鍵記憶（PKM）架構呢？PKM 最早提出了大記憶層（large memory layer）的概念。這個大記憶層包含了數量龐大的稀疏參數值（value），每個 value 都是一個向量。在推理時，每個 token 會根據一個「行路由」和一個「列路由」，定位到得分最高的幾個 value，然後激活這些 value 並做加權求和池化，再將結果作為記憶層的輸出。這種方法的好處是，每個 token 在推理的時候只會激活極少數的 value，所以不會遇到訪問瓶頸。但是，它也有自己的缺點，那就是模型的效果很差，且 scaling 的能力也比較差。這好比一個人做事很輕鬆，不會感到疲累，但他總是做不好，也很難透過增加工作量來提高整體的成果品質。

既然 MoE 和 PKM 都存在不足，那麼有沒有更好的辦法呢？豆包大模型 Foundation 團隊提出了 UltraMem 這個全新的稀疏模型架構。它參考了 PKM 的設計，但針對 PKM 的缺陷進行了補充，在保證模型效果的前提下，成功解決了推理的瓶頸問題，同時降低了顯存和部署成本。

那麼，UltraMem 是如何做到的呢？

首先，它在模型結構方面進行了優化。在 PKM 的設計中，記憶層只有一層，且插在整個 Transformer 的中間層。這種設計對於大規模的訓練不太友好，因為如此龐大的稀疏參數，應該盡可能多地參與到每次的殘差連接中，才能更好地發揮作用。這就像搭積木，每一塊積木都應該充分利用起來，才能搭建出更堅固、更複雜的結構。於是，UltraMem 團隊想到了一個巧妙的辦法，他們拆分出多個小的記憶層，然後以固定的間隔分佈在 Transformer 層中。這樣一來，模型在運行的時候，就可以並行地執行記憶層的訪問顯存操作和 Transformer 層的計算，大大提高了效率。不僅如此，他們還增加了 skip-layer 的操作，也就是當前記憶層的輸出會加到後面某個 Transformer 層的輸出。這就像是給模型內部搭建了一條「快速通道」，讓信息的傳遞更加高效。

其次，UltraMem 優化了 value 的檢索方式。在檢索時，只有分數最高的 m 個 value 會被激活。PKM 的分數是透過「行分數」加上「列分數」得到的，而 UltraMem 團隊探索了一種更為複雜的乘法方法——Tucker 分解查詢-鍵檢索（TDQKR）。這種方法的靈感來源於 Tucker 分解。具體來說，給定一組 values，其形狀是 (n, n, h)，其中 h 為隱藏層維度，那麼 values 的分數 S\_grid 可以進行如下分解。在這個結構下，每個 value 的分數是由 r 個行分數和 r 個列分數的乘積和相加組合而成的，複雜度更高，也就意味著檢索更加精準。打個比方，以前找東西可能只能大概看一下，而現在有了更為精確的方法，能夠更快、更準地找到需要的信息。

最後，UltraMem 還提出了隱式擴展稀疏參數的方法。通常來說，更多的稀疏參數通常會帶來更好的效果，但是過多的參數又會給顯存和部署帶來麻煩。這就像你想在房間裡放更多的東西，但是空間有限，你還得考慮能不能放得下。於是，UltraMem 團隊提出了隱式值擴展（IVE）的方法，同時引入了虛擬記憶體和物理記憶體的概念。以 4 倍擴展為例，虛擬記憶體的數量是物理記憶體的 4 倍。具體來說，給定多個（分數，索引）對之後，會首先根據虛擬記憶體位址表進行查找，4 個虛擬塊會查詢同一個物理記憶體表，之後各自做加權求和池化，然後經過不同的線性層，最後再求和輸出。由於最後的線性層和取 value 之間沒有任何的非線性操作，所以每個線性層都可以和物理記憶體表做融合，生成一個全新的記憶體表。這樣一來，實際上就隱式擴展了 4 倍的 value 數量，既增加了參數，又不會給顯存和部署帶來太大壓力。

那麼，UltraMem 的實際效果到底如何呢？研究團隊進行了一系列的實驗。他們在 151M、680M、1.6B 三個尺寸的激活參數上進行了實驗，並且保證 MoE、PKM 和 UltraMem 的總稀疏參數保持在激活參數的 12 倍，以使對比更加公平。

從實驗結果來看，UltraMem 在 680M、1.6B 規模的模型上展現出了顯著的效果優勢。在多個性能指標評估中，UltraMem 的表現都十分出色。舉例來說，在 TriviaQA 問答任務上，UltraMem - 680M - x12 模型的得分達到了 55.17，而 MoE - 680M - 2in3 模型只有 34.19，PKM - 680M - x12 模型為 46.31；在 HellaSwag 常識推理任務中，UltraMem - 1.6B - x12 模型得分 71.52，MoE - 1.6B - 2in3 模型是 67.34，PKM - 1.6B - x12 模型為 65.45。這些數據充分表明，UltraMem 在模型性能上超越了 MoE 和 PKM。

研究人員還關注了稀疏參數對 UltraMem 效果和推理速度的影響。從實驗數據來看，隨著稀疏參數的增加，UltraMem 的效果提升和損失值（loss）的下降呈現對數關係。也就是說，稀疏參數增加得越多，loss 下降得越快，但下降的幅度會逐漸變小。這說明稀疏度持續降低所帶來的收益在逐漸飽和。在推理速度方面，當持續增加稀疏參數的時候，UltraMem 的推理時間幾乎不變，而 MoE 的推理時間卻有了顯著增長的趨勢。這好比兩輛車在不同的道路上行駛，UltraMem 走的是一條平坦寬闊的大道，速度穩定；而 MoE 卻遇到了越來越多的阻礙，速度越來越慢。

為了進一步驗證 UltraMem 架構改進的有效性，研究團隊還進行了消融實驗。他們在 151M 激活、1.5B 總參數的稀疏模型上，從最原始的 PKM 開始，逐漸增加各種改進措施，比如增加 rm softmax、share query 等操作，以及我們之前提到的拆分大記憶層和 skip-layer、IVE、TDQKR 等關鍵改進。透過一系列的實驗對比，最終得到了 C4 驗證損失值為 -0.092 的顯著收益，同時稀疏參數和計算量幾乎不變。相比 MoE，UltraMem 可以實現最高達 6 倍的速度提升，推理成本最高可以降低 83%，而且在相同的參數和計算量情況下，UltraMem 比 MoE 展現出了更強的擴展能力。

不過，豆包團隊也指出，UltraMem 還有很大的提升空間，比如如何更高效地優化和激活稀疏參數，以及如何進一步提升稀疏模型的推理能力等等。

總而言之，UltraMem 架構為解決大模型的推理效率方面提供了一個新的方案和思路，可以提升模型在一些對延遲要求較高的推理場景，比如代碼補全和實時互動等場景下的應用。

好了，以上就是本期影片的內容了。大家若對論文內容有任何理解和想法，歡迎在評論區留言。感謝觀看，我們下期再見！

[model=gemini-2.0-flash,0]
