好的，這是我整理後的文稿，我主要做了以下修改：

*   **修正錯字和語法錯誤：** 修正了一些明顯的錯字和語法問題，使語句更流暢。
*   **調整標點符號：** 調整了標點符號的使用，使其更符合中文的習慣。
*   **分段和組織：** 將文稿分成更小的段落，使內容更易於閱讀和理解。
*   **提煉重點：** 在一些地方做了簡化，使其更加清晰。
*   **加入小標題：** 為了方便閱讀，我將文稿加上了小標題。

以下是整理後的文稿：

---

**大家好，這裡是最佳拍檔，我是大飛。**

**感謝與頻道近況**

首先感謝大家對頻道五月份運營情況影片的肯定和支持。最近頻道分享AI方面的內容比較多，主要是因為相關資訊量太大，我一個人實在無法完全消化並製作成節目。即使每天做四期節目，也講不完每天的新聞。

我希望能盡可能地分享這些資訊，所以只能將自己錄製的影片，連同找到的資料一起分享給大家。最近大佬們的觀點討論非常多，我整理起來吃力，且資訊可能會有誤，所以請大家多聽聽大佬們的原話。

**AI發展的個人觀察**

請大家理解包涵，不是我不錄節目，是真的錄不過來。因為我天天要看大量的AI資訊，從自身感覺來說，這次AI的發展進程跟以往不太一樣。這次科技突破密度高、強度大，而且持續時間長，我以前沒有見過。而且每天都有新的模型、方法、工具出現。

因此我個人判斷，這次AI的影響會非常深遠。以前科技進步傳導到大眾需要很長時間，但這次 ChatGPT 改變了這種狀況。雖然史上最快月活一億的數據不一定準確，但肯定規模很大。而且這次熱度持續時間長，快半年了還沒有退燒。

像以前的 AlphaGO、Alpha-Zero，新聞熱度也就是個把月就下去了，但這次不一樣。所以我對這個頻道方向很有信心，雖然現在轉做自媒體，但AI方面的内容足夠我做很久。HuggingFace上的模型，我一天講一個都夠講幾年。

再次感謝大家的支持和鼓勵，至少這個方向我會堅持做下去。當然也希望後續頻道能夠承接和製作更多其他類型的節目和內容，不斷豐富頻道的多樣性，給大家帶來更多更好更有價值的內容。

**北京智源大會**

說回正題，這兩天國內 AI 圈關注度最高的會議，應該就是2023年北京智源大會。它匯集了眾多傳說中的AI大佬，包括四位圖靈獎得主：傑弗里·辛頓、楊立昆、姚期智、約瑟夫·西法斯基，以及 OpenAI 創始人 Sam Altman、Midjourney 創始人大衛·霍爾茨。

還有像 PaLM-E、Meta AI 這些大模型項目的重要參與者，以及張钹、張宏江這些國內外久負盛名的科技圈大佬。即使楊立昆在法國，也在當地時間凌晨四點連線到現場直播並做了演講。

今年是第五屆大會，同時請來了楊立昆和傑弗里·辛頓。其實背後有個小插曲，在第三屆大會時，傑弗里·辛頓本來也想參加，但就在大會前幾天，他發現準備分享的新方法裡出現了 bug，所以只能取消。

本杰奧在2021年的智源大會上介紹過的關於 system 2 的机器学习理论，如今以思維鏈的形式用在了大模型提示工程方面。可以說深度學習的三巨頭都參加過這個大會。

除了三巨頭之外，還有很多圖靈獎得主也到這個大會做客，包括貝氏網路的提出者朱迪亞·珀爾、RISC-V 的掌門人大衛·帕特森、數據結構大師約翰·霍普克羅夫特，他更是擔任會議的學術顧問委員會會員。所以這個大會還是蠻有高度的，無論從中國還是世界範圍來看。

**智源大會的重點發布**

大會從 6 月 9 日開始，到 6 月 10 日結束，地點在中關村國家自主創新示範區的會議中心會展中心。

智源主要宣布了自己的悟道 3.0，有一系列的重磅發布，並且全面開源。其中的悟道天鷹 Aquila 大語言模型系列，支持中英雙語知識、支持商用許可。AquilaChat 不僅可以像 ChatGPT 一樣進行對話、生成命題作文，還可以通過定義可擴展的特殊指令規範，實現對其他模型和工具的調用。比如通過調用圖像生成 API 實現文生圖功能，或者配合 instructorface 這個多步可控的文生圖模型來實現對人臉的一步步編輯，以及調用 Aquila code 代碼模型的能力，在對話中實現文本到代碼的生成，比如說直接生成一個簡單的登錄頁面或者設計一個時鐘程序。

天秤 FlagEval 是一個大語言評測體系及開放平台，可以構建從能力到任務到指標這樣一個三維的評測框架，細粒度地去刻畫模型的認知能力邊界。目前它包括了 22 個主觀和客觀評測的數據集，以及 84,433 道題目，更多維度的評測數據集正在陸續集成，而且給出的評測結果都是可視化的。目前天秤 FlagEval 已經推出了語言大模型評測、多語言文圖大模型評測以及文圖生成評測等工具，並且對各種語言的基礎模型、跨模態基礎模型實現了評測功能。後續它還將全面覆蓋基礎模型、預訓練算法、微調算法等三大評測對象，包括自然語言處理、計算機視覺、音頻以及多模態等等四大評測場景及豐富的下游任務。

开源了悟道视界视觉大模型系列，发布了六个国际领先的成果。首先是视觉的基础模型 EVA，它是一种经过预训练的 vanilla ViT 模型，整体思路是将最强的语义学习 CLIP 模型与最强的几何结构学习 MIM 做了结合，再将标准的 Vit 模型扩大到了 10 亿参数的规模进行训练。其次是 EVA CLIP 模型，它可以显著提高 CLIP 训练的效果和效率。EVA CLIP 结合了表征学习优化和增强的新技术，与之前的 CLIP 模型相比，在具有相同数量参数的情况下，它的训练成本显著更低。据官方称，EVA CLIP 超越了此前最强的 open CLIP 模型，在 imageNet 1K 的零样本 top 1 测试中达到了最高的 82% 的准确率。

第三个是通用的视觉模型 painter，它的最大亮点是首创了上下文视觉学习的技术路径。它的核心思想就是将视觉任务的输出重新定义为图像，并且将任务 prompt 也指定为图像。在推理过程中，可以采用一对来自同一任务的输入和输出图像作为输入条件，来指示要执行的任务。

第四个是基于 painter 模型上衍生的分割一切的模型 SegGPT，它可以通过视觉 prompt 来完成任意分割的任务。与 painter 一样，SegGPT 也具备视觉上的上下文推理功能，只需要给出一个或几个视觉的 prompt，模型就能够理解用户的意图，有样学样地去完成类似的分割任务。

第五个是零样本的视频编辑方法 vid2vid-zero，在无需额外视频训练的情况下，它利用注意力机制动态运算的特点，结合现有的图像扩散模型，实现了可以指定属性的视频编辑。比如说，仅仅通过一句话，就可以换掉视频中的车和背景。

第六个是补全一切的生成式大模型 Emu，它可以接受多模态的输入，预测多模态的输出，支持在文字、图片、视频多模态序列内间进行理解、推理和生成。可以指定任意模态的输入组合，都能够在序列中补全下一项。Emu 的多模态上下文学习可以实现多轮的图文对话视频理解、少样本图文理解、文图生成、图图生成和少样本的文图生成等等。

除了悟道 3.0 本身的内容之外，這次會議的議程設置上，也聚焦了大模型、多模態生成式模型、AI 安全倫理問題和風險防範等最新話題，同時也包含了像自動駕駛、生命科學等專業細分領域的報告和研討。後續幾天會陸續編輯和發布相關的會議影片，供大家學習和討論。

**結尾**

好了，今天的分享就到這裡，感謝大家的觀看，我們下期再見。

---

希望這個整理對您有所幫助！

[model=gemini-2.0-flash,0]
