好的，以下是經過整理的文稿，主要目的是讓內容更清晰、易讀，並且更符合一般文章的格式。我做了以下調整：

*   **調整了語氣：** 將口語化的表達轉換為更書面化的語氣。
*   **更精簡的段落：** 將較長的段落拆分為更短的段落，使閱讀更輕鬆。
*   **標題層級：** 增加標題層級，以更清楚地展示文章的結構。
*   **調整一些詞彙：** 替換一些口語詞彙，使其更正式。
*   **調整標點符號：**修正了標點符號的錯誤使用。
*   **修正錯字:** 修正一些錯字。

**整理後文稿：**

---

## Stable Diffusion 生成原理詳解

大家好，這裡是最佳拍檔，我是大飛。前幾天，我們以較為通俗的方式講解了大語言模型的內部運行原理。今天，我們將進一步講解 Stable Diffusion 的生成原理。

Stable Diffusion 算法是這波 AIGC 浪潮崛起的基礎，無數的 AIGC 工具都由此衍生。本影片主要分為三個部分：

1.  **Stable Diffusion 原理：** 介紹擴散 (Diffusion) 的概念、如何穩定控制擴散，以及 CLIP 和 UNET 的原理，並理解 VAE 的編解碼過程。
2.  **模型訓練原理：** 介紹機器如何認識圖片，以及如何訓練模型。
3.  **大模型微調技術：** 介紹進行微調的原因、微調要解決的問題，以及常見的微調技術及其相關原理。

### 1. 什麼是擴散 (Diffusion)？

Stable Diffusion 是一個算法。我們所說的 Stable Diffusion Web UI 是基於此算法的一個工具。Stable Diffusion 的算法名稱直接點明了其核心概念：Stable（穩定） + Diffusion（擴散）。簡單來說，它是一種穩定的擴散算法。

在圖像領域中，擴散算法指的是透過一定規則進行去噪或加噪的過程。其中，去噪也稱為反向擴散，加噪則稱為正向擴散。

以提示語 "a red flower"（一朵紅色的花）為例，當我們要生成一朵紅色的花時，擴散過程會從最初的灰色噪點塊逐漸去噪，直到最終清晰地顯示一朵紅色的花。

那麼，對於 Stable Diffusion 算法來說，這個擴散過程如何能夠被穩定地控制住呢？

### 2. Stable Diffusion 的原理

我們以文生圖為例，展示 Stable Diffusion 的原理。這個過程可以抽象理解為一個大的函數 Fsd，其參數是 Prompt。也就是說，我們輸入一段自然語義 Prompt，經過一系列的函數運算和變化，最終輸出圖片。這個過程中會經過幾個部分，我們將逐一拆解講解。

#### 2.1 CLIP：文本編碼器 (Text Encoder)

輸入一段咒語（Prompt）後，如何使其起作用？實際上，讓咒語能夠起作用的是一個叫做 CLIP 的算法。CLIP 是 Text Encoder 算法的一種。Text Encoder 的主要功能是將自然語義 Prompt 轉變為詞特徵向量 Embedding。

例如，我們輸入 Prompt "cute girl"（可愛的女孩），CLIP 算法在進行自然語義處理時，會根據之前被程式設計師訓練的經驗，感知到可愛的女孩可能具有的特徵，如「大大圓圓的眼睛」、「白皙的肌膚」、「可愛的神態」等等。然後，這些可能的特徵會被轉化為 77 個等長的 Token 詞向量，其中每個詞向量包含 768 個維度，這就是我們所說的 Embedding。

為什麼同樣的關鍵詞，生成的圖片有的好看，有的醜？這是因為，當輸入同一個 Prompt 時，Text Encoder 過程是一樣的，也就是得到的詞向量是一致的。但是，後面的去噪算法依賴的模型不同，因此生成的結果也千差萬別。

#### 2.2 UNET：基於詞向量的擴散算法

接下來，我們來講解 Stable Diffusion 中最重要的 UNET 算法。UNET 是一種基於詞向量的擴散算法。CLIP 算法會根據輸入的 Prompt，輸出對應機器能識別的詞特徵向量 Embedding。可以將 Embedding 理解為一個包含 (Q、K、V) 三個參數的函數。這三個參數會根據輸入的擴散步長，在 UNET 去噪算法的每一步發生作用。

如果設定去噪步長為 20 步，就能看到圖片逐漸擴散生成的效果。實際上，UNET 去噪的原理比這裡描述的複雜很多。它並不是一步一步去噪就能得到對應效果的，如果僅僅是一步一步地去噪，效果往往很差，並不能精確得到描述文本的圖片。

為了保證 Prompt 最終的精確性，在 UNET 分步去噪時，它會在每一步都生成一個有 Prompt 特徵引導的圖，和一個沒有 Prompt 特徵引導的圖，然後將兩者相減，就得到了每一個去噪步驟中單純由文字引導的特徵信號。然後，將這個特徵信號放大很多倍，加強文本引導。同時，在第 N+1 步去噪結束後，它還會用第 N+1 步去噪的訊息特徵減去第 N 步的特徵，然後繼續放大很多倍。這樣可以保證 Prompt 在每一步都能有足夠的權重比參與運算。

此方法加強了 Prompt 的權重，在 Stable Diffusion Web UI 中被翻譯為「提示詞相關性」，是一個很常用的參數，其數值決定了生成的圖片與提示詞的相關程度。

#### 2.3 圖生圖 (Image to Image)

當在 Stable Diffusion Web UI 上使用圖生圖功能時，通常是給定一張圖片，然後輸入一段 Prompt。這時，它的原理是先把我們提供的圖片進行逐步加噪，提取圖片訊息，使其變成一張完全的噪點圖，再讓 Prompt 起作用，結合上面的 UNET 算法逐步去噪，得到既有素材圖片特徵、也有 Prompt 特徵的最終效果圖。

#### 2.4 VAE：變分自編碼器 (Variational Autoencoder)

最後，我們簡單理解一下 VAE 編解碼的過程。VAE 全稱是變分自編碼器。VAE 的原理是先壓縮後解壓。VAE 算法在一開始的時候，會把 512x512 的圖片壓縮到八分之一，變成 64x64，然後在經過 UNET 算法的時候，將圖形數據帶在噪點圖中，這個過程叫做 Encoder。然後等走完 UNET 算法後，得到一個帶有所有圖片特徵的噪點圖，此時 VAE 再進行 Decoder 的過程，把這張圖解析並放大成 512x512。

上述 UNET 算法不是直接在圖片上進行的，而是在「負空間」進行的，可以理解為在編碼層面進行。

### 3. 模型訓練

#### 3.1 機器如何認識圖片

最初，計算機視覺訓練認識物體時，採用的是成對訓練的方法，通過「圖」+「對應描述」，成對地進行訓練。然後運用圖像識別、自然語義處理與卷積神經網路等一系列技術，讓計算機能夠識別這個圖形。

如果要識別狗，我們會不停地給計算機餵成千上億張狗的圖片，然後告訴它：「這是狗，這也是狗，這還是狗。」然後機器會不斷歸納狗的特徵，重複學習上億遍後，它就認識狗了。

#### 3.2 訓練模型

假設經過我們的投餵，機器已經能夠識別萬物了。但這時，由於學習的圖片風格都不同，生成的圖片有可能不滿足我們的期望。

舉例來說，假設程式設計師經過十年的努力後，讓機器正確認識了小女孩的特徵，不會出現三個眼睛兩個嘴巴。當輸入提示 "a cute girl" 時，生成的是圖 A。但是，A 的風格與我們想要的完全不同，例如我們想要的是圖 B 這種風格。

這時，程式設計師們就會不斷調整函數中的各種算法和參數，使得產出的圖形 A 無限接近於圖像 B，然後停止訓練。此時，整個 Fsd(x) 函數的所有調整的參數，就被保存為一個 `.ckpt` 的檔案，這個叫做 checkpoint 的檔案，就是程式設計師訓練好的、可供調用的 AI 繪圖大模型。它能保證每次生成的圖片，都偏向於某種特徵集合。

了解過 Stable Diffusion 的人應該都知道，Stable Diffusion 可用的模型有兩類：

*   **safetensors：** 使用 NumPy 保存，僅包含張量數據，沒有任何代碼，加載 safetensors 檔案會更加安全和快速。
*   **ckpt：** 使用 Pickle 序列化，可能包含惡意代碼，如果不信任模型的來源，加載 ckpt 檔案可能會帶來安全風險。

因此，下載模型時，建議優先下載 safetensors。

### 4. 大模型的微調技術

#### 4.1 為什麼要進行大模型微調

UNET 模型是 SD 中最重要的模型網路，內部包含了上億個參數。要訓練這樣一個超大的模型，大概需要 15 億個圖像文本，使用 256 張 A100 顯卡跑 15 萬個 GPU 小時，成本約為 60 萬美元。一般的設計師無法像程式設計師那樣，頻繁調整函數參數來訓練模型。

由於 UNET 模型的泛化性極強，可能無法滿足特定風格的需求，因此我們往往會對 UNET 大模型進行微調，讓其更符合需要的使用場景。

#### 4.2 微調要解決的問題

所有的大模型微調技術，都是為了要解決兩個問題：

1.  **如何減少訓練參數，提高訓練效率和生成圖像的質量。**
2.  **如何解決模型泛化性差的問題。**

泛化性指的是模型對新數據的適應能力，具體會表現為過擬合和欠擬合兩種。

*   **過擬合 (Overfitting)：** 指微調模型對原始模型的語義理解程度發生了變化，整體訓練出現語義漂移的現象。例如，有一隻貓，名字叫 Jojo，我拍了他的幾十張照片，一直用 "a jojo cat" 這個名字來強化模型對這隻貓的認知。等以後我輸入 "a jojo cat" 的時候，確實能出現我想要的這隻貓。但是我輸入 "a cat" 的時候，出現的畫面卻很怪異，這就說明 cat 這個詞被污染了，出現了過度擬合的現象。
*   **欠擬合 (Underfitting)：** 指無法識別這個特徵。例如，它就完全識別不出 Jojo 和 cat 的關係。這往往是訓練樣本量不足，或者訓練樣本質量較差等因素導致，屬於訓練無效。

#### 4.3 常見的大模型微調技術

常見的大模型微調技術大概有四種：Dreambooth、LoRA、Embedding、Hypernetwork。

##### 4.3.1 Dreambooth

Dreambooth 如何解決過擬合的問題？Dreambooth 要求訓練過程中，「特徵詞 + 類別」和「類別」要成對出現。如果想訓練一隻叫 Jojo 的貓的模型，又不能讓它跟 "貓" 這個詞過度擬合，可以採用這樣的輸入方法："JOJO cat is a cat named JOJO"。

Dreambooth 是 Google 在 2022 年 8 月提出的一種新的圖像算法，可以完整地獲得你想要的模型的視覺特徵。它的提出既不是為了訓練人物，也不是為了訓練畫風，而是為了能在少量訓練圖像的基礎上，完美地還原細節特徵。

Dreambooth 實際上調整的是 UNET 算法每一層的內部參數，也就是將 UNET 每一層函數都要進行微調。優點是可以將視覺特徵完美融入，缺點是需要調整 UNET 所有內部參數，訓練時間長，模型體積也很大。

##### 4.3.2 LoRA：大模型的低秩適配器

LoRA (Low-Rank Adaptation) 的目的是減少模型的訓練參數，提升模型的訓練效率。LoRA 建議凍結預訓練模型的權重，並將訓練參數注入到 Transformer 函數架構的每個層中。它的優點是不破壞原有的模型，可以即插即用。

相較於 Dreambooth，LoRA 可以將訓練參數降低 1,000 倍，而且對 CPU 的要求也會下降 3 倍，所以訓練出來的 LoRA 模型就會非常小，通常只有幾十 MB，而一個大模型往往有幾 GB 甚至十幾 GB。

可以將 LoRA 理解成在原有的大模型上添加了一個濾鏡，讓這個底層的大模型往我們期望的效果走。例如，當 Prompt 不變的情況下，在一個偏動漫的底層模型 revAnimated 上加了一個盲盒效果的 LoRA，就相當於是給這個底層模型上了一個濾鏡，讓整個底層模型出來的效果往盲盒的方向靠。

##### 4.3.3 Embedding：文本反演

Embedding，也叫 Text Inversion，反映的是一個 Prompt 與對應向量的映射記錄關係的算法，可以用來訓練特定的人或者是物體。

可以針對從 Prompt 到向量這個映射過程進行訓練，修改它們之間的映射記錄關係，從而達到訓練特定的人或物體的效果。由於它生成的是一套純文字的映射記錄，所以體積非常小，通常只有幾百 KB。

例如，如果要通過描述守望先鋒裡的 Dva 的特徵，去把她完整地描繪出來，可能就需要幾千個 Prompt Tag 才行。如果每次想生成 Dva 的時候，都要再輸入一次這麼多的 Prompt，肯定是不科學的。所以，可以把這一串的 Tag 打包映射成一個新的詞彙叫做 "OW\_Dva"。由於這個詞是自創的，在 CLIP 映射集合裡是找不到對應的映射關係的，所以 CLIP 會默認給它創建一個新的映射空間。經過一系列的訓練之後，後續就可以通過輸入一個簡單的詞彙，來完成一系列 Tag 的打包效果。

##### 4.3.4 Hypernetwork

Hypernetwork 作為一種即將被 LoRA 淘汰的技術，在這裡僅做簡單介紹。Hypernetwork 是新建了一個單獨的神經網路模型，插入到原來的 UNET 模型的中間層。在訓練過程中，它會凍結所有的參數，只訓練插入的部分，從而使輸出圖像與輸入指令之間產生關聯關係。

這種方法更適合用於訓練某種畫風，例如像素畫之類的。將 Hypernetwork 理解為一個濾鏡即可。

Dreambooth 調整了整個 UNET 的函數和參數，所以它體積最大，適用範圍最全，但是訓練難度和訓練耗時和成本也都最大。LoRA 只是將訓練參數注入到了部分的 Transformer 函數中，所以它不改變原來的模型，可以即插直用，模型大小也可控。

### 5. 結語

以上就是對整個 Stable Diffusion 算法以及相關內容的介紹。由於這是一個通俗版的介紹，為了方便理解，我對算法原理和數學運算進行了一定的簡化，內容也不可能面面俱到。如有想深入了解一些技術細節的，可以在評論區留言，我們找時間再深入介紹。

感謝大家的觀看，我們下期再見！

[model=gemini-2.0-flash,0]
