好的，以下是整理後的文稿，更著重於重點摘要和結構化的呈現，並加入了標題和分點，方便閱讀和理解：

**李沐上海交大演講精華整理：大語言模型與個人生涯分享**

**引言**

*   本期節目整理李沐在上海交大關於大語言模型和個人生涯的分享。
*   雖然演講內容和影片網路上容易找到，但考慮到海外觀眾，故做此總結。

**一、 演講背景**

*   **上海交大：** 中國創業土壤濃厚的高校，培養出多位知名創業者。工科及計算機強校，在人工智能方面領先。ACM班號稱中國計算機人才的黃埔軍校。
*   **李沐：** 上海交大ACM班畢業，歷任百度高級研究員、卡耐基梅隆大學博士，後加入亞馬遜擔任Senior Principal Scientist，為深度學習框架MXNet的主要貢獻者之一。2023年與斯莫拉共同創辦Boson AI，為B端客戶提供定制化的大模型。

**二、 演講核心內容：大語言模型的現在與未來預測**

*   **大語言模型的本質：** 將數據透過算力和算法壓進模型，使模型具備能力，面對新數據能找到相似之處並修改輸出。
*   **類比：** 傳統機器學習像老中醫，深度學習像玄幻小說煉丹，數據是材料，算力是設備，算法是丹方。
*   **與傳統機器學習的區別：** 前者解決特定問題，後者解決多個問題。

**三、 大語言模型的未來發展**

*   **算力：**
    *   **硬體方面：**
        *   **带宽：** 分布式訓練瓶頸。
        *   **GPU服务器：** 供電、散熱問題，水冷散熱雖好但有漏水風險，但可提高算力密集度。
        *   **PCIe：** GPU和CPU之間的通訊瓶頸。
        *   **内存：** 模型大小受限於記憶體容量，Nvidia雖領先但不如AMD或Google的TPU。
        *   **供电：** 成為最大挑戰，單個芯片耗電量高。
    *   **價格：** Nvidia壟斷導致算力價格提升，但長期來看摩爾定律仍將發揮作用。
*   **數據：**
    *   預訓練數據量已達10T到50T token，模型大小100B到500B。
    *   未来100B到500B模型會是主流。
*   **模型：**
    *   **新模態模型發展：**
        *   **語音模型：** 可包含更多信息，延遲時間短，交互體驗更真實。
        *   **視頻模型：** 仍處於早期階段，数据處理成本高，通用視頻生成成本高，保持圖片一致性難。
    *   **多模態趨勢：** 將文本能力泛化到圖片、視頻和聲音等模態。
*   **人機交互：** 語音對話是趨勢，可處理複雜任務，但需時間養成用戶習慣。

**四、 應用層面**

*   AI的本質是輔助人類完成任務，提供無限人力資源。
*   **三類應用：**
    *   **文科白領：** 自然語言交互，AI已能基本完成簡單任務。
    *   **工科白領：** 程式設計師，簡單任務仍需努力，複雜任務存在困難。
    *   **藍領階級：** 機會最大，但目前僅限無人駕駛和工廠等特定場景，缺少足夠數據，尚無法完成簡單任務。

**五、 創業心得**

*   **模型預訓練與後訓練：** 預訓練是工程問題，後訓練才是技術問題，應將精力放在後訓練部分。
*   **垂直模型：** 實際上並不存在真正的垂直模型，需要通用能力。
*   **評估：** 模型在實際場景中的評估非常困難，要先把評估做好。
*   **數據：** 數據決定了模型的上限，算法決定了模型的下限，Anthropic在數據方面表現出色。
*   **算力：** 對於創業公司來說，買GPU或租GPU是主要方案，Nvidia利潤高昂。
*   **祛魅：** 大語言模型仍是機器學習範疇，只不過規模變大，數據和評估更難，本質上還是算法上的探索不夠。

**六、 工作經歷與個人發展**

*   **打卡式人生：** 經歷豐富，在不同機構工作有不同目標。
*   **讀PhD：** 除了研究能力，寫作和演講能力也很重要。
*   **創業：** 像海盜一樣，既有刺激也有痛苦，要學會延遲享受。
*   **選擇：** 需有強烈的動機，來自深層的慾望或恐懼，並解決問題。
*   **持續提升：** 不斷復盤，檢視動機、目標和努力空間。
*   **時代：** 既是最好也是最壞的時代，需付出更多努力。

**七、 總結**

*   建議大家有時間可以去看看原視頻。

希望這個整理對您有所幫助！

[model=gemini-2.0-flash,0]
