好的，這份文稿我將進行整理，目標是使其更易讀、更具條理，並且保留原意。我會進行以下操作：

*   **分段整理：** 將長段落分割成更短、更易於理解的段落。
*   **調整語氣：** 將口語化的表達轉換為更正式的書面語，同時保留文章的生動性。
*   **標點符號修正：** 修正標點符號，使其更符合書面規範。
*   **提取重點：** 使用標題、粗體等方式標示文章重點。
*   **歸納總結：** 在適當的地方加入總結性語句。

**整理後的文稿：**

---

大家好，這裡是最佳拍檔，我是大飛。

6月12日是著名的Transformer論文提交六週年的日子。應該說，人工智能領域最近的這波突破，或許都應該感謝Transformer。

六年前，這篇名字略顯浮誇的論文《Attention is all you need》被上傳到了預印版論文平台arXiv。這句話甚至成為了論文標題的一種潮流。當然，Transformer也不再是變形金剛的意思了，它開始代表著AI領域最先進的技術。

**Transformer的誕生背景**

六年後回看這篇論文，我們可以發現許多有趣或鮮為人知的地方。雖然論文的名字是“Attention is all you need”，我們也因此不斷推崇注意力機制，但實際上，並非Transformer的研究者們發現了注意力，而是他們將這種機制推向了極致。

**注意力機制的起源**

實際上，注意力機制是由深度學習的先驅約書亞·本傑奧帶領的團隊在2014年提出的。在這篇ICLR 2015的論文中，本傑奧等人提出了一種RNN+上下文向量（也就是注意力）的組合。雖然這是NLP領域最偉大的里程碑之一，但相比Transformer，它的知名度要低得多。本傑奧團隊的這篇論文至今被引用了2.9萬次，而Transformer的論文則被引用了7.7萬次。

**Transformer論文的早期評價**

Transformer的這篇論文雖然現在影響力很大，但在當年的全球頂級AI會議NeurIPS 2017上，連個Oral（口頭報告）都沒有拿到，更不用說獲得會議獎項了。當年大會共收到3,240篇論文投稿，其中678篇被選為大會論文。Transformer論文是其中之一，在這些論文中，40篇為Oral論文，112篇為Spotlight論文，3篇最佳論文，1篇時間驗證獎項論文，但沒有Transformer。

**Transformer的重要性**

不過事到如今，應該已經沒有人會否認Transformer的影響力了。在Transformer誕生之前，AI圈的人在自然語言處理中，大多會採用基於RNN循環神經網路的編碼器-解碼器架構來完成序列翻譯。然而RNN及其衍生的網路最致命的缺點就是慢，關鍵在於前後隱藏狀態的依賴性無法實現並行化。而Transformer完全摒棄了遞迴結構，依賴注意力機制，挖掘輸入與輸出之間的關係，進而實現了並行計算。

**Transformer的架構**

谷歌當年發布的一篇博客闡述了Transformer是一種語言理解的新型神經網路架構。具體來講，Transformer由四部分組成：輸入、編碼器、解碼器以及輸出。輸入字符首先通過Embedding轉為向量，再加入位置編碼來添加位置資訊，然後通過使用多頭自注意力和前饋神經網路的編碼器和解碼器來提取出特徵，最後再輸出結果。

**Transformer在機器翻譯中的應用**

谷歌曾以如何在機器翻譯中使用Transformer為例。機器翻譯的神經網路通常包含一個編碼器，在讀取完句子後會生成一個表徵。空心圓代表Transformer為每個單詞生成的初始表徵，然後利用自注意力從所有其他詞中聚合資訊，在整個上下文中為每個詞產生一個新的表徵（由實心圓表示）。接著將這個步驟對所有單詞並行重複多次，依次生成新的表徵。同樣，解碼器的過程與此類似，但每次從左到右生成一個詞，它不僅關注其他先前生成的單詞，還關注編碼器生成的最終標準。

**注意力機制的本質**

注意力機制其實是仿照人類的視覺注意力而來的。人類大腦有一種天生的能力，當我們看一幅圖的時候，先是快速掃過圖片，然後鎖定需要重點關注的目標區域。如果人類不放過任何的局部資訊，那必然會做很多的無用功，不利於人類的生存。同樣，在深度學習網路中的注意力機制也是為了簡化模型、加速計算。從本質上說，Attention就是從大量資訊中篩選出少量重要的資訊，並聚焦到這些重要資訊上，忽略大多數其他不重要的資訊。

**Transformer在NLP領域的崛起**

2019年，谷歌還專門為它申請了專利。從此以後，在自然語言處理中，Transformer便開始了它的逆襲之路。歸根到底，現在各種層出不窮的GBT（Generative Pre-trained Transformer）都起源於這篇2017年的論文。別忘了，GBT裡的“T”就是Transformer。

**Transformer在其他領域的應用**

然而，Transformer點燃的不僅僅是NLP的學術圈。2017年，谷歌的一篇博客中，研究人員就曾對Transformer未來應用的潛力進行了暢想，不僅涉及自然語言，還涉及非常不同的輸入和輸出，例如圖像和影片。自2012年以來，CNN逐漸成為視覺任務的首選架構。但是，隨著越來越高效的結構出現，使用Transformer來完成CV（電腦視覺）的任務就成了一個新的研究方向，它能夠降低結構的複雜性、探索可擴展性、提高訓練的效率。

**Vision Transformer (ViT) 的誕生**

2020年10月，谷歌提出了Vision Transformer（ViT）模型，不用卷積神經網路CNN，就可以直接用Transformer對圖像進行分類。值得一提的是，ViT的性能表現非常出色，在計算資源減少4倍的情況下，仍然超過了當時最先進的CNN模型。

**DALL-E 和 CLIP 的出現**

緊接著2021年，OpenAI連扔兩顆炸彈，發布了基於Transformer打造的DALL-E和CLIP模型。這兩個模型借助Transformer實現了很好的效果：DALL-E能夠根據文字輸出穩定的圖像，而CLIP能夠實現圖像與文本的分類。

**AI 繪畫的顛覆**

再到後來，DALL-E的進化版DALL-E 2還有Stable Diffusion同樣都是基於Transformer架構，再次顛覆了AI繪畫。這就是基於Transformer所誕生的模型的整條時間線。由此可見，Transformer有多麼能打。

**Transformer論文作者的現況**

那除了論文本身之外，當年發表這篇論文的作者們現在都怎麼樣了呢？Transformer論文的作者一共有8位，他們分別來自谷歌和多倫多大學。五、六年過去了，大部分的論文作者都已經離開了原來的機構。

*   **阿希什·瓦斯瓦尼（Ashish Vaswani）：**
    *   在南加州大學拿到博士學位，師從華人學者蔣偉和黃亮。
    *   主要研究現代深度學習在語言建模中的早期應用。
    *   2016年加入谷歌大腦，並領導了Transformer的研究。
    *   2021年離開谷歌。
*   **尼基·帕爾瑪（Niki Parmar）：**
    *   八位作者中唯一的女性。
    *   碩士畢業於南加州大學。
    *   2016年加入了谷歌。
    *   在工作期間，為谷歌搜索和廣告研發了一些成功的問答和文本相似度的模型。
    *   領導了擴展Transformer模型的早期工作，將它擴展到了圖像生成、計算機視覺等領域。
    *   2021年離開谷歌。
*   **Adapt 和 Essential AI：**
    *   2022年4月26日，瓦斯瓦尼和帕爾瑪參與創立了AI公司Adapt，共同創始人一共有9位，瓦斯瓦尼擔任首席科學家，帕爾瑪擔任首席技術官。Adapt的願景是創建一個被稱為人工智能隊友的AI，這個AI經過訓練可以使用各種不同的軟體工具和API。
    *   2023年3月，Adapt宣布完成了3.5億美元的B輪融資，公司估值超過10億美元，晉升獨角獸。
    *   不過在Adapt公開融資的時候，瓦斯瓦尼和帕爾瑪其實已經離開了Adapt，並且創立了他們自己的新公司EssentialAI。
    *   今年5月，EssentialAI宣布了由Thrive Capital領投的800萬美元融資。目前該公司還處於隱身模式，還沒有推出任何的產品。
*   **諾姆·沙澤爾（Noam Shazeer）：**
    *   是谷歌最重要的早期員工之一。
    *   在2000年底就加入了谷歌，直到2021年最終離職。
    *   之後成為了一家初創公司的CEO，公司的名字叫做Character.AI。
*   **Character.AI：**
    *   Character.AI的創始人除了諾姆·沙澤爾，還有一位是丹尼爾·德·弗雷塔斯。他們都來自於谷歌的LaMDA團隊。
    *   在創辦公司之前，他們在谷歌構建了支持對話程序的語言模型LaMDA。
    *   今年3月，Character.AI宣布完成了1.5億美元融資，估值也達到了10億美元，是為數不多有潛力和OpenAI競爭的初創公司之一，也是罕見的僅僅用16個月的時間就成長為獨角獸的公司。
    *   他的應用程式Character.AI是一個神經語言模型的聊天機器人，可以生成接近於人類的文本響應以及上下文對話。
    *   Character.AI於2023年5月23日在Apple APP Store和Google Play Store中發布，第一週的下載量就超過了170萬次。
    *   後來又增加了每月9.99美元的付費訂閱，允許用戶優先獲得更快的響應以及早期訪問新功能的特權。
*   **艾丹·N·戈麥斯（Aidan N. Gomez）：**
    *   他早在2019年就已經離開了谷歌。
    *   之後擔任了FOR.ai的研究員。
    *   現在是Cohere公司的聯合創始人兼CEO。
*   **Cohere：**
    *   Cohere是一家生成式AI的初創公司，於2019年成立。
    *   它的核心業務包括提供NLP的模型以及幫助企業來改進人機交互。
    *   三位創始人中有兩位是谷歌大腦團隊的前成員。
    *   2021年11月，谷歌Cloud宣布與Cohere合作，用谷歌的強大技術設施為Cohere平台提供動力。而Cohere則使用Google Cloud的TPU來開發和部署他們的產品。
    *   Cohere之所以備受業內矚目，除了創始團隊的背景之外，還有一部分原因在於他的投資者中還有圖靈獎得主傑弗里·辛頓，知名人工智能研究員李飛飛，UC伯克利大牛彼得·阿貝爾。這三位在當今的人工智能領域都是執牛耳的人物。
    *   Cohere剛剛也獲得了2.27億美元的C輪融資，成為了市值22億美元的獨角獸。
*   **盧卡斯·凱撒（Łukasz Kaiser）：**
    *   他在2021年離開了谷歌。
    *   在谷歌一共工作了7年零9個月。
    *   現在是OpenAI的一名研究員。
    *   在谷歌擔任研究科學家期間，他參與了機器翻譯和解析的工作，以及生成式任務的SOTA神經模型設計。
    *   是Tensorflow、Tensor2Tensor的共同作者。
*   **雅各布·烏什科里特（Jakob Uszkoreit）：**
    *   他被認為是發明了Transformer架構的幕後推手。
    *   在谷歌工作時間長達13年，於2021年離開谷歌。
    *   在谷歌工作期間，雅各布·烏什科里特參與組建了谷歌助理的語言理解團隊，早期還從事過谷歌翻譯的工作。
    *   之後他加入了Inceptive成為聯合創始人。
*   **Inceptive：**
    *   Inceptive是一家AI製藥公司，致力於運用深度學習去設計RNA藥物。
*   **伊利亞·波洛蘇欣（Illia Polosukhin）：**
    *   於2017年離開谷歌。
    *   現在是NEAR.AI的聯合創始人兼CTO。
*   **NEAR.AI：**
    *   這家公司主要是做區塊鏈的底層技術，可以託管去中心化應用程式和智能合約。
    *   被譽為以太坊殺手。
    *   目前估值約為20億美元。
*   **利昂·瓊斯（Llion Jones）：**
    *   唯一還留在谷歌的作者。
    *   今年是他在谷歌工作的第九年。
    *   只不過他的工作地點換到了日本。
    *   他曾打趣地說道，自己對論文做出了最有意義的貢獻，就是寫下了那個大逆不道的標題。

**總結**

雖然如今距離《Attention Is All You Need》這篇論文發表已經過去了六年，原作者們有的選擇離開，有的選擇繼續留在谷歌，但不管怎麼樣，Transformer還在繼續影響著整個AI技術的發展。

好了，本期的分享就到這裡，歡迎大家訂閱我們的頻道，我們下期再見。

---

**修改說明：**

*   將口語化的“大家好这里是最佳拍档我是大飞”改為更書面化的“大家好，這裡是最佳拍檔，我是大飛。”
*   將多個短句合併，使句子更流暢。
*   調整了部分詞語，使其更符合書面語習慣，例如“应该说”改为“應該說”。
*   增加了章節標題，使文章結構更清晰。
*   對作者們的介紹部分進行了結構化處理，使用項目符號，使其更易於閱讀。
*   增加了總結，概括了文章的重點。

希望這次的整理對您有所幫助！

[model=gemini-2.0-flash,0]
