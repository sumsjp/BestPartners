好的，我將這篇文稿整理如下，主要目標是：

*   **精簡冗詞贅句：** 刪除一些口語化的表達，讓文稿更精煉。
*   **組織結構優化：** 將內容重新組織，使其邏輯更清晰。
*   **專業術語統一：** 確保專業術語使用一致且準確。
*   **要點突出：** 更加突顯文章的核心觀點和實驗結論。

**整理後文稿：**

大家好，這裡是最佳拍檔，我是大飛。

近年來，大語言模型帶來了許多震撼。然而，這些看似強大的模型存在一個先天缺陷：本質上是靜態的，訓練後難以再學習新知識。它們只能依賴有限的上下文窗口進行即時適應，一旦超出範圍，就會忘記之前的信息。更關鍵的是，深度學習數十年來依賴的層疊結構，在解決複雜算法實現和持續學習等問題上，效果並不如預期。

今天，我們將解讀一篇來自谷歌研究院的重磅論文：《嵌套學習：深度學習架構的幻象》（Nested Learning: The Illusion of Deep Learning Architectures）。該論文提出了**嵌套學習 (NL)** 這一全新的學習範式，從數學上重構了深度學習的底層邏輯，並以此設計出了名為 **HOPE** 的模型。在語言建模和常識推理任務中，HOPE 超越了 Transformer、RetNet 等主流架構。

**一、當前深度學習的核心痛點**

作者指出，層疊思維存在以下四個局限：

1.  模型的計算深度並不會隨著層數增加而無限提升。
2.  部分參數的容量提升會隨著深度或寬度的增加而邊際遞減。
3.  訓練過程容易陷入次優解。
4.  模型的快速適應能力、持續學習能力和分布外泛化能力幾乎不會因為堆疊更多的層而得到提升。

這種情況導致模型訓練結束後，就無法再高效學習新知識，類似於順行性遺忘症患者。預訓練階段學到的知識被儲存在MLP層中，相當於發病前的長期記憶；而推理時的上下文信息只能暫時存在注意力機制中，相當於即時的短期記憶。

**二、嵌套學習（NL）：模仿人腦記憶機制**

論文的核心思路是，借鑒人腦的多時間尺度記憶鞏固機制，構建一種嵌套式的學習框架，讓模型的每個組件都能在不同的時間尺度上更新，形成多層次的記憶系統。人腦的持續學習能力源於神經可塑性，記憶的鞏固過程主要分為兩個互補的階段：

1.  **線上鞏固（synaptic consolidation）：** 學習後立即或不久進行，穩定新的、脆弱的記憶痕跡，並開始從短期記憶向長期記憶轉移。
2.  **離線鞏固（systems consolidation）：** 主要發生在睡眠中，通過海馬體的尖波漣漪（SWRs）與大腦皮層的睡眠紡錘波和慢波協調，重複回放最近編碼的記憶模式，強化並重組記憶，最終將其轉移到大腦皮層中進行長期存儲。

**三、嵌套學習 (NL) 的核心概念**

嵌套學習是一種將機器學習模型及其訓練過程表示為一組嵌套的、多層次的、並行的優化問題的範式。每個優化問題都有自己的上下文流（context flow）。

理解嵌套學習的關鍵在於明確以下兩個概念：

1.  **關聯記憶 (Associative Memory) 與學習的區別：** 記憶是輸入引起的神經更新，而學習是獲取有效且有用記憶的過程。
2.  **更新頻率 (Update Frequency)：** 對於模型的任何組件，其頻率是單位時間內的更新次數。

基於更新頻率，可以對模型組件進行排序。如果組件 A 的更新頻率高於組件 B (f\_A > f\_B)，或者 A 和 B 頻率相同、但 B 的計算依賴 A 的狀態，那麼 A 比 B 更快（記為 A ≻ B）。如果頻率相同且計算獨立，則記為 A ≡ B。

**四、嵌套學習的三大核心貢獻**

基於嵌套學習的視角，論文對傳統深度學習的組件進行了重構和升級：

1.  **深度優化器 (Deep Optimizers)：** 論文指出，常用的基於梯度的優化器本質上都是關聯記憶模塊。作者提出了四種更具表達力的優化器擴展方向：更具表達力的關聯、更具表達力的目標函數、更具表達力的記憶、非線性輸出。
2.  **自修改 Titans (Self-Modifying Titans)：** 自修改 Titans 將優化器的更新規則本身作為學習對象，通過嵌套的優化問題，讓模型在訓練過程中不斷調整更新策略，從而適應不同的任務和數據分布。
3.  **連續記憶系統 (Continuum Memory System, CMS)：** 傳統的記憶系統被劃分為短期記憶和長期記憶。論文提出的連續記憶系統是一組 MLP 塊的鏈，每個 MLP 塊都對應一個特定的更新頻率。

**五、 HOPE 模型**

論文最終構建出了 HOPE 模型，一個具有自指學習能力和連續記憶的嵌套學習模塊。HOPE 的架構核心是將自修改 Titans 與連續記憶系統融合。連續記憶系統提供多層次的記憶存儲，自修改 Titans 提供動態的更新算法學習。

**六、實驗結果**

論文在語言建模和常識推理兩大類任務上進行了實驗，對比的基線模型包括 Transformer++、RetNet、DeltaNet、TTT、Samba、Titans（LMM）等主流架構。

*   **語言建模：** 1.3B 參數的 HOPE 在 Wiki 數據集上的困惑度 (ppl) 達到了 15.11，優於同等參數規模的 Titans（LMM）、Transformer++ 和 RetNet。
*   **常識推理：** HOPE 在多個常識推理任務上都超越了基線模型，平均準確率達到 57.23%。

實驗結果證明了嵌套學習範式的有效性。通過多時間尺度的記憶更新和自修改的學習算法，模型能更好地捕捉數據中的依賴關係，提升表達力和泛化能力。

**七、嵌套學習的影響與局限**

嵌套學習為深度學習開闢了一個新的維度，即層級深度（level depth）。它為解決持續學習、長上下文推理等傳統難題提供了新的思路。此外，它將神經科學與深度學習的結合推向了新的高度。

然而，嵌套學習也存在一些局限性，例如計算複雜度較高，理論分析需要進一步完善等。

感謝收看本期視頻，我們下期再見。

[model=gemini-2.0-flash,0]
