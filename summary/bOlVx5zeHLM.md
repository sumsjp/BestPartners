好的，我來幫您整理這篇文稿，使其更清晰易讀，更適合作為文件保存：

**標題：大語言模型工作原理詳解：從詞向量到 Transformer (Tim Lee & Sean Trott 文章翻譯)**

**引言：**

大家好，這裡是最佳拍檔，我是大飛。近半年來，大語言模型無疑是最熱門的話題。但我們一直沒有深入探討其內部工作原理。最近，蒂姆·李 (Tim Lee) 和肖恩·特洛特 (Sean Trott) 聯合撰寫了一篇文章，以最少的數學知識和術語，對大語言模型進行了解釋。本文為該文章的翻譯，旨在幫助初學者理解大語言模型的內部機制，全文幾乎沒有太複雜的數學概念、公式和運算。

**作者簡介：**

*   **蒂姆·李 (Tim Lee)：** 曾任職於科技媒體 Ars Technica，現推出 Newsletter《Understanding AI》，探討人工智能的工作原理。
*   **肖恩·特洛特 (Sean Trott)：** 加里福尼亞大學聖迭戈分校助理教授，主要研究人類語言理解和語言模型。

**正文：**

**1. 大語言模型的崛起與挑戰**

ChatGPT 在去年秋天推出時引起了轟動。雖然機器學習研究人員已研發大語言模型多年，但普通大眾並未十分關注。如今，幾乎每個人都聽說過大語言模型，但了解其工作原理的人並不多。一般的解釋只停留在「訓練大語言模型是為了預測下一個詞，且需要大量的文本」這個層面，對於具體如何預測下一個詞，往往被視為謎題。

大語言模型的開發方式與傳統軟體不同。傳統軟體由人類工程師編寫明確指令，而 ChatGPT 建立在一個使用數十億個語言詞彙進行訓練的神经网络之上。因此，地球上沒有人完全理解大語言模型的內部工作原理。研究人員正在努力理解，但這是一個需要數年甚至數十年才能完成的過程。

**2. 詞向量 (Word Vector)**

*   **詞向量的概念：** 人類用字母序列表示單詞 (例如：C-A-T 表示貓)，而語言模型使用詞向量——一長串數字的列表來表示單詞。
*   **詞向量的意義：** 每個詞向量代表詞空間 (word space) 中的一個點，具有相似含義的詞的位置更接近。例如，在向量空間中，與貓 (cat) 最接近的詞包括 dog、kitten 和 pet。
*   **詞向量的優點：** 數字可以進行字母無法進行的運算。大語言模型使用具有數百甚至數千維度的向量空間，計算機可以對其進行推理，並產生有用的結果。
*   **Word2Vec (Google, 2013)：** Google 分析了數百萬篇 Google 新聞中的文檔，找出哪些單詞傾向於出現在相似的句子中。經過訓練的神经网络學會了將相似類別的單詞 (例如：dog 和 cat) 放置在向量空間中的相鄰位置。
*   **向量運算與類比：** 可以使用向量運算進行類比，例如 biggest - big + small = smallest。Google 的詞向量還捕捉到了許多其他的關係，例如：瑞士人 : 瑞士 :: 柬埔寨人 : 柬埔寨，巴黎 : 法國 :: 柏林 : 德國。
*   **偏見問題：** 詞向量反映了人類語言中的偏見。例如，在某些詞向量模型中，醫生 - 男人 + 女人 = 護士。減少這種偏見是一個新的研究領域。
*   **基礎作用：** 詞向量是大語言模型的基礎，它編碼了詞與詞之間微妙但重要的關係信息。如果一個大語言模型學到了關於 cat 的一些知識，那麼同樣的事情很可能也適用於 kitten 或 dog。如果模型學到了關於巴黎和法國之間的關係，那麼柏林和德國以及羅馬和意大利的關係很可能也是一樣的。

**3. 多重含義與上下文 (Homonyms & Polysemy)**

*   **問題：** 簡單的詞向量方案沒有捕獲自然語言的重要事實，即一個單詞通常有多重含義。例如，單詞 bank 可以指金融機構或河岸。
*   **解決方案：** 像 ChatGPT 這樣的大語言模型能夠根據單詞出現的上下文，用不同的向量來表示同一個詞。有一個針對金融機構的 bank 的向量，還有一個針對河岸的 bank 的向量。
*   **歧義的解決：** 大語言模型為特定段落的上下文中表示每個詞的準確含義提供了一種靈活的方式。

**4. Transformer**

*   **Transformer 的結構：** ChatGPT 最初版本背後的 GPT-3 模型由數十個神经网络層組成。每一層都會接受一系列的向量作為輸入，並添加一些信息來幫助澄清這個詞的含義，從而更好的預測接下來可能出現的詞。每個層都是一個 Transformer。
*   **輸入與隱藏狀態 (Hidden State)：** 輸入文本中的每個詞會對應著一個向量，並傳遞給第一個 Transformer。第一個 Transformer 確定了 wants 和 cash 都是動詞，並將信息存儲在「隱藏狀態」向量中，然後傳遞給下一個 Transformer。
*   **層數與功能：** 真實的大圓模型往往有更多的層。最强大的 GPT-3 版本有 96 層。前幾層的神经网络會專注於理解句子的語法，並解決上面所表示的歧義；後面的層則致力於對整個文本段落的高層次的理解。
*   **向量维度：** 現代大語言模型中的向量维度極為龐大，GPT-3 最强大的版本使用了有 12,288 個维度的词向量。

**5. Transformer 內部運作：注意力機制 (Attention) 與前饋網絡 (Feedforward Network)**

*   **注意力機制：** 詞彙會觀察周圍，查找具有相關背景並彼此共享信息的其他的詞。每個單詞都會製作一個檢查表稱為查詢向量 (Query Vector) 來描述他尋找的詞的特徵，還會製作一個檢查表稱為關鍵向量 (Key Vector) 描述他自己的特徵。神经网络通過將每個關鍵向量與每個查詢向量進行比較，通過計算他們的點積來找到最佳匹配的單詞。
*   **注意力頭 (Attention Head)：** 每個注意力層都有幾個注意力頭。比方說其中一個注意力頭可能會將代詞與名詞進行匹配，另外一個注意力頭可能會處理解析類似於 bank 這樣的一詞多義的含義，第三個注意力頭可能會將 Joe Biden 這樣的兩個單詞連接在一起。GPT-3 的最大版本有 96 個層，每個層有 96 個注意力頭，因此每次預測一個新詞的時候，GPT-3 將執行 9,216 個注意力的操作。
*   **前饋網絡：** 在注意力頭在詞向量之間傳輸信息之後，前饋網絡會思考每個詞向量並且嘗試預測下一個詞。在這個階段單詞之間沒有交換任何的信息。前饋層會獨立的去分析每個單詞，但是前饋層可以訪問之前由注意力頭复制的任何信息。前馈层通过模式匹配进行工作，隐藏层中的每个神经元都能够匹配输入文本中的特定模式。
*   **分工：** 注意力機制從提示的教導部分檢索信息，而前饋層讓語言模型能夠記住沒有在提示中出現的信息。可以将前馈层视为模型从训练数据中学到的信息的数据库。
*   **布朗大学的研究：** 前馈层通过添加将国家向量映射到其对应首都的向量，从而将波兰转化为了华沙。相同的向量添加到中国时候答案会得到北京。

**6. 大語言模型的訓練方式**

*   **無需顯式標記數據：** 大語言模型不需要顯式的標記數據，通過嘗試預測文本段落中的下一個單詞來學習幾乎任何的書面材料。
*   **流程：** 模型可能會拿到一個輸入，例如 i like my coffee with cream and 某某，並且試圖預測 sugar 糖作為下一個單詞。通過看到更多的例子 (數千億個單詞)，這些權重會逐漸的調整，從而做出更好的預測。
*   **訓練步驟：**
    1.  **前向傳播 (Forward Pass)：** 打開水源，並且檢查水是否從正確的水龍頭中流出。
    2.  **反向傳播 (Backwards Pass)：** 關閉水源，算法会逆向地通过网络，使用微积分来评估需要改变每个权重参数的过程。

**7. 大語言模型的規模效應**

*   **規模越大，效果越好：** OpenAI 的研究表明，语言模型的准确性与语言规模数据集规模以及用于训练的计算量呈幂率关系。
*   **案例：心智理論 (Theory of Mind)：** GPT-1 和 GPT-2 在這個測試中失敗了，但在 2020 年發布的 GPT-3 的第一個版本中正確率達到了接近於 40%，去年 11 月份發布的最新版的 GPT-3 將上述問題的正確率提高到了大約 90%，與 7 歲的兒童相當，而 GPT-4 对心智理论问题的回答正确率约为 95%。

**8. 预测與通用人工智能 (AGI)**

*   **預測的重要性：** 預測可能是生物智能以及人工智能的一個基礎。人腦可以被認為是一個預測機器。
*   **突破：** 语言模型能够通过找出最佳的下一个词的预测，来学习人类语言的运作方式。

**結論：**

本文詳細講解了大語言模型的推理過程和訓練方式。雖然我們目前還不能完全理解其內部運作方式，但重要的是關注其經驗表現。如果一個語言模型能夠在特定類型的問題中始終得到正確的答案，無論他們對語言的理解方式是否跟人類完全相同，這都是一個有趣而且重要的結果。

**感謝：**

感謝大家的觀看，希望這個影片能夠幫助到大家对现在的大语言模型有一个基础的理解。

**備註：**

*   本文件為蒂姆·李和肖恩·特洛特文章的翻譯整理，內容力求準確，但可能存在理解偏差。
*   如有任何錯誤或建議，請隨時指正。

**整理說明：**

*   **分層標題：** 使結構清晰，方便閱讀和查找信息。
*   **重點摘要：** 提煉關鍵概念和結論，便於快速掌握文章核心內容。
*   **案例分析：** 保留了文稿中的重要案例，以幫助理解抽象概念。
*   **術語解釋：** 對於重要的術語 (如詞向量、Transformer、注意力機制等) 進行解釋，避免讀者產生混淆。
*   **簡化語言：** 盡量使用簡潔明瞭的語言，避免過於專業化的術語。
*   **格式統一：** 統一格式 (如字體、字號、行距等)，使文件更加美觀易讀。

希望这份整理后的文稿对您有帮助！

[model=gemini-2.0-flash,0]
