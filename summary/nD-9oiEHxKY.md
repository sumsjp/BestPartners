好的，我幫您整理了以下文稿，主要做了以下修改：

*   **修正了錯字和語法錯誤。**
*   **添加了標點符號，使文稿更易讀。**
*   **稍微調整了語句順序，使表達更流暢。**
*   **將文稿分段，使結構更清晰。**
*   **統一了人名翻譯，如John Schulman統一為“約翰·舒爾曼”。**

**整理後的文稿：**

大家好，這裡是最佳拍檔，我是大飛。

假如在一到兩年內，一款真正意義上的AGI（通用人工智慧）能夠進入市場，你會期待它能做哪些事情呢？

不久前，ChatGPT首席架構師兼OpenAI聯合創始人約翰·舒爾曼（John Schulman）做客Dwarkesh的訪談，分享了他對AI模型未來發展的看法。在他的眼中，AGI的實現近在咫尺，人們將在三到五年內見證人工智慧的飛躍式發展，真正意義上的AGI將在這個期間問世。

圍繞這一預測，舒爾曼深入探討了關於大模型訓練的現狀、相關技術的進化方向，以及未來AI需要面對的監管問題。這100分鐘的訪談應該說是乾貨滿滿。今天大飛就來給大家分享一下舒爾曼眼中的AI模型會走向怎樣的未來。

**AI在未來人類社會中的定位**

先來說說AI在未來人類社會中的定位，這也是時下人們關注的焦點。在舒爾曼看來，AI始終是人類的助手，而相關的技術也會朝著讓AI成為更優秀的助手的方向發展。

在訪談開始時，他解釋道，所謂的AI訓練，就是對已經從互聯網中抓取了足夠內容的模型進行針對性的行為優化，來滿足人類的需求。經過訓練的模型可以非常精確，它不僅可以生成網上所有的內容，還可以按照要求排列內容的分布，從而實現特定的功能。

舒爾曼期望在不久的將來，AI能夠更像一個樂於助人的同事，而不僅僅是一個執行一次性查詢的工具。他期待AI能夠更加主動，能夠理解並且參與到用戶的整個專案中，甚至能夠主動提出建議和幫助。它既可以充當你的私人秘書，也可以做你的程式設計師。

但是，AI所能實現的功能都是建立在人的要求上，因此它始終都是人類的僕人，而非人類的主人。舒爾曼提到，目前的模型只是試圖產生一些人們會喜歡並且判斷為正確的東西，而不關心產出的到底是什麼。

**五年內AI模型的進化方向**

在此基礎上，舒爾曼預測，在未來的五年內，AI模型將向著“更好的助手”進化。他希望AI能夠獨立完成整個程式設計專案，根據人類的要求自行編寫代碼，而不是僅僅是提供編寫函數的建議。

同時，AI在執行多線程與長時間任務上的能力也將得到明顯的提高，並最終做到能夠長時間地編寫多個代碼文件。在一系列的進步下，模型將變得更有效率，可以更快地從錯誤中恢復，更好地處理臨界情況。

此外，舒爾曼還希望能夠為AI模型添加新的任務模式，通過預訓練和後訓練的結合，不斷改進功能，開拓新的用例。他預計隨著時間的推移，人工智慧將成為經濟的重要組成部分，人們將更好地理解如何將其整合到不同的社會活動中。

**AI技術通往未來的鑰匙**

舒爾曼對於未來的暢想絕非空穴來風。實際上，他認為自己已經找到了AI技術通往未來的鑰匙，那就是長期地進行強化學習訓練。通過投入更多的多模態數據與訓練成本，強化學習訓練計畫就會釋放AI在更長時間裡保持連貫的能力。一旦這種能力被解鎖，我們就可以期待AI對任務的處理能夠達到人類的水平。舒爾曼將這一質的變化稱為“相變”。

他相信，一旦模型的規模與學習訓練達到某個水平，就能夠處理更長的任務。通過使用特定的提示語，人類可以向AI描述任務所需要的時間尺度，然後它就可以開始制定學習計畫，嘗試朝著目標前進，無論這個目標是一個月還是十年後。

OpenAI當下的研究正是專注於AI的學習訓練，也就是基於人類反饋的強化學習（RLHF）的學習系統。在RLHF中，AI表現出了類似人類的心理驅動力，或者說對於目標的渴望。以人類的行為為例，當你有了某個特定的目標，比如說吃飯，你就會試圖轉向一個“尋找食物”的狀態，而不是其他狀態。這個過程在AI上，則表現為對人類正反饋的追求，模型“希望”自己的產出得到人類的認可。

舒爾曼認為，AI的驅動力或者目標概念還包括實現目標後的滿足感，這些因素可能與學習演算法有著強關聯關係。因此，他表示，在某種程度上，模型確實通過RLHF以某種有意義的方式實現了和人類相同的心理模式。

**學習方式的選擇**

在具體訓練方案的選擇上，舒爾曼則關注兩種學習方式：一種是上下文學習，這種方式雖然樣本效率高，但是會隨著每個實例的變化而被破壞；另一種是大規模訓練，雖然不會隨著實例變化而破壞，但是有可能過於淺薄。兩種方案各有優劣。

但是舒爾曼卻不滿足現有的技術路線，他探討了第三種可能：是否存在一種折中的學習方式，既不會隨實例變化而破壞，也不會過於淺薄，而是有著更強的主觀能動性？這種中間路徑可能涉及模型的中期記憶能力，即對於一百萬左右token的記憶調度。

如果能夠實現兼具前兩者優點的第三種學習模式，模型就能做到既能適應上下文，又不需要大量的預訓練資料。舒爾曼表示，在OpenAI之前，人們並沒有真正努力在大規模訓練和上下文學習之間找到平衡。他和他的團隊希望填補這一技術空白，構建一個能進行在線學習的系統，從而使得模型具有一些類似人類的認知技能，比如自我反思的能力，以及自推理的能力。

模型將能夠通過自推理實現如同人類一般“舉一反三”的能力，同時使用內省和自主知識來確定需要學習什麼。這些能力是當前大模型所缺少的，但是一旦實現，我們就離真正的AGI不遠了。

**技術瓶頸與挑戰**

當然了，任何尖端科技的發展都不可能是一帆風順的。舒爾曼也坦率地承認了技術瓶頸的存在。但是他本人對於這些難題抱有樂觀的態度。

目前大模型面對的問題主要有四個：

1.  **難以預測的雜項缺陷：** 舒爾曼表示，一旦開始進行長期的強化學習訓練，模型將能夠在更長時間裡保持連貫。然而不同的模型有著不同的訓練承受能力，這會導致它們經受過強的學習訓練，從而陷入瓶頸。
2.  **當下AI模型的局限性：** 我們依然沒有完全摸清AI這個“灰盒子”，這導致模型可能存在會幻覺等意料之外的問題。它們會錯誤地認為自己可以執行、甚至已經執行了某些它做不到的任務。比如，AI會表示自己已經幫助用戶發送了電子郵件或叫了輛出租車，但是實際並沒有這麼做。

不過，舒爾曼將以上兩點稱之為“雞毛蒜皮的小事”，這些事情也許會在初期減緩開發速度，但是不會持續太久。他相信，通過強學習訓練，以上問題都可以在兩到三年內獲得解決。屆時我們就會見證人工智慧發展的又一高峰。

3.  **訓練數據不足的問題：** 這被舒爾曼稱為“一個對科學研究的挑戰”，足可見它受重視的程度。從人類的視角看，互聯網上的知識浩如煙海，即使窮盡某個人的一生也不能完全了然。但是對於每秒吞吐上億數據的語言模型來說，互聯網中可供學習的資料的產出，遠遠趕不上它所消耗的速度。終有一天，大模型會陷入“輟學”的狀態，也就是說再也沒有足夠的訓練數據供它學習了。

舒爾曼承認數據量的有限性會帶來一些挑戰，但是這個問題還是被誇大了。首先，即使在數據有限的情況下，通過少量示例也可以改善AI的性能。AI的“泛化”就是一個例子，通過對西班牙語資料的學習，AI的英語能力也能夠得到提升。其次，只要模型的規模達到必要的程度，它就能從少量的数据中提取需要生成的功能。因此，对于GPT4级别的大模型而言，准确地识别需要的功能、并收集相关的数据比单纯的训练资料更为重要。

因此舒尔曼认为数据不足还不是OpenAI眼下最要紧的问题，他们有足够时间应对挑战。

4.  **AI的聊天功能：** 体验过AI的朋友都有类似的感受，AI的语言风格还是太过僵硬与公式化。Schulman相当关注聊天模型的发展，以及如何通过混合数据集，比如指令和聊天数据来优化模型。他认为，聊天模型更易于使用，并且展示出了一定程度的自省行为，这是AI智能的一种体现。眼下OpenAI正在努力改进AI的写作体验，让它更有“人情味”。

舒爾曼提到，他們不僅改進了ChatGPT的個性，也在探索語言模型如何影響語言的使用。在收集數據的過程中，研究者發現人們喜歡結構化的回應和大量信息，也注意到模型可能比人們需要的更加冗長。這可能是因為在標記階段，評分者更喜歡冗長的答案，又或者是因為預訓練的方式導致模型傾向於繼續生成文本。無論如何，針對AI聊天的功能優化已經被舒爾曼提上了日程。

**AGI與社會的平衡**

从舒尔曼对于科研瓶颈的态度不难看出，对于AI模型在未来发展，以及它能实现的功能，这位首席架构师都充满了信心。他甚至在访谈中预言，真正意义上的AGI会在2027年到来。

但是我們的社會真的能在三到五年內做好迎接更高性能的AI甚至AGI的準備嗎？如果人工智能變得足夠強大，比如能夠自己經營一家成功的企業，那麼人類是否還需要參與其中？

談及這一點，舒爾曼認為整個社會都需要拿出一套可以安全地處理AI的解決方案。政府要如何處理潛在的失業潮？AI在道德倫理上是否仍然具有風險？AI在企業中的應用是否會增加信息安全隱患？他甚至半開玩笑地說，如果一切開發順利，作為架構師的自己就會在一年之內丟掉飯碗。

對於這些問題的解決方案，在當下的社會中還沒有達成有效的共識。舒爾曼也承認，他不知道要如何在現在的社會中長期保持AI與社會秩序的平衡。我們必須要找到一種方案來確保AI始終隸屬於人的意志，與此同時，AI還得屬於秉持社會正義的團體。一旦被心懷惡意的人濫用，AI就會導致難以想像的災難。舒爾曼直言，在人們享受科技帶來的繁榮和進步之前，先要考慮好如何確保這些系統不會被壞人濫用，甚至於顛覆現有的社會秩序。

**技術解決方案與社會實驗**

面對AI可能造成的問題，作為信息工程領域的大拿，舒爾曼提出了一個技術性很強的解決方案，那就是一套強大的監管系統與一場穩定的社會實驗。他表示，既然步子邁大了容易扯到蛋，那還不如一步一走，先在社會中部署一個性能較弱的模型，再逐步對它的性能進行升級。一旦出現任何意料之外的惡性事件，整個部署流程可以隨時被終止。與此同時，舒爾曼也寄希望於監管技術的進步，以便在系統開始出現問題的時候能夠立即發現。

他舉了AI在公司管理中的應用作為例子。對於公司的運營者而言，海量的数据处理以及繁杂的管理问题始终是一座大山，压得人喘不过气来。如果管理者能够得到AI的辅助，那么整个公司的运营效率都将得到提高。但是如果AI表现出远超人类的管理效率，那么公司的运营还需要人类的参与吗？

至少在舒尔曼看来，即使人工智能展现出了超越人类的效率，人仍然是人工智能最终行动的驱动力。如果有人参与的公司，在市场竞争中败给了没有人参与的公司，那么这本身就是对社会秩序的动摇。在他看来，资本市场需要一套监察系统来维持AI与人之间的平衡，并且在必要的时刻禁止AI公司参与正常的市场竞争。

舒尔曼还表示，虽然AI运营的公司在许多方面可能会表现得更加高效，但是它们也存在更高的尾部风险。这是因为AI在处理非常少见的情况时，仍然缺乏足够的样本，所以更有可能出现大规模故障。哪怕是出于风险管理的考虑，他也不建议把AI大规模投入到公司管理。但是他也没有就此否定AI公司的可能。舒尔曼认为，假以时日，当社会做好准备，AI也证明自身在管理方面比人类更善于对人类负责，那么让AI管理公司也许是可以的。

**給中小企業和開發者的建議**

在訪談的最後，舒爾曼從AI模型在未來社會中的暢想中回到了當代，並給予了中小企業和個人開發者一些技術路線上的建議。他認為與社會科學等其他領域相比，對於機器學習的研究是一個相對健康的領域，因為它足夠“務實”，整個領域的事物在很大程度上基於技術的實用性和實證主義，只要你能做出成果，就能獲得回報。

因此他很推薦當代的有志者們投入這個領域來施展自己的才華。但是他也指出，訓練創建一個真正具有人們關心的所有功能的模型是相當複雜的，需要大量的專業人士和大量的研發積累。這個特點使得大模型的開發容易形成技術壁壘，就像企業的“護城河”一樣，入行的新人難以複製前人的成功。

对此，舒尔曼建议这些缺乏足够经验与人手的企业，可以考虑使用“蒸馏”模型来构建自己的AI系统。“蒸馏”模型的原理有点类似AI老师给AI孩子上课，开发者需要将一个复杂而且规模较大的教师模型中的知识提取出来，并传递给一个相对较小而且结构简单的学生模型。这样做

[model=gemini-2.0-flash,0]
