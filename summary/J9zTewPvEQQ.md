好的，我为您整理了这篇文稿，使其更清晰、更易读：

**标题：SambaNova RDU：大模型推理的更佳选择，超越GPU？**

**引言：**

大家好，我是最佳拍档的大飞。OpenAI 的 o1 发布后，AI 领域迎来了一场变革。o1 不仅具备人类般的复杂问题思考能力和优秀的通用推理能力，还能在未经专门训练的情况下，拿下数学奥赛金牌，甚至在博士级别的科学问答环节超越人类专家。

**大模型进化范式的转变：**

*   **关键：** 更多强化学习 + 更多推理 = 更强大的性能
*   **理论支撑：** Rich Sutton 的《苦涩的教训》指出，充分利用计算能力的方法最终才是最有效的。搜索和学习是两种会随着算力增加而持续扩展的方法。
*   **未来趋势：** Sam Altman 认为，新范式进化的曲线会非常陡峭。从训练 Scaling 到推理 Scaling 的范式转变，引发了对计算资源分配和硬件选择的重新思考。

**为什么需要重新思考硬件选择？**

*   **推理阶段需要更多计算资源：** AI 领域的研究者和从业者意识到这一点。
*   **优化硬件配置是关键：** 通过优化硬件配置来提升大模型推理的效率将是下一阶段的重点。
*   **并行处理能力的需求更高：** 大模型推理 Scaling 比训练 Scaling 对芯片并行处理能力的要求更高。

**GPU 的局限性：**

*   **GPU 的优势：** 由于优秀的并行处理能力，GPU 在过去几年里一直是训练大模型的热门选择。
*   **GPU 的不足：** 在全新的范式下，GPU 在延迟、功耗等方面表现不佳，不是进行大规模推理的最好选择。

**SambaNova RDU：一种更佳选择？**

*   **AI 芯片的各种流派：** ASIC、FPGA、DSP、Neuromorphic Chip 以及大量 DSA 芯片。
*   **SambaNova RDU 的优势：** 基于动态可重构数据流架构的芯片，通过并行处理和高效的数据移动来优化芯片的性能和效率，获得越来越多的关注。
*   **SambaNova 最新一代 RDU 产品：** SN40L。

**SambaNova 如何实现大模型的快速推理？**

*   **HBM 利用率是关键：** 大模型推理时，每生成一个 token，都需要把模型参数从 HBM 高带宽内存中搬运到片上进行计算。
*   **SambaNova 的独特架构：** 唯一一款采用紧耦合三层内存系统的 AI 加速器，由 SRAM、HBM 和 DDR DRAM 组成。
    *   **DDR：** 可以在单个插槽上托管数百个异构模型和检查点，支持万亿参数专家模型组合和其他 Agent 工作负载，并且可以在模型间快速切换，不受主机 PCIe 带宽的限制。
    *   **HBM：** 可以保存当前运行的模型，并且缓存其他模型。
    *   **大型分布式片上 SRAM：** 可以通过空间内核融合和库级并行，实现高强度的运算。
*   **极致的算子融合：** 架构可以自动做到极致的算子融合，实现 90% 以上的 HBM 利用率。
*   **性能优势：** RDU 相比 GPU 有着 2-4 倍的性能优势。
*   **Llama 3.1 405B 的推理速度：** SambaNova 是唯一一个能在 Llama 3.1 405B 上提供每秒超 100 个 Token 推理速度的平台，甚至超过第二名 Fireworks 将近一倍。
*   **解码器：** 整个解码器就是一个 Kernel 调用，调用开销显著减少，芯片对数据进行有效处理的工作时间增加。
*   **批处理能力：** RDU 参考了 GPU 的设计，在编码器 decoder0 进行批处理运算的时候，可以同时从 HBM 读取 decoder1 的参数。

**数据流架构的优势：**

*   **数据驱动计算：** 通过数据流动来驱动计算过程，而不是通过常规的指令流动。
*   **并行处理：** 每个节点在它的所有输入数据准备好以后，会立即执行，并且将结果传递给下游的节点。这种架构天然就支持并行处理，显著提高了计算性能。
*   **片上空间数据流：** 可以进行自动的算子融合，能够明显消除大量的内存流量和开销。

**GPU 的改进：**

*   **GPU 开始引入数据流功能：** 从 H100 开始，英伟达的 GPU 不仅开始加入了分布式共享内存，还加入了新的张量内存加速器单元，从某种程度上模仿了片上空间流水线运行的模式。
*   **改进不足：** 这种程度的改动还远远不够，GPU 速度的提升恐怕已经跟不上 AI 推理需求的暴涨了。

**GPU 的局限性（根本原因）：**

*   **设计初衷：** 很多 GPU 最初不是专门为 AI 而设计的。
*   **架构限制：** 很难在不影响主营业务的情况下，对 GPU 的基本架构做完全的重新设计，即使增加了一些修修补补的工作，也无法完全改用高效的数据流架构。

**SambaNova 的竞争优势：**

*   **数据流架构：** 几家主流的 AI 芯片初创公司都选择了数据流架构，SambaNova 的 RDU 展现出了独特的优势，被视为 GPU 的最有力竞争者。
*   **速度优势：** 与英伟达相比，Sambanova 在最新的 Llama 3.1 模型上生成 token 的性能已经快了不止 10 倍。
*   **可访问性：** 通过 cloud sambanova ai，公开提供给开发人员们使用。
*   **成本优势：** SambaNova RDU 不仅拥有大容量的片上 SRAM，同时也拥有 HBM，用户需要用来支持大语言模型的基础设施更少。
    *   **Llama70B 推理所需机架数量对比：**
        *   Groq：9 个机架 (576 个芯片)
        *   Cerebras：4 个机架 (336 个芯片)
        *   SambaNova：1 个机架 (16 个 SN40L 芯片)

**RDU 对 AGI 探索的加速：**

*   **推理算力 = 更强的智能：** 在同一时间单位内，推理速度越快，就能实现越复杂的推理，解锁越复杂的任务，大模型应用的天花板也就越高。
*   **基础设施建设是关键：** 想要更快的实现 AGI，本质上就需要建设足够的基础设施，并且持续的降低计算成本，同时计算资源还要更多地向推理侧增加。
*   **OpenAI 的困境：** OpenAI 在发布 o1 的时候，似乎就遇到了这个问题，由于最高的配置过于昂贵，没有对应的基础设施能够支持大规模的部署。
*   **SambaNova 的解决方案：** 在 o1 发布后不久，SambaNova 便在 Hugging Face 上发布了 Llama 3.1 Instruct-O1 的演示，这个项目由 SambaNova 的 SN40L RDU 提供算力支持，用户可以与 Llama 3.1 405B-instruct 模型进行实时的对话，体验风驰电掣般的 o1 推理过程。
*   **开源大模型的潜力：** 在强大算力的支持下，开源大模型的推理能力会不断提升，甚至触达更高级的智能也指日可待。

**SambaNova 公司介绍：**

*   **独角兽公司：** 在 AI 芯片赛道的诸多初创公司中，SambaNova 是目前估值最高的一家独角兽。
*   **成立时间：** 2017 年。
*   **CEO：** Rodrigo Liang，毕业于斯坦福大学，曾领导甲骨文和 Sun Microsystems 的工程团队，负责 SPARC 处理器和 ASIC 芯片的开发。
*   **创始人团队：** 其他两位创始人也都来自斯坦福大学。
*   **陈立武：** “芯片风险投资教父”陈立武自 SambaNova 创立之初，就作为创始投资人和董事会主席加入公司，并且于 2024 年 5 月出任执行主席，从而加速和扩大公司的发展。

**总结：**

自从大模型的 Scaling Law 开始从预训练向后训练和推理侧转移之后，一个新的时代正在开启。芯片厂商们在算力层面的分配与设计也会更为深刻的影响大模型领域的竞争格局。对于 SambaNova 或者其他以提供算力和计算基础设施为主的公司来说，相信接下来会迎来前所未有的机遇。

**结尾：**

今天的视频就到这里，感谢大家的观看，我们下期再见。

**整理说明：**

*   **精简语言：** 去除了一些口语化的表达和重复的信息。
*   **结构化：** 将内容分成了更小的段落，并添加了小标题，使文章结构更清晰。
*   **关键词突出：** 使用加粗字体突出了一些关键概念和结论。
*   **信息整合：** 将一些相关的信息整合到一起，使其更易于理解。
*   **内容逻辑优化：** 调整了一些段落的顺序，使逻辑更流畅。

希望这个整理后的版本对您有所帮助！

[model=gemini-2.0-flash,0]
