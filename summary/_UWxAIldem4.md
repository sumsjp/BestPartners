好的，我將這段文稿整理如下，主要針對語句流暢度、重複信息、及結構清晰度進行修改：

**TTT：有望超越Transformer的新型大語言模型架構**

大家好，這裡是最佳拍檔，我是大飛。

近期，人工智能領域最火熱的話題莫過於新型大語言模型架構TTT（Test-Time Training）。這項研究提出一種新的信息壓縮和模型記憶機制，以測試時訓練層取代RNN的隱藏狀態，通過輸入token的實際梯度下降，實現對上下文的壓縮。研究者相信，TTT將從根本上改變大語言模型。

**TTT的核心優勢：**

*   **線性複雜性架構：** TTT層直接取代Attention機制，通過表達性記憶，解鎖線性複雜性架構，使模型能夠在上下文中訓練數百萬甚至數十億個token。
*   **性能優越：** 在125M到1.3B參數規模的模型中，TTT在性能上匹敵甚至擊敗基於Transformers和Mamba架構的模型。與Mamba相比，TTT-Linear的困惑度更低，FLOPs更少，對長上下文的利用更好。

**RNN的瓶頸：**

RNN最本質的問題在於長上下文處理的挑戰。與自注意力機制不同，RNN層必須將上下文壓縮為固定大小的隱藏狀態，更新規則需要發現數千甚至數百萬個token之間的底層結構和關係。雖然RNN的線性複雜度是其相對於Transformer的主要優勢，但這種優勢僅存在於長上下文中。一旦上下文足夠長，現有的RNN反而難以有效利用額外的條件信息。

**TTT的設計理念：**

受到自監督學習將大量訓練集壓縮為大語言模型權重的啟發，研究團隊設計了一種新的序列建模層。該層的隱藏狀態是一個模型，更新規則是自監督學習的一個步驟。由於更新測試序列上的隱藏狀態的過程相當於在測試時訓練模型，因此稱為測試時訓練層（TTT層）。

**TTT的具體實例與改進：**

研究團隊引入了兩個簡單的實例：TTT-Linear和TTT-MLP，分別以線性模型和兩層的多層感知器MLP作為隱藏狀態。TTT層可以集成到任何網路架構中並進行端到端優化。為了提高TTT層的效率，研究團隊採取了以下改進技巧：

*   **小批量token：** 類似於常規訓練期間對小批量序列採取gradient step的做法，研究團隊在TTT期間也使用了小批量的token。
*   **對偶形式（Dual Form）：** 研究團隊為每個TTT小批量內的操作開發了一種對偶形式，以便更好地利用現代GPU和TPU，加快訓練速度。

**TTT的原理：**

所有序列建模層都可以看作是將歷史上下文儲存到隱藏狀態。RNN層將上下文壓縮為跨時間的固定大小狀態，這種壓縮雖然使得輸入token到輸出token的映射高效，但也限制了RNN層在長上下文中的性能。自注意力的隱藏狀態是一個隨著t線性增長的KV列表，無需壓縮，使得其在長上下文方面比RNN層更具表現力。然而，掃描這個線性增長的隱藏狀態所需的時間也是線性增長的。因此，為了保持長上下文的高效和表現力，需要一種更好的壓縮啟發式，將成千上萬甚至上百萬的token壓縮到一個隱藏狀態中，從而有效捕捉它們的底層結構和關係。

**實驗結果：**

*   **短文本（2k上下文）：** TTT-Linear (M)、Mamba和Transformer的性能相當。
*   **中等長度文本（8k上下文）：** TTT-Linear (M) 和 TTT-MLP (M) 的表現都明顯優於Mamba。隨著上下文長度的增加，TTT層相對於Mamba層的優勢也在擴大。Transformer雖然在每種模型尺寸下的困惑度依舊表現不錯，但是由於FLOPs成本的原因，已經不再具有競爭力。
*   **長上下文能力評估（32k上下文）：** TTT-Linear (M) 和 TTT-MLP (M) 的表現都優於Mamba。

**速度實驗：**

在TPU上，TTT-Linear比Transformer快了10%。在GPU上，儘管使用了更快的vLLM，TTT的速度仍然具有競爭力。

**總結與展望：**

如果scaling law依然存在，那麼TTT將帶來非常重大的影響，尤其對於長序列。Transformer的計算成本往往很高，而RNN則會發生遺忘。TTT巧妙地利用了神經網絡來解決了RNN的不足。

**研究團隊：**

三位主要作者分別來自於斯坦福大學、加州大學伯克利分校、加州大學聖迭戈分校。他們已經公開了代碼，有興趣的朋友可以深入研究。

**討論：**

大家對於TTT的發展有何看法？它是否會超越Transformer架構？歡迎在評論區留言。感謝大家的觀看，我們下期再見！

[model=gemini-2.0-flash,0]
