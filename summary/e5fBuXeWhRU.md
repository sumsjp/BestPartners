好的，這是經過整理的文稿，我盡可能保留了原文的口語化風格，同時使其更易於閱讀和理解：

**Llama 2 技术细节分析：开源大模型的关键启示**

大家好，這裡是最佳拍檔，我是大飛。

前幾天，Meta 發佈了免費可商用的 Llama 2，在整個 AI 圈引起了轟動。我們也做了一期節目介紹。之前的 Llama 1 版本因為開源協議的限制，一直無法免費商用。現在，隨著 Llama 2 的發佈，這個限制正式被打破。

在模型發佈之後，Llama-2-70B-Chat 迅速登頂 Hugging Face 的開源大模型榜單。但是，就在昨天，已經有新的模型把寶座搶走了。我們後續也會再介紹一下。用我同事的話說，「中午睡了半個小時的午覺，大模型又變天了」，真的是太誇張。

不過，今天我們主要是想給大家介紹一下 Llama 2 的更多技術細節。巧的是，除了官方公開的技術資料以外，來自伯克利大學，同時也是 Hugging Face 的人工智慧科學家内森·兰伯特 (Nathan Lambert) 也在自己的博客上發表了一篇文章。

他認為 Llama 2 是 Llama 架構的延續，但在數據質量、訓練技術、能力評估、安全訓練和責任發佈方面都進行了大量的技術更新。我們在這裡給大家分享一下其中的精華內容。

**Llama 2 的關鍵要點：**

首先，兰伯特 回顧了一下 Meta 在論文中提到的幾個要點：

*   Llama 2 相較於上一代模型，訓練數據提升了 40%，包含了 70 億、130 億和 700 億參數 3 個版本。
*   Llama 2 接受了 2 萬億個 token 的訓練。
*   上下文長度是 Llama 1 的兩倍，達到了 4K。
*   採用了分組查詢注意力機制 (GQA)。
*   對應的微調模型也接受了超過 100 萬個人類注釋的訓練。

雖然大家都叫 Llama 2 是開源模型，但實際從技術角度講，並沒有完全開源，不過對開源社區還是非常有用的。

通過一系列的基準測試，兰伯特 第一次確信開源模型的能力達到了 ChatGPT 的水平，當然除了程式設計能力以外。Llama 2 的成本至少超過 2500 萬美元。

此外，Meta 還提出了一種提高多輪一致性的新方法 GAtt，它的靈感來源於上下文蒸餾法。還有一些對獎勵模型、RLHF 流程、安全評估和許可聲明的看法，我們這裡先略過。

從 Meta 發表的研究論文來看，Llama 2 主要是在原有的基礎上進行了一次擴充，它的下一代模型應該也正在訓練中。

根據論文的顯示，Meta 在很大程度上傾向於通過開源實現人工智慧的民主化。這很可能意味著 Meta 正在爭分奪秒，争取在 Reddit 和 Twitter 等網站被完全封鎖之前，獲得所有可用的互聯網數據。

**技術細節：**

在基礎模型方面，除了增加上下文長度和分組查詢注意力（GQA）以外，Llama 2 在架構和所有方面都與一代非常相似。主要變化就是在數據和訓練過程中。

其中上下文長度提高了聊天用例的可用性，而 GQA 則提高了模型的推理速度。

論文的大部分內容都是關於評估和微調的。他們致力於在偏好數據上訓練獎勵模型，然後採用強化學習 (RL) 來進行優化，從而提高生成的質量。

兰伯特 還認為，這恰恰證明了他從 Anthropic 和 OpenAI 那裡聽到的一個傳言，那就是獎勵模型是人類回饋強化學習 (RLHF) 的關鍵，也是模型的關鍵。

為了得到一個好的獎勵模型，Meta 不得不花大力氣來收集偏好數據，這些數據遠遠超過了開源社區目前使用的数据量。

此外，Meta 采用了二分類的模型評價指標，並沒有使用更加複雜的回饋類型。數據收集的重點也放在了有用性和安全性上，而且對每個數據源使用不同的指導原則。而且，Meta 在收集的信息中還添加了額外的安全元數據，並且採取了迭代式的数据收集方式，每週分批收集人工注释。隨著收集到更多的偏好數據，獎勵模型也得到了改進。

如果按照市場價格來算的話，僅數據一項就可能花費了 2000 多萬美元。

關於獎勵模型的部分，主要可以用兩個重要細節來概括：

*   Meta 訓練了兩個獨立的獎勵模型，一個針對有用性進行了優化，另一個針對安全進行了優化。這兩個模型都建立在基礎語言模型上，用線性回歸層取代了普通語言模型。
*   迭代部署以及需要使用的偏好數據量。

在這個過程中還有一些值得注意的技術細節：

1.  在沒有詳細解釋為什麼需要的情況下，Meta 仍然保留了一些 Anthropic 的無害數據。
2.  為了避免獎勵模型容易出現過擬合，他們只訓練了一個 epoch。
3.  獎勵模型的平均準確率仍然只有 65-70%，但是當標註者的偏好一致性較強的時候，準確率可以達到 80-90%。

在準備微調部分的時候，Meta 隱藏了一個爆炸性的真相，那就是他們注意到獎勵模型的準確性是 Llama 2-Chat 最終性能的最重要代表之一，但是這部分並沒有詳細展開來講。

**RLHF 和微調：**

接下來，在論文的人類回饋強化學習 (RLHF) 和微調部分，Meta 採用了最佳獎勵模型，並且在這個基礎上對各種模型進行了評估。這麼做，也是為了說明獎勵模型會改善模型的最終輸出。

Meta 一共迭代訓練了 5 個 RLHF 版本，分別從 V1-V5。而且，從一開始，Meta 就指出了數據質量對這些模型的重要性。原話是 "Quality Is All You Need"。

Meta 在發現第三方數據集質量不高的情况後果斷放棄，採用了自己標註的、質量更高的数据，結果模型性能有明顯改善。而且僅僅對 27540 條高品質數據的監督微調，就可以達到接近 Anthropic 和 OpenAI 的效果。

Meta 還觀察到，不同的註釋平台和供應商提供的數據可能會導致下游模型性能的不同。這表明即使是供應商註釋的數據，後續檢查也是很重要的。

**強化學習：**

數據質量建立起來後，Meta 開始專注於強化學習組件，並且在論文中明確指出人類回饋強化學習 (RLHF) 能夠從根本上提高模型的性能上限。而其他的研究雖然也認為 RLHF 很重要，但是通常只是把它當作一種安全工具。

兰伯特 認為，要想切實有效地開展 RLHF，至少需要一個規模適中的團隊，至少需要 6-10 個人。隨著時間的推移，人數會逐漸減少，但是由於需要與外部公司保持緊密的合作和溝通，人也不會太少。

論文中使用了兩階段的 RLHF 方法：

1.  **拒絕採樣（Rejection Sampling）：**在模型輸出時採樣 K 個結果，選擇獎勵值最高的一個，在強化學習階段進行梯度更新。
2.  **近端策略優化 (PPO)：**標準的強化學習算法，進行拒絕採樣加近端策略優化的處理。

拒絕採樣的搜索範圍更廣，而 PPO 對每個獎勵模型的更新會更多，不過不同方法之間的最終差異並不明顯。

**模型評估：**

在模型的評估階段，論文從多個方面對模型進行了評估：

*   **自動基準評估：**Llama 2 在所有規模上都比任何其他的開源模型要好得多，但是閉源模型是一個都打不過。不過，Meta 在論文中並沒有詳細說明，大量的数据工作可能才是這些基本評估最重要的基礎。
*   **基本模型評估：**在某種程度上是在進行一場不公平的遊戲，這些模型可以在沒有開源驗證的情况下，很容易被提示和操縱來獲得高分。
*   **Llama 2-Chat 模型：**在單回合和多回合提示上都顯著優於其他開源模型。特別是 Llama 2-Chat 7B 模型在 60% 的提示上勝過了 MPT-7B-chat 模型，而 Llama 2-Chat 34B 模型在與容量相當的 Vicuna-33B 和 Falcon 40B 模型對戰中，總體勝率超過 75%。
*   **安全性方面：**論文中包括了偏差、紅隊、預訓練步驟等內容。這部分 兰伯特 以後會做更詳細的分析。

最後，對於 Meta 分別在 RSC 超級集群和內部生產集群上對模型進行預訓練，兰伯特 覺得這可能更多是出於計算能力的限制，而不是論文中說的是為了比較大模型訓練的適用性。

以及提到了 Llama 2 商業許可中對 7 億月活用戶以上必須向 Meta 申請許可的事。

**總結：**

通篇下來，Llama 2 我覺得最重要的一個啟示就是獎勵模型極其重要，不僅是人類回饋強化學習 (RLHF) 的關鍵，也是整個大模型效果的關鍵。其中數據的質量又是關鍵中的關鍵，在論文中被多次提到，卻又一直語焉不詳，希望能有人進行更多的研究。

好了，本期視頻內容就到這裡，後續有關 Llama 2 的一些更詳細的技術細節，我們有機會再跟大家分享。感謝大家的觀看，我們下期再見！

**整理說明:**

*   **分段與標題:** 將文稿按照主題分段，並添加了標題和子標題，使其結構更清晰。
*   **口語化調整:** 保留了大部分口語化的表達方式，但去除了一些重複的詞語和語氣詞，讓句子更精煉。
*   **重點強調:** 使用粗體字來突出重點內容，方便讀者快速抓取關鍵信息。
*   **術語解釋:** 對一些專業術語進行了簡單的解釋，例如 RLHF、GQA 等，幫助讀者理解。
*   **格式調整:** 統一了格式，例如數字的表達方式，使其更规范。
*   **錯誤修正:** 修正了部分明顯的錯別字。

希望這次整理對您有幫助！如果您需要對特定部分進行更深入的修改，請隨時告訴我。

[model=gemini-2.0-flash,0]
