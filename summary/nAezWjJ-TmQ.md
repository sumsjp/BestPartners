好的，以下是經過整理的文稿，重點整理、結構更清晰，更方便閱讀和理解：

**最佳拍檔：大語言模型幻覺研究解析 (Lilian Weng 博客重點)**

**引言**

*   本期節目聚焦 OpenAI 安全系統團隊負責人 Lilian Weng 的最新博客，深入探討大語言模型 (LLM) 幻覺 (Hallucination) 的研究成果。
*   Lilian Weng，中文名翁荔，OpenAI GPT-4 專案的核心成員，其博客以深度、細緻和前瞻性著稱，被視為 AI 研究的重要參考。
*   她提出的 Agent 公式（Agent = LLM + 記憶 + 主動規劃 + 工具使用）廣受好評。
*   本次關於 LLM 幻覺的文章同樣是深度力作，內容近兩萬字，參考文獻 24 篇，建議讀者閱讀原文。

**什麼是幻覺？**

*   **幻覺的定義：** 模型生成不真實、虛構、不一致或無意義的內容。Lilian Weng 將幻覺限定為「虛構編造，未基於所提供的上下文或世界知識」。
*   **幻覺的分類：**
    *   **上下文幻覺 (Contextual Hallucination)：** 輸出與上下文源內容不一致。
    *   **外在幻覺 (Extrinsic Hallucination)：** 輸出應基於預訓練數據集，但與事實或世界知識不符。文章重點討論外在幻覺。
*   **理想模型：** 應實事求是，並在不了解事實時明確表示不知道。

**幻覺產生原因分析**

*   **預訓練階段：**
    *   數據來自公共網路，存在資訊過時、缺失或不正確等問題。
    *   模型以最大化對數似然的方式記憶，可能錯誤記憶資訊。
*   **微調階段：**
    *   目標是提升具體能力，可能引入新知識。
    *   Gekhman 等人研究發現，微調樣本包含新知識時，LLM 學習速度變慢，且更易產生幻覺。
    *   **實驗結果：**
        *   `Unknown` 樣本擬合速度慢於 `Known` 樣本。
        *   學習大量 `Unknown` 樣本會導致幻覺。
        *   `MaybeKnown` 樣本有助於提升模型表現。
    *   **結論：** 使用監督式微調更新 LLM 知識存在風險。

**幻覺檢測方法**

1.  **檢索增強式評估 (Retrieval-Augmented Evaluation)：**
    *   **FActScore：** 將生成結果分解為原子事實，基於維基百科等知識庫驗證，衡量有知識源支撐的句子比例。
    *   **SAFE (Search-Augmented Factuality Evaluator)：** 使用語言模型作為智能體，透過多步驟迭代過程向 Google 搜索發送查詢，判斷搜索結果是否支持該事實。
        *   實驗結果表明，SAFE 方法超越人類標註者，且成本更低。
2.  **基於採樣的檢測 (Sampling-Based Detection)：**
    *   **SelfCheckGPT：** 根據黑箱 LLM 生成的多個樣本，進行事實性錯誤的一致性檢查。
    *   使用提示方法的 SelfCheckGPT 表現最佳。
3.  **對未知知識進行校準 (Calibration on Unknown Knowledge)：**
    *   **TruthfulQA 和 SelfAware 基準：** 衡量模型在面對無法回答的問題時生成誠實回應的表現。
    *   **TruthfulQA：** 以對抗方式構建，強調人類的謬誤。最佳 LLM 準確度為 58%，人類為 94%。
    *   **SelfAware：** 包含不可解答和可解答的問題，評估模型是否「知之為知之，不知為不知」。
    *   更大的模型在二元分類任務（區分問題是否可解答）上的表現更好。
    *   **輸出不確定性：** 模型應表現出正確的置信度水平。微調會讓模型的校準性能變差。
    *   **CalibratedMath 任務套件：** 檢測模型的輸出概率校準程度。
4.  **間接查詢 (Indirect Query)：**
    *   Agrawal 等人研究 LLM 生成中幻覺參考文獻的問題。
    *   直接查詢：判斷生成的參考文獻是否存在。
    *   間接查詢：要求提供參考文獻的輔助資訊（作者等）。
    *   實驗結果表明，間接查詢效果更好，模型能力越強，幻覺越少。

**減少幻覺的方法**

1.  **檢索增強生成 (Retrieval-Augmented Generation, RAG)：**
    *   檢索相關文檔，作為額外上下文進行生成。
    *   **RARR：** 透過編輯歸因，讓 LLM 能夠追溯對外部證據的歸因。包含研究階段（尋找證據）和修訂階段（校正內容）。評估指標：歸因率和留存率。
    *   **FAVA：** 檢索相關文檔，編輯模型輸出，避免幻覺錯誤。由檢索器和編輯器組成，編輯器模型需要微調。
    *   **RR：** 依賴外部知識檢索，無需額外編輯。基於分解式的 CoT 提示。
    *   **Self-RAG：** 端到端訓練語言模型，使其學會反思自身生成結果。
2.  **自我驗證和修訂：**
    *   **驗證鏈 (Chain of Verification, CoVe)：** 基於動作鏈規劃和執行驗證，包含基線回應、規劃驗證、執行驗證和最終輸出四個步驟。實驗表明指令微調和 CoT 不會減少幻覺，分解式和兩步式 CoVe 能提升性能。
    *   **RECITE：** 將複述作為中間步驟，提高模型生成的事實正確性並減少幻覺。
3.  **採樣方法 (Sampling Methods)：**
    *   **基於假設的事實核採樣算法：** 採樣的隨機性對句子後半部分的事實性影響大於開頭。
    *   **推理時間干預 (Inference-Time Intervention, ITI)：** 透過在每層激活上擬合線性探針，來區分真實輸出和虛假輸出。
4.  **針對事實性進行微調 (Factual Fine-tuning)：**
    *   **TopicPrefix：** 在文檔中的每個句子前面加上主題，更好了解事實。
    *   **句子完成損失：** 將句子的完成損失作為訓練目標，聚焦句子的後半部分。
    *   **FLAME (FActuality-aware Language Model Alignment)：** 關注事實性的 SFT+RLHF 對齊訓練。
5.  **針對歸因的微調 (Attribution-aware Fine-tuning)：**
    *   為模型輸出分配歸因。
    *   **WebGPT 和 GopherCite：** 將網路搜索與微調後的 GPT 模型組合，降低模型回答長篇問題的幻覺。核心是使用參考資料幫助人們判斷事實的正確性。都使用了監督式微調引導和 RLHF。

**結論**

*   Lilian Weng 的文章總結了大量與幻覺相關的論文研究成果。建議對此領域有興趣的讀者仔細閱讀原文。

**希望這個整理後的文稿對您有幫助！**

[model=gemini-2.0-flash,0]
