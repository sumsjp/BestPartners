好的，以下是经过整理的文稿，我主要做了以下几点：

*   **结构化内容：** 将文稿按照主要内容划分成段落，方便阅读。
*   **提炼要点：** 突出Janus Pro和JanusFlow的关键特性和创新之处。
*   **语言润色：** 微调部分语句，使表达更流畅自然。
*   **移除口语化表达：** 删除了部分口语化的词语，使文稿更书面化。
*   **加入标题层级：** 更清晰地呈现内容。

---

**DeepSeek Janus Pro 与 JanusFlow：多模态模型的新突破**

大家好，这里是最佳拍档，我是大飞。

近期，DeepSeek 的每一个举动都引发了全球科技圈的关注。在大家还在讨论 DeepSeek R1 缺少多模态能力之时，DeepSeek 团队于北京时间 2025 年 1 月 28 日凌晨发布了两款多模态模型：Janus Pro 和 JanusFlow，为大家献上了一份春节大礼包。

今天，我将为大家介绍这两个模型，看看它们到底有何过人之处，又会给多模态领域带来哪些变革。

**1. Janus Pro：统一的多模态理解与生成框架**

Janus（雅努斯）是古罗马人的门神，拥有两副面孔，一副看着过去，一副看着未来，象征着世界上矛盾的万事万物。DeepSeek 选择这个名字寓意深刻。

Janus Pro 是一款统一的多模态理解与生成的框架，是去年 10 月发布的 Janus 模型的升级版本。在图像生成基准测试中，仅用 1B 和 7B 的参数规模，就超越了 DALL-E 3 与 Stable Diffusion，再次用小模型颠覆了大模型的统治地位。同时，它和之前的 Janus 模型一样，再次选择了开源，DeepSeek 以开放的态度迎接全世界的关注。

**1.1 Janus Pro 的创新之处：回顾多模态大一统模型的发展历程**

多模态大一统模型的理念最早由谷歌提出，Gemini 就是代表之作。它运用 Transformer 架构，将文本、图像、音频等多种模态的数据进行统一处理，让模型能够同时实现对不同模态信息的理解与生成。这个创新的架构，打破了传统模型只能处理单一模态数据的局限，为多模态的融合发展开辟了新的方向。

在此之前，像 Stable Diffusion、Dall-E 这类主流的文生图模型，在处理文本和图像的时候，都需要另一套模型去理解文本，它们自身只管生成。这导致需要维护多个完整模型，不仅占用大量的存储空间和计算资源，模型之间还无法共享学习到的知识。而像 GPT-4V 这类模型，虽然能够理解图像并转译为文字，但是却无法生成图像。

**1.2 解决难题：扬弃 "编码器大一统" 的设计理念**

大一统多模态模型存在难训练、效果不够好的问题。最初，DeepSeek 尝试采用统一的 Transformer 架构来处理文生图任务，但在实际操作中遇到了严重的性能瓶颈。像智谱的 CogVLM，也尝试过用单一的 ViT 解码器来处理视觉理解和生成任务，但在高分辨率图像生成的时候，统一模型的计算复杂度会呈指数级增长，需要海量的多模态数据，训练过程还难以收敛。而且，模型在优化文本理解的时候，还会损害图像的生成能力，反之亦然。

为了解决这些难题，杨立昆和谢赛宁团队在 MetaMorph 项目中进行了大胆的创新，放弃了 "编码器大一统" 的设计理念，采用 "专门化" 方案，给模型配置两个不同的编码器。DeepSeek 的 Janus Pro 也采用了类似、但更为彻底的方案。

**1.3 Janus Pro 的架构**

正如雅努斯的两张脸，在 Janus Pro 中：

*   **第一张脸：SigLIP 编码器**，专门负责理解图像，精准提取图像的高层语义特征，关注图像的整体含义和场景关系，迅速抓住图像的要点。
*   **第二张脸：VQ tokenizer 编码器**，则专注于创作，将图像转换为离散的 token 序列，精心处理图像的细节。

这两张脸虽然各司其职，但是它们都共享同一个 "大脑"：Transformer。通过给 Transformer 加上图像理解的注意力头，DeepSeek 让两个编码器的知识实现了融合。

与 DeepSeek 从头开始训练不同，Meta 是给已有的语言模型加上视觉注意力头和视觉编码，再经过大约 20 万张图文对的微调训练，唤醒大语言模型自有的图像理解能力。而 DeepSeek 则更进一步，在图像方面使用生成和理解两个解码器，实现了图像生成和理解的大一统。

**1.4 训练策略的创新**

Janus Pro 采用三段式的训练方法，每个阶段都进行了独特的优化：

*   **第一阶段：预训练视觉编码器**，学习基础的视觉特征提取能力。DeepSeek 研究团队发现，即使将大语言模型的参数完全锁定，只训练适配器，模型也能掌握复杂的像素依赖关系。基于此，他们将第一阶段的训练时间延长到了总时长的 25-30%，使得模型的基础视觉理解能力实现了质的飞跃。

*   **第二阶段：模态对齐阶段**，传统方法会同时训练视觉和语言模型来实现模态的对齐，但 DeepSeek 团队发现 ImageNet 的数据分布与实际的应用场景差异很大，大量训练是无效的。于是，他们完全放弃在第二阶段使用 ImageNet，改为直接使用真实的文生图数据进行训练。这个改变直接让训练时间减少了 40%，生成质量提升了 35%，模型对真实场景的适应性也大幅提高。

*   **第三阶段：微调模型参数**，传统方法中，多模态数据、纯文本数据和文生图数据的配比通常是 7:3:10。DeepSeek 团队通过大量的实验，发现了一个更优的配比方案，于是将这三类数据调整为 5:1:4 的比例。在文生图的数据部分，他们还创新性地引入了合成美学数据，与真实数据形成了 1:1 的配比。这样带来的好处是，模型收敛更快，生成的结果更加稳定，输出图像的美学质量也得到了显著提升。

通过这三个阶段的创新训练方法，Janus Pro 7B 模型再次上演了算力极限，仅用了 32 个节点、256 张 A100、14 天的时间就完成了训练，1.5B 模型更是只用了一半 16 个节点和 7 天时间。

**1.5 Janus Pro 的性能表现**

在多模态理解 MMBench 测试中，Janus-Pro-7B 获得了 79.2 分的好成绩，超越了之前 Janus 的 69.4 分、TokenFlow 的 68.9 分和 MetaMorph 的 75.2 分。在图像生成评测方面，Janus-Pro-7B 在 GenEval 基准测试中达到了 0.80 分，大幅领先于 DALL-E 3 的 0.67 分和 Stable Diffusion 3 Medium 的 0.74 分。在 DPG-Bench 测试中，Janus-Pro-7B 获得了 84.19 分，超越了所有其他方法。

**1.6 Janus Pro 的实际使用效果**

从实际使用效果来看，Janus Pro 的多模态理解和图像生成能力也可圈可点。在多模态理解方面，它能准确识别杭州西湖的三潭印月景区，不仅能描述眼前的景象，还能理解更深层的文化内涵和历史意义。在文本理解上，面对一块写有 "自二十一世纪以来服务灵魂" 的黑板，它不仅能准确识别主要文字，还能注意到周边的细节信息。

在图像生成方面，虽然输出分辨率仅为 384×384，但是每一幅画面都展现出了细致的细节和准确的语义理解。

**1.7 Janus Pro 的局限性**

Janus Pro 也并非完美无缺。在多模态理解方面，它的输入分辨率被限制在了 384×384，这在一定程度上影响了在 OCR 等细粒度任务中的表现。对于文本到图像生成，低分辨率以及视觉分词器引入的重建损失，使得生成的图像虽然语义内容丰富，但是仍然缺乏精细的细节。

**1.8 Janus Pro 的重要意义**

Janus Pro 的出现对于多模态大一统模型的发展具有重要意义。它通过创新的架构设计和解耦的训练策略，首次证明了 "理解" 和 "生成" 这两个分离的任务，可以在一个统一框架下达到各自的最优状态。

Janus Pro 的架构设计仿佛是在向人脑学习，图像理解编码器类似左脑的分析功能，图像生成编码器则类似右脑的艺术创造能力，Transformer 则像胼胝体一样，将两路信息进行深度的统合，从而实现对多模态数据的统一理解。

**2. JanusFlow：多模态理解模型**

在发布 Janus Pro 的同时，DeepSeek 还发布了一个多模态理解模型 JanusFlow-1.3B。JanusFlow 的论文其实早在去年 11 月就已经发布，它将基于视觉编码器和大语言模型的理解框架，与基于校正流 Rectified Flow 的生成框架直接融合，实现了两者在单一大语言模型中的端到端训练。

DeepSeek 的研究表明，校正流可以在大语言模型框架内直接训练，无需进行复杂的架构修改。为了进一步提高统一模型的性能，他们还采用了两种关键策略：一是将理解和生成编码器解耦，二是在统一训练期间对齐它们的表征。

**2.1 JanusFlow 的训练阶段**

JanusFlow 依然采用了三阶段训练方式：

*   **第一个阶段：适应阶段**，只训练随机初始化的组件，包括线性层、生成编码器和生成解码器。

*   **第二个阶段：统一预训练阶段**，训练整个模型，但是不包括视觉编码器。训练数据包括三种类型：多模态理解、图像生成和仅文本数据。最初分配较高比例的多模态理解数据，来建立模型的理解能力，随后逐步增加图像生成数据的比例，来满足基于扩散模型的收敛需求。

*   **第三个阶段：监督微调阶段**，使用指令调优数据对预训练模型进行微调，包括对话、任务特定的交流，以及高质量的、基于文本的图像生成示例。这个阶段还需要解冻 SigLIP 编码器参数，让模型能够有效地响应用户指令，完成多模态理解和图像生成任务。

当时的实验结果表明，JanusFlow 在总体得分上达到了 0.63，超过了之前的统一框架以及多个生成特定模型，包括 SDXL 和 DALL-E 2。

**3. 总结与展望**

DeepSeek 在架构创新、训练策略和算力优化等方面，为多模态大一统模型的发展指明了新的方向。虽然目前还存在一些局限性，但是随着技术的不断发展和优化，相信多模态大一统模型也会越来越完善。期待看到 DeepSeek 带来更多的技术突破。

感谢所有观众朋友对频道和我一直以来的包容和支持，希望给大家带来更多好的节目。也祝愿大家在新的一年红红火火，万事如意！感谢大家的观看，我们下期再见！

[model=gemini-2.0-flash,0]
