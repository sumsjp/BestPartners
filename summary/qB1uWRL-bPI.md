好的，我來整理這篇文稿，使其更清晰、易讀，並突出重點。

**標題：安德烈·卡帕西 24小時 672美元 復刻GPT-2 全記錄**

**引言：**

大家好，這裡是最佳拍檔，我是大飛。OpenAI 的創始成員、前研究科學家安德烈·卡帕西（Andrej Karpathy）最近再次引起關注，他嘗試在 llm.c 中重現了 GPT-2 模型。

**背景：**

*   **GPT-2：** 15.58B 參數的完整版本，最初由 OpenAI 於 2019 年 2 月 14 日發布。
*   **2019 年 vs. 現在：** 卡帕西表示，2019 年訓練 GPT-2 需要整個團隊的規模化投入。但五年後的今天，由於計算、軟體和數據的改進，僅需八個 H100 的單節點，24 小時內即可成功復刻，總成本僅 672 美元。
*   **成本比較：** 當年訓練 GPT-2 的成本估計約 10 萬美元。

**關於安德烈·卡帕西：**

*   2017 年離開 OpenAI 加入特斯拉，擔任 AI 高級總監。
*   2023 年重返 OpenAI 組建團隊並推出 ChatGPT。
*   一年後離開 OpenAI，出於教育目的開發了 llm.c。

**什麼是 llm.c？**

*   一個簡單、純 C/CUDA 的大語言模型，約 5000 行程式碼。
*   無需 Python 解釋器或複雜的深度學習庫（如 PyTorch/JAX、Hugging Face Transformers 等）。

**GPT-2 復刻效果：**

*   卡帕西將新模型與 2019 年的 GPT-2 版本進行了比較。
*   按照當時博文介紹裡的提示詞，新模型的輸出結果相當連貫，質量大致與 GPT-2 相當。

**卡帕西的復刻過程：**

1.  **簡化：** 使用 llm.c 訓練 GPT-2 非常簡單，因為它是用 C/CUDA 編寫的，無需 minconda、Python、PyTorch 等。
2.  **硬體需求：** 一台八個 H100 GPU 的設備 24 小時即可，單張 GPU 則需 8 天。
3.  **多節點訓練：** 擁有 16 張 GPU，例如使用新的 Lambda 1 Click Clusters，前後只需 12 個小時。
4.  **完整說明：** 一分鐘內即可開始執行，腳本內容簡單且有詳細註釋，主要包括下載安裝 CUDA 和 MPI 相關的包、下載 llm.c 代碼倉庫、下載模型權重和訓練數據集，以及編譯。

**重要參數說明（參考 GPT-3 論文）：**

*   **-i -j：** 訓練和驗證分割標記文件（需要提前下載）。
*   **-o：** 指定輸出目錄（日誌和檢查點）。
*   **-v 250：** 每 250 步執行評估並記錄驗證 loss。
*   **-s 300000：** 關閉採樣（實際只在最後採樣一次）。
*   **-g 384：** 將最後需要採樣的 token 數設置為 384。
*   **-h 1：** 評估 HellaSwag 準確性。
*   **-b 16：** 微批次大小（如果内存不足，依次嘗試 8、4、2、1）。
*   **-t 1024：** 最大序列長度（與原版 GPT-2 保持一致）。
*   **-d 1048576：** 總批次大小（與 GPT-3 論文相同，代碼會確保滿足所需總批次大小並計算優化所需的梯度累積“內循環”步驟）。
    *   舉例：8 張 GPU，每張 GPU 執行 16 x 1024 個 token，則每個微步驟對應 131072 個 token，代碼計算梯度累積步數應為 8，才能滿足每步所需的 1M 的批次大小。
*   **-r 0：** 重新計算（0 為關閉，1 和 2 可節約内存但會降低速度）。
*   **-z 1：** 啟用 ZeRO-1（優化器狀態分片，適用於多張 GPU 訓練）。
*   **-c 0.1：** 權重衰減。
*   **-k "cosine"：** 余弦學習率計劃（默認值）。
*   **-l 0.0006：** 最大學習率（卡帕西將其增加了三倍，訓練速度更快）。
*   **-q 0.1：** 將學習率衰減到最大 LR 的 10%。
*   **-u 700：** 在前 700 次迭代中將學習率從 0 提升到最大值。
*   **-n 2000：** 每 2000 步保存一次模型檢查點。
*   **-x 32000：** 總共 32K 步（對應 24 個小時）。
*   **-ge 1：** 為 CublasLt 設置最近合併的 gelu 重新計算。
*   **-y 1：** 啟用“恢復”標記（訓練崩潰或掛起時可恢復）。
*   **-e "d48"：** 從頭開始初始化深度為 48 的 GPT-2 模型。

**執行結果：**

*   每個步驟約 2.75 秒，總共 3.2 萬個步驟（約 24 小時）。
*   每一步運行佔用約 100 萬個 FineWeb-EDU token，並更新模型的 15.58 億個權重。
*   總共處理 3.2 萬 x 1048576 = 33.6B 個 token。
*   隨著預測 token 能力的增強，loss 會隨之下降。
*   模型的 flops 利用率（MFU）約為 50%。

**結果評估：**

*   使用 vislog jupyter notebook 對 main.log 日誌文件進行可視化（需提前安裝 Mython 和 matplotlib）。
*   **FineWeb-EDU 驗證數據 loss：** 卡帕西模型快速超越了 OpenAI 发布的 GPT-2（loss 2.83）。
*   **HellaSwag 評估：** 大約 25K 步左右，與 GPT-2 模型的性能發生交叉。
*   **GPT-3 比較：** 綠線為同等參數規模的 GPT-3 模型（上下文長度為 2048，針對 3000 億 token 進行訓練）。

**記憶體不足的解決方案：**

*   調整微批次大小（-b 參數），嘗試 8、4、2 或 1。
*   調整重新計算設置（-r 參數）：0（最快，占用内存最大）、1（最慢，節約大量内存）、2（速度稍慢，節約内存較少）。

**llm.c 的優勢：**

*   C/CUDA 中最直接、最小且可讀的實現。
*   只需基本 CUDA 依賴即可運行。
*   約 5000 行 C/CUDA 代碼。
*   編譯和運行速度極快。
*   一次性分配所有 GPU 內存，訓練期間保持内存占用量的恒定。
*   運行效率高，MFU 略低於 50%。
*   支持多節點訓練（卡帕西見過最多支持到約 500 張 GPU）。

**llm.c vs. PyTorch：**

*   **内存占用：** PyTorch 約 80GB，llm.c 僅佔用 57GB（節省約 29%）。
*   **速度：** PyTorch 每次迭代約 3386 毫秒，llm.c 迭代約 2750 毫秒（速度快約 19%）。

**總結：**

卡帕西的復刻證明了 AI 領域的快速發展，四五年時間將訓練成本從十萬美元降至幾百美元。

**未來趨勢：**

*   AI 初創公司 Anthropic 的 CEO 達里奧·阿莫代伊（Dario Amodei）表示，GPT-4o 這樣的模型訓練成本約為 1 億美元，未來 AI 大模型的訓練成本可能高達 10 億美元，甚至 100-1000 億美元。
*   紅杉資本合夥人兼首席運營官大衛·卡恩（David Cahn）提出了 AI 行業的 6000 億美元問題，即 AI 公司的投入和產出之間存在巨大差額。

**結語：**

AI 行業仍然是一個極度燒錢的行業，從算力到人才再到數據都需要大量投入。AI 是否正處於泡沫破裂的邊緣，未來有時間會再做一期視頻來講解。感謝大家觀看本期視頻，下期再見！

---

**重點摘要：**

*   **卡帕西使用 llm.c 在 24 小時內以 672 美元的成本復刻了 GPT-2，大幅降低了訓練成本。**
*   **llm.c 是一個簡單、高效的 C/CUDA 大語言模型，比 PyTorch 更節省資源且速度更快。**
*   **儘管訓練成本有所降低，但 AI 大模型的整體訓練成本仍然很高，未來可能會達到數十億甚至數千億美元。**

希望這個整理後的版本對您有幫助！

[model=gemini-2.0-flash,0]
