好的，我幫你整理這篇文稿，使其更清晰、更易於理解：

**標題：OpenAI O3 模型解析：通往 AGI 的新曙光？**

**引言：**

大家好，這裡是最佳拍檔，我是大飛。今天我們來聊聊 OpenAI 連續 12 天聖誕發布會中最值得關注的產品——可能是迄今為止最強的推理模型，O1 的升級版，O3。之前的發布會內容平淡，但 O3 讓人眼前一亮，甚至讓人們看到了接近 AGI（通用人工智能）的曙光。

**O3 模型背景：**

*   今年 9 月，OpenAI 發布了 O1 模型，開啟了推理模型的大門。
*   之後，國內外許多大模型企業相繼推出推理模型。
*   為了避免與英國電信商 O2 重名，OpenAI 直接將模型命名為 O3。
*   O3 模型與 O1 類似，採用思維鏈進行思考，逐步解釋邏輯推理過程，總結出最準確的答案。
*   O3 模型分為完整版和 Mini 版。
*   新增功能：可以將模型的推理時間設置為低、中、高三種模式，推理時間越長，效果越好。
*   Mini 版更加精簡，針對特定任務進行微調，預計 1 月底推出。
*   完整版 O3 也將在之後不久推出。

**O3 模型性能：**

*   **ARC-AGI 基准测试突破：** O3 最引人矚目的是其在 ARC-AGI 基准測試下的性能表現。
    *   ARC-AGI 是一項評估 AI 系統推理首次遇到的、極其困難的數學和邏輯問題能力的基准測試。
    *   由 Keras 之父弗朗索瓦·肖莱 (François Chollet) 於 2019 年提出。
    *   該基准測試推出 5 年來一直未被攻克。
    *   O3 達到了優良的水平，成為首個突破 ARC-AGI 基准的 AI 模型。
    *   在高推理能力設置下，O3 取得了 87.5% 的分數。
    *   在低推理能力設置下，分數也達到了 O1 的 3 倍。
    *   相比之下，GPT-3 的評測結果為 0%，GPT-4 為 2%，GPT-4o 為 5%，O1 Pro 僅為 50% 左右。
*   **接近人類水平：** O3 能適應以前從未遇到過的任務，接近人類水平。

**弗朗索瓦·肖萊 (François Chollet) 的分析：**

*   弗朗索瓦·肖莱發布了 O3 的完整測試報告。
*   O3 在兩個 ARC-AGI 數據集中進行了測試，並在兩個具有可變樣本量的計算級別上進行了測試：高效率的 6 個樣本和低效率但計算量為 172 倍的 1024 個樣本。
*   75.7% 的高效率分數在 ARC-AGI-Pub 的 10000 美元預算規則範圍內。
*   87.5% 的低效率分數成本相當昂貴。
*   測試結果表明，新任務的性能確實會隨著計算量的增加而提高。
*   **非 AGI：** 肖莱認為 O3 還不是 AGI，在一些非常簡單的任務上仍然會失敗，表明其與人類智能存在根本差異。
*   **工作原理推測：** O3 模型的核心機制似乎是在 token 空間內進行自然語言程序的搜索和執行。模型會在可能的思維鏈空間中搜索，這些思維鏈描述了解決任務所需要的步驟，可能與 AlphaZero 風格的蒙特卡洛樹搜索（Monte-Carlo tree search）相似。
*   **DeepMind 的研究：** DeepMind 可能對此概念已經研究了很長時間。

**成本考量：**

*   目前 O3 模型的使用還不是很經濟。
*   每項任務約 5 美元（約 36 人民幣）。
*   在低效率推理模式下，完成每個任務需要花費 17-20 美元（約 124-145 人民幣）。

**未來展望：**

*   OpenAI 明年將與 ARC-AGI 背後的基金會合作，構建下一代的基准測試。
*   即將推出的 ARC-AGI-2 基准測試仍然將對 O3 構成重大挑戰，即使在高計算量下，O3 的得分也可能會降低到 30% 以下。
*   人類在不經過任何培訓的情況下仍然能夠得分超過 95%。
*   **其他基准測試表現：** O3 在其他基准測試中的表現也遠遠超過了競品。
    *   SWE-Bench Verified 基准測試（真實世界軟體任務）：O3 準確率約 71.7%，比 O1 高出 20% 以上。
    *   Codeforces（編程競賽）：O1 分數為 1891，O3 在低推理設置下超過 O1，在高推理設置下可達 2727（約等於人類智商 157）。
    *   AIME 2024（數學基准測試）：O3 準確率達 96.7%，O1 為 83.3%，GPT-4o 僅 13.4%。
    *   GPQA Diamond（博士級科學問題）：O3 準確率高達 87.7%，高於 O1 的 78%。
    *   EpochAI Frontier Math（全球數學家共同推出的數學基准）：O3 創下新紀錄，分數達 25.2%。

**O3 Mini 版本：**

*   更經濟高效的 O3 版本，專注於提升推理速度、降低推理成本的同時兼顧模型性能。
*   採用「自適應思考時間」（adaptive thinking time）機制，根據任務難度自動調整推理深度。
*   支持低、中、高三種推理強度選項。
*   在 Codeforces 上的性能具有顯著的成本效益。
*   在 AIME 2024 數學競賽測試中，性能超越 O1。
*   在延遲方面，O3-mini (low) 大幅降低了延遲，不到 1 秒，足以媲美 GPT-4 的即時響應。
*   提供全套 API 功能，包括函數調用、結構化輸出、開發者消息等等。
*   在 GPQA 數據集測試中展現出穩定的性能。
*   OpenAI 研究科學家任泓宇演示了使用 O3-mini 實現 Python 代碼生成和執行的示例，只用了 30 多秒寫出了一個自己的 ChatGPT UI。

**安全性：審議對齊（Deliberative Alignment）**

*   OpenAI 開始向安全研究人員開放 O3 的訪問權限，申請截止日期為 1 月 10 日。
*   OpenAI 公布了 O3 使用的、新的對齊策略的更多技術細節，稱為審議對齊。
*   **問題：** 傳統大語言模型基於監督微調（SFT）和基於人類反饋的強化學習（RLHF）進行安全訓練，但仍存在安全缺陷，因為模型需要立即響應用戶請求，導致沒有足夠時間推理複雜的安全場景，並且必須從大量標註樣本中推斷所需的行為。
*   **解決方案：** 審議對齊結合基於過程和結果的監督，讓大模型在產生答案之前明確地通過安全規範進行複雜推理。
*   **步驟：**
    1.  訓練一個只針對 O 系列模型的模型，不使用任何與安全相關的數據集。
    2.  構建含有（提示、補全）（prompt completion）對的數據集，補全中引用思維鏈，並在系統提示詞中為每個對話插入相關的安全規範文本。
    3.  從數據中刪除系統提示。
    4.  對這個數據集執行增量監督微調，讓模型學習到安全規範的內容，以及如何通過推理來生成一致的響應。
    5.  使用強化學習來訓練模型更有效地使用思維鏈，同時引入獎勵模型，讓它可以使用安全策略來提供額外的獎勵信號。
    6.  獎勵策略分為兩個階段：通過對思維鏈引用規範的示例進行監督微調，來教會模型在思維鏈中直接推理安全規範；使用高計算強化學習來訓練模型更有效地進行思考，並引入使用指定安全規範的裁判大模型來提供獎勵信號。
*   **優勢：** OpenAI 的訓練程序不需要人工標註，只需要依賴模型生成的數據，就能夠實現精確的規範遵守性。
*   **結果：** 在一系列內部和外部安全基准中，O1 模型通過了一些較難的安全評估，並且在拒絕不足等方面實現了帕累托改進。
*   **Anthropic 的播客訪談：** Anthropic 團隊也做了一期關於審議對齊的播客訪談。

**總結：**

以上是對 O3 和 O3-mini 的基本介紹。由於目前還未發布，具體使用情況還需等明年正式公開後再評測。不過，在 O3 發布前不久，OpenAI GPT 系列論文的主要作者亞歷克·拉德福 (Alec Radford) 宣布離職，將會轉向獨立研究。外媒也披露，GPT-5 的開發受阻，已經難產了 10 個月。O3 是否會跳票還很難說。至少目前來看，通往 AGI 的全球競賽還在加速進行。

**結尾：**

感謝大家觀看本期視頻，我們下期再見。

**整理說明：**

*   **更清晰的結構：** 將文稿分成幾個主要部分，每個部分都有明確的標題和副標題，方便快速瀏覽和查找資訊。
*   **更簡潔的語言：** 刪除了一些口語化的表達和重複的信息，使文稿更加精煉。
*   **更突出的重點：** 使用粗體、列表等方式強調重要的信息和結論。
*   **更易於理解的解釋：** 對一些專業術語和概念進行了簡單的解釋，方便非專業人士理解。
*   **保留關鍵資訊：** 確保所有重要的技術細節、測試結果和觀點都得到保留。

希望這樣的整理對您有所幫助！

[model=gemini-2.0-flash,0]
