好的，作為您的專業文件整理員，我已將這篇信息量極高的文稿進行了系統性的梳理和歸納，旨在讓您能更清晰、高效地掌握其核心內容。

---

## **萊克斯·弗里德曼 AI 對話：2025 回顧與 2026 展望**

**時間：** 2024年2月1日 (本文稿內容回顧2025年AI發展並預判2026年趨勢)
**主持人：** 知名科技播客博主 萊克斯·弗里德曼 (Lex Fridman)
**嘉賓：**
*   **塞巴斯蒂安·拉施卡 (Sebastian Raschka)：** 知名機器學習研究員與教育家
*   **內森·蘭伯特 (Nathan Lambert)：** 艾倫人工智能研究所後訓練負責人、RLHF領域權威專家

**核心摘要：** 本次訪談深度剖析了2025年AI行業的關鍵技術突破與競爭格局，並對2026年的發展趨勢做出了深刻預判，涵蓋了全球AI競爭、技術範式變革、編程領域重塑、AGI未來等多個維度。

---

### **I. 全球AI競爭格局：開源與閉源的博弈**

#### **A. 開源模型領域：中國崛起與美國反思**

1.  **2025關鍵轉折點：DeepSeek時刻**
    *   2 **時間：** 2025年1月。
    *   **事件：** 中國公司DeepSeek發布了開放權重的模型DeepSeek R1。
    *   **影響：** 以更低的算力成本達到接近SOTA的性能，震驚業界，被視為中美AI競爭的分水嶺。

2.  **中國開源公司的強勢表現**
    *   **代表：** DeepSeek、阿里巴巴通義千問、MiniMax、Kimi等。
    *   **優勢：** 發布大量高性能開源模型，憑藉出色性能和友好許可協議（大多為無限制的Apache 2.0），贏得全球開源社區廣泛青睞。
    *   **規模：** 數量多，規模更大，大多採用混合專家模型架構，峰值性能更高。

3.  **Meta Llama的戰略失誤與困境**
    *   **刷榜陷阱：** 拉施卡指出，Meta在2025年試圖通過巨大Llama 4在基準測試中擊敗ChatGPT。
    *   **策略偏差：** 忽略AI領域對輕量級、可用模型的真正需求。
    *   **後果：** Llama留下的生態空白正被中國開源模型迅速填補。蘭伯特認為其開源生態領先地位已不復存在，對未來是否會有Llama 5開源持懷疑態度。

4.  **中國堅持開源權重策略的考量**
    *   **務實需求：** 由於安全擔憂，許多美國頂尖科技公司不購買中國公司的API訂閱服務。
    *   **市場切入：** 開放權重模型成為中國企業參與全球AI市場、打入美國AI支出市場的重要途徑。
    *   **友好許可：** 與Llama、Gemma等帶用戶數量上限的許可協議形成對比。

5.  **競爭的積極循環與美國的回應**
    *   **良性循環：** 拉施卡認為，中國頂級開源模型的廣泛使用將鼓勵美國公司發布更好的開源模型。
    *   **美國行動：** 蘭伯特正在推動一項名為「亞當計劃」(Adam Project) 的美國版DeepSeek項目，旨在構建和託管高品質的美國開源AI模型及基礎設施。
    *   **預測：** 2026年全球開源競爭將更加激烈。

#### **B. 閉源模型領域：主流玩家的表現與2026展望**

1.  **Anthropic：熱門新星與組織有序**
    *   **模型：** Claude Opus 4.5 被視為頂流模型，尤其在編程領域表現出色，熱度一度蓋過Google Gemini 3。
    *   **優勢：** 組織有序，是所有主流實驗室中最不混亂的，為技術迭代提供有力保障。

2.  **Google：低調的強大與全棧垂直整合**
    *   **模型：** Gemini 3 性能極其強勁，儘管營銷聲量不如對手。
    *   **核心優勢：** 擁有從TPU芯片、雲計算到模型研發和應用落地的完整技術棧，不依賴Nvidia GPU，利潤率極高。
    *   **特點：** 在“大海撈針”等長上下文任務中表現突出，處理特定信息能力頂尖。

3.  **OpenAI：混亂中的強大與先發優勢**
    *   **運營：** 儘管內部常傳混亂，但交付能力依然極強。
    *   **技術：** GPT-5系列通過推理時計算優化，節省大量成本，並定義了新的技術範式。
    *   **市場：** 憑藉ChatGPT的先發優勢和飛輪效應，積累龐大用戶群，記憶功能、多場景適配能力使其短期內難以被超越。

4.  **Meta Llama系列：面臨困境**
    *   **內部問題：** 存在內部政治和激勵機制問題，研究人員想構建實用模型，管理層更傾向追求基準測試排名。
    *   **地位動搖：** 其開源生態的領先地位已經不復存在，嘉賓普遍對是否會有Llama 5開源持懷疑態度。

5.  **其他特色模型**
    *   **xAI Grok-4 Super Heavy：** 在硬核調試任務中表現出色，可解決其他模型無法處理的編程問題。
    *   **OpenAI GPT-OSS：** OpenAI發布的首個開源模型，在工具調用方面實現突破，是首個在訓練時就充分考慮工具調用的公開權重模型。

6.  **2026年閉源競爭格局預測**
    *   蘭伯特預測：Gemini將繼續追趕ChatGPT，Anthropic在軟件和企業端延續成功。
    *   Google Cloud憑藉豐富產品線和TPU芯片優勢，保持良好發展勢頭。
    *   OpenAI雖面臨競爭，但其在全新研究理念落地方面的核心能力依然難以被超越。

---

### **II. AI技術突破與範式變革：後訓練時代來臨**

#### **A. 預訓練的瓶頸與後訓練的崛起**

1.  **預訓練的挑戰：**
    *   **成本高昂：** 變得極其昂貴，DeepSeek預訓練費用約500萬美元，實際成本更高。
    *   **邊際效益遞減：** 隨著模型參數量達到萬億級別，繼續擴大規模帶來的性能提升越來越有限，“唾手可得的果實已被大量採摘”。

2.  **後訓練時代來臨：**
    *   **轉變：** 2025年的最大技術突破標誌著AI行業正式進入後訓練時代。
    *   **重心：** 模型能力的提升重點已從預訓練轉向後訓練階段的創新。

#### **B. 核心技術：帶可驗證獎勵的強化學習 (RLVR)**

1.  **RLVR原理**
    *   **對比RLHF：** 傳統基於人類反饋的強化學習(RLHF)更多是調整模型的語氣和風格，屬於微調偏好，容易觸及性能天花板。
    *   **核心：** RLVR讓模型在數學、代碼等有客觀答案的領域進行大規模試錯。
    *   **機制：** 通過從生成到評分的迭代循環，讓模型像人類學生一樣在數萬次練習中自我修正，從而解鎖預訓練中已有的知識。

2.  **RLVR的強大之處**
    *   **示例：** 拉施卡在Math500數據集上訓練通義千問3基座模型，僅用50步，幾分鐘內準確率從15%飆升至50%。這並非模型學會數學，而是RLVR解鎖了預訓練中已有的知識。
    *   **效果：** 能讓模型展示完整的推理步驟，甚至出現“頓悟時刻”，自我意識到錯誤並修正，提高準確性並增強用戶信任。

3.  **預訓練與後訓練的硬件需求差異**
    *   **預訓練：** 算力受限，核心指標是每秒浮點運算次數 (FLOPs)。
    *   **RLVR (後訓練)：** 更像是內存密集型任務，更看重GPU的運行時間而非單純算力堆疊。強化學習的運行天數正快速接近預訓練。

4.  **後訓練的完整體系**
    *   **中期訓練：** 介於預訓練和後訓練之間，本質是專業化的預訓練，例如專門訓練模型的長上下文能力，彌補特定領域數據不足。
    *   **RLVR：** 通過試錯深化模型能力。
    *   **RLHF：** 依然是點睛之筆，主要用於調整模型的語氣、風格和格式，使其更符合人類交互習慣。

---

### **III. AI對編程領域的重塑：從編寫者到設計者**

#### **A. Vibe Coding：新編程範式**

1.  **概念：** 開發者不再糾結具體代碼細節，而是通過自然語言描述需求，讓AI快速生成並修改代碼。
2.  **角色轉變：** 人類角色從代碼編寫者轉變為系統設計師和審查者。
3.  **嘉賓經驗分享：**
    *   **蘭伯特：** 用Claude Code構建數據分析工具，幾小時完成原本幾天的工作，只需簡單核查。
    *   **拉施卡：** 更傾向使用VSCode的Codex插件，認為其提供代碼幫助但未完全取代人類工作。
    *   **弗里德曼：** Cursor和Claude Code重度用戶，意外培養了“用英語編程”的能力，更關注代碼邏輯和結構。

#### **B. 軟件開發的工業化與自動化**

1.  **工業化趨勢：** AI正讓軟件開發變得高度工業化，工具門檻急劇降低。
2.  **未來展望：** 蘭伯特預測，未來不懂底層代碼的人，只要有清晰的系統設計思維，也能利用AI工具構建複雜軟件系統。
3.  **數據印證：** 弗里德曼分享調查數據，10年以上經驗的專業開發者中，AI生成代碼佔比更高；約80%的開發者表示AI讓他們工作更有趣。
4.  **挑戰：** 完全自動化的“超級智能編程”因數據分佈參差不齊，難以在短期內完美實現。

#### **C. 挑戰與人類核心能力**

1.  **新挑戰：**
    *   **維護壓力：** 拉施卡提到，其開源倉庫收到大量由大模型生成的Pull Request，維護者需花大量時間審核。
    *   **隱藏漏洞：** AI生成代碼可能存在潛在漏洞。
    *   **學習障礙：** 初學者如果過度依賴AI，可能失去親手構建代碼的機會，難以建立深刻的思維框架。

2.  **人類核心能力：**
    *   嘉賓們普遍認為，AI是編程的強大助手，但不是替代品。
    *   人類開發者需要保留核心的系統設計能力、調試能力和邏輯思維能力，在AI輔助下提升效率，而非完全依賴AI。

---

### **IV. Scaling Laws的演進：多維度擴展策略**

#### **A. 瓶頸與多維度擴展**

1.  **AI發展並未遇瓶頸：** 嘉賓們明確否定AI發展遇到瓶頸，Scaling Laws並未失效。
2.  **擴展維度豐富化：** 傳統的Scaling Laws主要關注模型參數規模和數據集規模。隨著預訓練規模擴展面臨成本高、邊際效益遞減，行業開始探索多維度的擴展策略。

#### **B. 三個維度**

1.  **傳統Scaling Laws：** 繼續堆疊模型參數和數據集規模，這依然是技術發展的基石。
2.  **強化學習規模：** 模型可以進行多長時間的試錯學習，這正是RLVR技術的核心，通過延長訓練時間來深化模型能力。
3.  **推理側算力擴展：** 讓模型在回答前思考更久，生成更多的思維鏈 (CoT) Token。
    *   **示例：** OpenAI的o1模型通過在推理時生成大量隱藏的思維鏈，顯著提升了複雜任務的解決能力。

#### **C. 成本效益權衡與未來**

1.  **理想與現實：** 在無限算力的理想世界裡，三維度旋鈕會全部拉滿；但在現實中，這是一場關於性價比的權衡遊戲。
2.  **OpenAI的策略：** 選擇了推理側擴展，GPT-5系列通過高端線路功能的路由機制，讓大多數用戶在通用場景不消耗昂貴GPU資源，節省成本；而對於奧數競賽、複雜編程等需要巔峰性能的特定任務，則可調用更多算力、生成更長思維鏈。
3.  **未來趨勢：** 嘉賓們普遍認為，Scaling Laws的多維度擴展將在未來幾年持續主導AI行業發展。隨著吉瓦級算力集群的落地，如何在三個維度之間找到最佳平衡點，將成為各大AI公司的核心競爭力。

---

### **V. AGI的未來：多智能體與專業化模型**

#### **A. 單一通用模型幻想破滅**

1.  **蘭伯特觀點：** 單一通用模型的夢想已經破滅，未來的AI生態將是分工明確、屬於多Agent與專業化模型的系統。
2.  **原因：**
    *   **任務需求差異：** 不同領域（法律、醫療、編程、數學）有獨特知識體系和任務特點，單一模型難以在所有領域達到頂尖。
    *   **數據分佈不均：** 某些領域訓練數據匱乏，模型難以掌握相關技能。
    *   **性價比考量：** 針對特定任務優化的專業化模型成本更低、性能更高；通用模型為覆蓋所有任務需巨大算力和數據成本。
3.  **未來格局：** 不再依賴單一ChatGPT處理所有事務，而是會有專門負責法律、醫療診斷、編程的垂直模型。人們將根據不同任務調用不同Agent，數據中心裡將是許多專業AGI相互交流、管理和執行任務。

#### **B. 人機協作的長期存在**

1.  **AI研究自動化時間線延後：** 《AI 2027報告》曾預測2027年出現超越人類水平的編程者，進而實現AI研究自動化。嘉賓們認為這個時間線可能需要推遲。
2.  **AI能力的“鋸齒狀”：** 蘭伯特認為AI能力在某些方面卓越，而在另一些方面表現糟糕，即使在編程領域也有強項與短板。
3.  **長期模式：** 人機協作的模式將長期存在。最頂尖的AI研究人員和開發者將是那些能夠充分發揮AI優勢、彌補AI短板的人。
4.  **長期樂觀：**
    *   拉施卡：大語言模型最終會像計算器解決計算問題一樣，解決編程問題，成為通用工具。
    *   蘭伯特：預測到2026年底，被自動化的軟件數量將非常高，軟件開發將更多地轉向系統設計及對結果目標的追求，任何人都能憑指尖靈感創造軟件。

---

### **VI. AI開發與研究：給初學者的實用建議**

#### **A. 拉施卡的核心建議：從頭開始構建一個簡單模型**

1.  **目的：** 不是取代現有模型，而是為了確切了解大模型的輸入輸出、預訓練的運作機制以及注意力機制、Transformer塊等核心組件的工作原理。
2.  **方法：** 在自己的電腦上從零構建模型，深入理解預訓練、監督微調等關鍵流程（這種底層理解無法從使用現有工具中獲得）。
3.  **學習陷阱：** 不要一開始就去閱讀Hugging Face Transformers庫源碼，其為兼容成百上千種模型和生產環境而極其複雜，不適合初學者學習原理。
4.  **建議路徑：** 採用逆向工程，先查看模型倉庫配置文件，了解模型層數、注意力機制類型等參數，然後從基礎模型開始逐步添加這些組件，加載預訓練權重，驗證自己的架構。
5.  **“掙扎的價值”：** 他曾花一天時間處理Llama 3的位置嵌入擴展，正是在這種掙扎中才真正理解相關技術。

#### **B. 蘭伯特的進階與低算力建議**

1.  **深入研究細分領域：** AI領域發展太快，頂尖人才往往轉向解決更大的難題，為初學者留下了許多細分領域的機會。
    *   **示例：** 性格訓練（通過調整數據讓模型變得幽默、諷刺或嚴肅）是一個有趣且有價值的方向，曾有牛津大學博士生在此方向深入研究並發表論文。
2.  **專注評估工作 (適用於沒有充足算力的初學者)：**
    *   方法：使用閉源模型或開放權重模型的補全結果，研究模型的失敗模式和能力邊界。
    *   認可：如果能發現一些頂尖模型難以處理的問題，同樣能在行業中獲得認可。
3.  **“掙扎的價值”：** 他在推導DPO論文的數學步驟時雖非常痛苦，但這種掙扎讓他對技術有了更深刻的理解。

#### **C. AI輔助學習建議：分階段進行**

1.  **初期：** 先進行一輪離線、專注模式的學習，努力克制立即查閱資料的衝動，讓問題沉澱一段時間。
2.  **後期：** 之後再使用AI作為復習工具，補充背景知識、解決遺留問題，而不是一開始就依賴AI獲取答案。

---

### **VII. AI行業趨勢：職場文化、商業整合與新技術方向**

#### **A. 職場文化：996與高薪酬**

1.  **996普遍化：** 在AI行業，早上9點工作到晚上9點，每周六天，總共72小時的工作模式正變得越來越普遍，尤其在硅谷AI公司中突出。
2.  **原因：** 激烈的行業競爭和互相超越的技術氛圍，模型更新迭代速度極快，一旦落後可能被市場淘汰。
3.  **代價：** 這種高強度工作也帶來高昂的人力資本代價，許多員工會感到職業倦怠。
4.  **高薪酬誘惑：** AI行業薪酬待遇非常誘人。蘭伯特透露，OpenAI員工平均薪酬僅股票部分每年就超過100萬美元。
5.  **人才流動：** 許多頂尖大學的博士生會選擇放棄學位，加入OpenAI、Anthropic等公司。
6.  **學術界與工業界：** 學術界提供更自由研究環境和個人認可，工業界提供更充足資源和更高薪酬，選擇取決個人規劃。

#### **B. 商業整合：收購與授權**

1.  **整合浪潮：** 2026年AI行業將迎來整合浪潮。
2.  **高溢價：** 目前AI初創公司溢價非常高，成立一年估值可達100億美元，八個月的Manus就以20億美元被收購。
3.  **收購趨勢：** 未來大型科技公司可能會收購更多有技術優勢的AI初創公司（如傳言Apple可能收購Perplexity）。
4.  **授權交易：** 也是重要整合方式，例如Grok與Nvidia之間200億美元的非排他性授權協議（此方式可能無法讓普通員工受益，與完整收購有本質區別）。
5.  **IPO展望：**
    *   **美國頂尖AI公司：** 嘉賓普遍認為Anthropic、OpenAI等短期內IPO可能性不大，只要融資容易，這些公司會避免公開市場帶來的業績壓力。
    *   **中國公司：** MiniMax和零一萬物等已提交IPO申請，反映中美AI公司不同的資本路徑。

#### **C. 新技術方向與硬件發展**

1.  **文本擴散模型 (Text Diffusion Models)**
    *   **來源：** 源於圖像生成領域的擴散模型。
    *   **原理：** 從一段隨機文本開始，通過多次迭代不斷填補和完善缺失部分，實現文本生成。
    *   **優勢：** 與傳統自回歸Transformer不同，可同時處理多個Token，實現並行化生成，效率更高。
    *   **權衡：** 要達到與自回歸模型相同質量，需增加去噪步數，最終算力消耗可能相差無幾；在推理、工具調用等非並行任務中表現不佳。
    *   **定位：** 不會取代自回歸大語言模型，但可能用於快速、廉價且大規模的文本生成任務。

2.  **工具調用 (Tool Calling)**
    *   **現狀：** 目前主要集中在閉源大模型上，但未來會有更多開源工具湧現。
    *   **核心價值：** 將某些依賴記憶的任務外包給工具（如調用計算器解決數學問題、調用搜索引擎獲取實時信息），能有效減少模型的幻覺問題。
    *   **挑戰：** 面臨信任和安全挑戰，賦予大模型訪問郵件、硬盤的權限可能帶來風險。

3.  **硬件發展**
    *   **Nvidia主導：** 依然佔據主導地位，核心護城河不僅是GPU硬件，更是演進二十年的Cuda生態系統，難以被複製。
    *   **挑戰者：** Google、Amazon、微軟等公司都在製造自己的芯片。
    *   **未來趨勢：** 訓練和推理算力可能會發生分離。Nvidia也推出了針對特定場景的新芯片（如專為推理預填充階段設計的低內存芯片）。
    *   **黃仁勳：** 作為Nvidia核心領導者，其對公司的高度掌控和對AI趨勢的敏銳判斷，將繼續推動Nvidia在AI硬件領域的領先地位。

#### **D. 人類文明與AI的長遠關係**

1.  **長期展望：** 嘉賓們保持樂觀態度。100年後，AI可能會被統稱為計算機，其核心依然是計算能力的提升，而深度學習作為術語會被銘記。
2.  **生活影響：** 蘭伯特預測，未來人類會隨身攜帶某種物理計算設備，用於保障隱私和實現與互聯網的接口。AI帶來的最大價值將是提升人類的自主性和社區感，讓人們有更多時間與親近的人相處，賦予生活更多意義。
3.  **倫理與安全挑戰：**
    *   **內容泛濫：** AI生成的垃圾內容可能會泛濫，需要建立有效內容驗證機制。
    *   **就業流失：** AI帶來的就業崗位流失可能會引發社會問題，需要建立完善的社會支持系統。
    *   **安全重中之重：** 尤其在機器人、自動駕駛等具身智能領域，必須確保AI系統的可靠性，避免造成人身傷害。

---

### **VIII. 總結**

這場長達數小時的硬核訪談為我們清晰地勾勒出了2026年AI行業的全景圖。從中美開源競爭的格局反轉，到主流AI實驗室的各有優劣；從後訓練與RLVR的技術革命，到AI編程帶來的行業變革；從多維度擴展的規模定律，到多智能體主導的AGI終局，我們看到了充滿活力、快速迭代的AI行業。

AI的發展不是一蹴而就的，它需要技術的持續創新、資本的理性投入、人才的不斷湧現，更需要行業對倫理和安全的堅守。作為普通人，我們不應畏懼AI的發展，而是應該主動擁抱這個變革，無論是學習AI知識，還是使用AI工具，都能讓我們在這個快速變化的時代中佔據主動。

---
希望這份整理能幫助您更高效地吸收和利用這份寶貴的資訊！

[model=gemini-2.5-flash,0]
