好的，這是我整理過後的文稿。我將其重新組織，並將重點放在使資訊更清晰和易於理解。我還嘗試將其精簡，刪除重複的內容，並提供更流暢的敘述。

**整理後文稿:**

**引言**

大家好，這裡是最佳拍檔，我是大飛。最近，圖靈獎得主Richard Sutton和Google RL專家David Silver合作發表文章《歡迎來到經驗時代》，引起廣泛關注。文章指出，人類數據已接近極限，AI Agent若要突破瓶頸，必須像人類和動物一樣，透過與環境的持續互動來生成經驗流，並藉由強化學習實現自主提升。這代表AI Agent將迎來經驗時代，是一項重大的範式轉變。

**挑戰與問題**

然而，在許多環境中，利用經驗數據透過強化學習訓練Agent仍然面臨挑戰。一方面，這些環境往往缺乏可驗證或密集的獎勵信號，尤其是在開放式場景，例如多數網頁環境通常不會回傳明確的任務回饋。另一方面，Agent可能需要在長時間跨度內進行低效的探索與泛化，例如多輪工具使用或複雜的互動流程。

目前多數語言Agent會採用監督微調（SFT）從專家示範中學習，以避免過度依賴獎勵信號。雖然這種方法訓練效率高，但缺乏與環境的互動，無法從失敗中學習或主動探索，且過度依賴高品質的專家數據，成本高昂且泛化性有限。

因此，關鍵問題在於：如何讓Agent在沒有外部獎勵的情況下，從自身經驗中學習成長？

**早期經驗（Early Experience）範式**

為了解決此問題，Meta超級智能實驗室MSL、FAIR、俄亥俄州立大學的研究提出名為「早期經驗」的中間路線。其核心概念為讓Agent在訓練時，同時從人類專家數據和自身試錯中學習。

具體來說，Agent在環境中提出替代行動，收集這些行動帶來的未來狀態（Future States），然後將這些未來狀態直接變成監督信號。Agent無需等待外部獎勵，也不用完全依賴專家，自己的行動後果就是最好的老師。

**理論基礎：馬可夫決策過程（MDP）**

研究團隊將語言Agent的決策問題形式化為馬可夫決策過程（MDP），這是RL和Agent研究中最常用的數學框架。一個MDP可用一個tuple M=(S, A, T, R, γ, ρ0) 來定義：

*   **S (狀態空間):** Agent能感知到的環境資訊，如網頁內容、工具輸出、環境文本描述。
*   **A (行動空間):** Agent可執行的離散行動，如點擊網頁按鈕、呼叫搜尋工具、生成文本回覆。
*   **T (轉移函數):** 描述在狀態s下執行行動a轉移到下一個狀態s'的機率。
*   **R (獎勵函數):** 很多場景下R是未知或不可驗證的，這也是早期經驗要解決的問題。
*   **γ (折扣因子):** 用來權衡當前獎勵和未來獎勵的重要性。
*   **ρ₀ (初始狀態分布):** Agent開始任務時環境可能處於的狀態集合。

Agent的核心是策略Policy，用π_θ表示，θ是模型參數，它的作用是把狀態s映射到行動a的機率分布。

**模仿學習的缺陷**

在無獎勵場景下，傳統模仿學習的訓練目標是最小化模仿損失，也就是讓Agent在狀態s_i下盡可能選擇專家行動a_i。然而，這種方法存在兩個問題：

1.  **分布偏移（Distribution Shift）：** Agent在實際部署時，策略π_θ與專家策略肯定會有偏差，導致Agent在遇到訓練數據沒有的狀態時，錯誤會不斷累積。
2.  **缺乏行動後果的認知：** Agent只見過專家行動和專家未來狀態的配對，從未見過自己行動和自己未來狀態的結果，因此遇到錯誤時不知道如何修正，也無法理解專家為何選擇該行動。

**早期經驗範式的解決方案**

早期經驗範式旨在解決這兩個問題。

*   **構建D_rollout數據集:** 讓Agent獲得從行動到後果的經驗。

    1.  從專家數據集D_expert裡取每個狀態s_i。
    2.  讓Agent的初始策略π_θ生成一個候選行動集A_i，包含K個替代行動（K是超參數）和專家行動a_i。
    3.  執行這些行動來收集未來狀態。執行專家行動a_i會得到專家下一步狀態s_{i+1}；執行每個替代行動a_i^j，會根據環境的轉移函數T(s_i, a_i^j)，得到對應的未來狀態s_i^j。
    4.  將這些狀態、替代行動、未來狀態的三元組收集起來，構成了D_rollout數據集。

    這個數據集不需要任何外部獎勵，未來狀態本身就包含了行動品質的隱含資訊。
*   **兩種訓練策略：**
    1.  **隱式世界建模（IWM）：** 讓Agent把學習環境動態當成一個輔助的預測任務，通過預測下一個狀態，在自己的策略裡內化環境的運行規律，而不是單獨建一個外部的模擬器。它的損失函數是讓Agent在給定s_i和a_i^j的情況下，盡可能準確地預測出s_i^j。 用IWM的損失函數訓練一個週期後，再用專家數據集D_expert做監督微調。
    2.  **自我反思（SR）：** 讓Agent思考行動的合理性，通過對比自己的替代行動和專家行動的後果，生成為什麼專家行動更好的解釋，再用這些解釋來優化策略。 Agent需要生成一段思維鏈c_i^j，解釋為什麼專家行動a_i比替代行動a_i^j更優，並包含任務目標、分析行動、對比專家行動合理性以及論證約束條件等。
        然後，訓練策略，Agent需要在給定s_i的情況下同時預測出反思c_i^j和專家行動a_i。
        在實際訓練中，研究團隊會把D_refl和專家數據集D_expert混合起來。

**實驗驗證**

研究團隊進行了全面的實驗，覆蓋8個環境、3種模型，從有效性、泛化性、RL兼容性三個維度驗證了早期經驗的價值。

*   **實驗設置：**

    *   **環境：** ALFWorld, ScienceWorld, TravelPlanner, SearchQA, BFCLv3, Tau-Bench, WebShop, WebArena-Lite。
    *   **模型：** 三種不同規模、不同家族的指令微調模型。
*   **實驗結果：**

    *   **有效性：** 兩種策略在所有8個環境中都顯著優於模仿學習基線。
    *   **領域泛化能力：** 早期經驗能夠挽回更多損失，自身試錯能夠覆蓋專家數據沒包含的場景，泛化性更強。
    *   **RL兼容性：** 早期經驗初始化的RL模型最終性能遠高於模仿學習初始化，而且優勢會持續到RL訓練結束。
*   **消融實驗：**

    *   **專家數據量的影響：** 早期經驗能用更少的專家數據達到更好的效果。
    *   **分支因子K影響：** 隱式世界建模的性能會隨K增大而穩步提升，而自我反思的性能在K=2-4時最優。
    *   **模型規模的擴展：** 早期經驗在大模型、有限計算資源下也適用。
*   **與基線方法的比較：**

    *   **長思維鏈（Long CoT）：** 提升幅度較小，且在複雜環境中會適得其反。
    *   **STaR風格的數據：** 解釋匹配率很低，導致模仿學習性能下降。

**總結**

早期經驗範式的核心價值：

*   解決了無獎勵學習難題，可以直接以行動-未來狀態為監督信號，無需外部獎勵，可以適配多數現實環境。
*   降低了數據依賴，能用更少的專家數據達到更好的效果，大幅降低數據成本。
*   連接了兩大範式，既是模仿學習的增強版，又是強化學習的優質初始化，為大語言Agent從人類數據時代過渡到經驗時代提供了可行的路徑。

**局限性與未來方向**

*   當前方法主要針對短互動序列，對於長序列任務，如何分配早期經驗的學習權重還需進一步研究。
*   如果環境存在危險行動，Agent的試錯可能帶來風險，如何在安全約束下生成替代行動也是未來要解決的問題。

未來研究方向包括結合自監督目標、進行跨環境的遷移、以及大規模的真實世界部署。

**結論**

早期經驗讓我們看到Agent在離自主學習的目標上更近一步。未來的AI助手也許不用人類一遍遍地教，就能透過自己的試錯和反思變得更加聰明。或許，Sutton等人提出的經驗時代真的會在某個時間點到來。

感謝收看本期視頻，我們下期再見。

[model=gemini-2.0-flash,0]
