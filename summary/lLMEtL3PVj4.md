好的，我整理後的文稿如下：

**标题：Thinking Machines Lab 在线策略蒸馏：小模型也能拥有大模型专家水平**

**引言**

大家好，这里是最佳拍档，我是大飞。今天我们来聊聊如何让小模型在特定领域达到大模型的专家水平。这是当前AI行业的一个核心痛点。Thinking Machines Lab 最新发布的技术文章——在线策略蒸馏，恰好就是解决这个问题的关键。

**什么是在线策略蒸馏？**

简单来说，在线策略蒸馏是一种将强化学习的纠错相关性与监督微调的奖励密度相结合的训练方法。Thinking Machines Lab 发现，把它用在数学推理和内部聊天助手的时候，在线策略蒸馏可以极低的成本超越其他方法。CEO Mira Murati 也表示，这种方法可以让小模型具备强大的领域性能和持续学习的能力。

**论文作者背景**

文章作者主要是凯文·卢（Kevin Lu），他之前曾在 OpenAI 工作，领导了 4o-mini 的发布，并且参与过 GPT-5 系列、GPT-oss 以及 o1、o3 和 o4 一系列模型的研发工作。

**大语言模型训练的三个阶段**

要理解在线策略蒸馏的价值，首先要明白一个大语言模型在特定领域达到专家级水平需要经历哪些训练阶段：

1.  **预训练 (Pre-training):** 模型的基础教育阶段，目标是掌握通用能力，比如理解语言结构、基础逻辑推理、常识性世界知识。
2.  **中期训练 (Mid-training):** 给模型填充专业知识，让它从通才向专才转变。
3.  **后训练 (Post-training):** 塑造模型的行为，让模型知道在特定场景下该怎么做。在线策略蒸馏是一种高效的后训练方法。

**当前主流后训练方法的问题**

当前主流的后训练方法包括在线训练（On-policy training）和离线训练（Off-policy training），各有优劣，却都存在难以解决的痛点。

*   **在线训练 (强化学习):**

    *   优点：学生模型在自己的轨迹上学习，能够直接避免自己会经常犯的错。
    *   缺点：反馈太稀疏，效率极低，计算成本很高。
*   **离线训练 (监督微调/离线蒸馏):**

    *   优点：反馈密集，成本比强化学习低。
    *   缺点：存在复合误差，容易偏离实际场景，学错重点，且可能只模仿到教师的风格，没学到准确性。

**在线策略蒸馏的原理**

在线策略蒸馏的核心逻辑是：让学生模型走自己的轨迹，然后让教师模型给每一步打分。

具体步骤：

1.  学生模型生成自己的轨迹。
2.  教师模型对学生轨迹的每一个 token 进行评分。
3.  学生模型根据这些 token 的分数来调整参数，下次尽量生成教师认可的 token。

在线策略蒸馏将在线训练的场景相关性和离线蒸馏的反馈密集性结合在一起。它借鉴了 DAGGER 算法和过程奖励建模 (Process Reward Modeling) 的思路，并针对大语言模型的特点做了优化。

**在线策略蒸馏的实现细节**

*   **损失函数：反向 KL 散度 (Reverse KL Divergence)。** 用于衡量学生模型和教师模型的 token 分布之间的差异。目标是最小化这个差距，让学生在自己的上下文里尽量预测出教师认可的 token。使用反向KL散度，而非正向KL散度的原因：
    *   **不可欺骗性：** 确保低差距一定对应教师认可的行为。
    *   **模式寻找 (Mode Seeking)：** 让学生专注于学习教师的核心行为，而不是分散在多个次优的行为上。
*   **折扣因子：** 设为 0，简化计算，且不影响性能。

**案例分析：数学题**

团队用一道数学题进行可视化案例分析，展示在线策略蒸馏如何精准定位错误的根源，而不是只看最终的结果。

**伪代码步骤（基于 Tinker API）：**

1.  初始化教师客户端（采样客户端）。
2.  采样学生轨迹，记录每个 token 的对数概率。
3.  计算奖励：用教师对每个 token 的 logprobs 减去学生的 logprobs，得到反向 KL 散度，取负，就是学生的奖励。
4.  用强化学习进行训练：将每个轨迹的 advantages 设置为反向 KL 散度取负，调用强化学习的 importance sampling 损失函数来更新学生模型的参数。

**实验结果**

*   **数学推理：**

    *   学生模型：Qwen3-8B-Base
    *   教师模型：Qwen3-32B
    *   基准测试：AIME24
    *   对比方法：离线蒸馏、强化学习、在线策略蒸馏
    *   结果：在线策略蒸馏以极低的成本超越其他方法，GPU 小时数只有强化学习的 1/10，计算效率提升 9 到 30 倍。
    *   结论：在线策略蒸馏对 LoRA 模型特别友好，能用 LoRA + 在线策略蒸馏的组合，在轻量微调的前提下达到接近全量微调的性能，进一步降低部署成本。
*   **个性化 (企业内部 AI 助手):**

    *   在线策略蒸馏是一个持续学习的有效工具，能够解决训练内部知识时指令遵循能力退化的问题。它通过固定教师模型，让学生在新的场景下学习，完美解决了学新忘旧的矛盾。

**四个更深层次的洞察**

1.  **密集监督是提升计算效率的关键：** 在线策略蒸馏比强化学习快 50-100 倍，核心原因是奖励的密度。
2.  **在线策略蒸馏能够高效的复用训练数据：** 即使只用一道数学题，让模型反复训练，也能让模型学到通用能力。
3.  **强化学习的本质是语义策略搜索，而蒸馏是直接学习最终的策略：** 在线策略蒸馏省去了搜索策略的时间，直接学习最终的成果。
4.  **在线策略蒸馏是持续学习的理想工具：** 因为教师模型是固定的，不会偏离。

**在线策略蒸馏的核心价值总结**

1.  **性能上：** 能让小模型在特定领域达到大模型级的性能。
2.  **成本上：** 计算效率高，还能复用数据、支持 LoRA，大幅降低开发和部署成本。
3.  **落地性上：** 基于现有工具和框架，步骤清晰，能够快速复用代码，还能解决持续学习、个性化等实际落地难题。

**总结**

在线策略蒸馏意味着，我们不用再依赖百亿参数的大模型，也不用承担强化学习的高额成本，只用中小模型 + 在线策略蒸馏的方式，就能在自己的领域做出高性能的 AI 应用。

**未来方向**

*   如何让教师模型的评分更精准？
*   如何在多任务场景下应用？
*   如何进一步降低教师模型的依赖？

**结语**

Thinking Machines Labs 的使命是用 AI 模型来赋能人，而在线策略蒸馏正是这个使命的体现。

感谢观看，我们下期再见。

**整理说明：**

*   **结构化：** 将文稿整理成包含标题、引言、原理、细节、实验结果、洞察和总结等清晰的结构。
*   **提取关键信息：** 提取了文稿的核心观点、实验数据和结论。
*   **简化语言：**  在不损失原意的基础上，精简了一些语句，使其更易于理解。
*   **重点突出：** 使用粗体和序号等格式，突出显示关键信息，方便读者快速抓住要点。
*   **保留术语：** 保留了原文中的专业术语，以确保准确性。

希望这个整理後的版本能幫助您更好地理解和使用這些信息。

[model=gemini-2.0-flash,0]
