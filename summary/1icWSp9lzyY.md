好的，我來幫你整理這篇文稿，使其更清晰、簡潔，並突出重點：

**標題：Transformer 新挑戰者：Liquid AI及其基於液態神經網路（LNN）的LFM模型**

**引言：**

*   介紹：這裡是最佳拍檔，我是大飛。
*   背景：Google 的 Transformer 架构是目前大模型的主流。
*   新秀：初創公司 Liquid AI 另闢蹊徑，旨在構建超越 Transformer 的基礎模型架構。
*   核心產品：Liquid AI 推出了基於第一性原理構建的新一代生成式 AI 模型——Liquid Foundation Models（LFM）。LFM 在各模型規模上實現 SOTA 性能，同時保持更小的記憶體佔用和更高效的推理能力。

**LFM 模型性能概覽：**

*   **LFM 系列模型三種尺寸：**
    *   **LFM 1.3B（密集型）：** 參數規模最小，適合資源受限環境。在各項基準測試中優於 Meta 的 Llama 3.2-1.2B 和微軟的 Phi-1。首次有非Transformer 模型超越 Transformer 模型。
    *   **LFM 3B（密集型）：** 適合部署在邊緣設備端，性能在 3B Transformer 模型、混合模型和 RNN 模型中名列第一。性能與 Phi-3.5-mini 相當，但規模小 18.4%，適合移動端和其他邊緣端文本應用。
    *   **LFM 40.3B MoE（專家混合模型）：** 目前 LFM 系列中最大的模型，處理複雜任務。LFM-40B 在模型大小和輸出質量之間實現平衡，運行時激活 12B 參數，性能媲美更大模型。MoE 架构實現更高的吞吐量和更低的硬體成本。
*   **LFM 的核心優勢：精簡**
    *   佔用内存更少：在基于 Transformer 的模型中，KV 缓存会随着序列的长度而线性增长，但LFM 可以通过高效地压缩输入，在相同硬件上处理更长的序列。舉例：LFM-3B 只需要 16 GB 内存，而 Meta 的 Llama-3.2-3B 則需要超過 48 GB 内存。
    *   更强的上下文能力：允许开发者打造更长的上下文窗口。
    *   新應用場景：首次在邊緣設備上實現長上下文任務，應用於文檔分析和摘要、上下文感知聊天機器人、提高 RAG 性能。
    *   理想選擇：具有很強的競爭力及運營效率，金融、生物技術、企業級應用、邊緣設備部署皆適用。

**LFM 核心架構：液態神經網路（Liquid Neural Networks, LNN）**

*   **LNN：Liquid 團隊提出的一種全新架構。** 與傳統深度學習模型相比，LNN 只需要更少的神經元即可實現相同結果。
*   **LNN 設計空間的兩個維度：**
    *   **核心運算符的特徵化：** 將輸入數據轉換為結構化特徵集/向量，以自適應的方式調節模型內部計算。
    *   **運算符的計算複雜度：** 完成操作所需的計算資源。
*   **LNN 設計原理：**
    *   LNN 根植於動態系統理論、信號處理和線性代數理論，設計了一種混合的計算單元。
    *   LFM 模型保留 LNN 的核心優勢，允許模型在推理過程中進行實時調整，避免計算開銷。可以高效處理多達 100 萬個 token，同時將記憶體使用量降到最低。
*   **LNN 的靈感來源：仿生學**
    *   液態神经网络的灵感来自于秀丽隐杆线虫的神经结构，它既是最简单的生命智能体也是通过生物神经机理模拟实现通用人工智能的最小载体。
    *   受線蟲啟發，Liquid AI 設計出「液態時間常數網路」（Liquid Time-constant Networks），神經元由微分方程 ODE 控制，時間常數可變。
    *   近似方法：可以用閉式解來高效地模擬神經元和突觸之間的相互作用，提高計算速度及可擴展性，在時間序列建模方面表現出色。

**Liquid AI 公司背景：**

*   **孵化：** 由 MIT 计算机科学和人工智能实验室 (CSAIL) 孵化。
*   **成立時間：** 2023 年 3 月。
*   **四位聯合創始人：**
    *   拉明·哈薩尼 (Ramin Hasani)：首席執行官，MIT CSAIL 机器学习研究合作伙伴。
    *   馬蒂亞斯·萊希納 (Mathias Lechner)：首席技術官，哈薩尼在 MIT CSAIL 的研究合作伙伴。
    *   丹尼爾·羅斯 (Daniela Rus)：MIT CSAIL 主任，著名機器人學家和計算機科學家。
    *   亞歷山大·阿米尼 (Alexander Amini)：首席科學官，丹尼爾·羅斯的博士生。
*   **融資：** 2023 年 12 月獲得種子輪融資 3750 萬美元，估值 3 億美元。
*   **發展方向：** 專注於為金融和醫學研究建模的企業客戶，暫無開發面向消費者應用程式的計畫。
*   **未來計畫：** 2024 年 10 月 23 日在麻省理工學院舉行正式發布會，並在發布會前發表一系列技術博客文章。

**總結：**

*   Liquid AI 的 LFM 和 Liquid 架構結合了高性能和高效的記憶體使用，為基於 Transformer 的模型提供了一個有力的替代方案，有望成為基礎模型領域的重要玩家。
*   邀請使用者回饋意見並持續改進模型。

**結語：**

*   關於基於蟲腦的 LFM 架構，歡迎大家在評論區留言。感謝大家的觀看，我們下期再見。

**總結改進：**

*   **更清晰的結構：** 將內容分成引言、模型性能、架構、公司背景和總結等部分，方便閱讀。
*   **突出重點：** 使用粗體字標記關鍵資訊，方便快速瀏覽。
*   **簡潔的語言：** 刪除冗餘的詞句，使內容更易於理解。
*   **概括歸納：** 將原文中的細節進行歸納，提煉出核心觀點。
*   **使用列表：** 使用項目符號和編號來組織信息，使其更易於閱讀。

希望這個版本更符合你的需求！

[model=gemini-2.0-flash,0]
