好的，我將根據您的要求整理這篇文稿，主要著重於內容的結構化和重點提煉，以便更好地理解和回顧。

**整理後的文稿：**

**主題：DeepSeek-GRM：通用獎勵模型與推理時擴展（Inference-Time Scaling）**

**介紹：**

*   大飛（最佳拍檔）介紹 DeepSeek 與清華大學聯合發表的論文，探討如何透過創新的架構設計和訓練方法突破模型性能。
*   預測 DeepSeek 可能提前推出 R2 模型，這篇論文或許能讓我們提前窺探到R2的一角。

**背景：**

*   當前主流 AI 模型大多採用強化學習，特別是基於人類反饋的強化學習 (RLHF) 作為後訓練的核心方法。
*   RLHF 的核心是訓練一個獎勵模型 (RM) 來模擬人類的偏好，指導大語言模型的優化。
*   傳統 RLHF 依賴大量人工標注，成本高昂且擴展性有限，難以處理複雜和主觀性強的任務。
*   因此，構建更強大、更通用的獎勵模型成為突破瓶頸的關鍵。

**挑戰：**

*   現有獎勵模型 (Scalar RM, Pairwise RM) 在通用性和靈活性上存在局限性。
*   隨著推理時擴展成為提升模型性能的重要途徑，獎勵模型需要在推理時透過更多計算變得更準確，以提升大語言模型的對齊效果。

**DeepSeek-GRM：通用獎勵模型**

*   DeepSeek 聯合清華大學研究團隊提出 DeepSeek-GRM 的通用獎勵模型，以及名為自我原則評價調優 (SPCT) 的訓練方法。
*   目標：解決通用獎勵模型的構建難題，系統性探索如何利用推理時擴展來提升模型性能。

**DeepSeek-GRM 的架構**

*   **生成式獎勵建模 (GRM)：** 選擇更靈活、表達能力更強的範式。
*   **逐點式 (Pointwise) 評分機制：** 針對輸入的查詢和待評價的回答，生成一段結構化的評價文本。
    *   **評價原則：** 根據輸入內容自適應生成一系列評價原則，定義評價的關注點和標準（附帶權重）。
    *   **詳細分析與評價：** 基於生成的原則，對每個回答進行詳細分析和評價，說明優缺點。
    *   **評分提取：** 通過預設的解析規則，從評價文本中提取對每個回答的具體評分。

**Pointwise GRM 的優勢：**

*   **輸入靈活性：** 統一框架處理評價單個回答、比較兩個回答或對多個回答進行獨立評分和排序。
*   **推理時擴展的潛力：** 生成文本的核心行為允許在推理時進行多次採樣，每次採樣產生不同的評價原則和分析角度，綜合多樣化的結果可獲得更全面、穩定和精細的最終評分。

**SPCT：自我原則評價調優**

*   **核心思想：** 讓模型根據具體輸入內容，動態、自適應地生成最相關的評價原則，並基於這些原則進行準確評價。
*   模型從被動地應用規則轉變為主動構建評價框架。
*   **訓練階段：**
    1.  **拒絕式微調 (RFT)：**
        *   使用預訓練的大語言模型作為基礎模型。
        *   利用包含查詢、回答和人類偏好標籤的獎勵模型數據集，讓模型生成「原則+評價」文本並提取評分。
        *   **拒絕式採樣策略：**
            *   拒絕與人類偏好不符的生成數據。
            *   拒絕多次生成結果與人類偏好一致的過於簡單的數據。
    2.  **基於規則的在線強化學習：**
        *   GRM 模型作為強化學習中的策略，根據實時輸入生成評價原則、進行評價並提取評分。
        *   設計簡單的準確性規則作為獎勵信號 (+1/-1)，更新 GRM 模型參數。
        *   通過調整 KL 散度的懲罰，保證生成文本格式的穩定性。

**推理時擴展 (Inference-Time Scaling) 策略：**

*   **基於投票的擴展：**
    *   使用訓練好的 DeepSeek-GRM 模型，設置隨機性，並行進行 k 次獨立推理，每次推理生成不同的原則、評價和評分。
    *   將 k 次推理得到的評分進行聚合（相加或取平均），得到最終的綜合評分。
    *   每次採樣前對輸入回答的順序進行隨機排列，以減少順序影響。
*   **基於元獎勵模型 (Meta RM) 引導的投票：**
    *   訓練 Meta RM 來評估 DeepSeek-GRM 生成的每次「原則+評價」輸出的質量或可靠性。
    *   Meta RM 判斷 GRM 的輸出是否與基準一致。
    *   推理時，DeepSeek-GRM 生成 k 份評價結果，Meta RM 對其評分，篩選出評分最高的 k_meta 份結果，最後基於這些高質量評價結果進行投票聚合。

**實驗結果：**

*   即使不進行推理時擴展，經過 SPCT 訓練的 DeepSeek-GRM 模型在整體性能上已優於同等規模的基線獎勵模型，並展現出與一些大型閉源模型競爭的實力。
*   SPCT 訓練方法有效，相比只進行 RFT 冷啟動，完整的 SPCT 流程帶來顯著的性能提升。
*   DeepSeek-GRM 展現出優秀的推理時擴展特性，隨著採樣次數 k 的增加，模型性能持續穩定提高，在元獎勵模型引導下提升效果更明顯。
*   通過推理時擴展，DeepSeek-GRM-27B 模型上進行 32 次採樣並使用元獎勵模型引導，性能提升的幅度有時可以達到甚至超過把模型參數增加幾倍所帶來的提升。
*   DeepSeek-GRM 在不同類型的任務和評價維度上的表現更為均衡，展現出更好的通用性和更少的領域偏見。

**局限性：**

*   GRM 的效率本質上落後於同等規模的 Scalar RM，限制了其在在線強化學習管道中的大規模使用。
*   在一些可驗證任務的特定領域上，DeepSeek-GRM 仍然落後於 Scalar RM。
*   GRM 需要更強的推理能力來全面檢查回答，但 Scalar RM 也存在嚴重的偏差和可擴展性問題。

**未來研究方向：**

*   工具集成
*   原則和評價生成範式的分解
*   在大模型離線評估中的應用
*   長視野推理的探索

**結論：**

*   DeepSeek-GRM 具有更強的可擴展性和效率，可以作為通用獎勵系統的多功能接口，推動大語言模型後訓練和推理的前沿發展。
*   如果 DeepSeek-R2 能夠整合這項技術，將推理時擴展做到極致，那麼它或許能夠進一步以更低的訓練成本挑戰 OpenAI 的 o 系列模型，實現「以小博大」的逆襲。

**总结：**
Deepseek 的新研究着重于通用奖励模型的建构，通过 GRM 架构和 SPCT 训练方法，配合推理时Scaling 策略，达到提升模型质量的目的，有望在低训练成本下挑战现有的大模型。

[model=gemini-2.0-flash,0]
