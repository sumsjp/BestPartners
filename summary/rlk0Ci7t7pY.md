好的，這是我整理後的文稿。我主要做了以下調整：

*   **簡化結構：** 將文稿分成更明確的段落，並加上小標題，讓讀者更容易抓取重點。
*   **精簡重複資訊：** 刪除一些重複的語句，讓文稿更簡潔。
*   **潤飾語言：** 調整一些口語化的表達，使文稿更正式一些。
*   **統一術語：** 統一使用「大型語言模型」或「大模型」等術語。
*   **加強重點：** 將一些關鍵資訊用粗體標示，方便讀者快速掌握。
*   **調整了部分語序:** 讓語句讀起來更流暢。

**整理後的文稿：**

**AI時代的到來：OpenAI CTO 米拉-穆拉提的訪談分享**

大家好，這裡是最佳拍檔，我是大飛。

最近，OpenAI的CTO米拉·穆拉提（Mira Murati）在達特茅斯學院的訪談中表示，AI在某些領域只需要一年半的時間就能達到博士級別的智能。這引發了人們對於AI發展的思考：人類真的要被AI超越了嗎？在高智能AI問世的前夕，人類又該何去何從？

**AI的性能與發展**

米拉認為，隨著大模型擁有更多的訓練數據和計算資源，其智能會穩定提升，且擴展過程相當線性。她將GPT-3比喻為幼兒級別的智能，而GPT-4更像是聰明的高中生。她預計，在接下來的一年半內，大模型在特定任務上的智能水平將能達到人類博士的水平。

市面上的一些大模型已經開始展現出驚人的能力。Anthropic發布的Claude 3.5 Sonnet在研究生級推理（GPQA）、本科級知識（MMLU）和編碼能力（HumanEval）上都刷新了紀錄。特別是在GPQA測試中，它首次突破了65%的分數，超越了領域專業博士的水平。

**AI的未來：合作還是威脅？**

米拉認為，AI在持續發展下去後，具備高智能體能力的系統肯定會出現，甚至會結成社群、連接到互聯網上相互交流、共同完成任務，或與人類無縫合作。她認為，AI安全問題必須在AI技術開發的過程中同步制定，就像訓練一隻聰明的狗一樣，AI越聰明，越容易理解安全的重要性。開發安全性和提升模型性能是一回事，大模型越聰明，就越安全。

米拉也承認，目前的研究還不能百分之百地掌控AI，因為大模型的黑盒子特性，有時會展現出開發人員意料之外的「湧現能力」。她認為，開發一套用於預測「湧現現象」的技術，對於未來的高智能AI是十分必要的。

**AI的價值觀與安全**

OpenAI也在努力為大模型塑造一套安全的價值觀。米拉提到，大模型擁有一個預設的價值觀系統，這些價值觀來自互聯網、授權訪問的數據，以及由人類標記的問題。她希望在未來製作一個自定義的價值觀系統，讓每個社群都可以有自己的價值觀。

米拉認為，創造零風險的AI是不可能的，AI安全的關鍵是如何將風險降到最低，並為人們提供工具來實現這一點。她認為，政府和監管機構的參與至關重要，企業需要為第三方訪問者提供足夠的知識和權限，以確保這些機構了解AI的現狀，並在出現問題後快速響應。公眾也是維繫AI安全的重要因素。

**AI帶來的挑戰：版權與虛假信息**

米拉承認，AI的發展也帶來了一些挑戰，例如版權和虛假信息。大模型訓練所用的數據來自互聯網，其繪畫和語音合成能力可能存在版權爭議。OpenAI正在測試和鑑別音頻，並讓人工鑑別專家提前使用大模型，來鑑別生成的圖片，制定應急措施，並逐步向更多人開放訪問。

為了防範虛假信息，OpenAI也在與民間社會、媒體和內容創作者合作，試圖解決這些問題。

**AI的樂觀願景**

儘管大模型會帶來一些麻煩，米拉依然願意保持樂觀的態度來迎接AI時代的到來。她認為，AI可以作為草稿製造機，為任何事情先制定一套草稿，再交給人類打理真正需要創意的成品。AI也可以當作秘書，幫助選擇最適合當前工作的工具。

米拉認為，AI象徵著人類在下一個生產力階段所使用的工具，AI與人類的合作會極大地擴展人類的創造力，讓每個人都參與到創作中來。

**結語**

米拉·穆拉提對於AI威脅論的樂觀態度與CEO薩姆·奧特曼一致，二人都覺得AI不會威脅到人類，並且會和人類展開有益的合作。大家贊同米拉對於未來AI的看法嗎？歡迎在評論區留言。感謝收看，我們下期節目再見。

**備註：**

這是我根據您的文稿進行整理的版本。如果需要，我可以根據您的具體需求進行更進一步的修改。例如，可以增加或刪減某些段落，或是調整語言風格等等。

[model=gemini-2.0-flash,0]
