好的，我將這篇文稿整理如下，主要目的是使其更易於閱讀，並使其結構更清晰。我將會：

*   **分段並加入標題**：將文稿分成更易於理解的段落，並添加標題以概述每個段落的內容。
*   **修正錯別字和語法**：修正文中可能存在的錯別字和語法錯誤，使語句更流暢。
*   **簡化口語化的表達**：將一些口語化的表達轉換成更書面化的表達方式，使其更適合閱讀。
*   **提煉重點**：在適當的地方，強調重點信息，使其更突出。
*   **格式化**：使用適當的格式，例如粗體和列表，以提高可讀性。

**整理後的文稿：**

**Meta推出SAM：計算機視覺的GPT-3時刻**

大家好，這裡是最佳拍檔，我是大飛。如果您看過漫威電影，一定對鋼鐵人的頭盔印象深刻。通過這個頭盔，可以一眼識別並標記出眼前的所有人和物品，並且可以看到這些事物的特徵和數據。現在Meta正在將這個科幻的設想變成現實。

當各大巨頭在AIGC領域混戰時，Meta默默地在人工智能的另一個重要分支——計算機視覺上展開了大動作。上週三，Meta發表了一篇名為 "Segment Anything" 的論文，中文翻譯過來就是「分割一切」。這篇論文介紹了一個全新的 Segment Anything Model，簡稱 SAM，可以用來識別圖像和視頻中的物體，甚至是AI從未訓練過的物品。

**SAM：強大的圖像分割能力**

所謂的分割，用最通俗的話來說就是摳圖。但Meta這次所展示的AI的摳圖能力遠比我們之前想像的要更加強大，甚至在人工智能的領域被認為是計算機視覺的GPT-3時刻。

雖然智能摳圖並不是一個新鮮事物，但是如果你嘗試過一些p圖軟件，就會發現如果想把照片摳的快、摳的準、摳的自然，其實是一件費時又費力的事情。而且對於單個主體的圖片來說可能還好處理一點，但是如果一個圖片裡有幾十個物品都要摳出來，那麼目前的摳圖軟件和工具都很難處理好。

從技術的角度來講，圖像摳圖一直是計算機視覺領域的一項經典而且複雜的任務，其中關鍵的難點在於識別的時間和精準度。而Meta這次發布的SAM模型可以說給出了一個幾乎完美的解決方案。對於任何一張照片，Meta都可以快速識別照片中的所有物體，並且智能地將其分割成不同的形狀和板塊。你甚至可以點擊圖中的任何物品進行單獨的處理。

這次的SAM還有一個很大的突破，在於即使是訓練過程中從未遇到過的物品和形狀，它也能夠將它準確的識別並且分割出來。而且除了簡單的識別圖片中的物體之外，這一次SAM還支持用戶使用交互性的方式來分離出你想要的物體。比如說，你可以將鼠標懸浮在這個物體之上，就能夠自動的定位出這個物體的輪廓，即使是顏色非常相近，或者是在連人眼都難以快速分辨的倒影的圖像中，SAM都能夠非常準確的找到輪廓的邊線。

再比如，你也可以直接通過輸入文字來查詢，SAM就可以幫你找到並且標記出這個圖片中你想找的文字所標記的對象。

**SAM：不僅限於靜態圖片**

不僅僅是靜態圖片，SAM也能夠準確識別視頻中的物體，並且還能夠快速標記出這些物品的種類、名字、大小，並且自動用ID給這些物品進行記錄和分類。Meta表示未來這一技術會跟AR、VR的頭盔進行廣泛的結合。這聽上去確實有點鋼鐵人頭盔的味道了。

聽到這裡，您是不是覺得這個模型已經很厲害了？但是別着急，Meta這次還有大招。除了能夠把物品從圖像中精準的分離出來，SAM還能夠支持對這個物品的編輯。也就是說，你可以把這件衣服從這個模特身上換下來，再換一個顏色，改個大小放到另一個模特身上。你還可以把你從靜態圖片中摳出來的椅子進行3D渲染和編輯，讓他從一個圖片立刻就能夠動起來。接著你還可以改變他的形狀或者進行更多的創意操作。

**業界評價：計算機視覺的GPT-3時刻**

在Meta發布了SAM之後，立刻就吸引了大量的關注。英偉達人工智能科學家 Jim Fan 表示，這次 SAM 最大的一點突破是，它已經能夠基本的理解 "物品" 的一般概念，即使對於一些未知對象或者是不熟悉的場景，比如說水下和顯微鏡裡的細胞，它都能夠比較準確的理解。因此，他表示相信 SAM 的出現會是計算機視覺領域裡的 GPT-3 時刻。

不僅是 Jim 有這樣的觀點，一些 AI 研究專家甚至也表示 SAM 之於計算機視覺，就像是 GPT 之於大語言模型。

**實際應用：SAM的廣闊前景**

就在 SAM 發布之後，很多人也在第一時間上網進行了實測。不僅大部分網友基本都表示驚嘆，一些網友還結合自身的工作領域，打開了 SAM 更廣闊的應用想像空間。

*   有人將包含了眾多複雜元素的圖片上傳之後，SAM 可以毫無壓力的把他們都識別出來，無論是近景還是遠景，都可以基本準確的找到大量的複雜的細微的元素。
*   自然科學的研究者將 SAM 和衛星圖像結合在了一起，表示 SAM 能夠很好的識別和找到他標記的風貌類型。
*   還有神經外科影像學的專家將 SAM 用到了一個脊髓血管病的案例文件之中，認為 SAM 在幫助判斷和分析病情上有很大的幫助。
*   有生物學家輸入了一張顯微鏡下的組織圖片，即使圖中的形狀特徵毫無規律，但憑藉著 Zeroshot 的技術，SAM 也能夠自動識別多細胞結構中的腺體、導管、動脈等等。這名生物學家認為 SAM 的產出結果已經非常接近完美，未來應該能夠節省大量手動註釋的時間。
*   還有騎行愛好者將地圖和 SAM 結合起來，認為能夠幫助自己未來更快更高效的給地圖做標記。

**SAM的關鍵特性**

總體來看，跟過去的一些計算機視覺模型相比，SAM 在幾個方面有著顯著的提升和不同：

1.  **Prompt結合**：SAM 開創性的跟 Prompt 結合了起來。它可以接受各種輸入提示，例如點擊框選，或者指定想要分割的對象。這種輸入並不是一次性的指令，你可以不停的對圖像下達不同的指令，從而達到最終的編輯效果。這意味著此前在自然語言處理中的 prompt 模式也開始被應用在了計算機視覺領域。

2.  **海量數據集**：SAM 這次是基於 1,100 萬張圖片和 11 億個掩碼的海量數據集進行訓練。這是迄今為止最大的分割數據集，是 OpenImage V5 數據集的 6 倍。這個數據集涵蓋了廣泛的對象和類別，包括像動物、植物、車輛、家具、食物等等。這些圖像的分辨率都達到了 1500x2250 的像素，平均每張圖像約有 100 個掩碼。

3.  **輕量級解碼器和零樣本性能**：這次 SAM 採用了輕量級的掩碼解碼器，可以在每次提示僅僅幾毫秒內的網絡瀏覽器中運行。同時 SAM 在各種分割任務上具有很強的零樣本性能。零樣本也就意味著 Sam 可以在不對特定任務或者領域進行任何額外的訓練或者微調的情況下就可以對對象進行分割。例如 SAM 可以在沒有任何先驗知識或者是監督的情況下，來分割人臉、手、頭髮、衣服和配飾。SAM 还可以以不同的方式来分割对象，例如红外图像或者是深度图。

**Meta的應用和開源策略**

Meta 表示目前公司內部已經開始使用 SAM 的相關技術，用在了 Facebook、Instagram 等社交平台對於照片的標記、內容審核和內容推薦上。而生成式人工智能作為創意的輔助工具，今年也將被作為重點優先事項納入到 Meta 更多的應用程序中。

這次最讓很多業內人士驚喜的地方在於無論是 SAM 模型還是巨大的訓練數據集都是開源的。也就是說，目前任何人都可以在非商用許可下下載和使用 SAM 模型以及它的數據。

Meta 這次推出的 SAM 模型是希望進一步的加速整個行業對圖像分割以及更通用圖像以及視頻理解的研究。隨著 SAM 的演進和發展，這項技術可能會變成未來 AR、VR、內容創作、設計等等更多領域的強大的一個輔助工具。

**體驗SAM**

最後，除了開放模型和數據集以外，Meta 還推出了一個 SAM 的演示平台。即使你是一個完全不懂 AI 的普通用戶，也可以在這個網站上親身體驗一下它神奇的摳圖功能。網址我放到評論區裡了，感興趣的小夥伴們快去試試吧。

好了，今天的分享就到這裡，感興趣的小夥伴們歡迎訂閱我們的頻道，我們下期再見。

**總結**

我盡量保留了原文的風格，但使其更易於理解和閱讀。希望這個版本對您有所幫助！

[model=gemini-2.0-flash,0]
