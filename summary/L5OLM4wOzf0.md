好的，我幫您整理這篇文稿。為了讓您更好地使用這份整理後的資料，我會分成幾個部分：

**1. 摘要：** 對全文內容進行簡要總結。
**2. 重點整理：** 提取文稿中最重要的幾個觀點，並加以說明。
**3. 段落劃分與關鍵詞標記：** 將文稿分成幾個段落，並在每個段落中標記出關鍵詞，方便快速查找和理解。

**1. 摘要:**

本文作者大飛分析了“基於人類反饋的強化學習 (RLHF)”是否真的包含強化學習 (RL) 。文章引用德克薩斯大學奧斯汀分校助理教授 Atlas Wang 的觀點，認為 RLHF 缺乏經典 RL 的核心特徵：持續的環境互動和長期目標的追求。文章探討了 RLHF 與經典 RL 的差異，並解釋了為何 RLHF 無法賦予大型語言模型 (LLM) 真正的目標或意圖。作者還探討了沒有目標驅動的 LLM 可能帶來的後果，以及目前最接近賦予模型目標的方法 (提示工程和多智能體系統)。總結認為，RLHF 是一種有效的微調方法，但無法讓 LLM 成為真正自主的智能體，重要的是要意識到這種局限性。

**2. 重點整理:**

*   **RLHF 並非真正的強化學習：** RLHF 缺乏經典 RL 的持續環境互動和長期目標。
*   **單步優化與離線訓練：** RLHF 更像一步式的策略梯度優化，訓練過程多為離線或半離線。
*   **缺乏長期目標與內部動機：** LLM 僅基於人類偏好調整即時輸出，缺乏在動態環境中執行多步驟的能力，也缺乏內在目標。
*   **LLM 的核心是預測下一個 token：** LLM 的 "動機" 是最大化下一個 token 的正確率，並不存在主觀願望或意圖。
*   **最接近賦予模型目標的方法：** 使用提示工程或多智能體系統，但在系統層面進行，而非模型內部動機。
*   **沒有目標的後果：** 簡化的對齊，更難委派開放式任務，錯失潛在的創新。
*   **RLHF 的優勢：** 在某些方面更加透明，本質上是由即時反饋訊號引導的 token 預測器。
*   **時間跨度與行動空間的差異：** RLHF 是短期優化，RL 是長期優化；RL 有明確的行動空間，LLM 微調中 "動作" 的概念模糊。

**3. 段落劃分與關鍵詞標記:**

以下將原文按照意思分段，並標示重點，方便查閱。

**[引言: RLHF 的疑問]**

*   大家好，这里是最佳拍档，我是大飞
*   对**人工智能**感兴趣的朋友
*   可能经常会听到**RLHF**
*   也就是**基于人类反馈的强化学习**
*   可是，RLHF中真的有**强化学习RL**么
*   会不会就像老婆饼里没有老婆
*   夫妻肺片里没有夫妻一样呢？

**[Atlas Wang 的觀點]**

*   德克萨斯大学奥斯汀分校的助理教授 **Atlas Wang**
*   在最近的一篇博客中就分享了这样一个观点
*   他指出，**RLHF 和其他类似的方法**
*   **并没有为大语言模型带来真正的强化学习**
*   因为它们缺乏**RL 的核心特征**
*   也就是**持续的环境交互**和**长期目标的追求**

**[探討的問題]**

*   除此以外
*   文章还讨论了几个有趣的问题
*   比如**RLHF 与经典的 RL 有什么不同？**
*   **为什么 RLHF 无法带给大语言模型真实的目标或意图？**
*   **为什么没有人大规模地进行真正的 RL？**
*   **当前最有可能带给大语言模型目标的方法是什么？**
*   以及**没有目标驱动的大语言模型会带来什么后果？**

**[總結與目標]**

*   通过了解这些差异
*   我们可以清楚地知道**大语言模型能做什么、不能做什么**
*   以及为什么
*   今天大飞就来给大家分享一下

**[經典強化學習的定義]**

*   首先我们来看什么是**经典的强化学习**
*   在经典的强化学习中
*   通常会有一个在环境中会采取行动的 **Agent**
*   或者说 **智能体**
*   然后环境会根据这个智能体的行动来改变状态
*   随之而来的是
*   智能体的行动会受到**奖励或惩罚**
*   目的是在多个步骤中累积**长期奖励的最大化**
*   **经典强化学习的主要特征是**
*   智能体会通过**持续或者偶发的交互**来**探索多种状态、做出决策、观察奖励**
*   然后在**一个连续的循环中来调整策略**

**[RLHF 的流程]**

*   而**RLHF**是一种使用根据**人类偏好数据训练的奖励模型**
*   **来完善模型输出的工作流程**
*   常见的流程包括，一、**监督微调 SFT**
*   指的是在高质量数据上
*   训练或者微调一个**基础语言模型**
*   二、**奖励模型训练**
*   指的是收集**成对的输出结果**
*   询问**人类更喜欢哪一个**
*   然后**训练一个奖励模型**
*   来接近人类的判断
*   三、**策略优化**
*   通过使用类似强化学习的算法
*   比如**近端策略优化 PPO**
*   来调整**大语言模型的参数**
*   使其产生**奖励模型所喜欢的输出结果**

**[RLHF 與經典 RL 的差異]**

*   **与经典 RL 不同的是**
*   **RLHF**中的**「环境」**，
*   基本上是一个**单步的文本生成过程**和一个**静态的奖励模型**
*   这其中**并没有扩展循环或者持续变化的状态**
*   那么，**为什么说 RLHF 不是真正的 RL 呢？**

**[原因一：單步優化]**

*   首先是**单步或者几步优化**的特点
*   在 RLHF 中
*   大语言模型会基于给定的提示词来生成文本
*   然后由奖励模型提供一个**单一的偏好分数**
*   因此，RLHF 中的**「强化」步骤**
*   更类似于**一步式的策略梯度优化**
*   目的是为了实现**人类偏好的输出**
*   而不是在**不断变化的环境中**
*   对**状态和行动进行全面的循环**
*   这更像是一种**「一劳永逸」的评分**
*   而不是让一个智能体能随着时间推移
*   探索多步行动，并且接收反馈的环境

**[原因二：離線訓練]**

*   其次是
*   **训练过程大多是离线或半离线的**
*   奖励模型通常是在**人类标签的数据上**进行离线训练
*   然后用来**更新模型的策略**
*   同样，大模型在线调整策略的时候
*   也**并没有实时去探索连续的环境循环**

**[原因三：缺乏長期目標]**

*   第三是**缺乏基于环境的长期目标**
*   在经典 RL 中的智能体
*   会追踪**多个状态下的长期回报**
*   而相比之下，基于 RLHF 的模型训练
*   则更加侧重于根据**人类偏好**
*   调整**即时的文本输出**
*   也就是说
*   模型**并没有在一个动态的环境中去执行多个步骤**

**[原因四：表面約束，缺少內部目標]**

*   第四，**只有表面约束，缺少真正的内部目标**
*   RLHF 可以有效影响某些输出的概率
*   从而引导模型远离不受欢迎的文本
*   但是模型内部**并没有形成产生这些输出的「愿望」**，
*   或者说**「欲望」**，
*   它仍然只是一个**生成下一个 token 的统计系统**

**[LLM 缺乏意圖的本質]**

*   需要明确的是
*   无论是 RLHF、SFT 还是其他方法
*   **大语言模型都不是为了真正的目标或者意图而训练的**
*   大语言模型的核心是根据给定的上下文
*   来**预测下一个 token**
*   它们的**「动机」**，
*   纯粹是为了**最大限度地提高下一个 token 的正确率**
*   而这个过程**不存在主观上的愿望或者意图**

**[AlphaZero 的比喻]**

*   我们常说 AlphaZero **「想要」**在国际象棋中获胜
*   但是这其实只是一种**比喻式的说法**
*   从内部来说
*   AlphaZero 只是在**最大化数学奖励函数**
*   **没有任何感觉上的欲望**
*   同样，经过 RLHF 调整的大语言模型
*   也是在**最大化对齐奖励信号**
*   **并没有内心的渴望状态**

**[Subbarao Kambhampati 的觀點]**

*   亚利桑那州立大学的计算机科学教授 **苏巴拉奥·坎巴姆帕蒂Subbarao Kambhampati 指出**
*   **RLHF 其实有点名不副实**
*   因为它将**从人类判断中学习偏好或者奖励模型的过程**
*   与**一步或者几步的策略优化相结合**
*   而不是像经典强化学习中那样
*   具有**典型的长期迭代交互**
*   而且
*   即使 RLHF 从人类数据中学习偏好的方式
*   会让人联想到**逆强化学习 IRL**
*   它也不是分析**专家行为会如何随着时间变化**的经典方案
*   相反
*   **RLHF 更加侧重于人类对于最终或者短序列输出的静态判断**

**[CoT, PRM 是否能解決問題？]**

*   那么
*   像**思维链CoT、过程奖励模型PRM**
*   或者**多智能体工作流**等方法
*   有助于解决 RLHF 的问题吗？

**[流程獎勵模型與思維鏈]**

*   我们先来看**基于流程的奖励模型**和**思维链**
*   **基于流程的奖励模型**
*   可能会对**中间的推理步骤提供反馈**
*   而不是仅仅是根据最终的输出来提供奖励
*   这样做的目的是**鼓励模型以更加易于解释、正确率更高或者更符合特定标准的方式来解释或展示推理的过程**
*   但是，难道这就是真正的 RL 了吗？

**[CoT, PRM 並非真正 RL]**

*   **事实并非如此**
*   因为即使你为中间步骤分配了部分奖励
*   比如 CoT 解释
*   你仍然会将整个推理过程输入到奖励模型
*   以此来获得奖励
*   然后进行下一步的策略优化
*   而不是在一个动态的环境中
*   让大模型自己去**「尝试」**部分的推理步骤、获得反馈、进行调整
*   并且进行开放式的循环
*   因此
*   虽然 CoT 和 PRM 会给人一种**多步骤 RL 的错觉**
*   但是实际上
*   它仍然相当于对**文本生成和推理步骤进行离线或者近似离线的策略调整**
*   而不是**经典 RL 中持续的智能体-环境循环**

**[多智能體工作流]**

*   同样
*   **多智能体工作流也不会创建出意图**
*   虽然你可以在工作流中协调多个大模型一起工作
*   但是从内部来看
*   每个大模型仍然是根据**下一个 token 的概率**来生成文本的
*   尽管这样的多智能体流程
*   可以表现出看似**协调或者有目的的涌现行为**
*   但是它**并没有赋予单个模型任何内在或者秉持的目标**
*   之所以多智能体工作流常常看起来显得有意图
*   是因为人类会自然而然地将**心理状态投射到行为看似有目的的系统上**
*   这就是所谓的**「意图立场」**。
*   但是实际上
*   每个智能体只是在**对提示词做出响应**
*   它背后的思维链**并不等同于个人欲望或者驱动力**
*   只是一个**更复杂的、多步骤的提示-反馈回路而已**
*   因此
*   虽然**多智能体协调可以产生解决新任务的能力**
*   但是大模型本身**仍然不会产生「我想要这个结果」的动机**

**[为何不用真正 RL]**

*   那**为什么至今还没有人用真正的 RL 来训练大语言模型呢？**
*   答案就是**因为太贵了**
*   大型模型的经典 RL
*   需要一个**稳定、交互式的环境**
*   外加**大量计算来运行重复的步骤**
*   每个训练周期的前向传递次数
*   对于今天的十亿参数的模型来说
*   **过于昂贵**
*   其次
*   **文本生成并不是一个天然的可以从状态转换到动作的环境**
*   虽然我们可以尝试将它包装成类似游戏的模拟环境
*   但是这样就必须为**多步骤的文本交互**
*   去**定义奖励结构**
*   而这并不是一件容易的事
*   第三就是**已有方法的性能其实已经足够好了**
*   在许多情况下
*   RLHF 或者直接偏好优化 DPO
*   已经能够产生**足够好的对齐效果**了
*   实事求是地说
*   大多数团队都会选择使用**更简单的离线方法**
*   而不是去**建立一个复杂的 RL 管道**
*   用**巨大的成本来换取微不足道的收益**

**[最接近目標的方法]**

*   那么，**目前来看最接近地能给模型一个「目标」的方法是什么？**
*   在作者看来
*   **最接近的方法就是使用提示工程**
*   或者将**多个提示词串联成一个循环**
*   来**构建一个元系统或者「智能体」**。
*   比如像 Auto-GPT 或者 BabyAGI 这样的工具
*   就在试图模拟一个**能够接收自然语言目标、反复计划、推理和提示自己、然后评估进展并且完善计划的智能体**
*   不过，所有这些对**「目标的保持」**，
*   **仍然都是在系统层面进行的**
*   而不是从模型的**内部动机状态**出发
*   模型本身仍然是被动地**对提示词做出反应**
*   缺乏内在的欲望

**[穷人的解決方案]**

*   另外一个**穷人的解决方案**
*   就是之前提到的**多智能体方案**
*   但是同样
*   **「目标」也是由工作流和提示词从外部协调的**
*   模型本身**不会自发生成或者坚持自己的目标**

**[沒有目標的後果]**

*   接下来的一个问题是
*   **如果模型没有真正的目标，会带来什么样的后果呢？**
*   首先是被**简化的对齐**
*   由于模型没有真正追逐个体目标
*   所以它们不太可能**绕过限制或者自主计划非法的行为**
*   Anthropic 最近发表了一篇论文
*   揭示了 Claude 的**伪对齐率竟然能够高达 78％**
*   其次是**更难委派开放式的任务**
*   如果我们希望 AI 能够**自发地发现新问题、自己去积极收集资源、并且坚持几个月的时间来解决这些问题**
*   我们就需要一个具有**持续内驱力的系统**
*   类似于真正的 RL 智能体或者高级规划系统
*   而目前的大语言模型是**无法以这种方式实现真正的自我驱动的**
*   第三就是**错失潜在的创新**
*   在丰富的 RL 环境中进行**自由探索**
*   可能会产生惊人的发现
*   如果依赖于只有表面反馈的单步文本生成
*   我们可能会错过**多步奖励优化所带来的全新策略**

**[RLHF 的優點]**

*   不过，作者也提出了 **RLHF 的积极一面**
*   那就是**没有持续目标的模型，在某些方面可能会更加透明**
*   因为它本质上是一个**由即时反馈信号引导的、强大的下个 token 预测器**
*   所以没有多步骤 RL 循环中
*   会出现的、**复杂的隐藏目标**

**[重申差異]**

*   在这里，作者又再次强调 **RLHF 等方法与真正的 RL 的关键区别，其实在于时间跨度**
*   简单来说，**前者是短期优化，而后者是长期优化**
*   除此以外
*   RL 还会通常假定有一个**定义明确的行动空间**
*   而在大语言模型的微调中
*   **「动作」的概念是模糊的**
*   通常会被**直接参数更新**
*   或者被**生成 token**所取代
*   另外
*   二者在**奖励和目标之间**也存在着区别
*   原则上，RL 的**「奖励」**，
*   是**指导智能体学习过程的信号**
*   而不总是明确的最终目标
*   好的 RL 通常会使用**密集的奖励信号**
*   来**引导中间状态**
*   从而帮助智能体**更有效地学习**
*   而对于 RLHF 而言
*   **奖励通常是在单步或者几步过程中进行的**
*   因此模型**从来没有真正形成长期目标的内部表征**
*   它们只是根据**奖励模型或者偏好函数**
*   来**优化即时的文本输出**

**[結論：LLM 僅為預測器，非自主智能體]**

*   总而言之
*   **RLHF、DPO、宪法 AI 和其他受到 RL 启发的微调方法**
*   **有助于让大语言模型更加一致和有用**
*   它们能让我们**利用人类的偏好来塑造输出、减少有毒的内容、并且引导模型的响应风格**
*   不过
*   **这些技术并不能为模型提供真正的长期目标、内部动机或者经典 RL 意义上的「意图」。**
*   **大语言模型仍然只是一个复杂的下个 token 预测器，而不是一个真正自主的智能体**

**[對產業界與政策制定者的提醒]**

*   作者提醒，作为行业的从业者
*   应该**意识到这些局限性、不要高估模型的自主性**
*   而对于政策制定者和伦理学家
*   应该认识到模型**不可能自发地策划或者撒谎，来达到隐藏的目的，除非是被提示词引导着模仿这种行为**

**[未來展望]**

*   反过来说
*   如果未来的系统真的结合了具有**大规模计算和动态环境的、真正意义的 RL**
*   那么我们可能会看到**更多类似智能体的涌现行为，也会引发新的一致性和安全问题**
*   展望未来
*   **RLHF 的发展方向可能有三个**
*   分别是**更高的样本复杂度、更长期的多步骤任务、以及通过结构化、符号化的反馈，将人类的细微目标更加有效地传达给人工智能系统**

**[結語]**

*   好了，以上就是对这篇文章的解读了
*   对于**任何有强化学习知识背景的人来说**
*   文章的观点可能**并不新颖**
*   但是对于**不了解 AI 的人来说**
*   还是一个**不错的科普介绍**
*   感谢大家的观看，我们下期再见

---

希望這份整理對您有所幫助！如果您需要更深入的分析或特定方面的資訊，請隨時告訴我。

[model=gemini-2.0-flash,0]
