好的，我將盡力用專業的方式整理您提供的文稿，使其更易讀、更有條理。以下是整理後的版本，我主要做了以下調整：

*   **精簡重複資訊：** 去除一些重複描述，例如多次強調「蘇媽」和「AMD」等。
*   **調整段落結構：** 將內容更清晰地分段，使其更易於理解。
*   **潤飾文字：** 修改一些語氣詞和口語化表達，使其更正式。
*   **突出重點：** 用粗體標示關鍵詞，方便快速抓取重點。

---

**最佳拍档：AMD Advancing AI 2024 盛會解析**

大家好，我是大飞。上周，AMD 在美國舊金山舉辦的 Advancing AI 2024 盛会上，展示了一系列令人印象深刻的 AI 成果，包含全新旗艦 AI 晶片、伺服器 CPU、AI 網卡、DPU 和 AI PC 行動處理器，旨在擴大 AI 運算的應用範圍。

AMD 在會上展示了其 AI 生態系統的合作夥伴，包括 Google、OpenAI、Microsoft、Meta、xAI、Cohere 和 RekaAI 等。AMD 預計到 2028 年，AI 加速器市場規模將達到 5000 億美元，並希望成為該市場的領導者。

AMD 認為 AI 平台有四大核心：最強大的訓練和推理計算引擎、開放的軟體解決方案、深度共同創新的 AI 生態系統，以及集群水平之上的系統設計。本次發布會的三大重點產品，正體現了在集群水平之上的系統設計。

**第一支箭：第五代 EPYC 伺服器**

蘇姿丰執掌 AMD 後，重點發展了 Ryzen 晶片和 EPYC CPU 伺服器。EPYC 伺服器經過四代升級，在 2024 年第一季度達到了 34% 的市佔率，成功從 Intel 手中奪取了 CPU 伺服器市場的三分之一。

為形成一體化的 AI 伺服器陣列，EPYC 迎來了第五代升級，即 EPYC 9005 系列，代號 "Turin"。它採用台積電 3/4nm 製程和 Zen 5 架構，最高配置擁有 16 個 Zen5 CCD 核心，內含 192 個核心和 384 個執行緒，時鐘頻率可達 5GHz。Turin 支援 AVX512 指令集，提供完整的 512 位元資料路徑，並實現了 17% 的 IPC 效能提升。它還使用 SP5 平台，可以兼容前代的 "Genoa" 處理器。

Turin 在記憶體方面引入了 DDR5 支援，頻寬提升至 6400 MT/s。在 I/O 能力上，它支援 PCIe Gen5 和更多的 PCIe 通道。在安全性方面，增加了硬體級別的根信任和可信 I/O 功能。

Turin 的效能提升相當明顯。與上一代 Intel Xeon 伺服器相比，Turin 在 SPEC CPU 測試中效能提升了 2.7 倍，企業效能最高提升了 4 倍，HPC 效能最高可提升 3.9 倍。特別值得注意的是，Turin 在 AI 方面的能力提升和對 GPU 節點控制的優化，其基於 CPU 的 AI 效能最高提升了 3.8 倍，而作為 GPU 主機節點時的效能最高能夠提升 1.2 倍。

AMD 表示，使用 Turin 伺服器只需 131 個即可達到之前 1000 個 Xeon 伺服器的效果。AMD 還對 Turin 的 AI 適用性加強，使其在 AI 時代成為更好的選擇。為了提高算力，AMD 這次還優化了 CPU 在 AI 工作流程中的關鍵動作，使得 CPU 在處理 GPU 協調任務時更加高效，比前代產品快了 28%。

AMD 將 Turin 和 Xeon 8592 進行了比較，Turin 讓 MI300X 的推理效能提升了 8%，訓練效能提升了 20%。針對 Nvidia H100，Turin 更讓 GPU 集群的推理效能提升了高達 20%，訓練效能提升了 15%，甚至比 AMD 自家的 MI300X 都強。

第五代 EPYC 的表現和側重，顯示了 AI 戰略對於 AMD 的重要性，也是 AMD 對 Intel 最近兩代 Xeon 伺服器都在大力強調 AI 能力的回應。

**第二支箭：AI 晶片 Instinct MI325X**

AI 晶片正在成為 AMD 業務增長的重點。Instinct MI300X 加速器已成為 AMD 歷史上增長最快的产品，不到两个季度销售额就超过了 10 亿美元。

AMD 在发布会上公布了 MI300 系列的第二代产品 MI325X，它曾在 2024 ComputerX 大会上被简短地介绍过，但从未公开技术细节。MI325X 加速器采用了 AMD CDNA 3 架构，配备 256GB 的 HBM3E 高带宽内存，内置 1530 亿个晶体管，可以提供高达 6TB/s 的内存带宽。在 FP8 和 FP16 精度下，分别可以达到 2.6 PF 和 1.3 PF 的峰值理论性能。

与 Nvidia 上一代的旗舰 GPU H200 相比，MI325X 的内存容量更大，内存带宽也更高。在算力方面，虽然 Nvidia 官方宣称 H200 的 FP16 算力可以达到 1.9 PF，但经过 semianalysis 实测，实际算力大约也就是 1 PF，与 H100 持平，比 MI325X 低了 30%。因此，AMD MI325X 在推理方面的表现平均要超越 H200 30%，与算力比提升相符。

由 MI325X 核心集成的 GPU 平台包含了 8 个 MI325X，总共可以提供 2TB HBM3E 的高带宽内存，FP8 精度下的理论峰值性能可以达到 20.8 PF，FP16 精度下可以达到 10.4 PF。系统还配备了 AMD Infinity Fabric 互连技术，带宽高达 896 GB/s，总内存带宽达到 48 TB/s。相比于 H200 的集成平台 H200 HGX，MI325X 平台提供了 1.8 倍的内存量、1.3 倍的内存带宽和 1.3 倍的算力水平，在推理方面相较 H200 HGX 能提升最多 1.4 倍的表现水平。

AMD 的 GPU 软件系统 ROCm 在过去一年内一直在和主流 AI 开发平台的适配性磨合，这导致了它的训练效果不佳。但是这一年来，AMD 一方面在不断升级 ROCm，一方面加强与 AI 开发平台的深度合作，总算是让它有了一倍左右的提升。这个提升的结果是，针对 Meta Llama-2 这种主流模型，MI325X 在单 GPU 上的训练效率也终于超越了 H200，而在集群中的训练效率仍然和 H200 HGX 相当。

MI325X 预计将于 2024 年的第四季度开始出货，与 H200 的大规模交付相差仅一个季度。考虑到目前 Nvidia 遇到了 B200 和 B100 的封装瓶颈，大规模发货被延迟，即便交付给 OpenAI 的也不过是工程样机。如果 MI325X 的发货规模能够快速爬升，那理论上的代差就会被实际的出货情况所抹平。MI325X 在市场上的实际对手就是 H200，而且比 H200 的性能还稍微更高。现在就看 AMD 能否抓住这个窗口期、保证供应链，趁机扩大市场了。

除了 MI325X 以外，AMD 还详细介绍了更下一代的 MI300 系列 GPU，MI350 系列。它采用了 AMD 的 CDNA 4 架构，使用先进的 3nm 制程工艺，配备高达 288GB 的 HBM3E 高带宽内存。MI350 系列的一个重要创新是新增了 FP4 和 FP6 数据类型的支持，这样可以在保持计算精度的同时，进一步提高 AI 训练和推理的性能。根据 AMD 表示，MI355X 的在 FP16 数据格式下的算力可以达到 2.3PF，比 MI325X 提升 1.8 倍，与 B200 的算力持平，而在 FP6 和 FP4 格式下，算力可达 9.2PF，比 B200 在 FP6 格式下的算力提升将近一倍，与 B200 在 FP4 格式下的算力持平。

MI355X 可以被视为 AMD 真正剑指 B200 的 GPU 芯片。MI355X 的集成平台则配备了 2.3TB HBM3E 高带宽内存，内存带宽高达 64 TB/s。在计算性能方面，MI355X 在 FP16 精度下可以达到 18.5 PF，FP8 精度下达到 37 PF，在新增的 FP6 和 FP4 数据类型下，它甚至能达到 74 PF 的理论峰值。不过这款产品需要等到 2025 年的下半年才能发售。

AMD 还在发布会上公布了自己的路线图，除了我们已经介绍过的产品以外，2026 年 AMD 预计会发售基于新架构的 MI400 系列 GPU 芯片。

除了硬件，AMD 也提了一下自己在软件栈上的进展。最近这一年来，AMD 打通了所有主要的 AI 开发平台，获取了 PyTorch 的零日更新，可以让客户在软件升级当天就使用到新的功能。与此同时，PyTorch 还支持了 Triton 的 AMD 硬件兼容。在模型层面，AMD 加强了与 Huggingface 和 Meta 的合作，对于超过 100 万种主流模型都能做到开箱即用。在这一系列合作的加持下，ROCm 的最新版本 6.2，相较于旧版，在推理和训练上也都有了超过 2 倍的提升。可以说，如今的 AMD 已经在硬件和软件层面，都有了和 Nvidia 叫板的资本。

在 2024 年第二季度的财报中，MI300 在单个季度内就实现了超过 10 亿美元的销售额，远超市场预期。虽然 AMD 服务器业务的综合销售额目前仅为 Nvidia 同期的 13%，但是就发展形势来看，MI325X 很有可能会扩大 MI300 带来的市场占有率。

**第三支箭：DPU**

对于大多数公司来讲，数据传输可能是他们模型训练中最大的拦路虎。想要构建一个好的数据服务器集群，除了算力扎实以外，核心任务是实现高效的数据传输，确保能够快速处理和分发海量的训练数据，从而最大化 GPU 利用率。与此同时，支持大规模的 GPU 并行计算也成为一项关键能力。

Meta 在训练 Llama 3.1 的时候，就专门搭建了一个相当复杂的集群，力图增加并联 GPU 的数量和数据效率，并且选择了 RoCE v2 传输协议来解决网络问题。经过多次分路和调整数据包的大小实验，Meta 的工程团队才成功达成了一个相对高效和稳定的数据传输水平。

AMD 推出的第三代可编程 P4 引擎，传输速度可达 400GB/s，与 Nvidia 最新的 DPU BlueField-3 持平，且支援每秒 120M 的可程式化資料包和每秒 5M 的並發服務速度。該晶片的核心特性，就是在處理並聯 GPU 時的後端網路優化，能針對高負載數據進行負載平衡和擁塞管理，從而避免在同一數據通路上產生資料包阻塞，還能在丟包時僅重發遺失的包，而非一口氣把所有數據重發一遍。此外，它還支持快速故障恢復，可以繞過故障 GPU 所在的数据通路，避免整个集群直接瘫痪，并且试图自动修复当前的数据包故障。

AMD 为前端网络提供的解决方案是 Pensando Salina 400 DPU，它采用了 400G PCIe Gen 5 接口，配备 232 P4 多服务 MPU、双通道 DDR5 内存以及 16 个 N1 ARM 核心。这款产品还支持软件定义网络、有状态防火墙、加密、负载均衡、网络地址转换和存储卸载等功能。

Pensando Salina 400 DPU 的核心数量与 Nvidia BlueField-3 持平，但内存和带宽都有提升，在网络调节中也更加自由。后端网络的网卡则为 Pensando Pollara 400，这是业界首款支持 UEC（Ultra Ethernet Consortium）标准的 AI 网卡，具有可编程硬件管道，性能提升最高可达 6 倍，支持 400Gbps 的网络速度。它采用开放生态系统设计，支持 UEC Ready RDMA 技术，可以缩短作业的完成时间，并且提供高可用性。

通过 AMD 这次发布的 DPU 产品，训练时对于 AI 服务器网络的利用率可以达到 95%，而一般没有经过优化的数据网络还很难达到 50%。

这些提升背后的秘密武器就是 UEC 协议，也被称为超级以太网联盟协议。AMD 宣称，UEC 相比于 Meta 训练时使用的传统 RoCE v2 协议，服务器中信息的传输速度可以提高 6 倍，集群间信息传输速度可以提高 5 倍，而且之前的智能分路等多种功能也都是内嵌于 UEC 协议之中的。

目前 AMD 的新款 DPU 是唯一支持 UEC 协议的数据网络传输产品，而 Nvidia 的 BlueField-3 目前还只能支持 RoCE v2 协议，而且它想要转换协议并非易事。除了需要面对 AMD 的专利瓶颈以外，硬件兼容性也需要一个较长的过程才能完成。

虽然 AMD 在 2022 年就收购了 Pansando 公司，并且推出了两代 DPU 产品，但是都没能打破 Nvidia 由 BlueField 系列构建的 DPU 霸权。毕竟跟据 Nvidia 的官方介绍，搭配 BlueField，Nvidia 的 GPU 集群表现可以提升 1.7 倍。但是，如果 UEC 被实际证明确实更加高效，那么 AMD 就成功抢占了 DPU 上的先发优势。

**總結**

AMD 正在沿着自己的路线图，将 AI 基础设施所需的各种高性能解决方案加速推向市场，并且不断证明它能够提供满足数据中心需求的多元化解决方案。AI 已经成为了 AMD 战略布局的焦点。这次新发布的一系列产品与持续增长的开放软件生态系统也形成了一套组合拳，有望帮助 AMD 进一步增强在 AI 基础设施竞赛中的综合竞争力。

感謝大家收看本期視頻，我們下期再見。
---

希望以上整理對您有所幫助！如果您對某些部分有特定的修改需求，請隨時告訴我。

[model=gemini-2.0-flash,0]
