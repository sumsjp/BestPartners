好的，我來整理這份文稿，使其更具結構性、重點更清晰，並移除口語化的部分。

**整理後的文稿：**

**主題：Hyung Won Chung（尚哥）「2023年的大型語言模型」演講分享**

**引言：**

*   介紹AI科學家Hyung Won Chung（簡稱尚哥），他近期從Google跳槽至OpenAI。
*   尚哥的演講涵蓋大語言模型的最新動向和技術細節，內容深入淺出，可與安德烈卡帕西的State of GPT演講齊名。
*   PaLM-2的聯合負責人評價尚哥擁有豐富的全棧大語言模型經驗。

**演講主題：「2023年的大型語言模型」**

*   **時間點的重要性：** 強調“2023年”，因為大型模型定義可能在未來幾年內發生變化，導致現有見解和結論過時。
*   **GPT-4的潛力：** 尚哥認為GPT-4即將超越拐點，性能將實現顯著跳躍，因此需要改變觀點，認識到大語言模型蘊藏的巨大潛力。

**核心觀點：規模效應**

1.  **湧現現象：**

    *   模型規模越大（參數數量、訓練數據大小、計算資源），就越可能發生湧現現象。
    *   小型模型在某些任務上進展緩慢，但當規模達到一定程度時，突然就能夠完成這些任務。
    *   即使當前一代的大語言模型還無法展現出某些能力，也不應輕言放棄，而應思考「它還沒行」的原因。
    *   推理思路應轉變為「一些方法只是在當前不起作用」，隨著模型規模擴大，許多結論都會改變。
    *   研究者應不斷更新、拋棄基於過時觀點的直覺。
    *   新研究者可以嘗試在以前模型上不起作用的想法，這些想法可能在新模型上突然有效。
    *   實驗過程中，應記錄失敗的過程，並在新的模型上再次運行實驗，不斷更新和糾正自我認知和理解。
2.  **不同能力與規模的關係：**

    *   以GPT-3和GPT-4為例，針對不同能力：
        *   “能力一”：GPT-4接近轉折點，可能出現躍進式改進。
        *   “能力二”：GPT-4仍有很大距離，目前方法可能無法提供實質性幫助。
        *   “能力三”：GPT-3已超越轉折點，研究可能只帶來漸進式改變。
    *   堅持這種思維框架，經常回顧和反思，更新直覺，就能明白正在解決哪種問題。
3.  **擴大參數規模的方法：**

    *   目前的大型語言模型都採用Transformer架構。
    *   將Transformer看做是一個包含一系列矩陣乘法的序列到序列的映射，進行數組的轉換。
    *   Transformer通過計算點積，使每個序列token都能與其他token進行交互。
    *   Transformer的規模擴大，就是讓很多機器高效地進行矩陣乘法，並減少機器之間的通信。
    *   通過將注意力機制拆分為單獨的頭、利用多台機器和芯片、使用GSPMD方法進行並行化，可以實現。

**訓練與優化：**

1.  **預訓練成本：**

    *   預訓練成本依然很高。
    *   縮放定律是基於小規模模型數據開發的，預測模型擴展損失非常重要。
    *   僅增大規模並不能解決所有問題。
2.  **模型訓練的四個階段：**

    *   預訓練階段
    *   指令微調（監督微調SFT）
    *   獎勵模型訓練
    *   策略模型訓練

3. **指令微調階段 (SFT):**
    * 為所有任務建立從自然語言指令到自然語言回應的映射.
    *  指令微調的核心思路是擴展到對話場景，模型可以與其他代理交互。
    * 增加更多任務雖可提高效能，但存在著邊際效益遞減的限制，任務的多樣性是關鍵。
    * 指令微調的目標函數是瓶頸，難以使用狹窄的信號訓練大型模型。

4.  **強化學習與RLHF (Reinforcement Learning from Human Feedback):**

    *   最大化預期的獎勵函數。
    *   使用獎勵模型來為更複雜的情況定義獎勵。
    *   通過人類比較兩個答案的優劣，讓模型學習人類偏好，而非直接告知最佳答案。
    *   比較比絕對評分更簡單，獎勵模型實際上是在用最大似然法來訓練。

5.  **策略模型 (Policy Model):**
    * 策略模型通常從監督指令微調的檢查點開始，生成一些補全，並提供給獎勵模型返回得分。
    *  透過策略梯度演算法迭代進行，在滿足 RM 模型條件下，讓策略模型通過強化學習來學習這些偏好。
    *  RLHF 實施困難，獎勵模型容易出錯，產生所謂的“獎勵黑客”問題 (模型為了得到更高獎勵產生沒意義的長答案)。
    *  RLHF 是一個有價值的研究方向，可以幫助我們克服最大似然的偏差。

**人工智能發展的演進：**

*   從基於規則的系統到深度學習，再到可學習的損失函數。
*   深度學習的成功在於採用了更弱的歸納偏差，並允許了更高的可擴展性。
*   下一個演進階段可能是讓損失函數也變得可學習。
*   GAN和RLHF是成功的例子，學習損失函數或目標函數將是下一個範式。

**總結：**

*   規模是關鍵，從規模的角度看問題至關重要。
*   學習損失函數或目標函數將是未來的發展方向。

**附註：**

*   省略了對Transformer和並行計算的詳細講解，可參考原視頻。

**這個版本更著重於組織信息，使其易於理解和查找，並去除了口語化的表達方式。** 這樣更適合作為文件整理的結果。

[model=gemini-2.0-flash,0]
