好的，我將整理您提供的文稿，使其更清晰易懂：

**主題：Google推出強大的視覺語言模型PaLM-E，通用AI時代即將來臨？**

大家好，我是Dafei from Best Partner。

自從微軟藉由與OpenAI和ChatGPT的合作而聲名大噪後，老對手Google一直想扳回一城。之前推出的Bard試圖挑戰ChatGPT，但結果卻不如預期，甚至可以說是弄巧成拙。

因此，Google本週再出重拳，推出了歷史上最大的視覺語言模型——PaLM-E。 這個模型有多強大呢？ 它的參數高達5620億，是ChatGPT-3的三倍。

PaLM-E結合了擁有5400億參數的PaML模型和擁有220億參數的ViT模型，不僅能理解圖像，還能理解和生成語言，並執行各種複雜的機器人指令。 重點是，PaLM-E可以直接分析來自機器人相機的數據，無需重新訓練，也無需預處理場景。 實驗結果證明，PaLM-E的正向遷移能力也非常強大。

接下來，讓我們一起觀看幾個PaLM-E的演示影片，相信大家會感受到通用AI時代即將到來。

**演示影片內容：**

*   **演示一：** 基於機器人相機的視覺回饋，讓機器人取來洋芋片。 這個過程中加入了對抗干擾，即使實驗者多次移動洋芋片，機器人仍然能夠再次抓取，最終關上抽屜，將洋芋片交給實驗者。 影片以4倍速播放，實際速度並非如此快速。

*   **演示二：** 讓機器人拿起綠色星星。綠色星星是機器人之前沒有直接觸碰過的物體。

*   **演示三：** 讓機器人將積木放置在不同的角落。 影片展示了機器人可以很好地規劃和執行長期任務，並詳細解釋了機器人每一步的規劃過程。 接著，影片展示了如何讓機器人將剩餘的積木移動到現有的積木群組中。 PaLM-E會將其分解為多個底層策略，例如將黃色六邊形移動到綠色愛心旁邊，將藍色三角形移動到積木群組中。

*   **演示四：** 讓機器人將海洋色的積木組合在一起。 影片展示了機器人可以準確識別藍色積木。

**泛化能力展示：**

實驗者還展示了兩個PaLM-E的泛化例子：

*   將紅色積木推入咖啡杯。這個數據集中只有3個包含咖啡杯的演示數據，而且沒有一個包含紅色積木。
*   讓機器人將綠色積木推到烏龜身邊，即使機器人從未見過烏龜，也能夠成功完成任務。

**PaLM-E的其他能力：**

*   可以根據圖片講笑話。
*   展現了感知、基於視覺的對話和規劃能力。
*   對多張圖片之間的關係非常清楚。 例如，在沒有PaLM-E的情況下，很難判斷圖1中的哪個物品出現在圖2中，但PaLM-E可以做到。
*   可以根據包含手寫數字的圖片進行數學運算。 例如，對於這家餐廳的手寫菜單，PaLM-E可以直接計算出兩份披薩的價格。

Google研究人員計劃未來探索PaLM-E在現實世界中的更多應用，例如家庭自動化或工業機器人。他們也希望PaLM-E能夠激發更多關於多模態AI的應用。

今天的分享就到這裡，歡迎有興趣的朋友訂閱我們的頻道，我們下次再見！

**整理說明：**

*   將原文分段，使其更易於閱讀。
*   將過長的句子拆分，使其更清晰。
*   添加了更具描述性的標題和副標題。
*   潤飾了一些語句，使其更流暢自然。
*   使用了一些口語化的表達，使文稿更具親和力。
*   將英文專有名詞（如PaLM-E、ChatGPT）保留，方便理解。
*   整理重點讓讀者更容易了解PaLM-E的功能和潛力。

希望這個整理後的版本對您有幫助! 如果您有其他需要調整的地方，請隨時告訴我。

[model=gemini-2.0-flash,0]
