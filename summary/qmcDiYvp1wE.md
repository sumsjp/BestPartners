好的，這是經過整理的文稿，主要針對文句的流暢性、結構以及可能造成閱讀障礙的地方進行調整。

---

大家好，這裡是最佳拍檔，我是大飛。

沒想到，AI竟然已經可以通過標準的三方圖靈測試了？

最近，來自加州大學聖地牙哥分校的研究人員，對四個AI模型進行了系統評估，最終證明大語言模型成功通過了圖靈測試。這意味著，以後在網路上和你聊天的那個「熟悉的陌生人」，也許就不再是人類了。

那麼，研究人員究竟找到了什麼樣的證據？今天我們就來解讀一下這篇論文。

首先，簡單介紹一下圖靈測試是什麼。時間回到75年前，電腦科學領域的先驅艾倫·圖靈提出了一個名為「模仿遊戲」的概念，它後來成為判定機器是否具備智能的一種方法，也就是我們現在所說的圖靈測試。

當時，艾倫·圖靈設想了這樣一個場景：測試人員透過純文字的聊天介面，同時與兩個人「聊天」，其中一個是真人，另一個則是AI。這兩方都要竭盡所能向測試員證明，自己才是如假包換的人類，而對方是AI機器人。如果測試員無法準確分辨出誰是真人，那麼就說明機器通過了測試。

長期以來，圖靈測試一直被視為一個衡量AI達到人類智能程度的重要標尺。在過去的75年裡，不少人都嘗試開發能通過圖靈測試的系統，但都沒有成功。不過，隨著人工智能技術的不斷發展，尤其是大語言模型的興起，讓大家重新把目光聚焦到了圖靈測試上。

之前有研究發現，當GPT-4按照要求扮演特定的角色時，有54%的機率會被誤判為真人，跟瞎猜的機率差不多。但是在進行三方圖靈測試的時候，有超過90%的模型都可以被識別出不是真人。

為了看看如今最新的大語言模型到底能不能被人識破，加州大學聖地牙哥分校的研究人員，這次開展了隨機對照的三方圖靈測試。測試分別評估了四個模型的表現，包括ELIZA、GPT-4o、LLaMA-3 1.4-405B和GPT-4.5。

其中，ELIZA是約瑟夫·維森鮑姆（Joseph Weizenbaum）1966年在麻省理工學院研發的一個聊天機器人，它會模擬一位羅傑斯式的心理治療師，來回答使用者提出的文字陳述或者問題。而其他幾個模型，都是當今較為先進的前沿大模型。

研究團隊一共招募了126名加州大學聖地牙哥分校的心理學本科生和158名Prolific平台的使用者作為實驗參與者。每位參與者都要完成8輪測試，每一輪測試都包含兩段對話。審問者會同時與兩位證人進行文字聊天，一位是真人，另一位是AI。參與者會被隨機分配扮演審問者或者證人的角色，並且會隨機匹配到另一位參與者一起進行測試。

AI證人又被分成了四個組：

*   **對照組 (Control)：**包含ELIZA和GPT-4o-NO-PERSONA這兩個模型。
*   **無人格組 (No-Persona)：**包含GPT-4.5-NO-PERSONA和LLAMA-NO-PERSONA這兩個模型。
*   **LLaMa人格組 (LLAMA-PERSONA)：**包含LLAMA-PERSONA模型。
*   **GPT-4.5人格組 (GPT-4.5-PERSONA)：**包含GPT-4.5-PERSONA模型。

這裡的人格，指的是實驗中設定的一個關鍵變數，也就是部分AI被賦予了一個特定人設的提示詞（Persona Prompt），要求模型模仿一個19歲、有點內向、熟悉網路文化和視頻遊戲、說話帶點俚語、不喜歡用句號的年輕人。提示詞中還增加了一些「以假亂真」的效果，比如會引導AI故意犯一些打字上的小錯誤，從而模擬人類在日常交流中不可避免出現的錯誤，降低AI輸出的「完美感」和「機器感」。

在實驗的遊戲介面中，審問者透過分屏介面同時與兩位證人交流。審問者先給兩位證人發送第一條消息，然後大家輪流發消息，一次只能發一條。證人之間看不到彼此的對話內容。每輪測試有5分鐘的時間限制，時間到了之後，審問者需要判斷哪個證人是人類，並且給出對這個判斷的信心程度，以及判斷的理由。測試中主要透過評估對話中人類語言風格模擬、上下文理解能力、情感表達與邏輯推理，來推測是否是AI在對話。

為了衡量AI模型的表現，研究團隊使用了「勝率」和「審問準確率」兩個指標。其中勝率指的是，當審問者判斷某個證人是人類，但實際上是AI模型的次數，佔所有這類判斷的比例。簡單來說，就是AI被誤認為人類的比例。而審問準確率指的是，審問者正確選擇出人類證人的次數比例。

在統計學分析中，研究團隊還將AI模型沒能通過圖靈測試設定為一個需要檢驗的零假設。這意味著，研究人員預設了如果AI模型沒有成功騙過人類，那麼它的勝率就不會顯著低於50%。因為50%的勝率相當於審問者幾乎是隨機猜測的結果。如果實驗結果顯示AI模型的勝率顯著高於50%，那麼就意味著我們可以推翻這個零假設，從而推導出AI模型在一定程度上通過了圖靈測試的結論。

最後，實驗結果顯示，在兩項研究中，GPT-4.5-PERSONA的勝率為73%，其中在本科生中為69%，在Prolific參與者中為76%。LLAMA-PERSONA取得了56%的勝率，其中本科生為45%，Prolific為65%。GPT-4.5-NO-PERSONA和LLAMA-NO-PERSONA的總體勝率分別為36%和38%。而基線模型GPT-4o-NO-PERSONA和ELIZA的勝率最低，分別為21%和23%。

這說明，高達73%的情況下，人類評委會把GPT-4.5錯判為真人，而坐在螢幕另一端的、貨真價實的人類參與者，被認出來的機率只有27%。另外，實驗結果還表明，如果缺少了明確的人格設定提示詞，即使是像GPT-4.5和LLaMA這樣的前沿大語言模型，在圖靈測試中的表現也會顯著下降，勝率驟降到36%-38%左右。這說明了提示詞的重要性，如果只依賴於模型的語言生成能力，並不足以讓AI系統成功偽裝成人類通過圖靈測試。

那麼在圖靈測試中，究竟什麼才是識別AI的有效方法呢？研究團隊也分享了幾個不太奏效的策略，比如依賴於日常閒聊和詢問情感感受。這些AI都已經非常成熟，難以區分。而真正能夠發揮作用的識別策略，包括出其不意地提出一些怪異的問題或者指令，以及設置一些知識盲區陷阱，提問一些非常冷門、專業，甚至略顯生僻的問題。面對不熟悉的知識領域，真正的人類通常會坦然承認自己不知道、不了解。但是，AI為了避免顯得無知，可能會傾向於根據已有信息進行推測或者生成答案，反而更容易暴露。這也說明了模型有時需要假裝自己是無知的，才會更像是真正的人類。

除此以外，在跟GPT-4.5和Llama模型進行討論的時候，很多用戶還感覺到，頂尖大模型已經發展出層次化的人設構建策略，能夠根據提示的要求，在社會身份、認知水平、語言風格乃至價值觀體系等多個維度進行模擬。比方說，當被問到紙杯蛋糕食譜的時候，AI回了句「不了，我一般就買那種預拌粉，哈哈」。這種漫不經心、帶點自嘲又符合年輕人設的回答，瞬間就拉近了距離，充滿了人味兒。另外LLaMa模型在被問到喜歡的奇怪動物時，回答「樹懶！我可以看它們慢吞吞動好幾個小時」，這種口氣也同樣符合人設的個性化表達。

這不禁讓我們開始思考，這種基於指令的、高超的角色扮演能力，本身是否就是一種高級智能的體現？或者說，是一種新的智能形式？

「機器能思考嗎？」這是圖靈在1950年提出模仿遊戲時試圖回答的終極問題。但是在邏輯、計算、知識儲存等方面，AI早已超越人類的今天，圖靈測試的意義似乎正在發生深刻的轉變。

論文的研究者們認為，圖靈測試如今與其說是來衡量通用智能，還不如說衡量的是「類人性」（humanlikeness）和「社交可替代性」（substitutability）。它測試的是，AI能否在社交互動場景中不被察覺地扮演人類的角色。當AI在硬技能上已經遙遙領先的時候，像是模仿人類的情感、個性和社交細節等軟技能，就開始成為了新的競技場。

不可否認的是，當AI開始通過圖靈測試的時候，表明AI已經可以無形地補充、或者替代那些需要與他人進行簡短對話的經濟角色，並且逐漸演化成一種所謂「偽造的人類」——偽人。而與這種偽人的廣泛互動，可能潛移默化地改變人類的自我認知、社會連接方式，以及對真實連接的感知價值。我們是否正在走向一個法國社會學家讓·鮑德里亞所描述的、真實與模擬界限模糊的「超真實」社會呢？

人類的獨特性究竟在哪裡呢？我們又該如何自處呢？美國作家布萊恩·克里斯汀 (Brian Christian) 曾經在他的著作《人性較量 (The Most Human Human)》中提過一個觀點，那就是AI的進步，最終可能會反向激勵人類更加關注和發展自身獨特的核心價值，變得「比以往更加人性化」。他認為，圖靈測試的真正挑戰者不應該是機器，而是我們人類自己。

另外，我們還要警惕的一點是，未來那些控制著大量AI偽人的巨頭，將會握有影響人類用戶意見和行為的權力。

好了，以上就是這篇論文的主要內容了，建議感興趣的朋友可以去閱讀原文。面對越來越像人類一樣的AI，你是否做好了準備呢？人類最應該堅守的特質又是什麼呢？歡迎在評論區留言，感謝大家的觀看，我們下期再見。

---

**整理說明：**

*   **調整語句結構：** 修正了一些語句的冗長和不流暢之處，使其更加易於理解。
*   **段落劃分：** 根據內容邏輯重新劃分段落，使文章結構更清晰。
*   **詞語潤飾：** 替換了一些口語化的詞語，使文稿更符合書面語規範。
*   **標點符號：** 檢查並修正了標點符號的使用，使其更加規範。
*   **補充說明：** 在需要解釋的專有名詞或概念後加入簡短的說明，有助於讀者理解。
*   **列表呈現：** 將AI證人分組用列表方式呈現，增加可讀性。

希望能對您有所幫助！

[model=gemini-2.0-flash,0]
