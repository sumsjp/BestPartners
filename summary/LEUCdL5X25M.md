好的，以下是經過整理的文稿，重點更清晰，結構更完整：

**Meta SAM-2 图形分割模型详解：分割一切，更进一步**

大家好，我是最佳拍档的大飞。繼上周 Llama 3.1 發布的熱潮後，Meta 乘勝追擊，推出了 SAM-2，對一代 SAM 的架構、功能和準確率進行了重大更新，並正式開源。SAM-2 在 SAM 一代分割圖形能力的基礎上，進一步升級，可即時分割影片和圖像，朝著「分割一切 (Segment Anything)」的目標邁進了一大步。

**一、什麼是圖形分割？**

對於不熟悉 SAM 模型的朋友，聽到「分割」這個詞可能會感到困惑。分割並不是真的「切東西」，而是圖形視覺領域的一個專有名詞，是「對象分割 (Object segmentation)」的簡稱。它指的是電腦識別圖像中與感興趣的物體相對應的像素，簡單來說就是「摳圖」。

如果你用 PS 摳過圖，應該知道在沒有輔助工具的情況下，把圖片摳好是很麻煩的。需要一個像素一個像素地把需要的圖案從圖片裡切割出來。但在 2023 年，Meta 的 SAM 就可以一次性識別任何類型圖片中的所有對象，然後輕鬆切割每個像素。而今天的 SAM-2 在性能上更進一步，不僅是圖像，影片也可以分割了。

**二、SAM-2 的主要特性和優勢**

*   **分割影片能力提升:** SAM-2 不僅能分割圖像，也能分割影片。它甚至可以分割以前沒見過的對象和視覺域，無需自定義適配，支援各種不同的使用場景。
*   **首個可即時、可提示的統一模型:** Meta 強調 SAM-2 是首個可用於即時、可提示的圖像和影片對象分割的統一模型，大幅改變了影片分割體驗，並可在圖像和影片應用程式中無縫使用。
*   **準確度提升:** SAM-2 在圖像分割準確率方面超越了之前的版本，並實現了比現有更好的影片分割性能，所需的人機交互時間僅為原來的 1/3。
*   **創新的流式記憶體設計:** SAM-2 的架構採用了創新的流式記憶體 (streaming memory) 設計，使模型能夠按順序處理影片幀。簡而言之，就是剪影片又快又準。
*   **能追蹤鏡頭:** SAM2甚至還可以實時追踪所有镜头

**三、SAM-2 的實際應用**

*   **影片編輯：** 能夠快速、精準地摳出影片中需要的對象圖像，並保證圖像的流暢播放。
*   **特效添加：** 演示預覽中，AI 可以輕鬆將滑板的人和背景圖像分割，然後在影片中追蹤被選定的對象，添加用戶需要的特效。

**四、SAM-2 的性能數據**

SAM 2 在以下方面表現優異：

*   **零樣本影片分割:** 在 17 個零樣本影片數據集的交互式影片分割方面，明顯優於以前的方法，所需的人機交互減少約三倍。
*   **零樣本基準測試:** 相較於自家的 SAM 一代，SAM-2 在 23 個數據集零樣本基準測試套件上的表現都更加優秀。
*   **影片處理速度:** 影片處理速度快了整整六倍。
*   **現有影片對象分割基準:** 在現有的影片對象分割基準（包括 DAVIS、MOSE、LVOS、YouTube-VOS）上，也都表現出色，各項數據都超越了之前的模型。

**五、SAM-2 的設備需求**

SAM-2 對算力的要求非常高，普通人很難在本地部署，只有 Meta 這樣能提供強大硬體資源的巨頭才能運行。

**六、SAM-2 的技術細節**

Meta 認為通用的分割模型應該同時適用於圖像和影片。在 Meta 的研究人員看來，圖像可以被視為具有單幀的、非常短的影片。處理影片的關鍵在於模型需要依靠記憶體來調用這個影片之前處理過的信息，以便在當前時間進一步準確地分割對象。

*   **可提示的視覺分割任務:** Meta 的研究團隊開發了可提示的 (promptable) 視覺分割任務，將圖像分割任務推廣到影片領域，並設計了能夠執行這個任務的模型，也就是 SAM-2。
*   **masklet和時空掩碼:** SAM-2 可以在影片的任何幀中，根據輸入提示來預測當前幀的時空掩碼，也就是「masklet」，一旦預測出初始的，就可以在任何幀中通過提供附加提示的方式來進行迭代完善。
*   **記憶機制:** 為了能夠準確預測所有影片幀的掩碼，研究團隊還引入了一種由記憶編碼器、記憶庫 (memory bank) 和記憶注意力模塊組成的記憶機制。
*   **流式架構:** 在 SAM-2 中，Meta 採用了流式架構，一次處理一個影片幀，並將有關分割對象的信息儲存在記憶中。

**七、SA-V 數據集**

為了訓練出史無前例的影片分割模型，Meta 開發了 SA–V 數據集，並使用了三大階段進行標注：

1.  **第一階段：** 使用 SAM 模型來輔助人類標注，平均標注時間為每幀 37.8 秒。
2.  **第二階段：** 引入了 SAM-2 Mask，標注時間下降到每幀 7.4 秒。
3.  **第三階段：** 使用了完全功能的 SAM-2，標注時間進一步下降到了每幀 4.5 秒。

SA–V 數據集在開發 SAM-2 過程中發揮了重要作用，也是目前最大的視覺分割訓練數據集之一。

**八、開源和許可協議**

SAM-2 已經被發布到了 Meta 自家官網和 GitHub 上。除了模型以外，Meta 也根據 CC BY 4.0 許可發布了 SA-V 數據集，包括大約 51,000 個真實世界的影片和超過 600,000 個掩碼標準。Meta 採用了寬鬆的 Apache 2.0 協議共享了 SAM-2 的代碼和模型權重，並根據 BSD-3 許可共享了 SAM-2 的評估代碼。

**九、SAM-2 的缺點**

根據 Meta 官方介紹，SAM-2 可能會在以下情況失去對象的追蹤：

*   攝影機視角發生劇烈變化
*   長時間遮擋
*   擁擠的場景
*   較長的影片

此外，當目標對象只在一幀中指定的時候，SAM-2 有時會混淆對象，無法正確的分割目標。對於複雜的快速運動對象，SAM-2 有時也會漏掉一些細節，而且預測結果在幀和幀之間可能會不穩定。

**十、總結**

SAM-2 是 Meta 在圖形分割領域的又一重大突破，它不僅提升了圖像分割的準確性，更將分割能力擴展到了影片領域。雖然 SAM-2 仍存在一些缺點，但其在影片編輯、特效添加等領域的應用前景廣闊，值得期待。

感謝大家的觀看，我們下期再見。

[model=gemini-2.0-flash,0]
