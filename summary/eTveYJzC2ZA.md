好的，這是我整理後的文稿，重點放在組織結構、修正錯別字、並提升可讀性：

**AI 繪畫原理與技術發展：從基礎到 LoRA 模型**

大家好，這裡是最佳拍檔，我是大飛。

相信大家都感受到，AI 繪畫領域發展實在太快了。去年 AI 繪畫還是某種畫風，今年已迅速進化到另一種程度，堪稱雲泥之別。 近期各種效果驚人的模型，已經有不少人介紹過了，但大部分技術點比較晦澀難懂。所以今天我盡可能用通俗直白的方式，來解釋 AI 繪畫的原理，希望能對大家有所幫助。

影片中涉及的概念可能包括 VAE (Variational Auto-Encoder)、GAN (Generative Adversarial Network)、Diffusion Model 等等。 但大家先不要被這些詞嚇住，我會盡量用淺顯易懂的方式來解釋清楚，即使是小白也應該可以聽得懂的。

**一、電腦如何生成圖畫？**

這涉及到兩個方面：

1.  **生成像真實圖片一樣的數據：** 大部分人可能只關注到這一點。
2.  **理解需求並給出對應結果：** 後者同樣重要。

**1. 生成式模型 (Generative Model)**

*   首先，我們來看看如何生成像真實圖片一樣的數據。這涉及到機器學習中的一個重要分支，也就是生成式模型。
*   生成式模型需要先「吞進」大量的訓練數據，也就是海量的人類所產生的真實圖片，然後再去學習這些數據的分布，去模仿著生成一樣的結果。
*   其實機器學習的核心無非也就是這麼回事。 難點終究還是如何在設計模型，以及讓模型能夠更好地學習到分布上。

**2. VAE (Variational Auto-Encoder) 變分自編碼器**

*   講到生成式模型，有一個不得不提的技術就是 VAE，全稱是 Variational Auto Encoder，中文稱為變分自編碼器。
*   其中的 Auto Encoder，也就是所謂的自編碼器，其實也包含了編碼器 (Encoder) 和解碼器 (Decoder)，它是一個對稱的網路結構。
*   對於一系列類似的數據，例如圖片來說，雖然整體數據量可能很大，但如果符合一定分布規律，那麼它們包含的訊息量其實會遠小於數據量。
*   編碼器的目的就是把數據量為 n 維的數據壓縮成更小的 k 維特徵，這 k 維的特徵盡可能包含了原始數據里的所有訊息，而你只需要用對應的解碼器就可以轉換回原來的數據。
*   所以在訓練的過程中，數據是透過編碼器壓縮，然後再透過解碼器解壓，然後盡量讓重建後的數據和原始數據的差距最小化。
*   不過訓練好以後，就只有編碼器被拿來當做特徵提取的工具，用於進一步的工作，例如圖像分類等等。所以我們稱之為 Auto Encoder。

*   這時會有人想到，既然 Auto Encoder 可以從 k 維特徵向量恢復出一整張圖片，那給你一個隨機生成的 k 維特徵向量，是否也可以隨機生成什麼畫面呢？
*   這個想法很好，不過實際的結果顯示，Auto Encoder 雖然可以記住見過的照片，但是生成新的圖像的能力很差。

*   於是就有了 Variational Auto Encoder (VAE)。VAE 可以讓 k 維特徵中的每個值都變成符合高斯分布的機率值。於是機率的改變可以讓圖片訊息也有相應的平滑的改變。
*   例如，透過控制某個代表性別的維度，讓它從 0 變到 1，就可以從一個男性的人臉開始，生成越來越女性化的人臉。
*   VAE 其實還是存在很多的統計假設，而且我們要判斷它生成的效果怎麼樣，也需要評估它生成的數據和原始數據的差距大不大。

**3. GAN (Generative Adversarial Network) 生成式對抗網路**

*   於是，有人就乾脆去掉了所有的統計假設，並且把這個評估真假數據 (原始數據和生成數據之間差異) 的判別器也放進來一起訓練，於是就創造了 GAN。
*   GAN 有兩個部分，分別是生成器和判別器。
*   生成器會從一些隨機的 k 維向量出發，用採樣網路合成一個大很多倍的 n 維數據。判別器就負責來判斷合成出來的圖片是真的還是假的。
*   一開始合成出來的都是一些意義不明而且沒有規律的結果，判別器就很容易分辨出來。 隨後生成器會發現一些生成的方向，例如成塊的色塊就可以騙過判別器，那麼它就會往這個方向合成更多的圖片。
*   而判別器發現被騙過去了，就會找到更複雜的特徵來區分真假。如此反覆，直到生成器生成的結果，判別器已經判斷不出來真假了，這就算是訓練好了。
*   這樣訓練出來的生成器可以生成非常逼真的、即使是人眼也難以分辨的圖片，但是都是現實中不存在的。
*   到了這個時候，電腦已經可以學會生成相當逼真的畫面了。

*   雖然 GAN 因為引進了判別器能夠生成非常逼真的圖片，但是由於它需要訓練對抗網路，這個過程其實非常不穩定。對於灌輸了大量海量數據的超大規模網路來說，更是難以控制。

**4. Diffusion Model 擴散模型**

*   所以，這個時候就出現了另一個更好的選擇，也就是現在 AI 繪畫普遍使用的生成式模型： Diffusion Model，中文稱為擴散模型。
*   Diffusion Model 生成圖片的過程看似很簡單，其實背後有一套非常複雜的數學理論作為支撐。
*   不過，我們先把複雜的理論放在一邊，先簡單聊一聊 Diffusion Model 是怎麼運行的。
*   Diffusion Model 運行有兩個過程：
    *   **前向擴散 (Forward Diffusion):** 從右到左的過程。做的事情就是在逐步疊加一個符合正態分布的雜訊，最後得到一個看起來完全是雜訊的圖片。這就是所謂的擴散過程。
    *   **反向擴散 (Reverse Diffusion):** 從左到右的過程。做的事情就是一步步去除雜訊，試圖還原圖片。這就是 Diffusion Model 生成數據的過程。

*   如果不嚴謹地說，你可以把它想像成你有一塊牛排，你一遍一遍地往上撒椒鹽，一直到整塊牛排都被椒鹽覆蓋到看不清原來的紋路了。
*   由於每次加雜訊只和上一次的狀態有關，所以這是一個馬爾科夫鏈的模型，其中的轉換矩陣可以用神經網路來預測。

*   為了達到去噪的目的，Diffusion Model 的訓練過程實際上就是要從高斯雜訊中還原圖片，以及學習馬爾科夫鏈的機率分布，再逆轉圖片的雜訊，使得最後還原出來的圖片符合訓練級集的分布。

*   這個去噪的網路是如何設計的呢？我們可以從疊加雜訊的過程中發現，原圖和加雜訊以後的圖片尺寸是完全一樣的。
*   於是我們很自然地能夠想到用一個 U-Net 的結構來學習。 U-Net 是一個類似於 Auto Encoder 的漏斗狀的網路，但是在相同尺寸的 Decoder 和 Encoder 層上增加了直接的連結，以便於圖片相同位置的訊息可以更好地透過網路來傳遞。
*   在去噪任務中，U-Net 的輸入是一張帶雜訊的圖片，需要輸出的是網路預測的雜訊，而 Ground Truth 就是實際疊加上的雜訊。
*   有了這樣一個網路，就可以去預測雜訊，從而去去除掉它還原圖片。因為帶雜訊的圖片就等於雜訊加圖片。

*   這也是為什麼 Diffusion Model 會比其他方法生成圖片的速度更慢，因為它需要一輪一輪地去雜訊，而不是透過網路可以一次性地推斷出結果。
*   以上就是 Diffusion Model 的生成圖片的原理，是不是很簡單呢？

**二、模型如何理解指令並生成結果？ (CLIP 模型)**

*   到這裡，我們就解釋了電腦是如何生成和真實圖片相似的圖畫的。接下來，我們就來解釋一下模型是如何理解我們想要它生成什麼，並如何給出對應的結果的。
*   玩過 AI 繪畫的人應該都知道，AI 繪畫最主流的模式是在網頁輸入框中輸入一長串的 Prompt 提示語，類似於施法前的吟唱咒語，其中包括想要生成內容的主體、風格、藝術家還有一些 Buff 等等。
*   點擊生成之後，就會得到一張非常棒的結果圖片，當然有時候可能也會出乎意料。
*   透過文字來控制模型生成畫面，最早的做法其實更像是先讓生成式的模型生成一大堆符合常理的圖片之後，再配合一個分類器來得到符合條件的結果。當然這種做法在海量的數據面前顯然是沒法用的。
*   這個領域的開山之作 DALL-E 最值得一提的就是引著了 CLIP 模型來連接文字和圖片。

*   CLIP 模型其實就是用了海量的文本和圖片數據對，把圖片和文本編碼後的特徵，再計算相似性的矩陣。
*   透過最大化對角線元素，同時最小化非對角線元素來優化兩個編碼器，從而讓最後的文本和圖片編碼器的語意可以強對應起來。
*   如果你不能理解 CLIP 的原理，那麼你只需要記住 CLIP 能夠把文字和圖片對應起來這樣就可以了。

*   它最大的成功之處不是用了多麼複雜的方法，而是用了海量的數據。 這樣帶來的好處是，很多現有的圖像模型可以很容易地擴展成文本控制的圖像模型。
*   原本需要大量人工標注的很多任務，現在就只需要用一個 CLIP 模型就可以了。 甚至還可以被用來生成新的數據。
*   例如在 StyleCLIP 中，用文本交互可以來控制生成的人臉。

*   一開始圖片的文字訊息大多都是以打標籤的形式，透過大量的人工標注來完成的。 有了 CLIP 模型之後，可以說是徹底打通了文字和圖片之間的橋樑，使得圖像相關的任務可以得到大大的擴展。
*   所以，即使說 CLIP 是 AI 繪畫的基石也不過分。
*   有了這個 CLIP 模型，就可以計算任何圖片和文本之間的關聯度 (即 CLIP Score)，再用它來指導模型的生成了。

*   這一步其實還分為了幾個發展階段。
    *   **Guided Diffusion:** 最早用的方法，很傻很天真。每次降噪後的圖片都需要計算一次和輸入文本之間的 CLIP Score。 原本的網路只需要預測雜訊，而現在不但要預測雜訊，還需要讓去噪後的結果圖片盡可能地跟文本接近，也就是 CLIP Loss 盡量小。這樣在不斷去噪的過程中，模型就會傾向於生成和文本相近的圖片了。
    *   由於 CLIP 是在無雜訊的圖片上進行訓練的，還有一個小的細節是需要對 CLIP 模型用加雜訊的圖片再進行 Fine-tune (也就是微調)，這樣 CLIP 才能夠看出來加噪後的牛排還是一塊牛排。

*   這樣做的好處是 CLIP 和 Diffusion Model 都是現成的，只需要在生成過程中結合到一起就行了。但是缺點是，本來已經很慢的 Diffusion Model 現在生成過程就會變得更慢，而且這兩個模型是獨立訓練出來的，沒有辦法聯合訓練，也就沒有辦法讓它們得到進一步的提升。
*   **Classifier Free Diffusion Guidance:** 這個模型可以同時支持無條件和有條件的雜訊估計，而且在訓練 Diffusion Model 時就加入了文本的引導。 這樣的模型當然也離不開很多很多的数据，以及很多很多的 GPU 卡。 除了網路爬取的數據以外，還有透過商業圖庫構造出巨量的圖片和文本對，最後作為成品的 GLIDE 在生成效果上又達到了一次飛躍。

*   雖然現在看的有點簡陋，但是在當時來說已經非常驚人了。
*   恭喜大家，到這裡我們已經追上了 AI 繪畫 21 年末的進度。

**三、以圖生圖 (Image-to-Image)**

*   我們再發散一下，如果是以圖生圖，那這個時候的輸入條件就變成了圖片，那麼要怎麼來控制生成結果呢？
*   其實有幾種不同的方法，也算是不同的流派吧。這裡我們先介紹 3 種。
    *   **方法一：提取圖片的 CLIP 特徵：** 就像文字特徵一樣去引導圖片，這樣生成出來的圖片內容就比較相近，但是結構不一定是相同的。
    *   **方法二：增加雜訊後去噪：** 對輸入的原圖增加幾層雜訊，再以此為基礎進行常規的去噪。透過使用你希望的畫風所對應的咒語，就可以生成和你原圖結構類似、但畫風完全不同的圖片。
    *   而且疊加的雜訊強度越高，生成的圖片和原圖的差距越大，AI 畫畫的發揮空間也就越大。
    *   **方法三： Fine-tune 生成網路 (Dreambooth)：** 用對應的圖片去 Fine-tune (也就是微調) 這個生成網路。打個比方就是，如果我們給模型看很多小狗狗的圖片，讓模型學到這隻小狗狗的樣子，這樣我們只需要再加上一些簡單的詞彙，就可以生成各式各樣的小狗狗了。

*   到這裡，我們已經解釋了電腦是如何生成和真實圖片相似的圖片，以及模型是如何聽懂我們想要它生成什麼，並且給出對應結果的。

*   關於 AI 繪畫的基本原理就已經介紹的差不多了。我們可以發現，其實大部分都是改進性質的工作，但是效果確實很驚人。 在這個過程中，其實會涉及到很多訓練網路的小技巧，這裡我就不再做過多的介紹了。

**四、Stable Diffusion 與 Latent Diffusion Model**

*   然後我們再聊一個比較火爆的模型： Stable Diffusion。 因為它開源而且效果比較好，所以得到了很多人的喜愛。基於 Stable Diffusion 也發展出了像二次元插畫的 NovelAI 等等模型，也都很火爆。
*   如果我們想要解釋 Stable Diffusion 為什麼這麼好，那還要先從 Latent Diffusion Model 談起。

*   讓我們來複習一下 Diffusion Model 的原理。對於一個帶雜訊的輸入圖片，訓練一個雜訊預測 U-Net 網路，讓它能夠去預測雜訊，然後再從輸入中減去雜訊，就得到了去噪後的圖片。
*   一般的 Diffusion Model 是對原始圖片進行加噪去噪，雜訊圖片和原始圖片的尺寸是一樣的。 為了節約訓練資源和生成時間，通常都會使用比較小的圖片尺寸來進行訓練，然後再接一個超分辨率的模型。
*   而在 Latent Diffusion Model 中，Diffusion 的模組被用來生成 VAE 的隱編碼。於是整個流程就變成了這樣：圖片先用訓練好的 VAE 的 Encoder 得到一個維度要小很多的圖片隱編碼。
*   你可以理解為將圖片的訊息壓縮到了一個尺寸更小的空間中，而 Diffusion Model 不再直接處理原圖，而是處理這些隱編碼，最後生成新的隱編碼，再用對應的 Decoder 還原成圖片。
*   這樣相對於直接生成圖片來說，就可以大幅度地減少計算量以及顯存，所以 Stable Diffusion 的生成速度會更快。

*   第二個改進點是增加了更多的訓練數據，而且還多了一個美學評分的過濾指標。簡單的來說就是它只選擇好看的圖片。就好像是，如果你要學會如何畫一個漂亮的畫，那你就要多看看一些大藝術家們的經典作品，這是一樣的。
*   同時，在訓練集裡邊也都是一些漂亮的圖片，一些模糊的有水印的圖片都已經被 Pass 掉了。所以只讓機器從好看的圖片裡邊去學習畫畫。
*   最後，Stable Diffusion 的另一個改進就是用了我們之前提到的 CLIP 模型，來實現了透過文本來控制圖片的生成方向。
*   這裡也順便提一下，二次元畫風的 NovelAI 其實在技術上並沒有什麼新的內容，就是拿海量的二次元圖片去微調原來的 Stable Diffusion 模型。 主要的一些改進包括把 CLIP 用在了倒數第二層，這樣就可以更貼近文本內容的特徵。然後就是把訓練數據擴展為一個長寬比不限的尺寸，這樣就可以容納下完整的人像，以及增加了可以支持的文本輸入的長度，這樣就能夠讓提示語變得更靈活，也更加複雜。

**五、ControlNet**

*   接下來我們再說一下 ControlNet 是如何來控制擴散模型的。
*   由於 Diffusion 超強的學習能力，理論上這個網路是可以還原出訓練集裡的每一張圖片的。所以只要數據足夠多足夠好，模型就可以生成非常好的圖片。

*   和人類學畫畫不同的點在於，如果人的難度是在於說畫不出來，那麼模型的難點就是它不知道該往哪個方向去畫。
*   所以控制模型生成，其實就是想辦法讓模型變得聽話，按照你的指示來生成結果。 之前我們提到 img2img，它的原理就是把左圖加一些高斯雜訊，相當於撒了一些黑胡椒面，然後再把它作為底圖基於它來生成新的圖，所以基本上色塊的分布是非常接近的，但是很難控制的更加細節。

*   而有了 ControlNet 之後，就可以透過任何的條件來控制網路的生成。原來模型只能得到一個文本的生成引導，但是現在它可以聽得懂任何基於圖片所提取出來的訊號，只要你拿一組成對的圖片去訓練就可以了。
*   這個方式出來以後，極大的拓展了這種可玩性，而且官方已經提供了非常多常用的已經訓練好的控制網路。 你可以用 Depth 來控制結構，生成各種場景。 比如你可以直接拿線稿來上色，也可以隨便塗幾筆就生成複雜的圖片，還可以透過姿態檢測來生成很好的多人結果。 甚至你可以自己訓練解決 AI 不會畫手的問題。

*   這些控制結果還可以一起使用。比如說結合人體姿態和深度圖來生成一張新的圖片，甚至它都不需要是來自於同一張圖。
*   雖然 ControlNet 效果很驚艷，但是原理上其實比較簡單。 為了給原始的模型增加一些額外的條件輸入，其實就是把整個網路複製了一份，固定讓原來的網路來保證輸出的穩定性。 原始的網路輸入依然是雜訊，而複製的 ControlNet 的輸入則是控制條件，比如說深度、姿態等等。

*   把這兩個輸入和輸出加起來，再用成對的數據集去訓練控制網路。這樣就能夠很好的去控制訓練的程度和結果。 這個訓練的本質上還是在做微調，所以耗時也不是很大，和直接微調整個網路也差不太多。
*   現在 ControlNet 對於結構生成已經能做到不錯了，這時候就會面臨另一個問題： 模型的細節，要怎麼讓它能夠生成的更好？

**六、超分 (Upscaling)**

*   想要得到更高質量的圖片，最直接的方式就是調大輸出的分辨率，分辨率越大細節畫的就越好，尤其是對於人臉來說。 但是實際上高分辨率的結果就非常容易崩掉，並且分辨率高了以後計算的成本會飆升，也會算的很慢。
*   於是，一種常見的做法是先生成一些較小分辨率的結果，然後對這些圖片做超分，也就是在保證清晰度的前提下把圖片放大。這麼做可以保證結構的合理性，而且速度會快非常多。
*   但是超分的模型對於細節的補充並不一定能做到很自然，而且容易有過於銳化的結果。
*   除了傳統的超分模型，還有同樣基於 Diffusion Model 的超分算法。由於 Diffusion 相當於重繪了這個圖片，所以可以得到更好的一些細節效果。但是圖片尺寸非常大跑起來就會更慢。

*   另一個現在被廣泛使用的方式是 Latent Upscale。 這個用 WebUI 自帶的 Hires.fix 就可以實現。
*   之前我們也提到過 Stable Diffusion 的結構優勢之一就是它是由一個壓縮圖片訊息的 VAE 和一個對 Latent 進行去噪的 U-Net 網路所組成的。
*   所以它天然適合基於 Latent 的超分算法。 對 Latent 做 Upscale 也有基於 Diffusion 的方法，並且效果應該是最好的，當然帶來的代價就是耗時也會增加。
*  (這裡省略超分方法的對比) 這張圖片就是 Latent Upscale 和 ControlNet 一起結合得到的。

**七、LoRA (Low-Rank Adaptation)**

*   除了 ControlNet 之外，AI 繪畫領域另一個不得不提的就是 LoRA 模型。要講 LoRA 模型，就要先解釋模型的 Fine-tune，也就是什麼是微調。
*   其實就是當你有一個現成的很厲害的大模型，也就是一個預訓練的模型的時候，你想要讓它學習一些新的知識，或者完成一些更面向於具體應用任務，或者只是為了適應你的這個數據分布的時候，就需要拿你的小樣本的數據去對這個模型進行重新的訓練。
*   這個訓練還不能訓太久，否則模型就會過擬合到這個你的小樣本數據上，從而喪失掉大模型的這個泛化能力。 Pretrain 和 Fine-tune 是機器學習中非常常見的組合，在應用上有很大的價值。
*   但是其中有一個問題就是它會遺忘。模型會在 Fine-tune 的這個過程中不斷忘記之前已經記住的內容。 常見的解決方式會有兩個：
    *   第一個是 Replay，就是把原始的知識再過一遍。
    *   第二個是正則化，就是透過正則項來控制模型的參數和原始參數盡量保持一致，不要變得太多。
    *   當然還有一個就是 Parameter Isolation (也就是參數孤立化)。 這個是透過獨立出一個模組來做 Fine-tune，使得這個原有的模型不再更新權重。
*   參數孤立化其實是最有效的一種方式，具體有好幾種實現方式。 例如 Adapter，就是在原模型中增加一個子模組，固定原模型只訓練這個子模組。 是不是聽起來有點熟悉呢？沒錯，ControlNet 就是一種類似於 Adapter 的方法。

*   而 LoRA 就是另一種參數孤立化的策略，也在 AI 繪畫這個領域找到了它的用武之地。 它利用了低秩矩陣來代替原來的全量參數進行訓練，從而提高了 Fine-tune 的效率。
*   我們可以和之前最常用的 Fine-tune 方法 DreamBooth 來對比一下。 對於 DreamBooth 來說，它是直接更新整個大模型的權重來讓模型學習到新的概念。 雖然可以透過正則化來避免遺忘，但是 Fine-tune 後的模型依然非常大，和原來的模型一樣大。
*   而使用 LoRA 之後，LoRA 影響的只是其中的一小部分權重，也就是透過低秩矩陣疊加到大模型網路上的那一部分。 所以微調起來就會更快，需要的資源更少，而且得到的微調模型會非常小，使用起來就會方便很多。
*   由於 LoRA 在結構上是獨立於大模型的，所以它有一個額外的好處，就是替換大模型可以得到不同的令人驚喜的一些結果。 比如說我們用水墨畫來訓練的 LoRA 模型墨心，結合國風美女的基礎大模型，就可以生成一個穿著中式服裝的水墨畫美少女。

*   在使用上來說，LoRA 就很像這個模型的一個插件，可以在基礎模型上去疊加想要的效果，或者把各種想要的效果加權組合疊加在一起，這樣就可以產生很多令人驚喜的結果。
*   當然，由於 LoRA 是一個微調模型，所以畫風會趨於單一，那這樣是好是壞也是見仁見智。但是如果你使用現實中的真人照片來訓練 LoRA 並且把這個模型公開了，那我覺得是一件非常缺德的事情，希望大家也不要這麼做。

**八、總結**

*   好了，今天的内容大概就介绍到这里。 我们大概完整介绍了AI绘画的整个流程，以及相关的模型算法。 当然由于时间有限，还有很多内容是没有被覆盖的，比如像Midjourney DALL-E等等，以后有机会大飞再给大家一一介绍，感兴趣的小伙伴们欢迎订阅我们的频道，我们下期再见。

**總結來說，這次整理做了以下調整：**

*   **組織結構：** 將內容分成幾個大章節，並在每個章節下使用小標題，讓結構更清晰。
*   **語言潤飾：** 修正錯別字，調整語句，使表達更流暢。
*   **重點突出：** 加粗重點詞語和概念，方便讀者快速理解。
*   **補充說明：** 在某些地方增加說明，使內容更容易理解。
*   **格式調整：** 調整了項目符號和排版，讓整體看起來更整潔。

希望這個整理後的版本對您有幫助！

[model=gemini-2.0-flash,0]
