好的，這是整理後的文稿，我將重點放在提煉文章主旨、架構，並簡化部分贅詞，使其更易於理解。

**整理後文稿：**

本期《最佳拍檔》解读 Semianalysis 万字独家爆料，主要揭露 OpenAI 的 o1 Pro 架构，并解答了以下问题：

*   **Claude 3.5 Opus 是否失败？** Anthropic 将其秘密应用于“内部数据合成”和“强化学习奖励建模”，显著提升模型性能。
*   **OpenAI 的 Orion 模型发展如何？** 用于生成海量“草莓训练”数据，及用于各种验证器和奖励模型。
*   **Scaling Law 是否能持续？** 新技术范式的出现和扩展将持续推动 AI 领域 Scaling Laws 发展。

**文章重点内容：**

1.  **o1 和 o1 Pro 推理架构：**

    *   o1 采用思维链 (Chain of Thought, CoT) 方法，但在推理过程中只沿单一 CoT 前进，不依赖搜索。
    *   o1 Pro 采用自洽性 (self-consistency) 或多数投票 (majority vote) 方法，虽然表面成本高，但实际成本增加低于价格上涨幅度。
2.  **草莓训练 (Berry Training) 系统：**

    *   OpenAI 通过蒙特卡洛树生成海量合成数据。
    *   模型基于过程奖励模型 (PRM) 或优化奖励模型 (ORM) 针对问题生成众多变体和轨迹。
    *   功能验证器 (functional verifiers) 检查数学计算或运行代码验证数据正确性。
3.  **推理训练 (Inference Training)：**

    *   OpenAI 正在训练介于 GPT-4o 和 Orion 之间的模型，后训练 FLOPs 将超过预训练。
    *   推理训练的兴起使得后训练不再局限于微调，需要更多计算量。
4.  **Token 经济学：**

    *   推理模型比同等规模的非推理模型生成更多输出 token，导致成本更高。
    *   序列长度增加会导致内存和 FLOPs 需求增加，进而影响批大小和每 GPU 吞吐量，使每 token 服务成本明显提高。
    *   长序列长度还可能导致可靠性问题，例如错误累积。
5.  **计算领域 Scaling Laws：**

    *   预训练规模将继续扩大，推理训练将需要更多计算量。
    *   调整 CoT 的长度和计算资源使用将成为测试时计算的关键技术。
    *   Scaling 预训练目前仍可大幅降低成本，且超大规模计算提供商将继续建设更大的集群。

**总结：**

文章深入剖析了 o1 Pro 的架构，探讨了预训练 scaling 和推理 scaling 的发展。Scaling Laws 会随着新技术范式的出现而持续。

**建议：**

建议阅读原文以了解更多细节。

**注意：**

*   我保留了關鍵詞彙，以便讀者能夠連結到原始材料。
*   我簡化了部分細節，使其更易於理解。
*   如果需要更詳細的整理，請提供更多指示。

希望這個版本對您有幫助！

[model=gemini-2.0-flash,0]
