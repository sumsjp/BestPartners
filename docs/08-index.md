<details>
<summary>803. [2025-11-11] 【人工智能】谷歌最新论文提出嵌套学习概念 | Nested Learning | 深度学习的幻觉 | 持续学习 | 人脑的神经可塑性 | 记忆巩固 | 嵌套优化 | 深度优化器 | HOPE模型</summary><br>

<a href="https://www.youtube.com/watch?v=ljYXKurry9o" target="_blank">
    <img src="https://img.youtube.com/vi/ljYXKurry9o/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【人工智能】谷歌最新论文提出嵌套学习概念 | Nested Learning | 深度学习的幻觉 | 持续学习 | 人脑的神经可塑性 | 记忆巩固 | 嵌套优化 | 深度优化器 | HOPE模型

好的，我將這篇文稿整理如下，主要目標是：

*   **精簡冗詞贅句：** 刪除一些口語化的表達，讓文稿更精煉。
*   **組織結構優化：** 將內容重新組織，使其邏輯更清晰。
*   **專業術語統一：** 確保專業術語使用一致且準確。
*   **要點突出：** 更加突顯文章的核心觀點和實驗結論。

**整理後文稿：**

大家好，這裡是最佳拍檔，我是大飛。

近年來，大語言模型帶來了許多震撼。然而，這些看似強大的模型存在一個先天缺陷：本質上是靜態的，訓練後難以再學習新知識。它們只能依賴有限的上下文窗口進行即時適應，一旦超出範圍，就會忘記之前的信息。更關鍵的是，深度學習數十年來依賴的層疊結構，在解決複雜算法實現和持續學習等問題上，效果並不如預期。

今天，我們將解讀一篇來自谷歌研究院的重磅論文：《嵌套學習：深度學習架構的幻象》（Nested Learning: The Illusion of Deep Learning Architectures）。該論文提出了**嵌套學習 (NL)** 這一全新的學習範式，從數學上重構了深度學習的底層邏輯，並以此設計出了名為 **HOPE** 的模型。在語言建模和常識推理任務中，HOPE 超越了 Transformer、RetNet 等主流架構。

**一、當前深度學習的核心痛點**

作者指出，層疊思維存在以下四個局限：

1.  模型的計算深度並不會隨著層數增加而無限提升。
2.  部分參數的容量提升會隨著深度或寬度的增加而邊際遞減。
3.  訓練過程容易陷入次優解。
4.  模型的快速適應能力、持續學習能力和分布外泛化能力幾乎不會因為堆疊更多的層而得到提升。

這種情況導致模型訓練結束後，就無法再高效學習新知識，類似於順行性遺忘症患者。預訓練階段學到的知識被儲存在MLP層中，相當於發病前的長期記憶；而推理時的上下文信息只能暫時存在注意力機制中，相當於即時的短期記憶。

**二、嵌套學習（NL）：模仿人腦記憶機制**

論文的核心思路是，借鑒人腦的多時間尺度記憶鞏固機制，構建一種嵌套式的學習框架，讓模型的每個組件都能在不同的時間尺度上更新，形成多層次的記憶系統。人腦的持續學習能力源於神經可塑性，記憶的鞏固過程主要分為兩個互補的階段：

1.  **線上鞏固（synaptic consolidation）：** 學習後立即或不久進行，穩定新的、脆弱的記憶痕跡，並開始從短期記憶向長期記憶轉移。
2.  **離線鞏固（systems consolidation）：** 主要發生在睡眠中，通過海馬體的尖波漣漪（SWRs）與大腦皮層的睡眠紡錘波和慢波協調，重複回放最近編碼的記憶模式，強化並重組記憶，最終將其轉移到大腦皮層中進行長期存儲。

**三、嵌套學習 (NL) 的核心概念**

嵌套學習是一種將機器學習模型及其訓練過程表示為一組嵌套的、多層次的、並行的優化問題的範式。每個優化問題都有自己的上下文流（context flow）。

理解嵌套學習的關鍵在於明確以下兩個概念：

1.  **關聯記憶 (Associative Memory) 與學習的區別：** 記憶是輸入引起的神經更新，而學習是獲取有效且有用記憶的過程。
2.  **更新頻率 (Update Frequency)：** 對於模型的任何組件，其頻率是單位時間內的更新次數。

基於更新頻率，可以對模型組件進行排序。如果組件 A 的更新頻率高於組件 B (f\_A > f\_B)，或者 A 和 B 頻率相同、但 B 的計算依賴 A 的狀態，那麼 A 比 B 更快（記為 A ≻ B）。如果頻率相同且計算獨立，則記為 A ≡ B。

**四、嵌套學習的三大核心貢獻**

基於嵌套學習的視角，論文對傳統深度學習的組件進行了重構和升級：

1.  **深度優化器 (Deep Optimizers)：** 論文指出，常用的基於梯度的優化器本質上都是關聯記憶模塊。作者提出了四種更具表達力的優化器擴展方向：更具表達力的關聯、更具表達力的目標函數、更具表達力的記憶、非線性輸出。
2.  **自修改 Titans (Self-Modifying Titans)：** 自修改 Titans 將優化器的更新規則本身作為學習對象，通過嵌套的優化問題，讓模型在訓練過程中不斷調整更新策略，從而適應不同的任務和數據分布。
3.  **連續記憶系統 (Continuum Memory System, CMS)：** 傳統的記憶系統被劃分為短期記憶和長期記憶。論文提出的連續記憶系統是一組 MLP 塊的鏈，每個 MLP 塊都對應一個特定的更新頻率。

**五、 HOPE 模型**

論文最終構建出了 HOPE 模型，一個具有自指學習能力和連續記憶的嵌套學習模塊。HOPE 的架構核心是將自修改 Titans 與連續記憶系統融合。連續記憶系統提供多層次的記憶存儲，自修改 Titans 提供動態的更新算法學習。

**六、實驗結果**

論文在語言建模和常識推理兩大類任務上進行了實驗，對比的基線模型包括 Transformer++、RetNet、DeltaNet、TTT、Samba、Titans（LMM）等主流架構。

*   **語言建模：** 1.3B 參數的 HOPE 在 Wiki 數據集上的困惑度 (ppl) 達到了 15.11，優於同等參數規模的 Titans（LMM）、Transformer++ 和 RetNet。
*   **常識推理：** HOPE 在多個常識推理任務上都超越了基線模型，平均準確率達到 57.23%。

實驗結果證明了嵌套學習範式的有效性。通過多時間尺度的記憶更新和自修改的學習算法，模型能更好地捕捉數據中的依賴關係，提升表達力和泛化能力。

**七、嵌套學習的影響與局限**

嵌套學習為深度學習開闢了一個新的維度，即層級深度（level depth）。它為解決持續學習、長上下文推理等傳統難題提供了新的思路。此外，它將神經科學與深度學習的結合推向了新的高度。

然而，嵌套學習也存在一些局限性，例如計算複雜度較高，理論分析需要進一步完善等。

感謝收看本期視頻，我們下期再見。

[model=gemini-2.0-flash,0]


---

</details>

<details>
<summary>802. [2025-11-10] 【人工智能】AI是产业革命还是一场泡沫 | 六位AI奠基人罕见齐聚讨论 | 杰弗里·辛顿 | 杨立昆 | 约书亚·本吉奥 | 黄仁勋 | 比尔·达利 | 李飞飞 | 顿悟时刻 | AGI何时到来</summary><br>

<a href="https://www.youtube.com/watch?v=Ca1Gxg3J_Lc" target="_blank">
    <img src="https://img.youtube.com/vi/Ca1Gxg3J_Lc/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【人工智能】AI是产业革命还是一场泡沫 | 六位AI奠基人罕见齐聚讨论 | 杰弗里·辛顿 | 杨立昆 | 约书亚·本吉奥 | 黄仁勋 | 比尔·达利 | 李飞飞 | 顿悟时刻 | AGI何时到来

好的，我將這篇文稿整理如下，重點在於提取核心信息，並將內容結構化，方便理解：

**文稿整理：AI 奠基人的世紀對談：泡沫、AGI 時間線與頓悟時刻**

**核心議題：**

1.  **AI 熱潮是產業革命還是泡沫？**
2.  **AGI (通用人工智慧) 何時到來？**

**對談背景：**

*   2025 年，六位 AI 領域奠基人齊聚一堂，共同獲得伊麗莎白女王工程獎。
*   他們回顧了 AI 從無人問津到席捲全球的 40 年，並辯論了以上兩個核心問題。

**參與者 (六位奠基人)：**

*   **深度學習三巨頭：**
    *   杰弗里·辛顿 (Geoffrey Hinton)
    *   楊立昆 (Yann LeCun)
    *   約書亞·本吉奧 (Yoshua Bengio)
*   **英偉達 (NVIDIA)：**
    *   黃仁勳 (Jensen Huang)，CEO
    *   比爾·達利 (Bill Dally)，首席科學家
*   **李飛飛 (Fei-Fei Li)**

**一、奠基人的頓悟時刻 (AI發展的導火索):**

*   **約書亞·本吉奧：**
    *   **第一次：** 研究生時期讀辛頓的論文，發現智能可以透過簡單的原則構建。
    *   **第二次：** GPT 問世後，意識到 AI 的目標不受控制可能帶來的問題，轉向 AI 安全和目標對齊。
*   **比爾·達利：**
    *   **第一次：** 1990 年代末，解決記憶體牆問題時，想到讓計算跟著數據走，設計了流處理架構 (GPU 的核心設計)。
    *   **第二次：** 2010 年和吳恩達的早餐會，發現 GPU 在訓練神經網路上的優勢，促使英偉達轉向 AI 卡。
*   **杰弗里·辛顿：**
    *   1984 年，語言模型實驗中，發現模型能自動理解單詞的含義，意識到這是讓機器理解語言的關鍵。
*   **黃仁勳：**
    *   2010 年，同時收到三個實驗室的信號，得知 GPU 在深度學習模型訓練上的良好效果，決定英偉達不僅做 GPU 硬件，還要打造 AI 軟件生態。
*   **李飛飛：**
    *   **第一次：** 2006 年，意識到數據是 AI 的瓶頸，創建了 ImageNet 數據集。
    *   **第二次：** 2018 年在谷歌，意識到 AI 是一項文明級技術，創立了以人為本的 AI 研究所。
*   **楊立昆：**
    *   本科時期，發現讓機器自我學習的想法，並在看到辛頓的論文後，堅定了這個方向。

**二、AI 熱潮是產業革命還是泡沫？**

*   **非泡沫派：**
    *   **黃仁勳：** GPU 的使用率高，算力需求真實存在。AI 增強的是腦力，是千年一遇的變革。
    *   **比爾·達利：** 模型效率提升、模型能力增強、應用場景爆發，多重指數增長的早期階段。
*   **部分泡沫派：**
    *   **楊立昆：** 大語言模型有價值，但認為過度依賴擴大模型參數規模可能導致資源浪費。缺乏對真實世界的理解。
    *   **李飛飛：** AI 是一個年輕的領域，過度集中投資於大語言模型可能導致局部過熱，忽略其他領域發展。
    *   **約書亞·本吉奧：** 很多企業對 AI 的短期預期過高，可能導致投資撤資。

**三、AGI 何時到來？**

*   **激進預測派：**
    *   **約書亞·本吉奧：** 5 年內，AI 就能達到一名員工的工作能力水平。
    *   **杰弗里·辛顿：** 20 年內，機器在辯論中總能勝過你。
*   **工程務實派：**
    *   **黃仁勳：** AGI 何時到來並不重要，重要的是 AI 現在能轉化為大量對社會有用的應用。
    *   **比爾·達利：** 這是一個錯誤的問題，目標不是取代人類，而是增強人類。
*   **細致解構派：**
    *   **李飛飛：** AI 和人類智能是互補而非替代關係，不存在誰達到誰的水平的說法。
    *   **楊立昆：** AGI 不會是一個單一事件，而是能力逐步擴展的漫長過程，需要基礎科學的突破。

**總結：**

*   六位先驅在泡沫、AGI 等問題上存在分歧，但都認為 AI 的未來需要技術突破與人文關懷的結合。
*   基於事實的爭論是科學進步的動力。

希望這個整理對您有所幫助!

[model=gemini-2.0-flash,0]


---

</details>

<details>
<summary>801. [2025-11-09] 【访谈】红杉资本新掌舵人的投资哲学 | Alfred Lin | 四个标准差的创始人 | 熔炉时刻 | Zipline | Airbnb | 同期群曲线 | AI的长期稳定点 | 重塑用户体验</summary><br>

<a href="https://www.youtube.com/watch?v=cXktR1AaRNY" target="_blank">
    <img src="https://img.youtube.com/vi/cXktR1AaRNY/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【访谈】红杉资本新掌舵人的投资哲学 | Alfred Lin | 四个标准差的创始人 | 熔炉时刻 | Zipline | Airbnb | 同期群曲线 | AI的长期稳定点 | 重塑用户体验

好的，我來幫你整理這份文稿。我會將其整理成一個更結構化、重點突出的版本，方便理解和回顧。以下是整理後的文稿：

**主題：紅杉資本新掌舵人Alfred Lin投資哲學拆解**

**核心觀點：紅杉不談風口，只找挑戰主流、重構世界的異類創始人。**

**一、什麼是「異類創始人」？**

*   **四個標準差：**
    *   紅杉平均看1000家公司，最終投1家，對應0.003%的機率，也就是「萬里挑一」。
    *   重點不在稀缺性，而在於「獨特且新穎的見解」。
    *   真正的異類能看到「世界上被錯誤解決的問題」。
*   **創始人分類與幫扶方式：**
    *   **夢想家：** 描繪宏大願景，但細節不清晰。紅杉會協助尋找工程師、梳理供應鏈。 (例如：馬斯克早期)
    *   **解決自身問題的人：** 了解用戶痛點，但不擅長融資。紅杉會協助對接資本、搭建商業模式。(例如：DoorDash)
    *   **技術型創始人：** 懂技術細節，但不明白商業價值。紅杉會協助做市場調研、翻譯成商業語言。

**二、轉型思維案例：**

*   **Zipline (無人機醫療配送)：**
    *   最初方向：機器人移動平台（玩具）。
    *   轉型方向：無人機醫療配送。
    *   Alfred Lin的提問：技術如何解決？美國法律限制如何解決？
    *   解決方案：學習技術，去美國以外的地方（盧旺達）飛行。
    *   結果：在盧旺達成功運營，獲得美國FAA許可，美國軍方開始合作。
*   **Airbnb (疫情期間)：**
    *   危機：疫情導致收入暴跌80%。
    *   關鍵決策：
        *   **聚焦新優勢：** 短途遊、虛擬體驗。
        *   **融資選擇：** 避免稀釋股權，選擇債務融資（銀團貸款）。
        *   **削減開支：** 先砍行銷費用、租金、承包商，最後裁員並給予補償。
    *   「熔爐時刻」：逆境才能看出公司文化的強度。

**三、反對「收入競賽」，重視潛在指標：**

*   用收入作為唯一衡量標準是懶惰的思維，可能燒錢燒出來的。
*   更關注「同期群曲線」（Cohort Analysis）：看用戶留存率，判斷是否為真實需求。
*   **飛輪效應：** 真實留存是飛輪效應的起點，形成正向循環。

**四、速率 (Velocity) vs. 速度 (Speed)：**

*   速度 (Speed)：僅僅追求快速成長。
*   速率 (Velocity)：有方向的成長。

**五、對AI的清醒看法：**

*   技術迭代快，對創始人是挑戰也是機會。
*   創業公司可以更靈活地all in新方向。
*   AI的長期穩定點是改變世界，重塑消費者體驗。
*   例子：個性化學習計劃，偏遠地區專家診斷。

**六、公司文化：**

*   不是標語，而是在艱難時刻的選擇。
*   適應技術迭代 (例如：Coinbase要求員工學AI)。
*   保持Day 1的理念（危機感和創新欲）。
*   在變化中保持核心 (例如：Zappos的客戶體驗)。
*   例子：Clay從程式設計教育轉型GTM服務再轉AI工具。

**七、投資邏輯與未來展望：**

*   不只投AI公司 (例如：Citadel Securities)。
*   看重風險控制，考慮最壞情況。
*   期待AI從自動化轉向重塑用戶體驗。
*   例子：每個孩子都有私人教師，癌症診斷提前半年。

**八、給創始人的建議：**

*   不要被短期熱點迷惑。
*   用AI解決具體行業痛點 (例如：AI預測病蟲害，AI優化生產流程)。

**九、總結：**

AI的價值在於重塑消費者體驗、商業模式，只有用留存率、飛輪效應與產品市場契合度這些指標，才能找出能創造長期價值的人。

**我的整理方式：**

*   **提煉重點：** 抓取文稿中的核心觀點和關鍵資訊。
*   **結構化呈現：** 將內容分點分段，方便閱讀和理解。
*   **使用例子：** 用實際案例來佐證觀點，使內容更具體。
*   **簡化語言：** 避免過多的專業術語，用更易懂的語言表達。
*   **關鍵字加粗：** 突出重要名詞和概念。

希望能對你有所幫助！ 如果您有其他需求，請隨時提出。

[model=gemini-2.0-flash,0]


---

</details>

