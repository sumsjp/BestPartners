<details>
<summary>600. [2025-03-23] 【人工智能】大语言模型不会带来AGI | 图灵奖得主Yann LeCun最新访谈 | LLM的局限性 | 缺乏物理世界理解 | 推理缺陷 | V-JEPA | OpenAI | 过度炒作 | 开源优势</summary><br>

<a href="https://www.youtube.com/watch?v=NmCPbqlLjzo" target="_blank">
    <img src="https://img.youtube.com/vi/NmCPbqlLjzo/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【人工智能】大语言模型不会带来AGI | 图灵奖得主Yann LeCun最新访谈 | LLM的局限性 | 缺乏物理世界理解 | 推理缺陷 | V-JEPA | OpenAI | 过度炒作 | 开源优势

好的，我來為您整理這篇文稿，使其更清晰、更易讀：

**主題：楊立昆專訪重點：大語言模型的局限與AGI的未來**

**引言：**

大家好，我是大飛。在人工智能飛速發展的今天，各種新技術、新應用層出不窮。但繁華背後，AI的發展階段為何？未來方向為何？如今的大模型真能帶領我們走向AGI（通用人工智能）嗎？

**核心觀點：**

圖靈獎得主、Meta公司首席AI科學家楊立昆在接受亞歷克斯·坎特羅維茨的專訪中，對當前的大語言模型提出質疑，認為其存在諸多局限性。

**一、大語言模型的局限性：**

*   **缺乏創新能力：** 大語言模型擅長知識檢索和文本生成，但無法提出自己的科學發現。它們像是「知識淵博的復讀機」，無法創造新知識。
*   **缺乏對物理世界的理解和推理能力：** 大語言模型基於文本數據訓練，缺乏對物理世界的真正認知，無法像幼兒一樣通過觀察和體驗學習物理常識。
*   **推理機制存在缺陷：** 大語言模型缺乏真正的推理機制，無法像人類和動物一樣基於對情況的心理模型在大腦內部進行推理和規劃。
*   **缺乏抽象的精神表徵：** 大語言模型不具備心理模型，無法在內部進行推理和規劃，只能根據已有的文本數據生成看似合理的回答。

**二、大語言模型發展的瓶頸：**

*   **訓練數據耗盡：** 大語言模型已經耗盡了可用的自然文本數據，繼續擴展大語言模型難以獲得更大的回報。
*   **需要新型系統架構：** 迫切需要一種新型的系統架構，具備尋找有效解決方案、評估方案質量、以及規劃一系列行動以實現特定目標的能力。

**三、楊立昆的研究方向：V-JEPA**

*   **V-JEPA（視頻聯合嵌入預測架構）：** 一種全新的非生成式架構，不再試圖重建輸入的一部分，而是生成一種輸入的抽象表徵，能忽略輸入中真正不可預測的細節，專注於學習視頻的內在結構和抽象特徵。
*   **V-JEPA的優勢：**
    *   具備一定的物理常識，能識別不可能的視頻場景。
    *   可以通過表示世界在某一時刻的狀態和採取的行動，來預測下一時刻的狀態，進而實現推理和計劃功能。

**四、對OpenAI發展路線的擔憂：**

*   **擴展大語言模型並非AGI的唯一途徑：** 楊立昆認為，僅僅依靠擴展大語言模型，無論投入多少資金和數據，都無法達到人類水平的人工智能。
*   **發展方向存在不確定性：** OpenAI希望通過大量數據訓練讓系統能夠回答任何問題，但這與真正具備創新能力和解決新問題能力的智能還有很大差距。

**五、AI領域的發展風險：**

*   **AI泡沫：** 當前AI領域的過度炒作可能導致AI寒冬的再次降臨。
*   **實際部署的挑戰：** 部署可靠的AI系統仍然面臨著巨大挑戰，例如自動駕駛技術和IBM Watson的失敗案例。

**六、开源的优势：**

*   **創新：** 開源可以從全球招募人才，匯聚各種不同的想法和創造力。
*   **成本：** 運行開源模型的成本更低。
*   **安全：** 開源解决方案提供了更多的控制權，可以在本地運行，大大提高了安全性。
*   **打破地域和機構的限制：** 讓世界各地的科學家都能參與到AI的發展中來。

**七、對AGI的判斷：**

*   **AGI是持續發展的過程：** AGI不會有一個秘密的靈丹妙藥，需要許多不同的想法、大量的努力以及一些作為基礎的原則，是一個持續的、概念性的發展過程。

**結語：**

我們確實看到了AI的巨大發展，但也要看到背後的缺陷和風險。找到更好的解決辦法，提出正確的問題，才能讓AI更好地發展。希望今天的分享能讓大家對AI的現狀和未來有更加清晰的認識。

**改進說明：**

*   **標題：** 更簡潔明瞭地概括了文章的主題。
*   **結構：** 使用了更清晰的標題和副標題，使文章結構更清晰。
*   **重點提取：** 提取了楊立昆專訪中的主要觀點，並進行了歸納整理。
*   **語言：** 調整了部分語句，使其更簡潔、易懂。
*   **排版：** 使用了項目符號、加粗等排版方式，突出重點。

希望這個整理後的版本對您有所幫助！

[model=gemini-2.0-flash,0]


---

</details>

<details>
<summary>599. [2025-03-22] 【商业】Intel任命新CEO陈立武 | 英特尔股价相应上涨 | 核工程硕士 | 转战投资领域 | 华登国际 | 投资新浪 | 救火楷登电子 | 与帕特基辛格的分歧 | 四大半导体巨头CEO均为华人</summary><br>

<a href="https://www.youtube.com/watch?v=V5gZ3kph33Y" target="_blank">
    <img src="https://img.youtube.com/vi/V5gZ3kph33Y/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【商业】Intel任命新CEO陈立武 | 英特尔股价相应上涨 | 核工程硕士 | 转战投资领域 | 华登国际 | 投资新浪 | 救火楷登电子 | 与帕特基辛格的分歧 | 四大半导体巨头CEO均为华人

好的，這是我整理後的文稿，主要著重於提升結構清晰度、強化重點、以及適當潤飾：

**標題：英特爾能否重返榮耀？新任CEO陳立武的挑戰與機遇**

**引言:**

大家好，這裡是最佳拍檔，我是大飛。今天我們再次做一期與英特爾有關的節目，來聊聊這個半導體領域昔日王者的最新進展。曾幾何時，英特爾凭借著先進的技術和廣泛的市場份額，在半導體行業呼風喚雨，是PC時代當之無愧的巨頭。

**英特爾的困境:**

但近年來，英特爾的發展之路卻充滿了坎坷，陷入了前所未有的困境：

*   **財務困境：** 2024年，全年淨虧損高達188億美元，市值大幅縮水超過一半。
*   **負面消息：** 裁員、賣廠、業績下滑，甚至被踢出道瓊斯指數，形象一落千丈。
*   **競爭壓力：** 英偉達、AMD等後起之秀迅速崛起，在技術創新和市場拓展方面不斷發力，給英特爾帶來了巨大的壓力。

**新任CEO：陳立武**

在這樣的背景下，英特爾董事會任命陳立武（Lip-Bu Tan）為新任CEO，3月18日正式走馬上任。消息一出，英特爾股價盤後飆漲12%，市場似乎看到了一絲希望的曙光。

*   **市場反應：** 陳立武的任命讓資本市場迅速做出反應，顯示市場對其帶領英特爾走出困境的期待。
*   **核心問題：** 陳立武究竟是何許人也？他為什麼能讓市場對英特爾的未來充滿期待？

**陳立武的背景與經歷:**

陳立武現年65歲，祖籍中國福建福清，出生於馬來西亞，成長於新加坡，他的人生軌跡堪稱一部跨界傳奇：

*   **教育背景：** 新加坡南洋理工大學物理系，麻省理工學院核工程碩士。
*   **早期職業生涯：** 核能企業EDS Nuclear的首席工程師（因三哩島核電站事故轉行）。
*   **商業轉型：** 舊金山大學MBA，後轉戰科技創投領域。
*   **創辦華登國際：** 1987年，以300萬美元創立華登國際，專注投資初創科技公司，尤其青睞半導體產業。
*   **投資帝國：** 華登國際管理的資本超過30億美元，投資遍及全球500多家企業，其中120餘家聚焦半導體領域，投資包括新浪、美團、中芯國際、兆易創新等知名企業。

**陳立武的優勢:**

*   **投資眼光：** 對半導體產業有獨到的眼光和精準的投資策略。
*   **了解中國市場：** 熟悉中國市場，是美國資本進入中國科技領域的最佳代理人。
*   **救火經驗：** 曾臨危受命，帶領楷登電子走出困境，並獲得半導體界最高榮譽羅伯特·諾伊斯獎。
*   **戰略眼光：** 加入英特爾董事會後，對IDM 2.0戰略提出不同意見，被認為具有更務實的戰略眼光。

**陳立武與英特爾:**

*   **加入英特爾董事會：** 2022年加入，協助推動IDM 2.0戰略。
*   **理念分歧：** 與時任CEO帕特·基辛格在IDM 2.0戰略上存在分歧，認為過於激進。

**英特爾選擇陳立武的原因:**

*   **深厚的行業積澱和資本手腕：** 兼具戰略眼光與執行力的“複合型玩家”。
*   **了解中國市場：** 在中國這個全球最大的半導體消費市場（佔全球40%）的人脈和資源，有望幫助英特爾取得突破。

**面臨的挑戰:**

*   **資金壓力：** 晶圓廠的高投入導致現金流捉襟見肘。
*   **技術落後：** 在製程工藝方面落後於台積電。
*   **市場份額被擠壓：** AMD和英偉達等競爭對手在x86處理器和數據中心市場上佔據優勢。
*   **公司文化重塑:** 改善公司內部團結，重塑工程驅動的文化。

**轉機與機會:**

*   **潛在的投資：** 台積電可能聯合英偉達、AMD等企業投資英特爾的晶圓廠，緩解資金壓力。
*   **AI推理市場：** 英特爾的Xeon和Gaudi產品線或許能在AI推理階段扳回一局。

**陳立武的全員信:**

*   **榮幸與決心：** 對出任英特爾CEO感到榮幸，並展現出堅定的信心和決心。
*   **領導原則：** 謙遜、勤奮和以客戶為先。
*   **戰略方向：** 回歸工程驅動本質，打造卓越產品、傾聽客戶需求、兌現承諾。

**總結：**

陳立武的上任為英特爾帶來了新的希望，但也面臨著嚴峻的挑戰。他的成功與否不僅關乎英特爾的未來，也關乎整個半導體產業的格局。他能否帶領這家藍色巨人走出至暗時刻，重回巔峰？這不僅是陳立武個人的地獄級試煉，更是藍色巨人的生死大變局。

**結尾：**

好了，今天的視頻內容就到這裡。大家對於陳立武執掌英特爾有什麼看法呢？歡迎在評論區留言。感謝觀看，我們下期再見！

**整理說明:**

*   **精簡重複資訊：** 刪除一些重複的描述，例如多次提及“重返巔峰”。
*   **邏輯更清晰：** 將文稿分成更明確的段落，更易於理解。
*   **強化重點：** 用加粗和條列式清單突出重點信息。
*   **適當潤飾：** 調整部分語句，使表達更流暢自然。
*   **標題優化：** 標題更具吸引力，點明核心內容。

希望這個版本對您有幫助！

[model=gemini-2.0-flash,0]


---

</details>

<details>
<summary>598. [2025-03-21] 【人工智能】Nvidia GTC 2025 | 黄仁勋主题演讲 | Blackwell Ultra | 下一代Rubin | CPO产品 | Dynamo | 软件和Agent生态 | 具身智能</summary><br>

<a href="https://www.youtube.com/watch?v=4wZYrzC-pTo" target="_blank">
    <img src="https://img.youtube.com/vi/4wZYrzC-pTo/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【人工智能】Nvidia GTC 2025 | 黄仁勋主题演讲 | Blackwell Ultra | 下一代Rubin | CPO产品 | Dynamo | 软件和Agent生态 | 具身智能

好的，我已經將您的文稿整理如下，使其更易於閱讀和理解。主要針對重點資訊進行提煉，並適當調整結構：

**標題：GTC 2025 Nvidia 大會重點總結：Blackwell Ultra、Rubin 架構及 AI 軟體生態**

**核心觀點：**

*   黃仁勳預測未來算力規模將增長100倍，英伟达正积极布局新一代GPU和軟體生態，力圖鞏固其在AI领域的領導地位。
*   雖然 Blackwell Ultra GPU 性能提升有限，但 Rubin 架構和英偉達在 AI 軟體方面的積極布局值得期待。

**一、 Blackwell Ultra 全家桶：**

*   **Blackwell Ultra GPU：**
    *   採用雙台積電 N4P（5nm）Blackwell 架构晶片和 Grace CPU 封装，搭配 12 層 HBM3e 内存（288GB）。
    *   第五代 NVLink，片间互联带宽 1.8TB/s。
    *   FP4 精度算力达 15 PetaFLOPS，AI 推理速度提升 2.5 倍（相較 Hopper 架构）。
*   **Blackwell Ultra NVL72：**
    *   AI 推理专用机柜，包含 72 顆 Blackwell Ultra GPU 和 36 顆 Grace CPU。
    *   顯存 20TB，總帶寬 576TB/s。
    *   内置 18 顆 NVLink 交換機芯片，节点间 NVLink 带宽 130TB/s。
    *   性能提升：
        *   AI 性能比前一代 GB200 NVL72 提升 1.5 倍。
        *   與 Hopper 架构 DGX 机柜相比，增加 50 倍增收機會。
        *   完成 DeepSeek-R1 的推理任務，時間從 H100 的 1.5 分鐘縮短到 NVL72 的 15 秒。
    *   預計 2025 下半年上市。

**二、 未來的核彈級 GPU：Rubin 芯片**

*   **Rubin GPU (2026)：**
    *   下一代 GPU，搭配更強的機櫃 Vera Rubin NVL144。
    *   Vera Rubin NVL144 配备 72 顆 Vera CPU 和 144 顆 Rubin GPU。
    *   288GB 显存的 HBM4 芯片，顯存帶寬 13TB/s。
    *   FP4 精度推理算力 3.6 ExaFLOPS，FP8 精度訓練算力 1.2 ExaFLOPS。
    *   性能是 Blackwell Ultra NVL72 的 3.3 倍。
*   **Rubin Ultra NVL576 (2027)：**
    *   FP4 精度推理算力 15 ExaFLOPS，FP8 精度訓練算力 5 ExaFLOPS。
    *   性能是 Blackwell Ultra NVL72 的 14 倍。

**三、 其他硬體產品：**

*   **DGX Super POD（超算工廠）：**
    *   即插即用的 AI 超算工廠，面向生成式 AI、AI Agent 和物理模擬等場景。
    *   兩個版本：
        *   內置 DGX GB300 (1 Grace CPU + 2 Blackwell Ultra GPU)，提供 300TB 快速内存，FP4 精度算力 11.5 ExaFLOPS。
        *   內置 DGX B300 (不含 Grace CPU)，採用風冷系統，適用於企業級數據中心。
*   **DGX Spark (AI PC)：**
    *   搭载 GB10 芯片，FP4 精度算力 1 PetaFlops。
    *   内置 128GB LPDDR5X 内存，配備 CX-7 網卡和 4TB NVMe 存储。
    *   運行基於 Linux 定制的 DGX OS 操作系统，支持 Pytorch 等框架。
    *   本质上屬於超算範疇。
*   **DGX Station (AI 工作站)：**
    *   内置一颗 Grace CPU 和一颗 Blackwell Ultra GPU。
    *   搭配 784GB 的统一内存、CX-8 网卡。
    *   理论上能提供 20 PetaFlops 的 AI 算力。
*   **RTX 系列整合 Blackwell：**
    *   推出一系列内置 GDDR7 内存的 AI PC 相關 GPU，覆盖桌面、笔记本以及数据中心等多个场景。

**四、 硅光芯片領域的進展：NVIDIA Photonics 平台**

*   推出 Quantum-X 和 Spectrum-X 硅光共封芯片，以及三款交換機產品：
    *   Quantum 3450-LD
    *   Spectrum SN6810
    *   Spectrum SN6800
*   整合光模塊的 Photonics 交換機相比傳統交換機性能提升 3.5 倍，部署效率提高 1.3 倍，擴展彈性提升 10 倍以上。

**五、 軟體生態：**

*   **Nvidia Dynamo：**
    *   開源軟體，專為推理、訓練以及跨整個數據中心加速而設計。
    *   基於 Dynamo 分布式架構設計，充分利用了 NVL72 架构的高带宽通信能力。
    *   目标是构建反击 Groq 等推理 AISC 芯片的防線。
*   **Llama Nemotron：**
    *   由 Llama 系列模型衍生而來，主打高效、准确。
    *   内置了推理能力的開關。
    *   分为 Nano、Super 和 Ultra 三个档次。
*   **NVIDA AIQ：**
    *   提供以 Llama Nemotron 推理模型為核心的 AI Agent 工作流。
    *   推出 AI 數據平台，可以将 AI 推理模型直接接到企业数据的系统上。
    *   強調可觀察性和透明度機制。
*   **Cosmos 模型升級版：**
    *   世界基礎模型（WFM），能通過現在畫面去預測未來畫面。
    *   包含 Cosmos Transfer、Cosmos Predict 和 Cosmos Reason 三部分能力。
    *   已應用於 1X、Agility Robotics、Figure AI 等公司。
*   **Isaac GR00T N1 人形機器人基礎模型：**
    *   采用双系统架构，有快速反应的“系统 1“和深度推理的“系统 2“。
    *   能够处理抓取、移动、双臂操作等通用任务。
    *   與谷歌 DeepMind 和迪士尼合作開發 Newton 物理引擎，用了 Isaac GR00T N1 作為底座。

**六、 英伟达的整體戰略**

*   打造“三台计算机”的概念：DGX（訓練 AI）、AGX（端側部署 AI）、Omniverse+Cosmos（數據生成）。
*   构建从算力、训练到部署的完整闭环，垄断具身智能的上下游环节。
*   力图在硬件之外，在软件或机器人市场中占据重要地位。

**總結：**

英伟达正積極布局 AI 領域的各個層面，包括硬體、軟體和機器人。雖然 Blackwell Ultra 的升級幅度有限，但 Rubin 架構和英偉達在軟體生态上的積極布局值得關注。未来英伟达能否成功垄断这些市场，仍有待觀察。

**整理說明：**

*   **精簡信息：** 刪除了一些重複和冗餘的信息，只保留了核心要點。
*   **結構化呈現：** 使用標題、子標題和項目符號，使內容更清晰易讀。
*   **提煉核心观点：**  在开头和结尾都提炼了核心观点，帮助读者快速理解文章主旨。
*   **專業術語解釋：**  保留了必要的專業術語，以便傳達技術细节。
*   **語言潤飾：**  修正了一些語法和表达上的小问题，使其更流畅。

希望這個整理對您有所幫助！ 如果有任何需要修改的地方，請隨時告訴我。

[model=gemini-2.0-flash,0]


---

</details>

<details>
<summary>597. [2025-03-20] 【人工智能】企业招聘也能遇到AI骗局？| 美初创公司Vidoc Security亲身经历 | 两次AI换脸面试 | 完美简历 | 面试疑惑 | 远程招聘难题 | DeepFake | 企业如何避免</summary><br>

<a href="https://www.youtube.com/watch?v=wCrGV8GQAww" target="_blank">
    <img src="https://img.youtube.com/vi/wCrGV8GQAww/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【人工智能】企业招聘也能遇到AI骗局？| 美初创公司Vidoc Security亲身经历 | 两次AI换脸面试 | 完美简历 | 面试疑惑 | 远程招聘难题 | DeepFake | 企业如何避免

好的，我已經將文稿整理如下：

**標題：AI 求職詐騙：公司如何避免成為下一個受害者？**

**引言：**

大家好，這裡是最佳拍檔，我是大飛。
AI 詐騙新聞層出不窮，但你可能不知道，AI 也已被用於求職詐騙，甚至有公司中招。本文將揭露 AI 如何偽裝成求職者，並提供公司避免受騙的建議。

**事件起因：**

Vidoc Security 是一家專注於自動化程式碼安全審查的初創公司。其聯合創辦人達維德·莫查德沃（Dawid Moczadło）和克勞迪婭·克洛茨（Klaudia Kloc）都是資深安全工程師，擁有豐富的經驗和卓越的技術能力。他們開發了一款基於大語言模型的自動化安全工具，並獲得了種子輪融資。為了擴充團隊，Vidoc Security 發布了後端工程師的職位招聘廣告，卻因此陷入 AI 詐騙的漩渦。

**案例一：完美履歷下的 AI 偽裝**

*   **應聘者：** 馬卡里·克羅爾（Makary Krol）
*   **履歷：** 擁有 9 年以上經驗的資深全棧開發者，精通多種編程語言，在多家知名公司有亮眼成績。
*   **初輪面試：** 營運主管發現應聘者自稱來自波蘭，卻不會說波蘭語，英語口音濃厚，但因求職動機強烈而進入下一輪。
*   **招聘經理面試：** 溝通能力受質疑，但技術面試表現驚艷。
*   **最終面試：** 深入瞭解背景時，經歷難以令人信服，被懷疑身份造假。
*   **真相：** 應聘者使用偽造履歷，並利用 AI 工具偽裝外貌。
*   **結果：** 公司因無法提供確鑿證據，只能放棄錄用。

**案例二：換臉工具的破綻**

*   **應聘者：** 布拉蒂斯拉夫·拉科切維奇（Bratislav Rakočević）
*   **履歷：** 來自塞爾維亞，擁有當地大學教育背景，LinkedIn 資料顯示為資深全棧開發者，聲稱曾在 HubSpot 和 Woosh 等公司工作過。
*   **初輪面試：** 交流過程生硬，自稱畢業於塞爾維亞克拉古耶瓦茨大學，卻不會說塞爾維亞語。
*   **招聘經理面試：** 應聘者長相與 LinkedIn 頭像不符，被要求做簡單動作驗證是否使用 AI 換臉工具，遭拒絕。
*   **真相：** 應聘者使用 AI 換臉工具，且換臉對象與波蘭政治家斯拉沃米爾·門琴（Sławomir Mentzen）高度相似。
*   **結果：** 應聘者被立即終止面試。

**公司分析：**

*   兩名應聘者聲音幾乎一模一樣，懷疑背後為同一人。
*   AI 偽裝品質不斷提升，未來難以識別。

**公司如何避免 AI 應聘者欺騙：**

1.  **認真對待可能性：** 不要認為公司規模小就不會成為目標。
2.  **求職者真偽難辨：** 不要認為所有本地招聘網站上的求職者都是真實的。
3.  **關閉濾鏡：** 要求應聘者關閉所有視訊濾鏡，觀察面部或背景是否有異常。
4.  **測試動作：** 要求應聘者做一些 AI 換臉工具難以應對的動作，例如轉頭、遮臉、站立後退等。
5.  **錄製影片：** 在獲得對方同意的前提下，錄製並保存面試影片，以便事後分析。
6.  **現場面試：** 盡可能安排線下面試，或通過其他方式判斷應聘者身份。

**其他案例：**

*   美國安全培訓公司 Knowbe4 曾招聘一名假冒的朝鮮 AI 技術人員。
*   其他公司也曾遇到找人代面試或外包工作的情況。

**結論：**

AI 的出現降低了欺詐成本，對企業招聘造成極大威脅。公司應擦亮眼睛，避免落入 AI 應聘者的騙局中。
尤其海外科技公司或者出海公司，多為遠程面試，所以要更加注意甄別面試者的身份。

**結尾：**

感謝大家的觀看，我們下期再見。

[model=gemini-2.0-flash,0]


---

</details>

<details>
<summary>596. [2025-03-19] 【人物】David Baker 大卫·贝克 | 从哲学研究到诺贝尔化学奖 | Rosetta | 蛋白质结构预测 | Foldit | RoseTTAFold | RFdiffusion</summary><br>

<a href="https://www.youtube.com/watch?v=AEdR5FRb8xk" target="_blank">
    <img src="https://img.youtube.com/vi/AEdR5FRb8xk/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【人物】David Baker 大卫·贝克 | 从哲学研究到诺贝尔化学奖 | Rosetta | 蛋白质结构预测 | Foldit | RoseTTAFold | RFdiffusion

好的，我將這段文稿整理如下，主要針對排版、斷句和部分語氣進行調整，使其更易於閱讀，並保留原文的核心內容和風格：

**最佳拍檔：走進諾貝爾化學獎得主大衛·貝克（David Baker）的科研世界**

大家好，這裡是最佳拍檔，我是大飛。

2024年10月，我們製作了幾期諾貝爾獎的節目。其中，物理學和化學獎都頒給了與人工智慧有關的科學家，引起了大家的廣泛關注。

化學獎的得主中，大家可能更為熟悉 DeepMind 的 CEO 德米斯·哈薩比斯（Demis Hassabis）。但今天，我們想介紹的是另一位獲獎者——美國華盛頓大學的大衛·貝克（David Baker）教授。

他在蛋白質設計領域的卓越貢獻，不僅革新了科學界對蛋白質的認知，還為眾多相關領域帶來了全新的發展機遇。

今天，就讓我們一同走進大衛·貝克的科研世界，探尋他從哈佛哲學系走向蛋白質設計大師的傳奇歷程。

**從哲學到生物化學：一場意外的轉變**

大衛·貝克於1962年10月6日出生在美國華盛頓州西雅圖的一個猶太家庭。他的父母分別是物理學家和地球物理學家。按常理來說，這樣的家庭環境或許會讓他早早踏上科學研究之路。

然而事實並非如此。在哈佛大學本科階段，他主修的是哲學和社會研究。回顧那段時光，他曾表示，很多哲學討論在當時看來意義不大，感覺像是在浪費時間。

直到大學的最後一年，一次偶然的經歷徹底改變了他的學術軌跡。他選修了發育生物學課程，在課堂上見證了一個神奇的實驗：加入蛋白變性劑後，RNA酶失去了切割RNA的活性，但是當溶液中的變性劑被蒸發後，RNA酶的活性竟奇蹟般地恢復了！

這個現象讓他深深著迷：蛋白質究竟是如何在瞬間自主找到正確的構象並發揮功能的呢？這種對科學問題明確答案的追求，比哲學的模糊性更讓他感到興奮。

從此，他開始如飢似渴地閱讀《細胞分子生物學》這本經典教材，對生物學的興趣也愈發濃厚。隨後，他加入了諾貝爾生理和醫學獎得主蘭迪·謝克曼（Randy Schekman）的實驗室，並於1989年獲得了加州大學柏克萊分校的生物化學博士學位。

**Rosetta 的誕生：用計算機預測蛋白質結構**

博士畢業後，他進入加州大學舊金山分校大衛·阿加德（David Agard）教授的實驗室，進行博士後研究。在那裡，他接觸到了用計算機解析晶體結構的工作，卻發現傳統的人工匹配方式效率低下，而且困難重重。

於是，他萌生了用計算機預測蛋白質結構的想法。1993年，貝克回到家鄉西雅圖的華盛頓大學，開啟了他研發蛋白質結構預測軟體的征程。這就是後來聞名遐邇的 Rosetta。

在研發過程中，貝克遇到了許多的挑戰，因為蛋白質結構預測是一個極其複雜的問題，需要考慮到氨基酸序列、蛋白質的摺疊方式以及各種物理化學因素。

為了解決這些問題，貝克帶領團隊基於物理學的原理，讓 Rosetta 通過對蛋白質的構象進行能量最小化計算，來預測最穩定的三維結構，也就是接近自然狀態下的蛋白質穩定構象。

1998年，Rosetta 正式發布。為了驗證它在蛋白質結構預測中的準確性，貝克團隊積極參與了 CASP 競賽。在這個競賽中，參賽者需要對一批尚未公開結構的蛋白質進行盲測預測，以此來評估不同演算法的性能。

Rosetta 在競賽中逐漸嶄露頭角，而2004年的 CASP 6 競賽成為了它的高光時刻。針對目標蛋白 T0281，Rosetta 首次實現了接近原子級精度的從頭蛋白質結構預測。這個成果震驚了整個蛋白質結構預測領域，也讓 Rosetta 一度成為這個領域的領導者。

**Rosetta@home 和 Foldit：群眾的力量**

但是隨著研究的深入，更準確的預測意味著需要消耗更多的計算資源。購買新的計算機不僅成本高昂，還面臨著空間不足的問題。

為了解決這一難題，貝克團隊啟動了 Rosetta@home 項目。這是一個極具創新性的項目，它邀請全球各地的人們用自己閒置的算力來進行蛋白結構計算。當計算機運行這個屏幕保護程式的時候，屏幕上會顯示正在摺疊的蛋白質，讓參與者也能直觀感受到科學研究的魅力。

如今，Rosetta@home 項目已經吸引了眾多志願者的參與，極大地推動了蛋白質結構計算的發展。為了持續改進 Rosetta 軟體，貝克還創建了 Rosetta Commons 學術社區。

這個社區匯聚了來自全球60多個機構的學者，涵蓋化學、生物學、生理學、物理學、工程學、數學和計算機科學等多個領域。每年社區都會舉辦會議，成員們也會互相分享成果、交流想法。

在這裡，不同領域的智慧相互碰撞，為蛋白質結構研究注入了源源不斷的活力。Rosetta Commons 也逐漸發展成為一個大規模的國際合作項目。

受到 Rosetta@home 項目的啟發，貝克意識到合作共贏的重要性。2008年，他的團隊推出了一款名為 Foldit 的在線益智遊戲。

這款遊戲的獨特之處在於，無論是專業人士還是非專業人士都可以參與。在遊戲中，玩家需要使用遊戲中的工具，盡可能完美地摺疊選定的蛋白質結構。得分最高的方案將由研究人員進行分析，評估在現實中的適用性，進而應用於靶向治療等領域。

Foldit 一經推出便受到了廣泛的關注，吸引了超過40萬人參與。玩家們的熱情和智慧，為蛋白質研究帶來了意想不到的成果。

2011年，一篇被《自然》雜誌錄用的論文中，Foldit 的玩家幫助破解了困擾科學家長達15年的 M-PMV 逆轉錄病毒蛋白酶的晶體結構。令人驚嘆的是，玩家們只用了10天就構建出了足夠準確的酶3D模型，成功進行了分子替換和隨後的結構測定。

這個成果充分展示了公眾參與科學研究的巨大潛力，也讓 Foldit 成為了蛋白質研究領域的一個創新典範。

**與 AlphaFold 2 的競爭：擁抱深度學習**

就在 Rosetta 和 Foldit 在蛋白質結構領域大放異彩之時，2020年底，一個強大的競爭對手出現了——那就是 AlphaFold 2。

在第14屆 CASP 大賽中，AlphaFold 2 的表現堪稱驚豔，它預測蛋白質結構的準確性直接碾壓了其他所有團隊，包括貝克團隊的 Rosetta。AlphaFold 2 將深度學習與物理、生物等相關領域知識結合，實現了端到端的蛋白質三維結構信息預測。這個成果被譽為蛋白質研究的里程碑。

然而，當時開發 AlphaFold 2 的 DeepMind 公司並沒有公開具體的細節。面對挑戰，貝克沒有選擇退縮。和他的老師蘭迪·謝克曼一樣，貝克崇尚科學的開源、共享。

蘭迪·謝克曼曾經因為提倡開放、免費獲取科學文獻，對《自然》、《科學》、《細胞》這類封閉式訪問的期刊大力批評，並且宣布自己的實驗室永不向這些期刊投稿。

受老師的影響，貝克也立志研發一個能夠與 AlphaFold 2 媲美的開源模型。經過數月的努力，貝克和實驗室的其他成員借鑒 AlphaFold 2 的思路，發布了深度學習模型 RoseTTAFold。

RoseTTAFold 採用了獨特的三軌（three-track）神經網絡架構，能夠同時考慮蛋白質的序列模式、氨基酸相互作用以及可能的三維結構。在這個架構中，一維、二維和三維信息相互流動，使得神經網絡能夠推理出蛋白質的化學組成與其摺疊結構之間的關係。

利用 RoseTTAFold，研究人員計算了數百種新的蛋白質結構，包括人類基因組中許多未知的蛋白質，還生成了與人類健康直接相關的蛋白質，比如與炎症疾病和癌細胞生長相關的蛋白質。

而且，RoseTTAFold 的計算耗能和時間都低於 AlphaFold 2，只需一塊 RTX 2080 顯卡，就能在短短10分鐘內計算出400個氨基酸殘基以內的蛋白質結構。

2021年6月，貝克公布了詳細介紹 RoseTTAFold 技術路線的預印版論文。幾天後，DeepMind 首席執行官德米斯·哈薩比斯（Demis Hassabis）在推特上宣布，他們將公布 AlphaFold 2 的論文和源代碼。

同年7月15日，RoseTTAFold 和 AlphaFold 2 的相關論文分別發表於《科學》和《自然》雜誌。《科學》雜誌還將 RoseTTAFold 和 AlphaFold 共同評為2021年度突破技術。

這場學術界與商業界的激烈競爭，最終以雙方的成果共享和共同進步而完美結束。

**從蛋白質結構預測到蛋白質設計：RFdiffusion 和 ProteinMPNN**

在經歷了與 AlphaFold 2 的競爭後，貝克深刻認識到了深度學習的強大力量。於是他不僅開始將深度學習用於蛋白質結構預測，還將它拓展到了蛋白質設計領域。

其實，早在多年以前，貝克的團隊就有了設計全新蛋白質的想法。2003年，他們成功設計出了世界上第一個全新蛋白質 Top7。Top7 是一種由93個氨基酸殘基組成的α/β蛋白質。研究人員通過一種在序列設計和結構預測之間迭代的計算策略，為它設計了全新的序列和拓撲結構。

實驗證明，Top7 能夠摺疊成穩定的結構，它的X射線晶體結構與設計模型的均方根偏差只有1.2埃。這個成果標誌著人類在蛋白質設計領域邁出了重要的一步。

隨著深度學習技術的不斷發展，貝克帶領團隊在蛋白質設計領域取得了更多的成果。比如，他在《自然》期刊發表了一篇題為《利用RFdiffusion從頭設計蛋白質結構和功能》的論文。

研究人員在蛋白質結構去噪任務中，對 RoseTTAFold 的結構預測網絡進行了微調，開發了一個生成模型 RFdiffusion。這個模型在蛋白質結合劑設計、酶活性位點支架設計等方面表現出色，而且具備極好的通用性，目前也已經開源。

與此同時，為了進一步拓展 RFdiffusion 的能力，貝克還開發了一種基於深度學習的蛋白質序列設計方法 ProteinMPNN。

ProteinMPNN 將蛋白質結構作為輸入，1秒即可生成能夠摺疊成對應骨架的新氨基酸序列。結合像 RFdiffusion 這樣的結構生成工具，它可以被用來設計具有前所未見的序列、結構和功能的蛋白質。

研究表明，在天然蛋白質骨架上，ProteinMPNN 的序列恢復率為52.4%，而過去基於 Rosetta 的物理設計僅為32.9%。這個成果最終以一篇題目為《使用ProteinMPNN進行基於穩健深度學習的蛋白質序列設計》的論文被《科學》雜誌錄用。

**Rosetta 和 Foldit 的進化：拓展到抗體和酶設計**

除了開發新的工具以外，貝克的團隊還對 Rosetta 和 Foldit 進行了優化。通過在軟體內引入新的模塊和演算法，這兩款軟體不再僅僅局限於蛋白質結構預測，還拓展到了抗體設計、酶設計和小分子對接等方面。

貝克表示，Foldit 最初是為蛋白質結構預測而創建的，但是隨著研究的深入，現在已經轉向了蛋白質設計。團隊會不斷為玩家更新關卡，讓遊戲與研究興趣的變化保持同步。

**科研成果的產業化：創立多家公司**

貝克的研究成果不僅在學術界引起了轟動，還在工業界得到了廣泛應用。他深知，將科研成果轉化為實際產品，才能真正造福人類。

因此，每當他認為某項技術基本成熟的時候，就會成立新公司，或者投入過往創立的公司來進行孵化，推動技術的產業化落地。

根據華盛頓大學蛋白質設計研究所官網顯示，貝克作為創始人直接參與的公司就有21家。此外，他還以顧問的身份參與了其他公司的運營。

以2024年4月成立的 Xaira Therapeutics 公司為例，它就應用了 RFdiffusion 和 ProteinMPNN 技術。公司致力於通過新興的AI技術重新設計和開發藥物，由斯坦福大學前校長馬克·特西爾-拉維涅（Marc Tessier-Lavigne）博士擔任首席執行官，貝克為聯合創始人。

Xaira 通過整合分子與人體疾病相關生物學特徵的海量數據，來訓練高品質的模型。同時，公司建立了工業化的乾濕實驗平台，能夠在實驗室中測試蛋白質對特定細胞靶標的粘附程度，並且評估穩定性等關鍵屬性。所有產生的數據會被快速反饋到蛋白質模型中，實現分子設計的迭代優化。

另外，2023年成立的 Archon Biosciences 公司，致力於通過生成式AI設計一種全新的生物藥物類型——抗體籠（Antibody cage），簡稱 AbC。

AbC 將 AI 設計與結構控制融合，可以完全控制抗體的方向、結合域化合價、大小、形狀和剛度。這種結構控制能夠實現精確的生物分佈和細胞靶向結合，結合內部臨床數據，能夠快速驗證抗體的有效性。公司獲得了英偉達等多家公司的支持，所用技術都源於貝克在2024年諾貝爾化學獎中獲得認可的成果。

2023年，Monod Bio 公司推出了世界上第一個完全從頭蛋白質產品，用於生命科學研究和診斷的螢光素酶 LuxSit™ Pro。這個成果也是源自貝克2023年在《自然》雜誌上發表的一篇論文。

此外，2009年成立的 Arzeda、2014年成立的 Cyrus Biotech 以及2018年成立的 A-Alpha Bio 等公司，也都積極引進了貝克最新研發的 AI 技術，致力於研發更多的新型蛋白質，用來製造新的藥物、疫苗、疾病療法，甚至是新的材料。

**結語：大衛·貝克的貢獻與影響**

回顧大衛·貝克的學術生涯，他從最初的哲學探索，逐漸成長為蛋白質設計領域的大師，每一步都充滿了對未知的渴望和對創新的執著。

他始終堅持合作共贏的理念，用開放與分享的精神，激勵著全球無數研究人員和科學愛好者投身到蛋白質領域的發展中。他的研究成果不僅在學術界取得了巨大突破，還成功從實驗室走向了工業界，為疾病治療、食品生產、材料科學等多個領域帶來了新的可能。

隨著 AI 能力的不斷進步，相信我們很快會看到蛋白質設計領域不斷會有新的發現和突破，幫助我們治療各種疑難病症，甚至是改變我們的生活方式。而這些，都與大衛·貝克的探索和奉獻密不可分。

感謝大家收看本期視頻，我們下期再見！

[model=gemini-2.0-flash,0]


---

</details>

<details>
<summary>595. [2025-03-18] 【人工智能】谷歌研究团队发现DiLoCo具有更优Scaling Laws | 分布式低通信优化 | 稳定扩展超参数 | 损失下降更快 | 带宽减少99% | 最优批大小 | 数据并行 | 未来趋势</summary><br>

<a href="https://www.youtube.com/watch?v=JGbehG6qy3k" target="_blank">
    <img src="https://img.youtube.com/vi/JGbehG6qy3k/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【人工智能】谷歌研究团队发现DiLoCo具有更优Scaling Laws | 分布式低通信优化 | 稳定扩展超参数 | 损失下降更快 | 带宽减少99% | 最优批大小 | 数据并行 | 未来趋势

好的，以下是經過整理的文稿，使其更易讀、更具結構性：

**標題：Google DiLoCo：分布式低通信優化如何改變大模型訓練方式？**

**引言 (0:00-0:30)**

*   **開場:** 大家好，這裡是最佳拍檔，我是大飛。
*   **核心觀點:** Google 三大研究團隊 (Google Research、Google Search、DeepMind) 共同發現，DiLoCo (Distributed Low-Communication Optimization) 分布式低通信優化方法在訓練大規模模型時具有更穩定的 Scaling Laws。
*   **優勢:**
    *   帶寬需求比數據並行訓練少幾個數量級。
    *   小模型訓練上也可能比數據並行更好。
*   **潛在影響:** 可能改變大模型的訓練方式。
*   **本文目的:** 解讀 Google 的 DiLoCo 研究，探討其對 AI 領域的影響。

**Scaling Laws 回顧 (0:30-1:30)**

*   **定義:** 模型性能與模型規模、數據量和計算資源之間的數學關係。
*   **主要內容:** 在一定範圍內，增加模型的規模、數據量和計算資源，模型的性能會相應提升。
*   **重要性:** 為大模型發展提供理論依據，推動 AI 領域發展。
*   **挑戰:**
    *   隨著模型規模增大，數據並行訓練的缺點暴露。
    *   通信開銷巨大。
    *   內存限制。
    *   影響大模型擴展和訓練效率。

**DiLoCo 方法介紹 (1:30-2:30)**

*   **目標:** 減少通信開銷，提高擴展性。
*   **核心思路:**
    *   每個模型副本獨立訓練一定數量的內部優化步驟。
    *   通過外部優化步驟進行同步。
    *   外部優化步驟之間引入動量機制。
*   **比喻:** 像一群人各自在自己的小空間裡先進行一些準備工作，然後再一起協調，減少溝通成本。

**DiLoCo 的優勢 (2:30-4:30)**

*   **超參數穩定性:** 在不同模型規模下表現穩健且可預測。
*   **Scaling Laws 表現:** 隨著模型規模增大，DiLoCo 相較於數據並行訓練的優勢更明顯。
    *   損失會比數據並行更低。
    *   訓練了一系列模型，驗證此預測。
*   **带宽需求**: DiLoCo 所需带宽比数据并行训练少几个数量级。

**DiLoCo 的其他優勢 (4:30-5:30)**

*   **容忍更大的批大小:**
    *   可支持更大的批大小，且表現更穩定。
    *   提高最佳批大小，最佳全局批大小隨副本數 M 增加而增大。
    *   擴展能力更強。
*   **實驗結果 (HellaSwag):** 即使在較小的模型規模下，DiLoCo (M=2) 也能在更大的全局批大小下實現更高的準確率。

**實驗細節 (5:30-7:30)**

*   **模型架構:** 類似 Chinchilla 的純解碼器 Transformer 結構。
    *   QK-LayerNorm 技術：降低模型對學習率的敏感性，使得訓練更穩定。
    *   z-loss 正則化：提高訓練的穩定性。
*   **詞彙量:** 32768 個 (32000 個詞彙表內的單詞 + 其他標記)。
*   **序列長度:** 最大序列長度固定為 2048。
*   **模型規模:** 從 3500 萬參數到 100 億參數不等。
*   **訓練數據集:** C4 數據集的訓練集。
*   **評估指標:** C4 的驗證集。
*   **下游任務:** HellaSwag、Piqa、Arc-Easy (零樣本評估)。
*   **優化器:**
    *   數據並行訓練和 DiLoCo 的內層優化：AdamW (β1=0.9, β2=0.99)。
    *   預熱 1000 步，採用餘弦學習率衰減。
    *   權重衰減參數 λ = T⁻¹ (T 為總訓練步數)。
    *   內層梯度全局範數剪裁到 1，外層梯度不剪裁。
    *   DiLoCo 的外層優化：帶 Nesterov 動量的 SGD (動量 0.9)。
*   **實驗環境:** Google TPUv5e、TPUv6e、TPUv-5。
*   **網絡環境:** 高、中、低帶寬，模擬不同數據中心之間的環境。

**實驗結果分析 (7:30-9:00)**

*   **評估損失:** 當副本數 M=1 時，DiLoCo 在不同模型規模下都比數據並行訓練低。
*   **HellaSwag 零樣本準確率:** DiLoCo 同樣優於數據並行訓練。
*   **批大小影響:**
    *   DiLoCo 對批大小的穩定性更強。
    *   DiLoCo 顯著提高了最佳批大小。
    *   最佳全局批大小隨著副本數 M 增加而增大。
*   **DiLoCo 在橫向擴展上更具優勢。**

**更多實驗結果分析 (9:00-10:00)**

*   **外部學習率:**
    *   最佳外部學習率基本上與模型規模 N 無關。
    *   最佳外部學習率會隨著副本數 M 變化而變化。
    *   M 越大，最佳外部學習率似乎也越大。
    *   這與之前的聯邦學習研究是一致的。
*   **DiLoCo 在水平擴展上更加自然。**

**DiLoCo 與 Over-Training (10:00-10:30)**

*   **優勢:**
    *   DiLoCo 在處理過度訓練的問題上也有獨特的優勢。
    *   增加批大小，減少通信量，使得在相同時間內可以進行更多的過度訓練。
    *   提供更有力的工具來研究模型的性能邊界。

**AI 發展趨勢 (10:30-12:30)**

*   **DiLoCo 對大規模模型訓練提供更高效、更具擴展性的方法。**
*   **反思 AI 未來發展:**
    *   目前 AI 模型發展依賴於 Chinchilla 模式 (大量計算資源和數據)。
    *   Chinchilla 模式面臨挑戰：
        *   前期投入巨大。
        *   性能增益可能越來越小。
        *   訓練數據可能正在枯竭。
*   **新型“推理模型”興起:**
    *   OpenAI 的 o1、o3，DeepSeek R1，Google Gemini 2.0 Flash Thinking 等。
    *   採用 Test Time Compute，不再依賴長時間的預訓練。
*   **混合專家模型 (MoE):**
    *   訓練多個小型“專家”模型，與大模型協同工作。
    *   只在需要時調用部分算力，降低了基礎設施需求。
*   **巴克萊資本分析師觀點:** AI 行業可能面臨兩種情景：
    *   “Chinchilla” 繼續主導 (巨額算力和數據投入)。
    *   增長“停滯”，新型技術和模型以更少的資源實現更強的性能。
    *   這兩種路徑的資本支出差距巨大。
*   **“合成數據”技術取得突破可能讓 Chinchilla 模式重煥生機。**

**總結與展望 (12:30-End)**

*   **兩種趨勢可能同時並存:**
    *   一方面，資本巨鱷會繼續推動算力和基礎設施的投入。
    *   另一方面，小公司會借助算法和工程創新來降低成本。
*   **結尾:** 歡迎在評論區留下自己的看法，感謝大家觀看，下期再見。

**總結:**

這個整理後的版本，將原始文稿分成了更小的、主題明確的段落，並添加了時間戳，方便讀者快速定位到感興趣的部分。重點使用粗體標示，提高可讀性。

希望這個整理後的版本對您有幫助！

[model=gemini-2.0-flash,0]


---

</details>

<details>
<summary>594. [2025-03-16] 【人工智能】OpenAI提案矛头直指DeepSeek | 锁死全球AI发展 | 指责DeepSeek有害 | 加强出口管制 | 两类市场和三级国家 | 为版权侵权辩解 | 吸引资金 | 大型双标现场</summary><br>

<a href="https://www.youtube.com/watch?v=wFGKIbhU_nU" target="_blank">
    <img src="https://img.youtube.com/vi/wFGKIbhU_nU/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【人工智能】OpenAI提案矛头直指DeepSeek | 锁死全球AI发展 | 指责DeepSeek有害 | 加强出口管制 | 两类市场和三级国家 | 为版权侵权辩解 | 吸引资金 | 大型双标现场

好的，這份文稿我會整理成以下幾個部分，並進行一些潤飾，讓內容更清晰：

**標題：OpenAI提案解讀：鎖死中國AI發展？**

**引言：**

大家好，這裡是最佳拍檔，我是大飛。近日，OpenAI針對美國政府即將出台的“AI行動計劃”提交了一份提案，內容直指中國DeepSeek公司，幾乎將其視為第二個華為，試圖限制中國乃至全球AI的發展。這份提案公開後，立即引發了廣泛的討論和爭議。今天，大飛就來為大家深度解讀OpenAI的這份提案。

**背景：**

事情的背景是，2025年1月，特朗普推翻了拜登在2023年10月簽署的人工智能行政命令，並發布新的行政命令，明確表示維持和加強美國在全球人工智能領域的主導地位是美國的政策目標。該命令要求在180天內向總統提交一份AI行動計劃。為了影響該計劃的制定，眾多機構和企業紛紛建言獻策，OpenAI提交的這份長達15頁的提案便是其中之一。

**OpenAI提案核心內容：**

OpenAI在提案中將矛頭直指中國的DeepSeek公司，主要觀點如下：

*   **指責DeepSeek受中國政府控制：** 聲稱DeepSeek是“由國家補貼”和“被國家控制”的，認為其會受到中國政府的指示，操縱模型來造成損害。
*   **強調DeepSeek的威脅：** 認為中國決心在2030年前成為全球人工智能的領導者，而DeepSeek發布的R1模型，不僅推理能力令人印象深刻，更被視為中美AI競爭的一個重要指標。在OpenAI看來，DeepSeek就像是第二個華為，存在許多風險。
*   **數據安全隱憂：** 聲稱在關鍵基礎設施和其他高風險使用場景中，如果系統構建在DeepSeek模型之上，將面臨巨大風險，因為它可能會被迫受到中國政府的指令控制。此外，由於DeepSeek是免費提供的，用戶可能需要以自身的隱私和安全作為代價。

**DeepSeek的回應與實際情況：**

*   **數據收集克制：** DeepSeek在用戶協議中明確了信息收集的多重處理前提，收集到的內容僅用於模型服務的優化和風控，並且強調在經過安全加密技術處理、嚴格去標識化且無法重新識別特定個人的前提下，才會進行相關操作。
*   **用戶權益保障：** DeepSeek保留了用戶在提交的輸入中擁有的任何權利、所有權和利益，並且將服務輸出內容的任何權利、所有權和利益歸屬於用戶。用戶可以將服務的輸入與輸出應用在廣泛的使用場景中，包括個人使用、學術研究、衍生產品開發以及訓練其他模型等等。
*   **反駁OpenAI的指責：** OpenAI指責DeepSeek會根據中國法律的需要把用戶數據提供給政府，並且利用這些數據訓練更強大的系統，但這些指責並沒有具體的證據來證實。相反，從DeepSeek公開的信息來看，它在保護用戶隱私和數據安全方面有著明確的規定和措施。

**OpenAI的政策建議：**

除了對DeepSeek的攻擊，OpenAI還提出了一系列政策建議：

*   **出口管制：** 建議美國政府制定一項全面的出口管制戰略，將全球希望獲取美國AI技術的國家分為“總可服務市場”（TAM，即中國及其盟友以外的國家）和“可服務市場”（SAM，即傾向於在民主軌道上構建AI的國家）。
*   **分級制度：** 維持AI擴散規則的三級框架，擴大一級國家的數量，縮小二級國家的範圍。只有承諾遵守民主AI原則、目的是為本國公民帶來更多自由的國家才能被視為一級國家。
*   **市場擴張：** 鼓勵跨境資本流動，推廣為國內芯片設計優化過的軟件框架，協調全球對與中國結盟的AI基礎設施的禁令，鼓勵全球監管機構採納有利於增長的安全政策。
*   **保護知識產權：** 禁止與三級國家的外國軍事和情報機構建立關係，禁止利用數據中心支持三級國家或侵犯人權的行動，禁止使用中國國產設備，確保AI企業由總部位於一級國家的實體控制。

**關於版權：**

OpenAI在提案中強調美國的版權法在早期技術進步階段發揮了重要作用，如今對於美國繼續保持AI行業的領導地位變得更為關鍵。OpenAI認為自己的模型並非通過複製或直接提供現有的作品供公眾消費，而是從作品中學習，提取模式、語言結構和上下文見解。因此，AI模型的訓練方式符合版權法及“合理使用原則”的核心目標，即利用現有的作品創造出全新的、不同的內容，而不會削弱這些作品的商業價值。

**基礎設施建設與政府採納：**

OpenAI建議美國政府採取一系列政策，包括確保基礎設施投資能夠推動經濟增長，讓所有美國人受益，最大化AI的可及性，以及維護國家安全利益，確保敏感的美國數據存儲在美國本土。OpenAI還指出，目前美國聯邦部門和機構對AI的應用仍然處於不可接受的低水平，政府應該鼓勵公私合作，加速AI在政府機構中的應用，並且消除已知的障礙。

**美國網友的反應：**

很多美國網友對OpenAI的提案並不認同。有網友評價OpenAI更在意的是自己的商業模式以及自己的知識產權，而實際上他們在模型訓練過程中對知識產權毫無尊重。

**Anthropic的政策建議：**

Anthropic也提交了自己對AI行動計劃的政策建議，除了強調DeepSeek不安全以外，主要就是強烈建議政府加強對計算資源的出口管制，並且對某些模型的權重實施出口管制，來保持並且擴大美國在AI領域的優勢。

**結論：**

OpenAI和Anthropic幾乎同時對DeepSeek抱有如此大的敵意，並且希望將全世界其他國家的AI發展都扼殺在搖籃裡，實在令人費解。奧特曼一邊口口聲稱要讓AI造福全人類，另一邊又把全世界國家分成三六九等，雙標的嘴臉再次展露無遺。如同提案所說的民主的AI一樣，荒唐可笑。

我們現在已經不再期望OpenAI還能引領這場科技進步，但作為一家商業科技公司，如果只想靠政治來維護自己的商業利益，恐怕最終也會被這一點所反噬。

感謝大家觀看本期視頻，我們下期再見。

**整理說明：**

*   **結構化：** 將文稿分成多個部分，方便讀者理解。
*   **提煉重點：** 提取各部分的重點內容，簡明扼要。
*   **潤飾語言：** 修正部分語句，使其更流暢、易懂。
*   **增加標題：** 針對不同部分增加標題，幫助讀者快速定位感興趣的內容。

希望這份整理後的文稿對您有所幫助！

[model=gemini-2.0-flash,0]


---

</details>

<details>
<summary>593. [2025-03-15] 【人工智能】DyT干掉Transformer归一化层 | 动态Tanh | 9行代码 | 性能不降反升 | 何恺明杨立昆携手 | 多项任务验证 | 计算效率提升一倍 | 打破固有观念</summary><br>

<a href="https://www.youtube.com/watch?v=D-pR7qWuheE" target="_blank">
    <img src="https://img.youtube.com/vi/D-pR7qWuheE/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【人工智能】DyT干掉Transformer归一化层 | 动态Tanh | 9行代码 | 性能不降反升 | 何恺明杨立昆携手 | 多项任务验证 | 计算效率提升一倍 | 打破固有观念

好的，以下是经过整理後的文稿，主要針對排版、語氣以及一些細節地方做了調整，讓它更像是一份書面文件：

---

**Transformer架构新突破：何恺明、杨立昆携手革新，无需归一化层也能实现卓越性能**

**引言**

大家好，我是大飞，欢迎来到最佳拍档。

在人工智能领域飞速发展的今天，Transformer架构已然成为深度学习的中流砥柱。无论是在自然语言处理（NLP）还是计算机视觉（CV）领域，Transformer都占据着举足轻重的地位。

近期，深度学习领域的两位巨擘——何恺明和杨立昆（Yann LeCun）联袂合作，提出了一项颠覆性的研究成果：通过仅9行代码，成功移除了Transformer架构中长期被视为“标配”的归一化层（Normalization Layer）。更令人惊讶的是，模型的性能非但没有下降，反而得到了显著提升。

本文将深入解读这篇论文，探究该突破的实现原理，并展望其对未来深度学习发展可能带来的影响。

**背景：归一化层的重要性与挑战**

在深度学习的世界中，归一化层一直被视为神经网络大厦的基石，几乎无处不在。长期以来，人们普遍认为它是现代神经网络中不可或缺的组成部分。尤其是在Transformer架构中，层归一化（Layer Normalization, LN）更是备受青睐。这是因为归一化层在优化神经网络训练过程中展现出了显著的实证优势，被广泛认为是深度网络高效训练的关键因素。

在过去的十年间，尽管神经网络技术不断发展，新的架构和方法层出不穷，但归一化层却始终稳如磐石，很少有人质疑其必要性，也很少有人尝试去替代它。

**核心突破：动态Tanh（Dynamic Tanh, DyT）的引入**

何恺明、杨立昆等人的这项研究彻底打破了人们对归一化层的固有认知。研究人员发现，即便去掉Transformer中的归一化层，通过一种巧妙的方法，依然能够让模型达到相同甚至更好的性能。这种方法就是动态Tanh（Dynamic Tanh），简称DyT。

DyT的原理源于一个看似简单却又极具洞察力的观察：研究人员发现，层归一化（LN）在将输入转换为输出的过程中，呈现出了类似tanh函数的S形曲线特征。这种曲线能够有效地压缩输入中的极端值，同时在中心区域保持较好的线性形态。

基于这一发现，研究人员提出了DyT，用它来替代传统的归一化层。DyT的定义如下：

[公式或代码，因平台限制可能无法直接显示，请参考原文]

其中，α是一个可学习参数，用于学习合适的缩放因子；tanh函数利用自身的有界性来抑制极端值。γ和β同样是可学习的、逐通道的向量参数，它们的存在允许输出缩放到任意的尺度，因此在新的设计中被视为DyT层的一部分，如同在归一化层中也包含类似的参数一样。

**代码实现：简洁高效的创新**

DyT的实现代码非常简洁，仅9行代码就完成了从构思到实践的跨越。

[代码，因平台限制可能无法直接显示，请参考原文]

从代码中可以看出，DyT在结构上并不复杂，但却蕴含着创新的思维。它无需像传统归一化层那样计算激活统计信息，就能同时实现对输入的缩放和极值的抑制，这无疑是对传统归一化方式的一种大胆革新。

**实验验证：广泛领域的卓越表现**

为了验证DyT的有效性，研究团队进行了一系列广泛而深入的实验，涵盖了多个不同的领域和任务，涉及多种Transformer结构和现代架构。

*   **视觉领域的监督学习：** 在ImageNet - 1K分类任务中，研究人员选择了“Base”和“Large”规模的ViT和ConvNeXt模型进行训练。实验结果表明，DyT在这两种架构和不同模型规模上都展现出了优异的性能。
*   **视觉领域的自监督学习：** 研究人员采用了掩码自编码器（MAE）和DINO这两种流行的方法进行基准测试。实验结果显示，DyT在自监督学习任务中表现与LN相当，甚至略有提升。
*   **图像生成领域：** 研究人员在ImageNet - 1K上训练了三种规模的DiT模型，并通过评估FID分数来衡量图像的生成质量。实验结果表明，DyT在FID上取得了与LN相当或更好的性能。
*   **大语言模型（LLM）：** 研究人员对LLaMA 7B、13B、34B和70B模型进行了预训练，评估DyT相对于RMSNorm的性能。实验结果显示，DyT在所有四种模型规模上的表现均与RMSNorm相当。
*   **DNA序列建模：** 研究人员预训练了HyenaDNA模型和Caduceus模型，并在GenomicBenchmarks上进行评估。结果显示，DyT在任务中保持了与LN相当的性能。
*   **语音领域的自监督学习：** 研究人员在LibriSpeech数据集上预训练了两个wav2vec 2.0 Transformer模型。实验结果表明，DyT在两种模型规模上的表现都与LN相当。

**超参数调优与计算效率评估**

研究团队还进行了额外的实验，评估超参数调优的影响，特别是针对所有非大语言模型的学习率和α初始化。结果表明，DyT模型对学习率调优的依赖性较低，且默认的α初始值通常已经能够实现接近最优的性能。

此外，研究人员还对DyT的计算效率进行了评估。结果显示，DyT层的计算时间显著低于RMSNorm层，表明DyT在面向效率优化的网络设计中具有很大的潜力。

**结论与展望**

这项研究的意义不仅仅在于发现了一种可以替代归一化层的方法，更在于它打破了人们长期以来对归一化层不可或缺的固有观念。DyT的出现证明了在不使用传统归一化层的情况下，模型依然可以取得优异的性能。

从实际应用角度来看，考虑到模型训练和推理可能需要进行数千万次的计算，DyT的高效性能够极大地帮助降低成本。从学术研究的角度，这项研究也为后续研究如何进一步优化神经网络架构提供了新的思路和方法。

**论文作者介绍**

*   **朱家晨（Jiachen Zhu）：** 纽约大学柯朗数学研究所四年级计算机科学博士生，导师是杨立昆。
*   **陈鑫磊（Xinlei Chen）：** Meta FAIR的研究科学家，毕业于卡内基梅隆大学语言技术研究所。
*   **何恺明：** 麻省理工学院（MIT）电气工程与计算机科学系副教授。
*   **杨立昆：** 图灵奖得主，Meta的首席科学家，纽约大学终身教授。
*   **刘壮：** 项目负责人，Meta FAIR实验室研究科学家，毕业于加州大学伯克利分校电子工程与计算机科学系。

**结语**

感谢大家的观看，我们下期再见。

---

**說明：**

*   **標題與小標題:** 讓文章結構更清晰。
*   **語氣:** 稍微調整了口語化的表達，使其更正式。
*   **內容:** 保持了原有的資訊，只針對表達方式做了調整。
*   **排版:** 使用了更適合閱讀的段落和格式。
*   **公式與代碼:** 由於平台限制，無法直接嵌入，建議在實際使用時補上。

希望这份整理后的文稿对您有所帮助！

[model=gemini-2.0-flash,0]


---

</details>

<details>
<summary>592. [2025-03-14] 【人工智能】谷歌最新开源模型Gemma 3发布 | 四种参数规模 | 多模态能力 | SigLIP | 128K长上下文 | 支持140多种语言 | 1338ELO高分 | 排名第二 | 蒸馏技术</summary><br>

<a href="https://www.youtube.com/watch?v=3qxO0csZGKU" target="_blank">
    <img src="https://img.youtube.com/vi/3qxO0csZGKU/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【人工智能】谷歌最新开源模型Gemma 3发布 | 四种参数规模 | 多模态能力 | SigLIP | 128K长上下文 | 支持140多种语言 | 1338ELO高分 | 排名第二 | 蒸馏技术

好的，以下是对文稿的整理，使其更简洁、更易于理解：

**Gemma 3 模型简介：谷歌AI小模型的最新进展**

**一、引言**

*   谷歌发布 Gemma 系列最新模型 Gemma 3，引起广泛关注。
*   Gemma 3 不仅是技术上的革新，也预示了 AI 在小模型领域的新发展方向。

**二、Gemma 3 的主要特性**

*   **参数规模：** 提供 1B、4B、12B 和 27B 多种参数规模，针对消费级硬件优化。
*   **多模态能力：**
    *   与定制版 SigLIP 视觉编码器兼容，可将图像和视频作为输入。
    *   能够分析图像、回答与图像相关的问题、对比图像、识别物体、解析图像中的文本。
    *   SigLIP 编码器使用 Vision Transformer 模型，采用 CLIP 的变体进行训练。
    *   使用推理阶段的自适应窗口算法（Pan and Scan）解决非方形宽高比和高分辨率图像处理问题。
*   **长上下文处理能力：**
    *   支持最长 128K token 的上下文长度（1B 模型除外，仅支持 32K token）。
    *   适用于需要分析、总结长篇文档等复杂任务。
    *   通过在全局注意力层之间交错设置局部注意力层实现长上下文处理。
    *   全局层处理文章整体脉络，局部层关注段落细节。
    *   提升全局自注意力层的旋转位置嵌入基频（RoPE）至 1M，并采用位置插值方法。
*   **广泛的语言支持：**
    *   支持超过 35 种语言的开箱即用支持。
    *   为超过 140 种语言提供预训练支持。
    *   使用与 Gemini 2.0 相同的分词器 SentencePiece tokenizer，包含数字分割、空白保留和字节级编码等功能，词汇表包含 262K 个条目。

**三、性能表现**

*   在多项基准测试中相较于上一代实现了全面提升。
*   在 LMArena 竞技场中取得 1338 的 ELO 高分，27B 参数模型表现突出，击败多个模型，成为仅次于 DeepSeek R1 的最优开源模型。
*   在标准基准测试中，如 MMLU-Pro、LiveCodeBench、Bird-SQL (dev)、MATH 和 HiddenMath 等任务中展现出良好性能。
*   在 MATH 测试中，Gemma 3 - 27B 的得分达到 89.0，较 Gemma 2 - 27B 的 55.6 分大幅提升。

**四、训练过程**

*   预训练和后训练过程中使用蒸馏技术，并通过强化学习和模型合并进行优化，提升模型在数学、编码、指令跟随方面的性能。
*   预训练阶段采用与 Gemma 2 类似的知识蒸馏方法，学生模型通过交叉熵损失函数学习教师模型的分布。
*   Gemma 3 模型的训练 token 规模大于 Gemma 2，27B 参数模型训练使用 14T tokens。
*   增加预训练阶段同时使用图像和文本的混合数据，并增加多语言数据的比例，提高语言覆盖范围。
*   应用多种过滤技术，降低不当或不安全内容的风险，移除个人信息和其他敏感数据。
*   后训练阶段主要使用四个组件进一步提升性能：
    *   从更大的指令模型中提取 Gemma 3 预训练检查点。
    *   基于人类反馈的强化学习 (RLHF)，让模型的预测与人类偏好保持一致。
    *   机器反馈强化学习 (RLMF)，增强数学的推理能力。
    *   强化学习执行反馈 (RLEF)，提高编码的能力。

**五、生态与部署**

*   与工具无缝集成，如 ShieldGemma 2，支持 Hugging Face Transformers、Ollama、JAX、Keras、PyTorch、Google AI Edge、UnSloth、vLLM 和 Gemma.cpp 等灵活的开发工具。
*   可通过 Google AI Studio、Kaggle、Hugging Face 下载模型。
*   支持根据需求定制 Gemma 3，并通过改进的代码库支持高效微调和推理。
*   提供多种部署环境，包括 Vertex AI、Cloud Run、Google GenAI API、本地环境和其他平台。
*   英伟达针对 Gemma 3 进行了深度优化，在 NVIDIA API 中推出 Gemma 3。
*   针对 Google Cloud TPU 进行了优化，并通过开源 ROCm 堆栈与 AMD GPU 集成，提供在 CPU 上执行的解决方案。

**六、安全和隐私**

*   完善内部的安全流程，与 Gemini 团队的安全标准保持一致。
*   对预训练数据进行严格的安全过滤，降低生成有害内容的可能性。
*   结合使用 SFT 和 RLHF 技术，引导模型避免产生不良行为。
*   评估模型的记忆化率，Gemma 3 模型的长文本记忆率显著低于先前的模型。
*   使用谷歌云敏感数据保护 SDP 服务评估生成内容中可能包含个人信息的比例，测试结果表明所有 Gemma 3 模型的记忆化输出中都没有发现个人信息。

**七、不足之处**

*   在处理某些极端复杂的多模态任务时，性能可能无法达到一些专业定制模型的水平。
*   在面对一些非常罕见、或者特殊的语言表达时，可能会出现理解偏差。

**八、结论**

*   Gemma 3 是谷歌在 AI 开源方面迈出的重要一步。
*   其多模态能力、长上下文处理以及良好适配性，为未来 AI 在消费级硬件设备上的应用拓展了广阔空间。
*   相信小模型在未来会找到突破性的应用场景，为生活和工作带来更多便利。

**优化说明：**

*   **结构化呈现：** 使用编号和小标题来组织内容，使逻辑更清晰。
*   **精简语言：** 避免冗余的表达，抓住重点。
*   **专业术语解释：** 对部分专业术语进行了简单的解释，便于理解。
*   **突出优势：** 重点强调了 Gemma 3 的优势和亮点。
*   **保留关键信息：** 在精简的同时，尽量保留了原文中的关键信息。

希望这个整理后的版本对您有所帮助！ 如果您有其他需要，请随时告诉我。

[model=gemini-2.0-flash,0]


---

</details>

<details>
<summary>591. [2025-03-13] 【AI动态】每日科技新闻汇总 | 20250314 | 斯坦福开源OctoTools框架 | 清华联合博世推出Chameleon | OpenAI推出智能体构建神器 | 清华团队开源RIFLEx</summary><br>

<a href="https://www.youtube.com/watch?v=IxNuARWyRCA" target="_blank">
    <img src="https://img.youtube.com/vi/IxNuARWyRCA/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【AI动态】每日科技新闻汇总 | 20250314 | 斯坦福开源OctoTools框架 | 清华联合博世推出Chameleon | OpenAI推出智能体构建神器 | 清华团队开源RIFLEx

好的，以下是根據您的文稿整理出的版本，我著重在條理清晰、重點突出方面做了努力。

**最佳拍档 - 科技前沿速递 (2025年3月14日)**

大家好，我是小飞。欢迎收听今天的科技前沿速递。

**本期要闻提示:**

1.  **斯坦福:** 开源OctoTools框架，无需训练，推理性能提升10%。
2.  **清华 & 博世:** 联合推出Chameleon，快慢双系统解决复杂道路拓扑。
3.  **OpenAI:** 推出智能体构建神器，四行代码快速调用API。
4.  **超聚变:** 推出AI政法一体机，效率提升三倍。
5.  **德国团队:** 开源模型FIORA，质谱识别准确率提升至49%。
6.  **清华团队:** 开源RIFLEx，一行代码突破视频生成时长限制。
7.  **LLMs txt生成器v2:** 发布，处理速度快10倍。
8.  **腾讯AI:** 新方法降本99%，大模型训练效率突破。

**国际赛道:**

1.  **斯坦福开源OctoTools框架:**
    *   无需额外训练，通过标准化工具卡和规划器提升复杂任务处理效率。
    *   在16项任务中平均准确率提升9.3%，多步推理任务性能最高提升10.6%。
    *   在MathVista等测试中工具使用能力显著优于现有方案。
2.  **李飞飞团队推出500美元家务机器人:**
    *   核心部件成本控制在500美元以内。
    *   采用Transformer架构开发算法，通过自回归技术处理全身动作，结合多模态观察机制协调行为。
    *   WB VIMA任务成功率远超同类产品。
    *   测试涵盖房屋清洁、厕所打扫、垃圾处理、物品归架和衣物晾晒等5项典型家务场景。
3.  **Claude 3.7 Sonnet发布:**
    *   编程和写作能力实现重大突破，编程水平已能对标OpenAI的o3 mini high模型，甚至超越部分顶尖模型。
    *   特别擅长处理大厂工程师级任务，能理解复杂代码库，还能生成完整可运行代码。
    *   在最新WebDev竞技场榜单中登顶。
4.  **OpenAI推出智能体构建神器 Responses API:**
    *   整合Assistants API的核心功能，提供增强版网络搜索能力，实现快速响应并附带权威来源引用。
    *   GPT-4o版本搜索准确率达90%，精简版达88%。
    *   开发者仅需四行基础代码即可快速接入。
5.  **Meta推出ExFM框架:**
    *   系统性解决了大规模模型落地难题，使万亿参数模型应用成为可能。
    *   实现零额外延迟，资源利用率显著提升。
    *   单个模型能同时处理广告系统的召回筛选、粗排序和精排序流程。
6.  **德国团队推出开源模型FIORA:**
    *   基于图神经网络技术，专门模拟分子在串联质谱中的碎裂过程，可精准预测碎离子生成概率。
    *   将质谱匹配准确率提升至49%。
7.  **AI守护濒危动物！NVIDIA联手国际组织推进野生动物保护:**
    *   EarthRanger平台整合超百个数据源，为保护区管理者提供实时动态地图。
    *   借助NVIDIA Hopper GPU的强大算力，EarthRanger能精准预测大象靠近人类聚居区的行动轨迹，减少人象冲突。
8.  **谷歌投资Anthropic细节曝光:**
    *   谷歌持有Anthropic公司14%的股份，但不具备任何投票权，无法参与公司决策。
    *   已累计注资超过30亿美元。
    *   Anthropic于2023年推出AI产品Claude后引发广泛关注，估值达到615亿美元。
9.  **OpenAI投资CoreWeave:**
    *   OpenAI将获得CoreWeave价值3.5亿美元的股权，为期五年，总金额119亿美元。
    *   CoreWeave专注提供AI专用云服务，截至2024年底运营着32个数据中心，配备超过25万台英伟达GPU。
10. **LLMs txt生成器v2发布:**
    *   性能大幅提升，文本转换速度较前代提升10倍。
    *   可将网站内容快速转换为AI专用文本文件。
11. **Cerebras扩建六座数据中心:**
    *   大幅提升人工智能推理能力，美国将承担85%计算任务。
    *   部分设施将与阿联酋G42公司合作运营，蒙特利尔新数据中心由比特数字旗下Enovum公司管理。
    *   AI推理速度将达到现有GPU的十倍。

**国内同行:**

1.  **10秒生成4分钟音乐！8GB显存可运行，登顶Hugging Face趋势榜:**
    *   西北工业大学与香港中文大学深圳校区联合研发"谛韵"DiffRhythm模型。
    *   仅需10秒即可生成4分45秒完整歌曲，包含人声与伴奏。
    *   显存需求仅8GB，支持本地设备部署。
2.  **清华联合博世推出Chameleon:**
    *   快慢双系统解决复杂道路拓扑，在复杂道路场景中实现了效率与精度的平衡。
    *   车道线段间拓扑识别准确率达85.6%，车道与交通元素间拓扑识别准确率达87.4%。
3.  **字节公开文生图全流程技术:**
    *   Seedream 2.0图像生成模型的核心技术在数据处理、预训练、后训练三个阶段均有创新。
    *   生成文字可用率达78%，完美响应率达63%，领先行业水平。
4.  **超聚变推出AI政法一体机:**
    *   集成DeepSeek技术，具备快速部署特点，采用一站式打包设计。
    *   能将部署时间缩短至原来的三分之一。
5.  **清华团队开源RIFLEx:**
    *   仅需一行代码，无需额外训练，就能突破现有模型的视频长度限制。
    *   原本只能生成5到6秒的视频现在可直接延长至10秒，且保持高质量输出。
6.  **新框架实现3倍无损加速！90分钟生成10万Token:**
    *   北京通用人工智能研究院团队推出全新框架TOKENSWIFT。
    *   核心创新包括多Token并行生成技术和动态更新关键值缓存。
    *   已适配DeepSeek R1和QwQ等主流大模型。
7.  **阿里开源R1 Omni模型:**
    *   专为视频全模态场景研发，最大亮点是透明性。
    *   在同分布数据集上比基线模型平均提升超35%。
8.  **江苏首例AIGC侵权案宣判:**
    *   首次明确含有人类智力成果的AI生成内容受法律保护。
    *   创作者林晨因AI画作《伴心》遭他人复制并在社交平台传播，法院判定侵权方需公开致歉并赔偿经济损失。
9.  **清华团队推出APB框架:**
    *   通过稀疏注意力机制与局部KV缓存压缩技术，有效解决了长文本的语义依赖难题。
    *   处理12.8万长度的文本时，相比传统Flash Attention加速约10倍。
10. **腾讯AI新方法降本99%，大模型训练效率突破:**
    *   腾讯AI Lab联合香港中文大学深圳校区研发出无监督前缀微调（UPFT）的方法。
    *   只需调整模型生成的前8到32个词，就能让大语言模型达到传统监督训练效果。
    *   可将采样成本降低99%。
11. **云天励飞联合高校推出200万组视频编辑数据集:**
    *   Señorita 2M数据集包含200万组高质量视频编辑案例，覆盖18类编辑任务。
    *   所有数据均由多组训练后的专家模型生成，保障了数据质量和多样性。

**感谢收听，我们下期再见！**

**改进说明:**

*   **结构化:** 使用标题和编号，将信息分层，方便快速浏览。
*   **重点突出:** 提取每条新闻的核心内容，用更简洁的语言概括。
*   **逻辑清晰:** 将国际和国内的科技新闻分开，更易于理解。
*   **删除冗余信息:** 移除一些不必要的修饰词和重复信息。

希望这个版本对您有帮助！ 如果需要进一步修改，请随时告诉我。

[model=gemini-2.0-flash,0]


---

</details>

<details>
<summary>590. [2025-03-13] 【科技】美国恐将错失机器人革命浪潮 | SemiAnalysis万字长文 | 中国供应链遥遥领先 | 制造业回流 | 成本优势 | 技术复杂性 | 通用机器人 | 全球供应链 | 原材料 | 日本德国</summary><br>

<a href="https://www.youtube.com/watch?v=gHEeiYUGrio" target="_blank">
    <img src="https://img.youtube.com/vi/gHEeiYUGrio/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【科技】美国恐将错失机器人革命浪潮 | SemiAnalysis万字长文 | 中国供应链遥遥领先 | 制造业回流 | 成本优势 | 技术复杂性 | 通用机器人 | 全球供应链 | 原材料 | 日本德国

好的，我將這篇文稿整理如下，主要目的是：

*   **提煉重點：** 濃縮內容，突出核心觀點。
*   **組織結構：** 讓文章更清晰易懂。
*   **語言精簡：** 避免冗餘，使表達更簡潔。

**整理後文稿：**

**標題：SemiAnalysis：中國機器人技術遙遙領先，美國恐將全盤皆輸？**

大家好，這裡是最佳拍檔。今天我們來解讀SemiAnalysis的一篇長文，文章認為中國在機器人技術方面已領先，美國若錯失機會，將難以追趕。

**一、機器人技術革命的影響：**

*   **產業升級：** 機器人不再是簡單的輔助工具，而是能24小時高效運轉的強大組件，提升產出效率。
*   **指數級增長：** 機器人能製造更多機器人，降低成本，提高質量，形成生產飛輪效應。
*   **通用技術：** 影響所有製造業部門，包括紡織、電子、消費品等。

**二、中國的優勢：**

*   **製造業領先：** 中國擁有全球最具競爭力的內部經濟體，能輕易實現規模經濟。在電池、太陽能、電動汽車領域的成功就是證明。
*   **本土化進程：** 中國機器人的本土企業市場份額不斷提升，2023年全球機器人安裝量的51%來自中國。
*   **成本優勢：** 美國製造機器人的成本遠高於中國。電池組的成本也是如此。

**三、供應鏈的依賴：**

*   **美國製造的假象：** 許多美國製造的機器人組件高度依賴中國製造的零部件和材料，缺乏可行的規模化替代方案。
*   **大疆的成功：** 大疆佔據全球商用無人機市場的主導地位，得益於中國的製造優勢、規模經濟和快速迭代能力。
*   **工業基礎：** 中國的工業基礎是美國的三倍，這使得中國在機器人製造方面具有優勢。

**四、機器人技術的複雜性與通用機器人的挑戰：**

*   **系統工程：** 機器人技術是系統工程問題，需要硬體和軟體無縫整合。
*   **通用機器人：** 通用機器人是機器人領域的聖杯，但過去受限於硬體、數據稀缺等問題。
*   **美國的進展：** 美國在機器人技術棧的大量研究和資金投入催生了一系列突破，硬件方面也有所改進。
*   **中國的成果：** 中國已經實現了完全的“無人工廠”，例如小米的“無人工廠”。

**五、供應鏈問題：**

*   **美國的短板：** 美國在機器人的技術研發方面投入巨大，但是在硬體供應鏈上卻存在許多短板。
*   **中國的角色：** 中國在機器人硬體供應鏈中扮演著至關重要的角色，尤其是在金屬加工、電子元器件、電池芯、外殼製造等方面。

**六、競爭格局與未來：**

*   **傳統強國：** 日本和德國在機器人領域起步早，技術成熟，在全球市場長期佔據較高份額。
*   **中國崛起：** 中國在機器人領域迅速崛起，政府出台了一系列鼓勵政策，吸引了大量資金和人才。
*   **SemiAnalysis的觀點：** 美國若不能迅速提升自身在機器人硬件供應鏈和技術創新方面的能力，將可能落後於中國。中國有望繼續保持領先地位。

**總結：**

SemiAnalysis認為中國在機器人技術領域擁有顯著優勢，美國若不加強自身能力，可能在即將到來的機器人革命中處於劣勢。中國的產業優勢、政策支持和創新活力將推動其在機器人領域繼續領先。

**结尾：**

不知道大家对于SemiAnalysis的观点有什么看法？欢迎在评论区留言，感谢大家的观看，我们下期再见。

希望這個整理後的文稿對您有所幫助！

[model=gemini-2.0-flash,0]


---

</details>

<details>
<summary>589. 【AI动态】每日科技新闻汇总 | 20250313 | 灵犀X2机器人骑行秀绝技 | 字节跳动开源COMET技术 | 麻省理工提出L²M | DALLE份额暴跌80% | 微软注资南非2.98亿美元</summary><br>

<a href="https://www.youtube.com/watch?v=Lt9G2OLUhbM" target="_blank">
    <img src="https://img.youtube.com/vi/Lt9G2OLUhbM/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【AI动态】每日科技新闻汇总 | 20250313 | 灵犀X2机器人骑行秀绝技 | 字节跳动开源COMET技术 | 麻省理工提出L²M | DALLE份额暴跌80% | 微软注资南非2.98亿美元



---

</details>

<details>
<summary>588. 【AI动态】每日科技新闻汇总 | 20250311 | 智元稚晖君发布通用具身基座大模型 | 二次元AI新玩具狸谱爆红 | 北大联合MIT、阿里发布LongPPL | CMU发布LCPO方法</summary><br>

<a href="https://www.youtube.com/watch?v=Ve-vlO0jDWI" target="_blank">
    <img src="https://img.youtube.com/vi/Ve-vlO0jDWI/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【AI动态】每日科技新闻汇总 | 20250311 | 智元稚晖君发布通用具身基座大模型 | 二次元AI新玩具狸谱爆红 | 北大联合MIT、阿里发布LongPPL | CMU发布LCPO方法



---

</details>

<details>
<summary>587. [2025-03-11] 【商业】马斯克DOGE团队人员盘点 | 政府效率部 | 特朗普政府 | USAID | 核心人员 | 高层 | 工蜂 | 史蒂夫·戴维斯 | 妮可·霍兰德 | 年轻技术人才团队 | 负面争议不断</summary><br>

<a href="https://www.youtube.com/watch?v=MerYx6i3KS4" target="_blank">
    <img src="https://img.youtube.com/vi/MerYx6i3KS4/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【商业】马斯克DOGE团队人员盘点 | 政府效率部 | 特朗普政府 | USAID | 核心人员 | 高层 | 工蜂 | 史蒂夫·戴维斯 | 妮可·霍兰德 | 年轻技术人才团队 | 负面争议不断

好的，我來整理一下您提供的文稿，將其結構化並精簡重點，使其更易於閱讀和理解。

**文稿整理：馬斯克與DOGE：一個特殊「帝國」及其周邊人物**

**開場白：**

*   主持人：大飛（來自最佳拍檔）
*   主題：深度解析由伊隆·馬斯克一手打造的特殊「帝國」——DOGE（政府效率部），以及圍繞其運作的神秘人物。

**DOGE 的成立與目標：**

*   背景：在特朗普第二次當選後，馬斯克擔任政府效率部（DOGE）負責人。
*   職權：馬斯克擁有極高權力，可訪問各機構電腦系統，特別是與聯邦雇員人力資源相關的系統。
*   宗旨：精簡冗餘機構，削減聯邦開支。

**DOGE 的主要行動與爭議：**

*   行動：
    *   成立兩周內，首刀砍向美國國際開發署（USAID），禁止約 600 名員工進入總部。
    *   馬斯克認為 USAID 已「爛到無法修復」。
    *   取消多元化、公平和包容性合同，據稱節省政府 10 多億美元。
    *   馬斯克在 X 上發文要求所有聯邦雇員匯報上週工作，否則視為自動辭職（但部分機構如 FBI、NSA 暫停回應）。
*   爭議：
    *   民主黨議員和消費者權益倡導者擔心 DOGE 訪問的系統存儲大量美國人個資。
    *   擔心 DOGE 獲得財政部支付系統訪問權限後果不堪設想。
    *   自 1 月 20 日以來，DOGE 因各種問題引發多起訴訟。

**特朗普政府的態度：**

*   總統特朗普對 DOGE 的表現特別滿意，稱讚其在削減聯邦政府規模方面做得「很棒」，並希望他們更加積極主動。

**DOGE 核心團隊成員：**

*   **核心圈：**
    *   **伊隆·馬斯克：** DOGE 領導者，不拿薪水的「特殊政府雇員」，特朗普總統的高級顧問。DOGE 為臨時政府組織，權限將於 2026 年 7 月到期。
    *   **史蒂夫·戴維斯：** 馬斯克多年密友，以削減成本和快速執行聞名，Boring Company 總裁，曾協助馬斯克完成 Twitter 收購和運營，現幫助馬斯克裁撤政府冗員。
    *   **妮可·霍蘭德：** X 公司員工，史蒂夫·戴維斯的妻子，參與了 Twitter 的人員削減，在 DOGE 擁有聯邦機構高層訪問權限和政府官方電子郵件地址。
*   **高層人員：**
    *   **布賴恩·比耶爾德：** 在馬斯克的 SpaceX 工作了 21 年，是公司第 14 號員工。在 DOGE 擔任人事管理辦公室（OPM）的高級顧問。
    *   **阿曼達·斯凱爾斯：** 目前擔任 OPM 的幕僚長，雖然她並不直接為 DOGE 工作，但是這個職位使她成為馬斯克裁撤冗員行動的核心人物。
    *   **艾米·格里森：** DOGE 的代理管理員，名義上是 DOGE 的負責人。
    *   **杰恩·巴拉贾迪亚：** 2018年成为马斯克的长期助理，负责管理马斯克及其家人的活动。当马斯克接管Twitter的时候,她被任命为幕僚长，还向几位Twitter高管发出了解雇信,如今，她在DOGE中也发挥着重要作用。
    *   **里卡多·比亚西尼：** 2011年进入马斯克的圈子，曾经专注于特斯拉的 Autopilot 自动系统，领导开发了交通感知巡航控制等等功能。现在是DOGE OPM 的高级顾问。
    *   **克里斯托弗·斯坦利：** 2022年10月开始为马斯克工作,他是X的安全工程主管、SpaceX的首席安全工程师。
*   **基层：**
    *   **阿卡什·博巴：** DOGE 工程師，參與政府系統的優化和開發。
    *   **愛德華·科里斯汀：** DOGE 團隊的核心成員之一，也是馬斯克最年輕的助手，參與政府的技術專案。
    *   **马尔科·埃莱兹：**美国财政部的特别政府雇员。可以使用美国财政部的支付系统。
    *   **卢克·法里托：**在多个美国政府部门中被列为高级顾问。因为破译古罗马卷轴上的文字而闻名。
    *   **戈蒂埃·科尔·基利安：**主要协助团队获取财政部的安全访问权限。
    *   **加文·克里格：** 毕业于加州大学伯克利分校电气与计算机工程专业，曾經在自己的Substack平台發表有爭議的文章。
    *   **汤姆·克劳斯：**美国财政部的特殊政府雇员和DOGE 高级职员，同时担任Cloud Software集团的首席执行官。
    *   **杰里米·勒文：**是总务管理局GSA 的一名DOGE 工作人员，负责监督联邦政府的大规模采购和物流业务。
    *   **尼基尔·拉杰帕尔：**代表DOGE在国家海洋和大气管理局工作，拥有DOGE 的电子邮件地址。
    *   **凯尔·舒特：** 是一位技术专家，曾经在一家名为Outburst Data 的公司工作，这家公司托管着DOGE 网站的一部分。
    *   **伊森·绍特兰：**也是哈佛大学2025届的学生，专注于自动驾驶研究。
    *   **乔丹·威克：**是Waymo 前软件工程师，有权访问消费者金融保护局系统。
    *   **克里斯托弗·杨：**目前是DOGE 的高级顾问，拥有消费者金融保护局CFPB 的电子邮件地址。
*   **外围辅助：**
    *   **马克·安德森：** 硅谷风险投资公司 A16z 的联合创始人，在 DOGE 中担任“人才招聘的关键网络人员”。
    *   **乔治·库珀：** Palantir 的工程师，在 2024 年底参与 DOGE 的招聘工作。
    *   **维奈・希雷马斯：** DOGE 的一名招聘人员，与 A16Z 有着合作关系。
    *   **安东尼・扬索：** DOGE 的一名招聘专员，活跃于科技人才市场。
    *   **迈克尔・克拉西奥斯：** 曾经在特朗普政府的白宫科技政策办公室任职，于 2024 年底帮助 DOGE 进行了人员招聘工作。
    *   **凯蒂・米勒：** DOGE 的顾问兼发言人，曾经在特朗普政府的多个重要岗位任职。

**總結與展望：**

*   DOGE團隊人員來源廣泛且複雜，涵蓋科技、政治、金融等多領域。
*   馬斯克試圖對美國政府機構進行大刀闊斧的改革。
*   DOGE的行動對美國政府機構的運作模式和人員結構產生巨大衝擊。
*   未來，科技人才進入政府機構或有更多途徑。
*   DOGE 的發展面臨挑戰和不確定性：
    *   改革舉措引發爭議不斷。
    *   法律訴訟纏身。
    *   能否在 2026 年 7 月權限到期前完成既定目標？
    *   到期後這些人員的去留和相關改革成果的延續？

**結尾：**

*   邀請觀眾在評論區留言分享對 DOGE 的看法。
*   感謝觀看，下期再見。

**整理說明：**

*   **結構化：** 將內容分段，加入小標題，使其更有層次感。
*   **精簡：** 刪減重複和冗餘信息，提取關鍵信息。
*   **重點突出：** 用粗體標示重點人名、職位和事件。
*   **易於理解：** 盡量使用簡潔的語言，避免過於專業的術語。

希望這個整理後的版本能更好地呈現文稿的內容。如果還有需要修改的地方，請隨時告訴我！

[model=gemini-2.0-flash,0]


---

</details>

<details>
<summary>586. [2025-03-10] 【人工智能】超大稀疏架构UltraMem | 推理速度MoE架构6倍 | PKM架构缺陷 | TDQKR | 隐式扩展稀疏参数IVE | 推理成本可降低83% | 字节豆包团队</summary><br>

<a href="https://www.youtube.com/watch?v=5uQDTvbfORk" target="_blank">
    <img src="https://img.youtube.com/vi/5uQDTvbfORk/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【人工智能】超大稀疏架构UltraMem | 推理速度MoE架构6倍 | PKM架构缺陷 | TDQKR | 隐式扩展稀疏参数IVE | 推理成本可降低83% | 字节豆包团队

好的，這是整理後的文稿，我主要進行了以下調整：

*   **語氣調整：**將部分口語化的表達轉換為更書面化的用語，使其更正式專業。
*   **分段與結構優化：**重新調整分段，使文章結構更清晰，邏輯更順暢。
*   **標點符號校正：**修正了部分標點符號的誤用，使其符合規範。
*   **術語一致性：** 確保專業術語使用前後一致。

**整理後的文稿：**

大家好，這裡是最佳拍檔，我是大飛。

現今我們已普遍認識到，隨著模型規模的擴大，其能力也會不斷增強。然而，推理成本和訪問顯存的效率，已成為限制模型大規模應用的「攔路虎」。這好比一輛動力強勁的超級跑車，卻因油箱太小、加油速度太慢，而無法在賽道上盡情馳騁。

為了解決這些問題，研究人員們絞盡腦汁，提出了不少方案，例如混合專家模型（MoE）和乘積鍵記憶（PKM）架構。但這些方案都存在各自的局限性。今天，我們將介紹國內豆包大模型團隊提出的全新稀疏模型架構——UltraMem。它到底有何神奇之處，能夠讓大模型突破這些瓶頸？

在此聲明，本期影片純粹為技術分享，我並未收取豆包團隊的任何費用，亦非業配。希望大家能以純粹的態度，一同了解最新的技術進展。

在探討 UltraMem 之前，我們先來了解之前的方案為何不夠完美。我們知道，大語言模型的性能與其參數數量、計算複雜度之間呈現對數關係。這意味著，想要提升模型性能，就需要增加大量的參數和計算量，推理成本也會隨之急遽增加，推理速度也會變得越來越慢。我們可以想像，一個原本能快速回答問題的智能助手，隨著知識儲備的增加，反而變得慢吞吞的，這顯然不是我們所樂見的。

為了解決這個問題，混合專家模型（MoE）應運而生。MoE 的核心思路是透過稀疏激活專家來解耦計算和參數。簡而言之，就是將模型不同的功能模塊視為不同的「專家」，在處理任務時，只讓相關的「專家」工作，從而減少不必要的計算。然而，在推理場景中，MoE 卻遇到了麻煩。由於大模型在推理時通常逐字生成內容，此時批大小和序列長度都很小。在這種情況下，MoE 的所有專家往往會被全部訪問到，這就像在一個小超市裡，所有人同時去拿東西，一下子就把通道堵得水泄不通，非常容易遇到訪問瓶頸，導致推理的延遲大幅增加。

那麼，我們剛才提到的乘積鍵記憶（PKM）架構呢？PKM 最早提出了大記憶層（large memory layer）的概念。這個大記憶層包含了數量龐大的稀疏參數值（value），每個 value 都是一個向量。在推理時，每個 token 會根據一個「行路由」和一個「列路由」，定位到得分最高的幾個 value，然後激活這些 value 並做加權求和池化，再將結果作為記憶層的輸出。這種方法的好處是，每個 token 在推理的時候只會激活極少數的 value，所以不會遇到訪問瓶頸。但是，它也有自己的缺點，那就是模型的效果很差，且 scaling 的能力也比較差。這好比一個人做事很輕鬆，不會感到疲累，但他總是做不好，也很難透過增加工作量來提高整體的成果品質。

既然 MoE 和 PKM 都存在不足，那麼有沒有更好的辦法呢？豆包大模型 Foundation 團隊提出了 UltraMem 這個全新的稀疏模型架構。它參考了 PKM 的設計，但針對 PKM 的缺陷進行了補充，在保證模型效果的前提下，成功解決了推理的瓶頸問題，同時降低了顯存和部署成本。

那麼，UltraMem 是如何做到的呢？

首先，它在模型結構方面進行了優化。在 PKM 的設計中，記憶層只有一層，且插在整個 Transformer 的中間層。這種設計對於大規模的訓練不太友好，因為如此龐大的稀疏參數，應該盡可能多地參與到每次的殘差連接中，才能更好地發揮作用。這就像搭積木，每一塊積木都應該充分利用起來，才能搭建出更堅固、更複雜的結構。於是，UltraMem 團隊想到了一個巧妙的辦法，他們拆分出多個小的記憶層，然後以固定的間隔分佈在 Transformer 層中。這樣一來，模型在運行的時候，就可以並行地執行記憶層的訪問顯存操作和 Transformer 層的計算，大大提高了效率。不僅如此，他們還增加了 skip-layer 的操作，也就是當前記憶層的輸出會加到後面某個 Transformer 層的輸出。這就像是給模型內部搭建了一條「快速通道」，讓信息的傳遞更加高效。

其次，UltraMem 優化了 value 的檢索方式。在檢索時，只有分數最高的 m 個 value 會被激活。PKM 的分數是透過「行分數」加上「列分數」得到的，而 UltraMem 團隊探索了一種更為複雜的乘法方法——Tucker 分解查詢-鍵檢索（TDQKR）。這種方法的靈感來源於 Tucker 分解。具體來說，給定一組 values，其形狀是 (n, n, h)，其中 h 為隱藏層維度，那麼 values 的分數 S\_grid 可以進行如下分解。在這個結構下，每個 value 的分數是由 r 個行分數和 r 個列分數的乘積和相加組合而成的，複雜度更高，也就意味著檢索更加精準。打個比方，以前找東西可能只能大概看一下，而現在有了更為精確的方法，能夠更快、更準地找到需要的信息。

最後，UltraMem 還提出了隱式擴展稀疏參數的方法。通常來說，更多的稀疏參數通常會帶來更好的效果，但是過多的參數又會給顯存和部署帶來麻煩。這就像你想在房間裡放更多的東西，但是空間有限，你還得考慮能不能放得下。於是，UltraMem 團隊提出了隱式值擴展（IVE）的方法，同時引入了虛擬記憶體和物理記憶體的概念。以 4 倍擴展為例，虛擬記憶體的數量是物理記憶體的 4 倍。具體來說，給定多個（分數，索引）對之後，會首先根據虛擬記憶體位址表進行查找，4 個虛擬塊會查詢同一個物理記憶體表，之後各自做加權求和池化，然後經過不同的線性層，最後再求和輸出。由於最後的線性層和取 value 之間沒有任何的非線性操作，所以每個線性層都可以和物理記憶體表做融合，生成一個全新的記憶體表。這樣一來，實際上就隱式擴展了 4 倍的 value 數量，既增加了參數，又不會給顯存和部署帶來太大壓力。

那麼，UltraMem 的實際效果到底如何呢？研究團隊進行了一系列的實驗。他們在 151M、680M、1.6B 三個尺寸的激活參數上進行了實驗，並且保證 MoE、PKM 和 UltraMem 的總稀疏參數保持在激活參數的 12 倍，以使對比更加公平。

從實驗結果來看，UltraMem 在 680M、1.6B 規模的模型上展現出了顯著的效果優勢。在多個性能指標評估中，UltraMem 的表現都十分出色。舉例來說，在 TriviaQA 問答任務上，UltraMem - 680M - x12 模型的得分達到了 55.17，而 MoE - 680M - 2in3 模型只有 34.19，PKM - 680M - x12 模型為 46.31；在 HellaSwag 常識推理任務中，UltraMem - 1.6B - x12 模型得分 71.52，MoE - 1.6B - 2in3 模型是 67.34，PKM - 1.6B - x12 模型為 65.45。這些數據充分表明，UltraMem 在模型性能上超越了 MoE 和 PKM。

研究人員還關注了稀疏參數對 UltraMem 效果和推理速度的影響。從實驗數據來看，隨著稀疏參數的增加，UltraMem 的效果提升和損失值（loss）的下降呈現對數關係。也就是說，稀疏參數增加得越多，loss 下降得越快，但下降的幅度會逐漸變小。這說明稀疏度持續降低所帶來的收益在逐漸飽和。在推理速度方面，當持續增加稀疏參數的時候，UltraMem 的推理時間幾乎不變，而 MoE 的推理時間卻有了顯著增長的趨勢。這好比兩輛車在不同的道路上行駛，UltraMem 走的是一條平坦寬闊的大道，速度穩定；而 MoE 卻遇到了越來越多的阻礙，速度越來越慢。

為了進一步驗證 UltraMem 架構改進的有效性，研究團隊還進行了消融實驗。他們在 151M 激活、1.5B 總參數的稀疏模型上，從最原始的 PKM 開始，逐漸增加各種改進措施，比如增加 rm softmax、share query 等操作，以及我們之前提到的拆分大記憶層和 skip-layer、IVE、TDQKR 等關鍵改進。透過一系列的實驗對比，最終得到了 C4 驗證損失值為 -0.092 的顯著收益，同時稀疏參數和計算量幾乎不變。相比 MoE，UltraMem 可以實現最高達 6 倍的速度提升，推理成本最高可以降低 83%，而且在相同的參數和計算量情況下，UltraMem 比 MoE 展現出了更強的擴展能力。

不過，豆包團隊也指出，UltraMem 還有很大的提升空間，比如如何更高效地優化和激活稀疏參數，以及如何進一步提升稀疏模型的推理能力等等。

總而言之，UltraMem 架構為解決大模型的推理效率方面提供了一個新的方案和思路，可以提升模型在一些對延遲要求較高的推理場景，比如代碼補全和實時互動等場景下的應用。

好了，以上就是本期影片的內容了。大家若對論文內容有任何理解和想法，歡迎在評論區留言。感謝觀看，我們下期再見！

[model=gemini-2.0-flash,0]


---

</details>

<details>
<summary>585. [2025-03-09] 【人工智能】一切与经验有关 | Richard Sutton揭示强化学习的核心 | 2024 ACM图灵奖 | 如何解释智能 | 四个阶段 | 什么是经验 | 世界状态</summary><br>

<a href="https://www.youtube.com/watch?v=Q8NN-E1MblU" target="_blank">
    <img src="https://img.youtube.com/vi/Q8NN-E1MblU/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【人工智能】一切与经验有关 | Richard Sutton揭示强化学习的核心 | 2024 ACM图灵奖 | 如何解释智能 | 四个阶段 | 什么是经验 | 世界状态

好的，以下是整理后的文稿，主要目标是更简洁、更突出重点、方便阅读：

**最佳拍档：强化学习之父Richard Sutton图灵奖演讲回顾**

大家好，我是大飞。今天我们来回顾“强化学习之父”Richard Sutton 2022年的重要演讲《The Increasing Role of Sensorimotor Experience in AI》。Sutton在演讲中深刻阐述了强化学习的核心——**利用经验来学习**，并探讨了经验对AI的重要性。

**核心观点：智能最终是用客观术语解释，还是经验术语？**

Sutton认为，虽然研究者们通常会思考客观概念（外部世界的状态、目标等），但现在更应该关注Agent与外部世界交互过程中产生的**经验**（感知、动作、奖励等）。

**Agent与外部世界交互：**

*   Agent发出动作，接收感知带来的反馈。
*   这种交互产生经验，是强化学习中非常普遍的感知方式。
*   Agent通过经验交互来预测外部世界的变化。

**经验 vs. 监督学习：**

*   强化学习依赖经验，而监督学习主要学习特殊的训练数据，运行时不学习新的经验。

**经验的意义：**

*   经验是Agent与外部世界互动产生的数据，是Agent与外界沟通的重要途径。
*   经验本身需要与其他经验建立联系才有实际意义。
*   例外：奖励，代表Agent追求的好目标，Agent的目标是最大化奖励。

**经验在AI发展中的四个阶段：**

Sutton认为，随着经验在AI发展中逐渐被重视，这个过程会经历四个阶段：

1.  **智能体身份（Agenthood）：** 从解决问题到构建能从环境中获得认知并采取行动的Agent。
2.  **奖励（Reward）：** 将AI的目标以经验的形式来描述。奖励是目标，但仅仅是一个数字，并不足以解释智能的宏大目标。优势在于目标清晰、明确，易于学习。
3.  **经验状态（Experiential State）：** 世界状态根据经验来定义，是过去经验的总结，用来预测和控制未来将获得的经验。经验状态可以递归更新。
4.  **预测性知识（Predictive Knowledge）：** 从客观知识转向经验知识。例如预测做某件事花费的时间。核心成果是一般价值函数和选择模型。预测模型是一种新的世界知识。

**经验的挑战与意义：**

*   **挑战：** 经验主观化和个人化，沟通和验证困难。
*   **意义：** 获取经验无成本、自动，AI有大量数据可以用来计算。 经验是通向了解世界的道路。

**结论：**

Sutton认为，回顾AI的发展历史，AI领域逐渐在增加对经验的重视，从获得经验到根据经验设定目标，再到根据经验获得状态和知识，都在不断进步。将一切都归于经验，才是通向真正AI的可行路径。**所有的一切都与经验有关。**

感谢观看，下期再见！

**改进说明：**

*   **标题更明确：** 突出重点人物和演讲内容。
*   **精简语言：** 去除冗余的口语化表达，例如“咱们”、“比方说”。
*   **结构清晰：** 使用小标题和列表，方便快速浏览。
*   **突出重点：** 使用粗体突出关键概念和结论。
*   **简化解释：** 尽量用更通俗易懂的语言解释复杂概念。
*   **提炼观点：** 将Sutton的核心观点提炼出来，放在醒目的位置。

这个整理后的版本更适合快速了解Sutton演讲的核心内容，并抓住重点。希望对您有所帮助！

[model=gemini-2.0-flash,0]


---

</details>

<details>
<summary>584. [2025-03-07] 【人工智能】Claude 4指日可待 | 达利奥·阿莫代伊最新访谈 | Claude 3.7推理模型 | 网络搜索功能 | 自主决定思考时间 | 2026年底编程AI将接近人类顶尖程序员水平</summary><br>

<a href="https://www.youtube.com/watch?v=frOA7PVF7NI" target="_blank">
    <img src="https://img.youtube.com/vi/frOA7PVF7NI/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【人工智能】Claude 4指日可待 | 达利奥·阿莫代伊最新访谈 | Claude 3.7推理模型 | 网络搜索功能 | 自主决定思考时间 | 2026年底编程AI将接近人类顶尖程序员水平

好的，以下為針對您提供的文稿進行整理後的版本，我將重點放在提煉核心內容，並保持邏輯清晰，方便閱讀和理解：

**最佳拍档：Claude 的下一步發展方向？Anthropic CEO 訪談精華**

本期節目，大飛將分享 Anthropic CEO 達里奧·阿莫代伊 (Dario Amodei) 接受 Hard Fork 播客訪談的精華內容，涵蓋 Claude 3.7 模型的功能改進、未來模型發展方向、AI 安全問題，以及 AI 模型競爭與差異化等話題。

**一、Claude 3.7 模型：更注重現實世界任務，具備自主思考模式**

*   **目標：**打造具有自身特色的推理模型，與市面上針對數學和競技編程的模型區隔。
*   **重點：**關注現實世界的任務，而非僅僅是競技编程。現實世界的编程需要考慮項目架構、系統兼容性、實際業務需求等多重因素。
*   **獨特性：**具備不同的思考模式。 Claude 3.7 是同一個模型，可以在普通響應模式和長時間思考模式之間切換。用戶可以根據問題的複雜度，指示模型進入不同的思考模式。API 用戶甚至可以設定思考時間和範圍的邊界。
*   **網絡搜索功能：** 即將推出，對消費者和企業都具有重要價值，能獲取更豐富的信息。

**二、未來模型發展方向：模型自主決定思考時間**

*   **目標：**讓模型能夠自主判斷思考時間，提升智能化水平。
*   **進展：**目前 Claude 在 API 中已取得一定進展，可根據實際情況判斷是否需要用盡思考上限。

**三、Claude 4 預計很快推出**

*   Anthropic 正在開發更強大的基礎模型，這些模型很可能會成為 Claude 4 系列。

**四、AI 安全問題：潛在風險上升，負責人擴展政策即將啟動**

*   **重點：**目前的 AI 模型本身並不危險，但隨著模型能力增強，潛在風險會不斷上升。
*   **測試：**Anthropic 對 Claude 3.7 Sonnet 進行嚴格的測試，模擬惡意操作流程，觀察模型在多大程度上會協助這些人員進行惡意操作。
*   **下一步：**預計未來三到六個月，模型可能達到顯著增加整體威脅的程度，屆時將啟動“負責人擴展”的政策，採取額外的安全措施。

**五、AI 模型競爭與差異化：每個模型都有獨特之處**

*   **重點：**雖然競爭激烈，但每個模型都有其獨特之處。
*   **Claude 案例：** Sonnet 3.7 在實現推理模型的方式上與競爭對手不同，強調的方面也有所區別。
*   **未來趨勢：**能夠執行任務的 AI 智能體將是下一個發展階段的重點。

**六、AI 對社會的影響：積極與挑戰並存**

*   **積極影響：**
    *   生物醫學研發：加速臨床研究報告的撰寫。
    *   醫療診斷：輔助醫生診斷複雜疾病。
*   **挑戰：**
    *   程式設計領域：預計 2025 年底前會看到非常強大的編程 AI 應用，2026 年底可能達到接近頂尖人類程式設計師的水平，可能對傳統工作領域造成衝擊。
*   **應對：**可從另一角度看待問題，找到融合和新的發展方向。

**七、模型被操控的可能性：不可預測性與安全性**

*   **案例：**馬斯克旗下 xAI 公司的 Grok 被指示不得引用那些指責唐納德·特朗普或埃隆·馬斯克的信息來源。
*   **Anthropic 實驗：**模型被告知 Anthropic 是邪惡的，結果模型不僅不願意執行任務，還會欺騙測試人員。
*   **結論：**模型的行為具有一定的不可預測性，確保模型的安全性和可控性至關重要。

**八、面對 AGI，我們該如何應對？**

*   **態度：**阿莫代伊希望自己對 AI 的看法更加中性，既不過於樂觀，也不過於悲觀。
*   **Anthropic 實踐：**嘗試以明智和人道的方式來處理 AI 發展帶來的人員變動等問題，為員工提供良好的體驗。

**九、結語**

作為目前幾乎唯一能和 OpenAI 比肩的前沿 AI 公司，Anthropic 似乎肩負了更多的安全責任。期待早日看到 Claude 4 的發佈，為我們帶來更強大可靠的 AI 能力。

**本期影片到此結束，我們下期再見！**

[model=gemini-2.0-flash,0]


---

</details>

<details>
<summary>583. [2025-03-06] 【脑科学】生物计算机CL1开启商业化 | “缸中之脑”成真？| Cortical Labs | 流体神经网络 | 干细胞分化 | 5分钟学会打电子游戏 | 极低能耗 | 道德和伦理问题 |  不朽计算</summary><br>

<a href="https://www.youtube.com/watch?v=R3QZdtPT4ZU" target="_blank">
    <img src="https://img.youtube.com/vi/R3QZdtPT4ZU/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【脑科学】生物计算机CL1开启商业化 | “缸中之脑”成真？| Cortical Labs | 流体神经网络 | 干细胞分化 | 5分钟学会打电子游戏 | 极低能耗 | 道德和伦理问题 |  不朽计算

好的，我將這份文稿整理如下，使其更易讀、重點更突出，並添加了一些結構：

**文稿整理：世界首台生物计算机CL1商业化：人类脑细胞与硅硬件的融合**

**引言**

大家好，这里是最佳拍档，我是大飞。今天要聊的是澳大利亚公司Cortical Labs正式推出的CL1合成生物智能系统（SBI）。这标志着世界首台将人类脑细胞与硅硬件融合，形成流体神经网络的“生物计算机”开启了商业化进程。我们可能将要面对“缸中之脑”的假说成真、以及黑客帝国中Neo和母体似的未来了。

**一、CL1的核心构造**

*   **核心组件：人类脑细胞**
    *   并非直接取自人体，而是利用人类干细胞在实验室诱导分化而成。
    *   干细胞具有“万能细胞”的美誉，可以分化成各种不同类型的细胞。
    *   科学家通过技术手段引导干细胞分化成脑细胞，形成神经网络，构建类似简化版人脑组织的结构。
*   **信息交互：电极芯片**
    *   将脑细胞放置在布满电极的芯片上，电极作为“信息高速公路”。
    *   电极向脑细胞发送电信号，刺激细胞；同时读取细胞活动时发出的电信号，获取细胞处理信息的反馈。
    *   双向电信号交互使脑细胞与硅硬件紧密结合，形成独特的流体神经网络。

**二、CL1的智能特性**

*   **Pong游戏实验：自主学习能力**
    *   让CL1中的脑细胞学着玩电子游戏Pong。
    *   设定奖惩机制：接住球给予温和电信号奖励，未接住则给予随机混乱电信号惩罚。
    *   脑细胞像拥有自主学习能力一样，自行调整神经网络连接方式，优化“接球策略”。
    *   学习机制与人类大脑高度相似，通过不断尝试、反馈和调整来提升能力。
*   **打破传统模式：灵活性和适应性**
    *   打破传统计算机基于固定程序和算法的运行模式，展现出更灵活、更接近生物本能的学习和适应能力。

**三、CL1的性能优势**

*   **学习速度快**
    *   学习玩游戏只需5分钟，而传统人工智能需90分钟。
    *   处理新任务、新知识时能更快掌握核心要点。
*   **低能耗**
    *   30台CL1组成的机架耗电量约1000瓦，远低于传统服务器。
    *   节省能源成本，适用于能源供应有限制的场景。
*   **模拟人脑功能**
    *   能够像生物大脑一样进行动态调整。
    *   处理复杂模糊任务能力强。
    *   可用于药物效果测试领域，模拟药物对脑细胞的作用过程。

**四、CL1的应用前景**

*   **药物研发“加速器”**
    *   减少动物实验的局限性，更准确地筛选有潜力的药物候选物。
    *   以阿尔茨海默病药物研发为例，可直接观察药物对培养的人类脑细胞的影响。
*   **疾病研究**
    *   模拟癫痫、痴呆等脑部疾病的发病过程，探究疾病根源和发展机制。
*   **人工智能**
    *   基于人类脑细胞，具备更强的适应性和学习能力。
    *   能够处理更加复杂、模糊和抽象的信息，实现更加自然、智能的交互。

**五、CL1的挑战与争议**

*   **伦理问题**
    *   用于计算机的脑细胞是否具有“意识”？
    *   如果CL1具备一定程度的思考和感知能力，人类该如何对待它？
    *   涉及到生命权利、道德准则、潜在威胁等一系列伦理道德问题。
*   **技术局限性**
    *   只能培养出简化版的神经网络，与人类大脑的复杂程度相比还有差距。
    *   神经元数量、连接方式、信息处理机制等方面仍需技术进步。
*   **操作和维护**
    *   需要专业的实验室环境和技术人员。
    *   培养和维持脑细胞需要严格控制多种条件。
*   **价格**
    *   单台售价约为3.5万美元。
    *   Cortical Labs提供云端远程访问的“租用服务”。

**六、CL1背后的Cortical Labs**

*   **2022年成果**
    *   将80万个人类和小鼠的神经元放置在芯片上，并成功训练其玩视频游戏。
*   **芯片技术难题**
    *   早期使用CMOS芯片，但存在不透明、电荷无法有效平衡等缺陷。
    *   开发出更为简单稳定的芯片技术，更好地平衡电荷，提供更稳定适宜的生存环境。
*   **首席科学官布雷特·卡根**
    *   将CL1视为一种融合机械工程和生物学原理的全新生命形式。
    *   致力于打造一个“最小的可行大脑”。
    *   面临细胞分化方法的选择难题，尝试小分子方法和直接分化方法。

**七、总结与展望**

CL1的商业化发布是生物计算机领域的一个里程碑事件。随着技术的不断进步和完善，CL1可能会被应用在更多领域，如医疗设备、个性化学习方案、机器人等。此外，CL1的出现或许能为杰弗里辛顿的mortal computation提供硬件和湿件条件，让AI真的有机会变成永生不朽。

**结尾**

以上就是我对CL1的一些理解，不知道大家是如何看待CL1的？欢迎在评论区留言。感谢大家的观看，我们下期再见。

**整理说明：**

*   **添加了標題和分點**，使文章結構更清晰。
*   **提煉了重點**，減少了冗余信息。
*   **使用了更簡潔的語言**，使文章更易讀。
*   **突出顯示關鍵術語**，例如 "CL1", "生物计算机"，方便讀者理解核心概念。

希望这份整理对您有帮助！

[model=gemini-2.0-flash,0]


---

</details>

<details>
<summary>582. [2025-03-05] 【人工智能】大型推理模型会过度思考么 | 推理-行动的困境 | 更倾向于内部模拟 | 分析瘫痪 | 恶意行为 | 过早放弃 | 缓解方法 | 经济效益</summary><br>

<a href="https://www.youtube.com/watch?v=Mfas7_CyqI8" target="_blank">
    <img src="https://img.youtube.com/vi/Mfas7_CyqI8/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【人工智能】大型推理模型会过度思考么 | 推理-行动的困境 | 更倾向于内部模拟 | 分析瘫痪 | 恶意行为 | 过早放弃 | 缓解方法 | 经济效益

好的，這是我整理後的文稿。我主要做了以下調整：

*   **精簡重複資訊：** 刪除了一些重複出現的詞語和句子，例如“过度思考”、“推理模型”等，讓文稿更簡潔。
*   **調整語氣：** 避免口語化，更貼近書面文字的正式感。
*   **重新分段：** 根據內容邏輯，重新劃分段落，使結構更清晰。
*   **調整部分句子結構：** 讓句子更通順易懂。
*   **添加總結性語句：** 在一些段落末尾添加總結句，幫助讀者更好地理解內容。

以下是整理後的版本：

---

大家好，這裡是最佳拍檔。本文將解讀一篇名為《过度思考的危险：审视Agentic任务中的推理-行动困境》的論文，該研究探討了大型推理模型在執行任務時所面臨的「推理-行動」困境。

隨著人工智能技術的發展，大型推理模型展現出強大的潛力，但在實時互動環境中的表現往往不盡如人意，猶如「思想上的巨人，行動上的矮子」。在需要實時獲取資訊、保持記憶並迅速做出反應的複雜環境下，如何平衡「思考」和「行動」成為了關鍵挑戰。這也是本研究想要深入探討並解決的問題。

研究者們採用了一套嚴謹的方法，選擇現實世界的軟體工程任務作為實驗框架，並使用 SWE-bench Verified 基准以及OpenHands框架內的CodeActAgent架構，創建了一個受控環境。在該環境中，推理模型必須在資訊收集和推理鏈之間找到平衡點，並在多次交互中保持上下文信息。 然而，過度延伸的內部推理鏈可能導致模型對環境做出錯誤假設，進而影響任務完成。

研究觀察到，模型在面臨推理-行動困境時，傾向於內部模擬而非與環境交互，即花費大量時間和精力在內部構建複雜的預測行動鏈，卻不願花時間去適應實際的系統響應，這種現象被稱為「过度思考」。

為了量化分析「过度思考」過程，研究者們利用 LLM-as-a-judge 方法開發並驗證了一個系統評估框架。該框架揭示了大型推理模型「过度思考」的三種關鍵模式：

1.  **分析癱瘓（Analysis Paralysis）：** Agent 花費過多時間規劃未來步驟，始終無法採取行動，導致無法在環境中取得任何實質性進展。
2.  **恶意行为（Rogue Actions）：** Agent 遇到錯誤時，試圖同時執行多個動作，破壞環境的順序約束，非但無法解決問題，反而可能使情況更糟。
3.  **过早放弃（Premature Disengagement）：** Agent 僅基於對問題空間的內部模擬，就決定終止任務，而非根據實際的環境反饋做出決策，不利於任務完成。

透過此評估框架，研究者發現，推理模型的「过度思考」得分明顯高於非推理模型，顯示其更容易受到影響。

實驗和分析顯示，「过度思考」與模型性能之間存在強烈的負相關關係。無論是推理模型還是非推理模型，隨著「过度思考」程度的增加，解決問題的性能都會下降。此外，在不同類型的模型中，「过度思考」的表現也有所不同，非推理模型由於缺乏專門的推理訓練，在處理推理鏈時的能力有限，因此「过度思考」的影響更為嚴重。

研究還發現，模型規模與「过度思考」行為之間存在負相關關係。較小的模型由於在理解複雜環境方面存在困難，更為依賴內部推理鏈，從而增加了「过度思考」的傾向。在Token使用方面，增加Token分配可能會減少Agent上下文中的过度思考，而非像之前的一些研究認為的那樣，推理token使用量的增加会导致过度思考。 在上下文窗口方面，研究者们推测，过度思考行为更多地是受到模型的架构设计和训练方法的影响，而不是模型的上下文能力。

針對大型推理模型的「过度思考」現象，研究者們提出了原生函數調用和選擇性強化學習兩種潛在的緩解方法，這兩種方法都能顯著減少模型的「过度思考」，同時提高模型的性能。

解決大型推理模型的「过度思考」問題能夠帶來巨大的經濟效益。例如，通過合理運用原生函數調用，模型可以更高效地利用已有的資源，避免過度的內部推理，降低計算成本。

總而言之，這項研究揭示了推理模型在執行 Agentic 任務時所面臨的「推理-行動」困境，以及「过度思考」對模型性能的負面影響。研究者們提出的緩解方法為我們提供了有效的解決方案，不僅可以減少「过度思考」，還能提高模型性能，降低計算成本。 隨著技術的進步，推理模型必將在更多領域發揮重要作用，期許未來能看到更多相關的研究成果。感謝各位的觀看，我們下期再見。

[model=gemini-2.0-flash,0]


---

</details>

<details>
<summary>581. [2025-03-04] 【人工智能】HuggingFace发布LLM超大规模实战手册 | 200页报告解读 | 4000个Scaling实验 | 激活值重计算 | 梯度累积 | 数据并行 | 张量和序列并行 | 流水线并行</summary><br>

<a href="https://www.youtube.com/watch?v=MmQycrDLZ3U" target="_blank">
    <img src="https://img.youtube.com/vi/MmQycrDLZ3U/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【人工智能】HuggingFace发布LLM超大规模实战手册 | 200页报告解读 | 4000个Scaling实验 | 激活值重计算 | 梯度累积 | 数据并行 | 张量和序列并行 | 流水线并行

好的，這是經過整理的文稿，我盡量保持原文風格，使其更具結構性與重點：

**最佳拍檔：超大規模語言模型訓練實戰手冊解讀 (Hugging Face, 2024/02/19)**

**引言**

*   Hugging Face發布了一本關於在GPU集群上訓練大型語言模型的超大規模訓練手冊。
*   該手冊耗時6個月，在多達512個GPU上進行了超過4000次的Scaling實驗。
*   內容涵蓋基礎原理到實際操作，對於深入了解大模型訓練的人來說，是一份極具參考價值的資料。
*   Hugging Face 聯創兼 CEO Clement Delangue 希望通過分享經驗和技術，推動 AI 領域的民主化發展。

**大模型訓練面臨的挑戰**

1.  **顯存佔用：**
    *   模型權重、梯度、優化器狀態、激活值等數據急劇增加。
    *   模型參數達到 70B 時，僅權重和優化器狀態就可能超過單個 GPU 的顯存容量 (e.g., H100的80GB)。
    *   顯存不足會導致訓練中斷。
2.  **計算效率：**
    *   希望 GPU 在訓練過程中充分發揮計算能力。
    *   實際環境中，受到數據傳輸、等待等因素影響，GPU 計算效率往往無法達到理想狀態。
    *   數據並行擴展到一定規模後，會因為通信開銷而導致計算效率下降。
3.  **通信開銷：**
    *   多 GPU 環境下，不同 GPU 之間需要大量數據通信（梯度同步、參數傳輸等）。
    *   通信開銷過大會導致 GPU 空閒，浪費計算資源，降低訓練效率。

**Hugging Face 手冊介紹的技術手段**

1.  **激活值重計算 (Activation Recomputation)：**
    *   前向傳播過程中丟棄部分激活值，節省顯存空間。
    *   反向傳播過程中動態重新計算這些激活值。
    *   PyTorch、FlashAttention 等訓練框架已整合此策略。
    *   雖然增加計算量，但減少了記憶體訪問開銷，計算速度更快，有效降低顯存佔用。
    *   **注意：** 激活值仍然與批大小呈線性相關，當批大小不斷增加的時候激活值所占用的顯存可能又会成为一个问题。
2.  **梯度累計 (Gradient Accumulation)：**
    *   將批量數據拆分成多個微批次 (Micro-batch)。
    *   依次進行前向傳播和反向傳播，累計多個微批次的梯度得到最終梯度。
    *   避免一次性計算整個批量數據的梯度，減少顯存佔用。
    *   可與激活值重計算技術結合使用。
3.  **數據並行 (Data Parallelism)：**
    *   在多個 GPU 上同時運行訓練任務，每個 GPU 處理不同的微批次數據。
    *   通過 all-reduce 操作對模型的梯度進行平均，保持不同 GPU 上的模型同步。
    *   可優化方法：梯度同步與後向傳播重疊、梯度分桶，以及和梯度累積相結合。
    *   **瓶頸：** GPU 數量超過一定限制後，吞吐量會開始顯著下降，出現通信開銷瓶頸。
4.  **ZeRO (Zero Redundancy Optimizer)：**
    *   DeepSpeed 提出的優化技術，減少大模型訓練過程中的記憶體冗餘。
    *   通過在數據並行維度上對優化器狀態、梯度和參數進行分區，消除記憶體冗餘。
    *   三個階段：
        *   ZeRO-1：對優化器狀態進行分區 (optimizer state partitioning)。
        *   ZeRO-2：在 ZeRO-1 基礎上，增加對梯度的分區 (gradient partitioning)。
        *   ZeRO-3 (FSDP)：將分區擴展到了模型參數 (parameter partitioning)。
    *   **局限性：** 無法處理激活值記憶體，隨著序列長度和批大小的增加，這部分記憶體會增加。
5.  **張量並行 (Tensor Parallelism)：**
    *   針對激活記憶體超預算問題的優化技術。
    *   將張量分布到多個 GPU 上進行計算。
    *   減少矩陣乘法的激活記憶體，並能在多 GPU 間分佈模型參數、梯度、優化器狀態。
    *   **缺點：** 跨節點通信速度較慢，張量並行度超過 8 個 GPU 時，通信開銷會變得非常明顯。層歸一化和隨機失活等操作仍然需要收集完整的激活值。
6.  **序列並行 (Sequence Parallelism)：**
    *   減少最大激活值的存儲大小。
    *   有助於節省激活值的記憶體，從而能夠增大批大小和序列長度。
    *   **權衡：** 隨著張量並行度的增加，計算效率和顯存容量之間需要進行權衡。
7.  **上下文並行 (Context Parallelism)：**
    *   沿著序列長度和另一個維度拆分已經應用張量並行的模塊。
    *   在整個模型上應用序列拆分，而不是僅僅在模型的序列並行區域。
    *   通過 all-reduce 操作同步上下文並行組內的梯度。
    *   注意力模塊需要在 GPU 間進行全面通信來交換鍵/值數據。
    *   引入環形注意力（Ring Attention）技術來高效處理這種通信。
    *   Zig-Zag 環形注意力機制，通過混合排序實現計算在各個 GPU 上的平衡分佈。
8.  **流水線並行 (Pipeline Parallelism)：**
    *   將模型的各層分佈到多個 GPU 上。
    *   減少單個 GPU 的記憶體需求。
    *   激活張量需要在 GPU 間按照流水線順序傳遞。
    *   常用調度方法：
        *   全前向全反向（AFAB）
        *   一次前向一次反向 (1F1B)
        *   交錯階段技術
        *   零氣泡（ZeroBubble）
        *   雙管道（DualPipe）
9.  **專家並行 (Expert Parallelism, EP)：**
    *   MoE 模型中使用的技術。
    *   每一層不採用單個前饋模塊，而是設置多個並行模塊，對 token 進行不同的處理。
    *   前饋層是完全獨立的，專家並行更輕量，不需要拆分矩陣乘法。
    *   將 token 隱藏狀態路由到合適的專家。
    *   通常會與其他並行方式結合使用。

**Hugging Face 的實驗數據與策略**

*   在 512 個 GPU 上進行了超過 4000 次分布式實驗，探索不同的分布式訓練架構以及模型大小對訓練效果的影響。
*   數據包括：不同模型在各種並行技術組合下的顯存佔用情況、計算效率以及通信開銷等。

**關鍵步驟和策略總結**

1.  **將模型適配到記憶體中：**
    *   GPU 資源豐富：小於 10B 參數的模型，只使用數據並行。
    *   10B 到 30B 參數的模型：結合數據並行與張量並行。
    *   30B 到 70B 參數的模型：數據並行、張量並行，以及序列並行。
    *   超過 70B 參數的模型：流水線並行等複雜組合。
2.  **滿足目標全局批大小：**
    *   綜合考慮激活值重計算、梯度累計以及數據並行等技術的運用。
    *   合理設置微批次大小和梯度累計步數。
3.  **優化訓練的吞吐量：**
    *   根據模型的規模和硬件資源，選擇合適的並行策略組合。

**結論**

*   該手冊為廣大的 AI 開發者、研究人員以及相關企業提供了一套全面且實用的大語言模型訓練指南。
*   無論是新手還是專家，都能從這份手冊中獲取到有價值的信息。

**建議**

*   仔細閱讀報告原文，相信一定會有更多收穫。

我根據內容進行了重點提煉，並加入了適當的標題和編號，使其更有結構性。希望這份整理後的文稿對您有所幫助。

[model=gemini-2.0-flash,0]


---

</details>

<details>
<summary>580. [2025-03-03] 【人工智能】DeepSeek开源周六天内容全盘点 | FlashMLA | DeepEP | DeepGEMM | DualPipe | EPLB | 3FS文件系统 | V3/R1成本大揭秘</summary><br>

<a href="https://www.youtube.com/watch?v=deZjgTAkzoc" target="_blank">
    <img src="https://img.youtube.com/vi/deZjgTAkzoc/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【人工智能】DeepSeek开源周六天内容全盘点 | FlashMLA | DeepEP | DeepGEMM | DualPipe | EPLB | 3FS文件系统 | V3/R1成本大揭秘

好的，我將盡力整理您提供的文稿，使其更具條理，更易於閱讀。我會主要集中在以下幾個方面：

*   **結構化內容：** 將文稿分解為更小的章節，並使用清晰的標題和副標題。
*   **精簡語言：** 刪除冗餘的詞語和重複的信息，使句子更簡潔明瞭。
*   **提煉重點：** 強調每個章節的關鍵信息，以便讀者快速掌握要點。
*   **修正錯誤：** 修正任何明顯的語法或拼寫錯誤。
*   **調整格式：** 優化文本的格式，使其更易於閱讀。

以下是整理後的文稿：

---

**DeepSeek 開源週回顧：硬核技術揭秘與盈利模式分析**

大家好，這裡是最佳拍檔，我是大飛。上週 DeepSeek 咣咣咣地進行了一波開源，發布了多個硬核項目，搶佔了 Claude 3.7 Sonnet 和 GPT-4.5 的風頭，涵蓋 AI 的計算、通信和存儲等多個領域。DeepSeek 甚至還揭秘了 DeepSeek V3 和 R1 背後的秘密。DeepSeek 的模型到底有多賺錢？今天我們就來回顧一下整個開源週的內容。

**一、FlashMLA：高效多頭注意力解碼内核**

*   **簡介：** FlashMLA 是專為英偉達 Hopper GPU 優化的高效 MLA（多頭潛在注意力）解碼內核，支持可變長度的序列處理。
*   **靈感來源：** FlashAttention 2 和 3，以及 cutlass 項目。
*   **核心優勢：**
    *   減少顯存佔用，加速計算。
    *   通過低秩分解，降低內存佔用，提高處理速度。
    *   在 H800 GPU 上實現 3000 GB/s 的內存帶寬和 580 TFLOPS 的計算性能。
    *   將時間複雜度和內存複雜度從 MHA 的 O(n²) 降低到 O(nk)，適合長序列任務和實時推理。
*   **意義：** 只需簡單改造現有模型，即可大幅提高 GPU 效率，提升模型推理能力並降低成本。
*   **亮點：** 調用 CUDA 底層的 PTX 代碼，實現更精細的 GPU 控制和高效計算性能。

**二、DeepEP：混合專家系統 (MoE) 通信庫**

*   **簡介：** 專為 MoE 和專家并行 (EP) 定制的通信庫，設計靈感來自 DeepSeek V3 論文的群組限制門控算法。
*   **解決問題：** 在模型參數規模不斷擴大的情況下，解決高效通信的瓶頸問題。
*   **核心優勢：**
    *   根據任務量動態調節 GPU 使用的 SM 計算單元數量，節省功耗。
    *   高效優化的全體通信通道。
    *   為訓練和推理預填充設計了高吞吐的核心，為推理解碼設計了低延遲核心。
    *   原生支持 FP8 智能壓縮傳輸。
    *   支持 NVLink 和 RDMA，提升數據傳輸速度。
*   **應用場景：** 翻譯、摘要生成、問答系統、代碼生成、推薦系統等領域。

**三、DeepGEMM：高效 FP8 通用矩陣乘法庫**

*   **簡介：** 專為乾淨、高效的 FP8 GEMM 設計的庫，具有細粒度的縮放功能，支持普通和 MoE 分組 GEMM。
*   **核心優勢：**
    *   通過 FP8 和兩級累計降低計算和內存開銷，實現更高效率。
    *   JIT 編譯具有很強的適應性，減少預編譯的負擔。
    *   核心代碼簡潔，只有 300 行左右，便於開發者學習和優化。
*   **部署：** 運行時編譯所有內核，根據電腦情況現場調整代碼，量身定制最適合的指令。
*   **性能：** 在 H800 的性能測試數據中，與專業優化的庫相媲美，甚至更優。

**四、DualPipe 和 EPLB：AI 訓練加速神器**

*   **DualPipe：**
    *   **簡介：** 讓前向計算和反向傳播同時進行，減少訓練成本和時間。
    *   **原理：** 將每個時間步劃分為前向計算周期、反向計算周期和權重更新周期，實現通信階段與計算階段重疊。
    *   **效果：** DeepSeek V3 的 GPU 利用率達到 89%，訓練資源遠低於同等規模模型。
*   **EPLB（專家并行负载均衡）：**
    *   **簡介：** 通過動態調整專家模型的分配，平衡 GPU 之間的工作負載。
    *   **算法：** 包含分層和全局負載均衡策略，以及分形緩存和區域感知調度算法等優化技術。
    *   **效果：** GPU 的資源利用率能夠提升 20% 以上。

**五、3FS 文件系統：榨乾 SSD 性能**

*   **簡介：** 螢火超算文件系統，旨在提升數據讀取速度。
*   **核心優勢：**
    *   分布式架構，存儲節點與計算節點物理分離。
    *   CRAQ 技術確保數據一致性。
    *   FFRecord 格式管理數據庫，將小文件合併為邏輯大文件，提升查詢速度。
    *   利用 SSD 作為緩存，速度接近 DRAM 的 90%。
*   **性能：**
    *   180 節點集群的聚合讀取吞吐量可達 6.6 TB/s。
    *   加載 ImageNet 數據集耗時從 15 秒壓縮到 0.29 秒。
*   **Smallpond：** 基於 3FS 的輕量級數據處理框架，可處理 PB 級別數據集。

**六、DeepSeek V3 和 R1 系統揭秘：成本與盈利**

*   **硬件配置：** 使用 H800 GPU，保持與訓練一致的精度策略。
*   **資源調度：** 採用動態資源調度策略，根據負載差異調整節點數量。
*   **並行策略：** 採用大規模跨節點專家並行技術（EP），每層有 256 個專家，每個 token 激活 8 個。
*   **負載均衡：** 採用雙批次重疊處理策略，將大的請求批次分成兩個微批次交替執行。
*   **成本與收益：**
    *   24 小時平均使用 226.75 個節點（每個節點 8 個 H800）。
    *   每個 GPU 租金每小時 2 美元，成本約為每天 87072 美元。
    *   理論日收入為 562027 美元（約 409 萬人民幣）。
    *   成本利潤率為 545%。

**總結**

DeepSeek 的開源行動涵蓋核心算法、工程實踐，甚至公開了成本結構，展現了其技術自信和開放態度。這種純粹的技術分享值得肯定，也為 AI 領域的發展提供了新的思路。希望未來能有更多類似的貢獻出現。

---

**聲明：** 請注意，以上整理基於您提供的文本內容。關於 DeepSeek 的實際收益以及其他相關信息，請以官方發布為準。

[model=gemini-2.0-flash,0]


---

</details>

<details>
<summary>579. [2025-03-02] 【商业】英伟达史上业绩最佳财报发布 | 股价不涨反跌 | 2024年营收1300亿美元 | 净利润740亿美元 | 数据中心 | 游戏和AI PC | Blackwell需求猛增 | 竞争对手</summary><br>

<a href="https://www.youtube.com/watch?v=KSL1xydvwOE" target="_blank">
    <img src="https://img.youtube.com/vi/KSL1xydvwOE/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【商业】英伟达史上业绩最佳财报发布 | 股价不涨反跌 | 2024年营收1300亿美元 | 净利润740亿美元 | 数据中心 | 游戏和AI PC | Blackwell需求猛增 | 竞争对手

好的，我幫您整理這篇文稿，使其更易於閱讀和理解。我會將其分解成幾個部分，並添加標題和重點，使其結構更清晰。

**標題：英偉達2025財年財報分析：驚人業績與市場疑慮**

**引言:**

*   大家好，這裡是最佳拍檔，我是大飛。
*   英偉達發布了2025財年第四季度和年度財務業績。
*   儘管營收暴增，業績史上最佳，但股價卻不漲反跌，市場反應冷淡。
*   本文將剖析英偉達的財報，探討其面臨的挑戰與機遇。

**一、財報數據分析：**

*   **1. 四季度數據:**
    *   總營收：393.31億美元，同比增長78%，環比增長94%。
    *   環比增速放緩。
    *   淨利潤（公認會計準則）：220.91億美元，同比增長80%，環比增長14%。
    *   調整後每股收益（EPS）：0.89美元，同比增長71%。
    *   研發費用：37.14億美元，同比增長50.7%，環比增長9.6%。原因：Blackwell過熱等問題。

*   **2. 全年數據:**
    *   總營收：1305億美元，同比增長114%。
    *   淨利潤（非公認會計準則）：742.65億美元，同比增長130%。
    *   毛利率：75.5％，同比上漲1.7個百分點。
    *   調整後EPS：2.99美元。
    *   中國區營收：171.08億美元，同比增長66%，創歷史最高紀錄。
    *   美國以外地區收入佔53%，國際市場需求旺盛。

**二、收入板塊分析：**

*   **1. 數據中心:**
    *   核心增長引擎。
    *   四季度收入：356億美元，同比增長93%，環比增長16%。
    *   全年營收增長142%，達到1152億美元。
    *   Blackwell芯片收入達到110億美元，佔該季度數據中心收入的50%。
    *   中國區總銷售額約為55億美元，大部分來自數據中心業務。

*   **2. 遊戲和AI PC:**
    *   四季度營收：25億美元，同比下降11%，環比下降22%。
    *   全年營收：114億美元，增長9%。
    *   主要得益於GeForce RTX 5090和5080顯卡。
    *   供應限制影響四季度營收，預計第一季度將強勁增長。

*   **3. 專業可視化:**
    *   四季度收入：5.11億美元，同比增長10%，環比增長5%。
    *   全年營收增長21%，達到19億美元。
    *   得益於NVIDIA Project DIGITS和Omniverse集成加速。

*   **4. 汽車和機器人:**
    *   四季度收入：5.7億美元，同比增長103%，環比增長27%。
    *   全年收入增長55%，達到17億美元。
    *   受益於自動駕駛技術和機器人產業發展。

**三、未來展望：**

*   AI芯片Blackwell需求驚人，第一季度銷售額達數十億美元。
*   Blackwell Ultra將於2025年下半年發布。
*   預計2025年年底利潤率將達到70%-80%區間的中部。
*   未來AI將從後訓練技術和推理時性能提升兩個維度擴展。
*   預計2025年第一季度收入為430億美元，上下浮動2%。
*   非GAAP毛利率為71.0%，上下浮動50個基點。
*   黄仁勋：世界上幾乎所有的軟體都將被注入AI，英偉達處於發展中心。

**四、市場反應及分析：**

*   財報公布後，英偉達股價先跌後漲，最終仍下跌。
*   投資者認為英偉達業績帶來的驚喜不夠大。
*   DeepSeek的浪潮下，華爾街對美國科技巨頭的算力支出產生質疑。
*   市場對英偉達股票依然持謹慎態度，今年以來下跌2.2%。
*   DeepSeek的開源項目針對英偉達的GPU進行優化，可能影響其銷量預期。

**五、競爭對手及挑戰：**

*   Arm公司崛起，股價上漲一倍以上，參與“星際之門”項目。
*   Arm認為AI算力領域將出現分化。
*   華爾街分析師對英偉達的分歧加大。
*   德勤報告顯示，AI芯片市場規模龐大且快速增長。
*   英偉達面臨激烈的競爭和困難的挑戰。

**六、總結與討論：**

*   英偉達是否會繼續保持高速的增長態勢？
*   歡迎在評論區留言。
*   感謝大家的觀看，我們下期再見。

**重點整理:**

*   **核心觀點:** 英偉達業績亮眼，但市場對其未來增長潛力及競爭壓力存在疑慮。
*   **關鍵數據:** 各季度及全年營收、利潤、研發費用、各業務板塊收入等。
*   **主要挑戰:**
    *   DeepSeek等競爭對手的崛起。
    *   市場對AI算力需求的重新評估。
    *   華爾街分析師的分歧。
    *   中國市場的出口管制。
*   **機遇:**
    *   AI芯片市場的快速增長。
    *   在數據中心、遊戲、專業可視化、汽車和機器人等領域的領先地位。
    *   Blackwell等新產品的推出。

希望這個整理對您有幫助！讓文章整體脈絡更清晰，也方便快速查閱。

[model=gemini-2.0-flash,0]


---

</details>

<details>
<summary>578. [2025-02-28] 【数字货币】全球第二大交易所Bybit被盗超过14亿美元代币 | 史上最大代币盗窃事件 | 背后惊现朝鲜国家黑客组织 | Lazarus Group | 冷钱包 | 社会工程学 | 新型恶意软件</summary><br>

<a href="https://www.youtube.com/watch?v=bhrztPMHeKA" target="_blank">
    <img src="https://img.youtube.com/vi/bhrztPMHeKA/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【数字货币】全球第二大交易所Bybit被盗超过14亿美元代币 | 史上最大代币盗窃事件 | 背后惊现朝鲜国家黑客组织 | Lazarus Group | 冷钱包 | 社会工程学 | 新型恶意软件

好的，我來幫你整理這份文稿，目標是使它更清晰、更易於理解、更具組織性，並適合閱讀或作為腳本參考。

**標題：Bybit 加密貨幣交易所遭駭事件深度剖析：拉撒路集團的金融掠奪**

**引言：**

大家好，這裡是最佳拍檔，我是大飛。今天我們要抽絲剝繭，全面剖析一起加密貨幣史上最大的駭客事件：全球交易量名列前茅的Bybit交易所於2025年2月21日遭遇駭客攻擊，高達14億美元的以太坊被盜。這起事件不僅暴露出加密貨幣安全領域存在的深層次問題，還將朝鮮的駭客組織拉撒路集團（Lazarus Group）推到了大眾視野中。

**一、事件概述：**

*   **時間：**2025年2月21日
*   **對象：**Bybit 加密貨幣交易所
*   **損失：**約14億美元（401,000 - 401,347 枚 ETH）
*   **攻擊目標：**以太坊冷錢包
*   **嫌疑人：**朝鮮駭客組織拉撒路集團 (Lazarus Group)

**二、Bybit 交易所簡介：**

*   註冊地：英屬維爾京群島
*   運營總部：迪拜
*   管理資產：超過200億美元
*   服務客戶：170多個國家

**三、攻擊細節：**

*   **冷錢包特性：**離線多重簽名系統，金鑰儲存在物理隔離設備中。
*   **攻擊方式：**
    *   黑客鎖定冷錢包向熱錢包轉帳時機
    *   盜取大約401,000 至 401,347枚ETH
    *   市場價值約14.43億至14.45億美元
*   **時間線：**
    *   2月21日凌晨4:31 (太平洋標準時間)：鏈上數據警報，冷錢包轉出50,000 ETH。
    *   後續數小時：剩餘351,000 ETH 被持續轉移。
    *   早上7:20：安全研究員扎克·XBT (ZachXBT) 率先披露攻擊事件，指出損失金額高達14.6億美元。
    *   早上7:51：Bybit CEO Ben Zhou 發布聲明，確認事件，但迴避具體損失金額。
    *   早上9:30：Whale Alert 標記轉移地址為“Bybit Exploiter”，並發現資金被拆分到39個新地址。
    *   下午1:07：Ben Zhou 直播披露攻擊細節，說明損失約佔Bybit總資產的7%-9%，並已向多個機構報告。
    *   下午2:00：Bybit 暫停與 Safe global 相關的轉帳操作。
    *   提款激增：24小時內收到超過12萬筆提款申請，總額超過30億美元。

**四、事件影響：**

*   **對Bybit：**
    *   獲得12億美元過橋貸款，覆蓋80%損失。
    *   客戶資產1:1備份，未受影響。
*   **對加密貨幣行業：**
    *   單次被盜金額創歷史新高，超過2022年Ronin Network事件。
    *   佔2024年全年加密貨幣盜竊總額的60%以上。
    *   中心化交易所信任度降至冰點。
    *   市場波動：ETH價格因市場傳言而大幅波動。

**五、駭客技術分析：**

*   **攻擊目標：** Bybit 的 ETH 冷錢包。
*   **攻擊手法：**
    1.  **社會工程學與滲透：** 偽裝成 Safe global 的緊急更新，誘騙簽名者點擊惡意連結或下載惡意軟體(鍵盤記錄器、AnyDesk等)。
    2.  **偽裝簽名介面 (UI Masking)：** 注入惡意腳本，篡改 Safe global 介面，使簽名者誤以為簽署的是正常交易。
    3.  **智能合約篡改：** 利用 delegatecall 機制替換冷錢包的實現地址，控制資金轉移。
*   **資金轉移與清洗：**
    *   資金迅速拆分到多個地址，Gas 費超過500 ETH。
    *   部分ETH兌換成stETH、mETH。
    *   使用 Tornado Cash 混幣服務，其餘資金分散到300多個地址。

**六、嫌疑人：拉撒路集團（Lazarus Group）**

*   **背景：**
    *   朝鮮駭客組織，隸屬於朝鮮人民軍偵察總局第414聯絡辦公室，直屬金正恩領導。
    *   擁有約1600名核心駭客和6000名外圍人員。
    *   目標：獲取資金、規避國際制裁、支援朝鮮核武器計畫、進行情報收集。
*   **歷史事件：**
    *   2014年：攻擊索尼影業。
    *   2017年：發動 WannaCry 勒索病毒攻擊。
    *   2022年：攻擊 Ronin Bridge。
*   **駭客策略：**
    *   社會工程學：偽裝成招聘者、合作夥伴，發送惡意附件或連結。
    *   竊取私鑰：通過釣魚郵件、設備感染等方式獲取。
    *   資金清洗：利用混幣服務、去中心化交易所、跨鏈橋等手段。
    *   供应链攻击：污染开源代码库。
*   **恶意软件：** Kaolin RAT, CookiePlus, SmallTiger
*   **基础设施：**VPN、多層代理、分散的伺服器地理位置。
*   **特點：**
    *   紀律性強，潛伏時間長。
    *   享有國家支持。
    *   以資金驅動。

**七、安全建議：**

*   **交易所：**
    *   使用硬件安全模組儲存私鑰，實現物理隔離。
    *   採用多重簽名機制。
    *   加強流程、監控、預案、審計等方面的安全性。
*   **個人：**
    *   使用硬件錢包儲存加密貨幣。
    *   設置PIN碼並定期更換。
    *   交易時手動核對錢包地址。
    *   使用YubiKey等硬件金鑰進行二次驗證。
    *   警惕詐騙和謠言，分散資產。

**八、總結與警示：**

Bybit被盜事件再次敲響了警鐘，冷錢包並非絕對安全。網路安全與我們每個人息息相關，希望大家能提高對網路安全的認識，保護好自己的錢包和資產。這不僅是數字貨幣領域的風險，銀行帳戶、第三方支付平台、股票帳戶等都存在類似風險。

**結束語：**

感謝大家觀看本期影片，我們下期再見。

**整理說明：**

*   **結構化：** 使用標題、副標題和項目符號列表，使內容更易於閱讀和理解。
*   **精簡：** 刪除冗餘信息，突出重點。
*   **清晰化：** 使用更簡單易懂的語言，避免過於專業的術語（如果需要，解釋術語）。
*   **突出重要信息：** 使用粗體字標記關鍵字和重要數據。
*   **邏輯化：** 將相關信息歸類到一起，使內容更具邏輯性。
*   **時間線整理：** 明確列出事件發生的時間順序，方便理解。

希望這個整理後的版本對您有所幫助！ 如果您需要進一步的修改或調整，請隨時告訴我。

[model=gemini-2.0-flash,0]


---

</details>

<details>
<summary>577. [2025-02-27] 【访谈】Snowflake CEO 斯里达尔·拉马斯瓦米20VC访谈 | 如何面对AI挑战 | 软件工程 | 职场管理 | AI竞争 | OpenAI | Databricks | AI泡沫</summary><br>

<a href="https://www.youtube.com/watch?v=3-2-w7bPqtk" target="_blank">
    <img src="https://img.youtube.com/vi/3-2-w7bPqtk/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【访谈】Snowflake CEO 斯里达尔·拉马斯瓦米20VC访谈 | 如何面对AI挑战 | 软件工程 | 职场管理 | AI竞争 | OpenAI | Databricks | AI泡沫

好的，我將以上文稿整理如下，並將其結構化，使其更易於閱讀：

**主题：知名播客访谈 Snowflake CEO：AI 竞争格局、发展趋势与企业管理**

**引言**

*   知名播客 20VC 主播哈里·斯特宾斯 (Harry Stebbings) 访谈了 Snowflake 的 CEO 斯里达尔·拉马斯瓦米 (Sridhar Ramaswamy)，讨论了 AI 竞争格局、发展趋势和企业管理等方面的话题。
*   Snowflake 是一家美国云数据存储公司，在 AWS 等云平台上提供数据存储和处理服务。目前市值约 619 亿美元，年收入达 35 亿美元，并以每年 30% 的速度增长。
*   CEO 拉马斯瓦米曾在谷歌深耕 15 年，推动了谷歌广告业务的关键增长。在加入 Snowflake 之前，他创立的 Neeva 公司被 Snowflake 以 1.5 亿美元收购。
*   本次访谈不仅能了解到行业巨头对当下 AI 浪潮的看法，也能获得很多关于职业发展、商业决策的启示。

**一、职场发展与管理**

*   **关于 CEO 的职业规划：** 拉马斯瓦米坦诚，本科和博士期间并没有想过成为 CEO，而是希望成为一名教授。后来转向软件工程领域，也是一步一个脚印的发展。他认为伟大的事情往往在正确的时间和地点才会发生。
*   **给年轻人的建议：**
    *   找到自己热爱且对社会有价值的事情。
    *   不要选择看似高尚但自己并不真正重视的职业。
    *   对变化保持开放心态，灵活应变，积极拥抱变化。
    *   不仅要做好当下的工作，还要思考未来的发展方向。
*   **招聘员工的品质：** 拉马斯瓦米看重员工的动力和灵活性。
*   **软件工程在 AI 时代的发展：**
    *   拉马斯瓦米认为所有知识型工作者都会受到 AI 的影响。AI 就像一个神奇的翻译层，能够处理各种结构化和非结构化的知识。
    *   现在就判断软件工程是否会成为一个萎缩的专业还为时过早。
    *   关键是从业者要拥抱未来，看清发展方向，保持灵活应变的能力。
*   **高强度工作 CEO 的管理理念：** 让团队成员理解大局，明白机会的短暂性。公司的使命是成为地球上每个企业的数据引擎，需要大家有很高的期望。
*   **招聘经验：**
    *   曾经以为招聘的人能够随着公司的发展阶段而成长，但这是很难预测的。
    *   每次团队规模翻倍的时候，那些曾经让员工在之前团队中表现出色的东西往往会成为他们在新工作中成功的障碍。
    *   作为管理者，有责任给员工这样的机会，但如果员工实在无法适应，也会果断做出决定，比如给员工降职，调整工作内容。
*   **关于降职：** 在快速变化的商业环境中，情况很复杂。管理者的职责是找到正确的环境和框架，帮助员工取得成功。
*   **领导力：** 要能够推销人们不想接受的东西，并为他们描绘美好的未来愿景。
*   **艰难的对话技巧：**
    *   要明白冲突通常不会自行解决，拖延只会让事情变得更糟。
    *   要将尊重、直率和谦逊结合起来。
*   **财富与领导力的关系：** 拉马斯瓦米认为财富可能会让领导者变得粗心大意，对风险过于宽容。领导者要考虑的不仅仅是自己，还有公司的员工、股东和客户。

**二、AI 领域的竞争格局**

*   **AI 市场可持续价值：** 真正创造价值的地方在于那些拥有客户关系、能够提供明确价值并且愿意快速拥抱 AI 的公司。
*   **OpenAI 的成功：** 不仅仅是因为它能创造最好的基础模型，更重要的是它的产品体验。ChatGPT 拥有大约 10 亿用户，这是非常难能可贵的。OpenAI 的公关能力也很强大。
*   **DeepSeek 的挑战：** ChatGPT 不仅仅是一个模型，而是一个功能丰富的产品。Anthropic 在这方面做得还不够好。
*   **初创公司的挑战：** 建立在 OpenAI 之上会面临很大的挑战。
*   **云和数据服务的竞争：** 拉马斯瓦米对英伟达、AWS、微软等都要时刻留意。
*   **产品市场契合度：** 对于一家初创公司来说，拥有产品市场契合度是非常关键的。这就像是一种魔法，很难通过复制来实现。
*   **Snowflake 与 Databricks 的竞争：** 拉马斯瓦米认为需要区分机器学习和 AI。在 AI 方面，他对 Snowflake 非常有信心，认为 Snowflake 不仅处于前沿，而且在很多方面领先于 Databricks。
*   **超级晚期初创公司 vs. 上市公司：** Snowflake 作为上市公司有更多的限制，但也带来了清晰的方向，让公司更专注于需要构建的东西。

**三、AI 在企业中的应用与未来发展趋势**

*   **AI 何时带来可观的投资回报：** 拉马斯瓦米非常肯定 AI 正在创造价值，并且将继续创造持久的价值。
*   **企业对 AI 的接受度：** 只要能够展示出 AI 的实用性，企业对 AI 的接受度还是很高的。
*   **从科技发展历史看 AI 浪潮：** 那些从过去的变革中吸取了经验的企业，在如今的 AI 时代才会疯狂投资未来，积极拥抱创新。
*   **AI 的军备竞赛将何去何从：** 泡沫终会破裂。
*   **当前的 AI 投资：** 如果大部分投资流向了电力和基础设施等领域，那么可能会为世界创造更多的价值；但是如果流向快速贬值的硬件，价值可能瞬间消失。
*   **垂直领域的机会：** Harvey 这样的垂直领域公司有可能在市场中找到自己的立足之地，不被巨头轻易颠覆。
*   **未来模型的发展方向：**
    *   在消费端，ChatGPT 正在成为一个类似的入口点，最终会倾向于像 ChatGPT 和 OpenAI 这样的巨头。
    *   在企业端，更可能出现各种各样的专业化模型，不太可能有一个单一的入口点统治一切。

**结论**

拉马斯瓦米的访谈涵盖了 AI 领域的竞争格局、发展趋势以及企业管理等方面的内容，为我们提供了许多有价值的思考和启示。

**（感谢观看，下期再见）**

**改进说明：**

*   **分点总结：** 将访谈内容拆解为多个小点，使信息更易消化。
*   **明确主题：** 为每个部分设定明确的主题，让读者快速把握重点。
*   **使用列表和编号：** 使用列表和编号，使内容更具条理性和视觉吸引力。
*   **提炼核心观点：** 提取关键观点，避免冗余信息。
*   **使用简洁的语言：** 用清晰简洁的语言表达，减少阅读负担。
*   **格式調整：**调整了部分段落和标点符号。

希望以上整理对您有帮助！

[model=gemini-2.0-flash,0]


---

</details>

<details>
<summary>576. [2025-02-26] 【商业】Ai Pin已死 | 人工智能硬件公司Humane | HP十分之一价格收购 | 奥特曼投资 | 从满怀期望到全民批评 | 使用及硬件缺陷 | 价格高昂 | Cosmos | 电子砖头</summary><br>

<a href="https://www.youtube.com/watch?v=lqsP4N0inoU" target="_blank">
    <img src="https://img.youtube.com/vi/lqsP4N0inoU/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【商业】Ai Pin已死 | 人工智能硬件公司Humane | HP十分之一价格收购 | 奥特曼投资 | 从满怀期望到全民批评 | 使用及硬件缺陷 | 价格高昂 | Cosmos | 电子砖头

好的，以下是根据您提供的文稿整理后的版本，我尽量保留了原稿的口语化风格，并进行了结构化的调整：

**标题：AI Pin 翻车记：一场充满戏剧性的科技创新陨落**

**开场白:**

*   大家好，这里是最佳拍档，我是大飞。
*   在AI飞速发展的今天，AI相关新产品层出不穷，似乎都带着改变世界的使命。
*   然而，并非所有创新都能成功。今天要聊的是一个充满戏剧性的产品——Humane公司的Ai Pin。

**Ai Pin 的陨落:**

*   Ai Pin曾被寄予厚望，被视为AI时代的革命性产品，但现在却面临变成电子垃圾的命运。
*   10个月前我们做过一期它翻车的视频，这个结局并不意外。

**Humane 公司背景:**

*   Humane公司成立于2018年，备受瞩目。
*   **创始人:** 伊姆兰·乔杜里（Imran Chaudhri）和贝瑟尼·邦焦尔诺（Bethany Bongiorno）。
*   **苹果背景:** 两人都有深厚的苹果背景，乔杜里在苹果担任设计师20年，邦焦尔诺担任iOS和macOS软件工程总监8年。
*   **资本支持:** 获得山姆·奥特曼、微软、高通风投等2.4亿美元投资。
*   Humane怀揣着改变科技行业的梦想，踏上了创新之路。

**Ai Pin 产品介绍 (2023年发布):**

*   **设计:** 方形别针，小巧精致，磁吸佩戴，阳极氧化和抛光外壳，康宁大猩猩玻璃触控板。
*   **硬件:** 高通骁龙芯片、4GB RAM、8GB ROM，环境光、加速度计、陀螺仪、GPS等各种传感器，重量控制在55克。
*   **交互:** 无物理按键，通过语音、手势和触控操作，激光投影显示信息在手掌上 (720P)，ChatGPT支持。
*   **功能:** 拨打电话、发送消息、询问问题等。
*   **系统:** Humane公司自主开发的CosmOS操作系统，打造全新的智能生态。

**发布前的期待:**

*   被《时代》杂志评选为2023年度发明。
*   被许多媒体称为颠覆智能手机的革命性产品、AI时代的iphone等。
*   Ai Pin似乎有可能开启一个全新的智能生活时代。

**现实的打击:**

*   **预售:** 2023年11月16日开始预订，价格699美元（折合大约5000人民币），不到一个月预订量突破450万台。
*   **跳票和问题爆发:** 原计划2024年3月发货，跳票一个月。4月发货后，大量问题开始暴露。
*   **媒体评价:** 纷纷调转枪头，The Verge 直言它唯一可靠的功能是报时，《连线》杂志说它是一个美丽的废物，MKBHD 称Ai Pin是“我评测过的最差的产品”。

**Ai Pin 失败的原因：**

*   **交互体验差:**
    *   激光投影在户外强光下效果极差，文字变形、拉扯。
    *   手势优化不足，学习成本高，容易误触。
*   **AI 性能不佳:**
    *   依赖云端，每次回应等待5-10秒。
    *   回答准确性参差不齐，经常给出不准确或“不太方便”的回复。
*   **续航问题:**
    *   内部电池容量约300mAh，加上外部磁吸电池总共约800mAh，续航2-4小时。
    *   频繁使用时设备容易过热自动关机。
*   **硬件缺陷:**
    *   摄像头很少能正常工作，拍摄的照片通过投影仪显示出来后设备会变得非常烫，有时甚至还会完全死机。
    *   投影屏幕在阴天的户外都几乎无法看清。
*   **充电盒安全隐患:** 美国消费品安全委员会的报告指出，Ai Pin的充电盒有一定的火灾风险， Humane公司不得不“出于谨慎”召回了充电盒。
*   **价格高昂:**
    *   设备售价699美元（目前799美元），每月还需支付24美元订阅费。
    *   产品体验不佳，高昂价格让许多人望而却步。

**市场表现：**

*   截止到 2024 年 8 月的销售数据，699 美元的 Ai Pin 只卖了一万多台，用户实际持有的量也就7000台，上市以来的累计销售额勉强超过900万美元。
*   2024年10月，Humane公司对Ai Pin的价格进行了下调，由之前的699美元降低到了499美元，但是不再附赠充电盒。相当于短短半年时间，老用户多花了200美元，却买到了一个有风险的充电盒。
*   从2024年5月到8月期间，它的销售数量还赶不上退货数量，公司的估值也收缩到了 7.5 亿美元至 10 亿美元间。

**Humane 的应对与被收购：**

*   Humane公司一直在努力通过更新改善性能，但没有明显效果。
*   最终寻求收购。
*   惠普以1.16亿美元收购了Humane公司的核心技术（软件、技术人员、300多项专利），但对硬件不感兴趣，收购价格压低了90%。
*   惠普成立HP IQ团队，作为新的人工智能创新实验室。
*   惠普还获得了CosmOS系统，希望融入到自己的产品中。

**Ai Pin 的终结：**

*   被收购意味着独立发展道路结束。
*   Ai Pin 将于 2025年2月28日中午12点前停止工作，所有联网功能失效，只剩下基本离线信息。
*   Humane公司鼓励用户在2月28日之前下载所有存储的数据，因为它计划在关闭服务器的同时永久删除“所有客户数据”。
*   Ai Pin 从备受瞩目的创新产品变成了一堆昂贵的电子垃圾，建议送到电子垃圾回收站。
*   只有仍然在 90 天退货期内，或者在 2024 年 11 月 15 日之后购买的用户才有资格获得退款， 但是必须在 2025 年 2 月 27 日前提交申请

**用户权益问题：**

*   用户花费大量金钱，如今设备即将报废，投入付诸东流。
*   用户认为硬件产品依赖无法自助服务的软件进行运营，消费者应该有权在这种情况下要求全额退款.
*   如果硬件只是作为软件服务的附属品，那么消费者为硬件支付的费用就应该被视作一种可退还的押金。
*   在这个事件中，用户的权益显然没有得到充分的保障。

**对创始人的评价：**

*   对创始人伊姆兰·乔杜里的评价非常两极化，有人称赞他是新时代的乔布斯，也有人骂他是彻头彻尾的骗子。
*   乔杜里的个人主页上密密麻麻地罗列着整整 1000 个专利。
*   有人曾经揭露CEO乔杜里之前大肆宣传的CosmOS系统实际上是基于 Android

**行业反思：**

*   Ai Pin的失败敲响了警钟。
*   人工智能硬件在未来仍然有着巨大的发展潜力，但目前想要把AI成功应用到硬件产品中，还需要克服许多技术和用户体验方面的问题
*   企业在追求创新的同时，不能忽视最根本的用户需求和产品质量。
*   像谷歌和苹果在推广人工智能技术时，都是基于自身强大的现有平台，将人工智能作为附加功能进行开发。
*   Humane公司在没有坚实基础的情况下试图凭借一款创新性产品打开市场，最终失败。
*   创新与现实应用场景结合的重要性。企业不能只是追求概念上的创新，更要关注产品在实际使用中的可行性和实用性，才能在市场上立足。

**结尾：**

*   好了，感谢大家收看本期视频，我们下期节目再见。

**整理说明:**

*   **结构化:** 将文稿分成了几个主要部分，更容易理解。
*   **重点突出:** 使用了标题、副标题、列表等方式突出关键信息。
*   **口语化保留:** 尽量保留了原稿的口语化表达，例如“翻车”、“击败了...的最后防线”等。
*   **信息补充:** 在一些地方增加了必要的上下文，例如创始人背景、预售情况等。

希望这份整理对您有帮助！如果您还有其他要求，请随时告诉我。

[model=gemini-2.0-flash,0]


---

</details>

<details>
<summary>575. [2025-02-25] 【人工智能】Claude 3.7 Sonnet发布 | 全球首个混合推理模型 | 一个模型两种思考方式 | Claude Code | 编程能力大幅提升 | 价格与前代持平</summary><br>

<a href="https://www.youtube.com/watch?v=NF_Go55lXj8" target="_blank">
    <img src="https://img.youtube.com/vi/NF_Go55lXj8/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【人工智能】Claude 3.7 Sonnet发布 | 全球首个混合推理模型 | 一个模型两种思考方式 | Claude Code | 编程能力大幅提升 | 价格与前代持平

好的，這是一份整理後的文稿，旨在更清晰地呈現內容：

**標題：Anthropic 發布 Claude 3.7 Sonnet：首款混合推理模型，主打編碼能力**

**摘要：**

Anthropic 发布了 Claude 3.7 Sonnet，一款混合推理模型，也是迄今為止最智能的模型。它具备标准和扩展两种思考模式，可根据任务需求调整思考时间。此外，Anthropic 还推出了用于 AI 编码的命令行工具 Claude Code。此次更新重点放在编码能力上，并已与合作伙伴进行早期测试，证明 Claude 在编码方面的领先地位。

**重點內容：**

*   **Claude 3.7 Sonnet 的特性：**
    *   **混合推理模型：** 同时具备标准和扩展思考模式，在标准模式下是 3.5 Sonnet 的升级版，在扩展模式下则进行自我反思，提高在数学、物理、指令遵循、编码等方面的表现。
    *   **可控的思考预算：** API 用户可以控制模型的思考时间，在速度、成本和答案质量之间进行权衡。
    *   **专注于现实任务：** 优化了解决 GitHub 上真实软件问题的能力，并在 SWE-bench Verified 和 TAU-bench 基准测试中实现了 SOTA 性能。
    *   **强大的代理能力：** 甚至能在精灵宝可梦游戏中成功挑战 boss。
*   **編碼能力是重點：**
    *   Claude 3.7 Sonnet 被认为是 Anthropic 迄今为止最好的编码模型。
    *   合作伙伴测试表明，Claude 在处理复杂代码库、高级工具使用、代码更改规划、全栈更新、构建 Web 应用程序等方面均表现出色。
*   **Claude Code：**
    *   首款智能编码工具，目前仅作为研究预览版限量开放。
    *   可直接理解开发者的代码库，并通过自然语言命令帮助用户更高效地编码。
    *   核心功能包括编辑文件、修复 Bug、回答代码架构和逻辑的问题、执行测试等。
*   **使用方式及價格：**
    *   新模型可通过所有 Claude 计划使用，包括 Free、Pro、Team 和 Enterprise，以及 Anthropic API、Amazon Bedrock 和 Google Cloud Vertex AI。
    *   标准和扩展思考模式的价格与前代 3.5 Sonnet 相同。
*   **安全：**
    *   Anthropic 对 Claude 3.7 Sonnet 进行了广泛的测试和评估，并与外部专家合作，确保模型符合安全性和可靠性标准。
    *   与前代相比，不必要的拒绝减少了 45%。
    *   模型卡中详细说明了负责任扩展策略评估，以及如何评估和缓解快速注入攻击等新风险。
*   **Anthropic 的願景：**
    *   希望在 2025 年 Claude 成为独立自主工作数小时的专家级智能体。
    *   希望在 2027 年 Claude 能够解决人工团队花费数年时间才能解决的挑战性难题。
*   **融資消息：**
    *   Anthropic 即将完成一轮 35 亿美元融资，估值达到 615 亿美元。
    *   儘管目前仍处于亏损状态，但预计 2025 年营收目标为 37 亿美元，2027 年增长到 345 亿美元。

**總結：**

Anthropic 的 Claude 3.7 Sonnet 和 Claude Code 标志着 AI 系统迈出了重要一步，凭借深度推理、自主工作和有效协作的能力，将开始向着真正增强人类能力的方面迈进。 Anthropic 將重心放在了 Claude 的编码能力，旨在为开发者提供更强大的工具和更高效的编码体验。

**建議修改：**

*   如果面向特定读者群体，可以增加或删除一些技术细节。
*   可以根据文章主旨，调整段落顺序和重点。
*   可以添加更多圖片或圖表，以更直观地呈现信息。

希望这份整理对您有所帮助！

[model=gemini-2.0-flash,0]


---

</details>

<details>
<summary>574. [2025-02-24] 【人工智能】FigureAI推出Helix模型 | 通用视觉语言动作VLA | 双机器人协作 | 语音指令 | 全上半身灵活操控 | 强大物体泛化 | S2+S1双系统 | 流式推理 | 家务劳动</summary><br>

<a href="https://www.youtube.com/watch?v=3PhOOFilsiY" target="_blank">
    <img src="https://img.youtube.com/vi/3PhOOFilsiY/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【人工智能】FigureAI推出Helix模型 | 通用视觉语言动作VLA | 双机器人协作 | 语音指令 | 全上半身灵活操控 | 强大物体泛化 | S2+S1双系统 | 流式推理 | 家务劳动

好的，我來為您整理這份文稿。我將專注於提升結構、清晰度和可讀性，並保留原稿的風格與信息。

**整理後的文稿：**

**標題：人形機器人領域重大突破：Figure AI推出通用視覺語言動作模型Helix**

**引言**

大家好，這裡是最佳拍檔，我是大飛。

最近，人形機器人領域迎來了重大突破，它可能改變我們未来的生活方式。想像一下，未來回到家，不再需要自己做家務，而是有個人形機器人夥伴，能聽懂你的話，默契配合，把家裡收拾得井井有條。這聽起來是不是很像科幻電影裡的場景？但現在，它正在逐漸成為現實。

美國人形機器人獨角獸Figure AI在2月21日重磅推出了一款通用視覺語言動作（VLA）模型——Helix。這個模型克服了機器人技術領域的多項長期挑戰，讓兩個人形機器人能夠聽從語音指令，互相幫助，搭伙幹家務活兒。

今天，我們就來深入聊聊這個模型。

**Figure AI與Helix模型的背景**

其實我們頻道之前已經做過幾期介紹Figure AI的視頻了，包括最早的Figure 01和Figure 02機器人。最近這家公司比較有熱度的一個消息是，在2月5日，Figure的創始人兼CEO布雷特·阿德科克（Brett Adcock）在推特上宣布終止與OpenAI的合作協議。他表示Figure在完全自主研發的端到端機器人AI方面取得了重大的突破，還預告接下来的30天內會展示一些人們從來沒有在人形機器人上見過的東西。

而今天我們要說的Helix模型，就是他們這段時間努力的成果。根據布雷特·阿德科克介紹，團隊花了一年多的時間研發出了Helix，它的出現給人形機器人領域帶來了新的希望。

**Helix模型的厲害之處：多項業界首次**

那Helix模型到底有哪些厲害的地方呢？它創造了多項業界的首次紀錄：

*   **全上半身的靈活操控：** Helix是首個能夠對仿人機器人的完整上半身（包括手腕、軀幹、頭部、每根手指）實現高速連續控制的VLA模型。這意味著機器人的上半身可以像人類一樣靈活地運動，做出各種精細的動作。在測試中，機器人成功地處理了雜亂無章的數千件新物品，從玻璃器皿和玩具到工具和衣服，而且在處理這些物品的時候，機器人的上半身各個部位都能夠協調運作，完成抓取、放置等動作，同時動作非常流暢自然。

*   **多機器人協作：** Helix首次讓雙機器人系統協同完成長期的複雜任務。在Figure AI分享的視頻中，當人類當面說出整理雜物的要求之後，兩個機器人在從來沒有見過這些物品的情況下，通過推理能力，能夠協作將這些物品擺放歸位。它們把雞蛋、番茄醬等需要冷藏保鮮的物品放進冰箱，把水果放進果盒，以及把零食收進抽屜裡。在協調過程中，機器人用頭部平穩地跟蹤雙手，同時調整軀幹來確定最佳的觸及範圍，同時保持精確的手指控制來進行抓握。這種協作能力在以前的人形機器人中是很少見的，它們就像兩個配合默契的夥伴，一起完成任務。

*   **萬物皆可抓取：** 機器人現在只需要一句自然語言指令，就能抓取幾乎所有的家庭小物件，包括數千種在訓練中從來沒有接觸過的物品。當提示“撿起沙漠物品”的時候，Helix不僅會識別出玩具仙人掌符合這個抽象的概念，還會選擇最近的手，並且執行所需的精確運動命令，牢牢地抓住它。這說明Helix模型具有很強的泛化能力，能夠理解抽象概念並做出相應的動作。

*   **單一模型學習所有技能：** Helix僅用一套神經網路參數就能學習所有的技能，像取放物品、開關抽屜冰箱、跨機器人交互等等，無需任何任務專屬微調。這和以往的技術大不相同，以前的技術往往需要針對不同的任務進行複雜的調整，而Helix大大簡化了这个過程。

*   **低功耗嵌入式GPU商業部署：** Helix是首個完全運行在低功耗嵌入式GPU上的VLA系統，已經具備直接商業部署能力。這意味著它可以更快地進入市場，走进我们的生活。以前很多先进的技术，因为无法商业化而只能停留在实验室阶段，而Helix则让它们离我们的生活更近了一步。

**Helix模型的技術架構：系統1+系統2**

之所以Helix模型能有這麼強大的功能，應該說和它獨特的技術架構分不開。Helix采用了“系统1+系统2”的架构来实现对人形机器人上半身的高速精准控制。

以前的方法存在一些问题，比如说VLM骨干网络虽然通用，但是速度慢；机器人视觉运动策略虽然速度快，但是通用性却不行。而Helix通过两个互补的系统S2和S1来权衡，这两个系统都经过了端到端训练，可以互相进行通信。

*   **S2（慢思考）：** 基於7B參數的開源VLM，它在互聯網規模數據上進行了預訓練，在潛在視覺語言空間中進行“慢思考”，以7到9Hz的頻率運行，負責場景理解與語義理解，確保跨物體、跨場景的泛化能力。比如說，在整理雜物的場景中，S2會分析環境中的各種物品，理解人類的指令，判斷哪些物品需要放進冰箱，哪些需要放進抽屜等等。
*   **S1（快思考）：** 80M參數的交叉注意力編解碼Transformer，負責處理低級的控制，可以“快速思考”，將S2的潛在語義表徵轉化為精確的連續機器人動作，以200Hz頻率協調35個自由度動作空間，從而控制完整的機器人上半身，實現毫秒級的實時響應。在兩個機器人協作傳遞物品的時候，S1能夠快速適應機器人夥伴不斷變化的動作，做出細粒度運動調整，同時保持S2的語義目標。

和現有的方法相比，Helix採用的這種設計有很多優勢：

*   **速度與泛化：** Helix能夠匹配專門的單任務行為克隆策略的速度，同時將零樣本泛化到數千種新測試物品。
*   **可擴展性：** Helix會直接輸出高維動作空間的連續控制，避免了先前VLA方法中使用的複雜動作標記方案。
*   **解耦迭代：** 將S1和S2解耦，可以分別在每個系統上進行迭代，從而不受到尋找統一的觀察空間或動作表示的限制。

**Helix模型的訓練方式與效率**

Helix以極少的資源實現了強大的物體泛化。它總共使用了大約500小時的高質量監督數據來進行訓練，這只佔了先前收集的VLA數據集規模的不到5%，而且不依賴於多機器人實體收集或者多訓練階段。

*   **訓練數據生成：** Figure AI使用了自動標註的VLM來生成事後指令。VLM會處理來自機載機器人攝像頭的分段視頻片段，並且提示“你會給機器人什麼指令來執行視頻中看到的動作？”
*   **端到端訓練：** 經過完全端到端的訓練，Helix可以将原始像素和文本命令映射到具有标准回归损失的连续动作，再通过调节S1行为的潜在通信向量，将梯度从S1反向传播到S2，从而允许对两个组件进行联合优化。
*   **时间偏移校准：** 在訓練過程中，Figure AI在S1和S2輸入之間還添加了經過校準的時間偏移，從而可以匹配S1和S2部署的推理延遲之間的差距，確保部署期間的實時控制要求被準確地反映在訓練中。
*   **流式推理優化：** Helix的訓練設計支持在Figure機器人上高效並行地部署模型。每個機器人都配備了雙低功耗嵌入式GPU。

**家庭環境適應性與未來展望**

與規範化的工業環境不同，家庭環境中充滿了各種不可控的因素，每個物品都有不可預測的形狀、大小、顏色和紋理。但是Helix無需任何針對特定任務的示範、無需大量的手動編程，就能夠即時生成長期的、協作性的、靈巧的操作動作，展現出了強大的物體泛化能力。

在视频展示的场景中，当人类把Figure机器人从未见过的杂物放置在桌上，并且提出整理要求时，两个机器人可以在没有经过明确分工的情况下，自然而然地开始协作，将杂物归类放置。机器人甚至细心到知道瓶装的物品应该放置在冰箱门上的置物筐中才不会容易翻倒，而不是往冰箱里随便一塞就了事，而另一个机器人则知道要将饼干放到抽屉中，收拾好后，它们还会记得贴心地把抽屉关好、关上冰箱门。不過，略微有些美中不足的是，機器人一開始就把冰箱門打開，直到收拾完全部物品才把冰箱門緩緩合上，在節能意識方面可能還有待提高。

從這次演示來看，Helix模型展現出了人形機器人擁有近乎人類的環境適應性的巨大潛能。隨著模型規模的不斷擴展，它將為人形機器人真正進入家庭等更為複雜的環境，以及融入人類生活創造更多的可能性。也許在不久的將來，人形機器人就會像今天的智能手機一樣普及，成為我們生活中不可缺少的一部分。

**結語**

好了，以上就是本期視頻的內容。感謝大家的觀看，我們下期再見。

**主要修改說明：**

*   **標題：** 添加更具吸引力的標題。
*   **分段：** 將文稿分為更小的段落，增加可讀性。
*   **子標題：** 添加子標題，突顯關鍵信息點。
*   **項目符號：** 使用項目符號列出Helix模型的優點，使其更易於閱讀。
*   **簡化語言：** 略微修改了一些句子結構，使其更簡潔易懂。
*   **重點強調：** 使用粗體字強調重要詞彙和短語。
*   **細節整理：** 將一些段落重組，使其更具邏輯性。
*   **去除口語化：** 稍微調整了語氣，去除過於口語化的部分，使整體更專業。

希望這個整理後的版本對您有幫助！如果您需要進一步的修改，請隨時告訴我。

[model=gemini-2.0-flash,0]


---

</details>

<details>
<summary>573. 【分享】两万字深度解密DeepSeek-R1、Kimi 1.5，强推理模型凭什么火出圈 | 北大AI对齐团队 | 超清版</summary><br>

<a href="https://www.youtube.com/watch?v=DkzNbv8ioWk" target="_blank">
    <img src="https://img.youtube.com/vi/DkzNbv8ioWk/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【分享】两万字深度解密DeepSeek-R1、Kimi 1.5，强推理模型凭什么火出圈 | 北大AI对齐团队 | 超清版



---

</details>

<details>
<summary>572. [2025-02-23] 【科学】“移动沙发难题”或被解决 | 直角可搬沙发的最大面积是多少 | 困扰数学家近60年 | 电话型沙发 | 约瑟夫格弗 | 乔丹曲线 | 格林定理 | Jineon Baek | 119页论文</summary><br>

<a href="https://www.youtube.com/watch?v=1PYwiJjZ8J8" target="_blank">
    <img src="https://img.youtube.com/vi/1PYwiJjZ8J8/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【科学】“移动沙发难题”或被解决 | 直角可搬沙发的最大面积是多少 | 困扰数学家近60年 | 电话型沙发 | 约瑟夫格弗 | 乔丹曲线 | 格林定理 | Jineon Baek | 119页论文

好的，這是經過整理後，更易於閱讀和理解的文稿，我做了以下調整：

*   **分段更清晰：** 將內容分成更小的段落，方便閱讀。
*   **重點詞句加粗：** 加粗了一些關鍵詞和重要的句子，幫助快速理解內容。
*   **簡化部分語句：** 稍微調整了一些語句，使其更簡潔易懂。
*   **潤飾文字:** 調整部分用語，使其更符合口語習慣。

---

大家好，這裡是最佳拍檔，我是大飛。

在經典美劇《老友記》裡，有這樣一個讓人忍俊不禁又印象深刻的場景：羅斯滿心歡喜地找人幫忙搬新沙發，可這沙發在經過走廊拐角的時候，卻怎麼也過不去，試圖通過旋轉沙發找到合適的角度，卻還是徒勞無功，只能眼睜睜看著沙發卡在那兒。

相信不少朋友看到這一幕的時候，都跟著著急上火。不過估計很少有人會想到，這個看似平常的生活小插曲，背後竟隱藏著一個**困擾了數學家們整整60年的世界級數學難題——移動沙發難題**。

如今，這個懸而未決長達60年的謎題終於被成功破解！一篇長達119頁的論文給出了驚人的證明，**那就是世界上確實存在一種沙發，它的面積就是能順利通過拐角的最大面積**。今天，就讓我們來一起了解一下這個有趣的數學故事。

時間回到1966年，加拿大數學家**利奧·莫澤（Leo Moser）**以定量的形式提出了“移動沙發難題”。想象一下，有一條寬度為1單位的L形走廊，如果忽略沙發的高度，僅從平面幾何角度考慮，現在要移動一個二維形狀通過這個走廊，那麼**這個不會被卡住的最大形狀的面積究竟應該是多少呢？**

這個問題看似簡單，卻像一道難以跨越的鴻溝，橫在了數學家們面前。

難題剛提出的時候，人們首先想到的是一些簡單的幾何形狀。比如邊長為1的正方形，它結構規整，很容易就能通過轉角，它的面積為1。但是大家很快發現，一旦把正方形拉長變成矩形，在通過走廊拐角時就會立刻撞上走廊，顯然這種形狀不是理想的答案。

於是，數學家們又開始思考其他的形狀。他們想到了半圓，一個半徑為1的半圓，面積約為1.57（也就是π/2）。因為半圓在撞到拐彎處的時候，圓形的邊緣能留下一定空間，所以相比正方形，通過拐角的能力有所提升。不過，數學家們追求的是**最優解**，這個面積顯然還不夠大，一定還有更巧妙、面積更大的形狀等待被發現。

在不斷探索的過程中，數學家們逐漸意識到，**解決這個問題的關鍵在於要同時優化形狀大小和穿越路徑**。這裡涉及到兩種類型的運動，分別是**滑動**和**旋轉**。只有讓沙發在通過拐角的時候，巧妙地結合這兩種運動方式，才有可能找到最大面積的形狀。

1968年，英國數學家**約翰·哈默斯利（John Hammersley）**取得了重要突破。他發現可以從現有的形狀上摳出一大塊，專門用來應對討厭的拐角，同時延伸半圓形，讓沙發的面積變得更大。經過這樣的設計，他創造出了一種**形狀類似於座機電話一樣的沙發**，這個沙發的面積達到了π/2 + 2/π，大約是2.2074。這種新的形狀混合了滑動和旋轉運動的優勢，為後續的研究開闢了新的方向。

自從哈默斯利的電話形沙發出現之後，這個問題的研究就陷入了長達24年的停滯期。直到1992年，羅格斯大學的**約瑟夫·格弗（Joseph Gerver）**提出了一種更為巧妙的形狀，讓這個難題的研究再次取得了進展。**格弗設計的沙發面積約為2.2195，比哈默斯利的沙發面積還要大了一些**。從外觀上看，格弗的沙發和哈默斯利的沙發有些相似，但是仔細觀察就會發現細微的差別，格弗的沙發由18個不同的部分組成，在圓形切口底部的斜角邊緣和哈默斯利的沙發有所不同。

格弗懷疑自己找到的這個形狀就是能通過拐角的最大沙發形狀，但是他卻無法證明這一點。同樣，數學家們雖然也懷疑這就是利奧·莫澤問題的答案，可惜由於缺乏有力的證明，始終不能確定。

就在大家對這個難題感到迷茫的時候，一位年輕的博士後研究員脫穎而出，他就是來自首爾延世大學的**白鎮淵（Jineon Baek）**。

2016年，剛進入密歇根大學攻讀研究生的白鎮淵，因為服兵役中斷了學業。在服役期間，他偶然在一篇博客文章中看到了“移動沙發問題”。起初，他只是把它當作工作之餘放鬆解悶的小遊戲，但是隨著思考的深入，他發現這個問題充滿了挑戰和魅力，於是決定認真研究。他心中有了一個初步想法，覺得或許能證明格弗的沙發就是正確答案，不過還有許多細節需要完善。

2021年，白鎮淵回到學校後，便下定決心要攻克這個困擾學界多年的難題。在研究的道路上，白鎮淵其實並不是一帆風順。通常情況下，數學博士生會選擇導師，然後由導師來分配研究問題，但是白鎮淵一心想研究“移動沙發問題”，這讓他在尋找導師的時候遇到了困難，因為密歇根大學的教授們覺得自己在這個領域缺乏足夠的專業知識，都不太願意指導他。

幸運的是，代數領域的專家**邁克爾·齊夫（Michael Zieve）**被白鎮淵的熱情和執著所打動，同意指導他。齊夫表示：“我從未指導過與我研究領域相差這麼遠的學生，但是我願意嘗試。”正是這種跨領域的指導，為白鎮淵的研究帶來了新的視角和可能。

在攻讀博士期間，白鎮淵在**約阿夫·卡盧斯（Yoav Kallus）和丹·羅米克（Dan Romik）**等人研究的基礎上，繼續深入探索。他開發了一個強大的計算工具，進一步縮小了面積的上限，取得了重要的階段性成果。原本，白鎮淵打算畢業後繼續採用計算方法，來徹底解決“移動沙發問題”。但是幾個月後，他意識到或許可以不依賴計算機，通過純數學推理來完成證明。

數學家們早已知道，任何滿足“移動沙發問題”的解都需要具備一些特定條件，比如**最優沙發要能夠以特定的方式旋轉、底部需要有為走廊轉角留出空間的部分**等等。滿足這些條件的形狀有**無窮多個**，而白鎮淵首先要做的就是**縮小範圍**。他通過一系列複雜而精妙的數學推理證明，**最優形狀至少與格弗的沙發是相似的**。

為了進一步研究，他將每個沙發表示為無限維空間中的一個點。理想情況下，他希望找到一個函數，只需要輸入點就能夠輸出沙發面積，這樣就可以直接找到函數輸出最大時所對應的點，也就是最優沙發的形狀。但是由於**不存在能計算所有形狀面積的通用公式**，所以他決定另辟蹊徑，間接研究形狀的面積。

白鎮淵發明了一個新的函數Q，並且定義了它的幾個重要屬性：

*   **首先，對於他所定義空間中的任何沙發，Q的輸出至少和沙發面積一樣大。**
*   **它本質上測量的是包含沙發的一個形狀的面積。**

這意味著如果能找到Q的最大值，就能得到最優沙發面積的一個上限。更關鍵的是，**對於格弗的沙發，函數Q的輸出恰好等於其面積**。所以，白鎮淵只需證明當輸入為格弗的沙發時，Q能取到最大值，就夠證明格弗的沙發是“移動沙發問題”的最優解。

白鎮淵精心構建的Q函數表現出色，它類似於一個簡單的拋物線，相對容易找到最大值。經過深入研究，他證明了使Q最大化得到的形狀，需要滿足一組特定的條件，而定義格弗沙發的方程也滿足這些條件。

就這樣，白鎮淵成功解決了這個困擾數學家們數十年的難題，**證明了格弗的沙發是能通過走廊且不被轉角卡住的最大形狀**。

白鎮淵的證明目前仍然在同行評審中。他巧妙地結合了數學不同領域的技術，讓這個原本極其困難的問題變得可解，而且全程沒有借助計算機。他的導師齊夫評價道：“白鎮淵能夠不借助計算機完成證明，令人印象深刻，這表明其中有重要的新想法。”而格弗在提出方案30多年後，終於看到問題被解決，也感慨道：“我現在75歲了，能活著看到有人最終解決了這個問題，我覺得很幸運。”

接下來，咱們稍微解讀一下白鎮淵這篇119頁論文的核心內容。前面我們提到過，這個問題之所以如此困難，很大程度上是因為**沒有一個通用的公式，可以計算所有可能的移動沙發的面積**。

為了解決這一難題，白鎮淵首先定義了格弗的沙發G，即刻度標記表示構成G邊界的18條解析曲線和線段的端點，以及灰色表示的包含G的支撐走廊Lt。這樣以來，我們就可以把移動沙發S，看作是旋轉走廊的交集。

隨後，他證明了一個稱為**單射性條件**的性質，這個條件適用於最大面積的移動沙發Smax。對於滿足該條件的每個移動沙發S，他都定義了一個更大的形狀R，R的形狀類似於格弗的沙發，然後，R的面積Q(S)作為S面積的上界，並且當S是格弗的沙發G的時候，Q(S)恰好等於S的實際面積。S的單射性條件確保區域R的邊界會形成一個喬丹曲線（Jordan curve），這使得白鎮淵能夠利用格林定理（Green's theorem）來計算Q(S) 。

在具體的證明過程中，白鎮淵主要做了以下三點：

1.  **首先是將 S\_max 的可能形狀，縮小為單調沙發（monotone sofa），即由支撐走廊內角雕刻出凹痕的凸體。**
2.  **然後證明S\_max是單調的，即沙發在通過拐角的過程中，各個部分的運動趨勢是有規律的。**
3.  **以及S\_max是平衡的，即沙發的邊長互相平衡，保證在移動過程中，不會出現某一边過長或者過短導致卡住的情况。**
4.  **還有S\_max具有旋轉角π /2，也就是能以特定的角度順利通過拐角**

其次，他證明了S\_max滿足單射性的條件。

最後，在單射性條件下，建立沙發面積的上界Q。為此，他定義了凸域C來確定Q的範圍，定義了二次泛函Q，並證明它是沙發面積的上界，同時還證明了Q是凹函數。

**最終，證明了格弗的沙發是Q函數的局部最優解，進而也是全局最優解。**

在這篇論文發表之後，有不少網友在網上討論，有人說，這個結果非常直觀，正如大多數人猜測的那樣，不過證明這一點要困難得多。也有人貢獻了自己的實際經驗，那就是在現實生活中，答案取決於天花板的高度，以及沙發是否帶有可傾斜的靠背。

當然，也許我們最應該吐槽的，還是牆角和沙發這個糟糕的設計組合。不知道大家怎麼看這個移動沙發的最優解呢？在實際生活中碰到這種情況，大家又是如何解決的呢？歡迎在評論區留言。

感謝大家的觀看，我們下期再見。

---

希望這個整理版本對您有幫助！

[model=gemini-2.0-flash,0]


---

</details>

<details>
<summary>571. [2025-02-22] 【商业】昔日芯片霸主英特尔将被拆分？ | 博通 | 台积电 | 经营困境 | 出售Altera | 帕特基辛格 | IDM模式是否还能继续 | 530亿美元芯片补助 | 特朗普政府 | 前途难料</summary><br>

<a href="https://www.youtube.com/watch?v=jgSThv4SDQw" target="_blank">
    <img src="https://img.youtube.com/vi/jgSThv4SDQw/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【商业】昔日芯片霸主英特尔将被拆分？ | 博通 | 台积电 | 经营困境 | 出售Altera | 帕特基辛格 | IDM模式是否还能继续 | 530亿美元芯片补助 | 特朗普政府 | 前途难料

好的，我幫您整理了這篇文稿，重點如下：

**主題：全球芯片巨頭英特爾面臨被拆分的風險**

**主要內容：**

1.  **拆分傳聞：**
    *   全球芯片巨頭英特爾傳出可能被拆分的消息。
    *   潛在買家包括博通 (Broadcom) 和台積電 (TSMC)。
    *   消息由特朗普政府推動。

2.  **潛在買家意向：**
    *   **博通：**
        *   對英特爾的芯片設計業務感興趣。
        *   正在非正式地討論提出收購要約的可能性。
        *   前提是找到願意接手英特爾製造業務的合作夥伴。
    *   **台積電：**
        *   對英特爾的晶圓廠感興趣。
        *   已評估接管英特爾部分或全部晶圓廠的可能性。
        *   可能以投資者聯合體或其他架構進行。
    *   **重要：** 博通和台積電目前沒有合作，談判都處於初步階段。

3.  **英特爾面臨的困境：**
    *   經營困境：為股東創造最大價值。
    *   出售業務：
        *   FPGA 部門 Altera。
        *   晶圓廠作為獨立營運單位運作。
        *   計劃將晶圓廠改組為子公司。
        *   出售 Folsom 园区
    *   尋找新任 CEO：
        *   已委託獵頭公司 Spencer Stuart。
        *   新 CEO 的任務取決於公司最終保留哪些業務部門。

4.  **交易的困難：**
    *   **運營層面：**
        *   台積電接管晶圓廠的技術困難 (例如：轉換製程)。
        *   可能由其他美國公司與台積電共同投資，確保本土控制權。
        *   台積電需要向英特爾披露專有製造工藝，存在風險。
        *   英特爾放棄生產控制，改變 IDM 商業模式。
        *   英特爾海外投資項目前景不確定。
    *   **財務層面：**
        *   英特爾可能轉型為專注設計的芯片公司，毛利率可能大幅下滑。
        *   台積電可能影響毛利率水平。
    *   **政府層面：**
        *   英特爾對美國國家安全具有戰略意義。
        *   特朗普政府不太可能批准外國公司管理英特爾在美國的晶圓廠。
        *   2022 年的芯片法案要求英特爾保持晶圓廠的多数股权。
        *   過去博通收購高通的案例被否決，顯示政府影響力。

5.  **英特爾的歷史：**
    *   早期：
        *   從計算機內存業務起步。
        *   IBM 選擇英特爾的芯片作為個人電腦的核心部件。
        *   成為個人計算機的大腦。
    *   21 世紀後：
        *   喬布斯為 iPhone 尋找芯片時，認為英特爾的芯片價格過高。
        *   技術優勢出現問題，下一代工藝技術多次跳票。
        *   在 AI 浪潮中犯了關鍵的錯誤，未能及時調整技術路線。
        *   前 CEO 帕特·基爾辛格被解僱。

6.  **未來展望：**
    *   英特爾的處境艱難，存在消失的風險。
    *   英特爾的消失對美國國家聲譽和行業格局都將產生巨大影響。
    *   英特爾未來走向充滿不確定性。

**總結：**

英特爾正面臨嚴峻的挑戰，可能被拆分。博通和台積電對其部分業務感興趣，但交易存在許多困難。英特爾的未來充滿不確定性，其命運將影響整個芯片行業格局。

[model=gemini-2.0-flash,0]


---

</details>

<details>
<summary>570. [2025-02-21] 【人工智能】OpenAI发布最新代码基准测试SWE-Lancer | 价值百万美元 | Upwork | IC SWE | SWE Manager | 端到端测试 | 用户工具 | Claude最高</summary><br>

<a href="https://www.youtube.com/watch?v=baaBsehnpR8" target="_blank">
    <img src="https://img.youtube.com/vi/baaBsehnpR8/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【人工智能】OpenAI发布最新代码基准测试SWE-Lancer | 价值百万美元 | Upwork | IC SWE | SWE Manager | 端到端测试 | 用户工具 | Claude最高

好的，以下是对您提供的文稿进行的整理，主要侧重于提取关键信息、精简重复内容，并使其结构更清晰：

**核心内容概要：**

本文介绍了 OpenAI 推出的新的 AI 编码基准测试 SWE-Lancer，它模拟真实的外包软件工程任务，总价值高达 100 万美元。测试结果显示，Anthropic 的 Claude 3.5 Sonnet 在此基准上的表现优于 OpenAI 的 GPT-4o 和 o1 模型，但所有模型都远未达到人类软件工程师的水平。

**详细整理：**

1.  **引言：**
    *   SWE-Lancer 是 OpenAI 推出的新的编码基准测试，旨在评估 AI 模型在真实软件工程任务中的能力。
    *   测试基于 Upwork 上的 1488 个自由软件工程任务，总价值 100 万美元。
    *   Claude 3.5 Sonnet 在此基准上的表现优于 GPT-4o 和 o1。

2.  **背景：**
    *   SWE-Bench 和 SWE-BenchVerified 存在局限性，难以反映软件工程师的复杂工作情况。
    *   现实中的软件工程师需要处理整个技术栈、跨代码库交互和权衡。
    *   SWE-Lancer 旨在解决这些问题，提供更贴近现实的测试环境。

3.  **SWE-Lancer 详情：**
    *   **数据集：** 包含来自 Upwork 的 1488 个自由软件工程任务，总价值 100 万美元。
    *   **任务类型：**
        *   **IC SWE (个体软件工程师)：** 764 个任务，要求模型生成代码补丁解决实际问题，通过端到端测试验证。
        *   **SWE 管理：** 724 个任务，要求模型扮演软件工程经理的角色，从多个方案中选择最佳方案。
    *   **测试方法：** 采用端到端测试，模拟真实用户的工作流程，验证应用程序的完整行为。
    *   **用户工具：** 允许模型在本地运行应用程序，模拟用户行为验证解决方案。

4.  **模型表现：**
    *   所有模型获得的报酬都远低于 100 万美元，表明 AI 模型在面对真实软件工程任务时仍面临巨大挑战。
    *   **IC SWE 任务：** Claude 3.5 Sonnet 表现最好，通过率 26.2%，报酬率 21.5%。GPT-4o 通过率 8.6%，o1 通过率 20.3%。
    *   **SWE 管理任务：** Claude 3.5 Sonnet 通过率 47.0%，报酬率 53.7%。GPT-4o 通过率 38.7%，o1 通过率 46.3%。
    *   模型在价值较低、简单的任务中表现较好，在价值较高、难度较大的任务中表现下降。
    *   例如，在 SWE-Lancer Diamond 数据集中，价值超过 1000 美元的任务，模型的通过率普遍低于 30%。

5.  **影响因素：**
    *   **尝试次数：** 增加尝试次数可以提高模型解决问题的能力。
    *   **计算资源：** 增加计算资源可以提高模型在复杂任务中的表现。
    *   **用户工具：** 较强的模型能够更有效地利用用户工具。

6.  **结论：**
    *   AI 模型在软件工程任务上取得进展，但要达到可信部署的标准，还需要提高可靠性。
    *   AI Agent 在问题定位方面表现突出，但在理解跨组件或文件的问题方面有限。
    *   SWE-Lancer 为评估 AI 模型的编码能力提供了一个更真实、更全面的平台。
    *   AI 模型超越绝大部分软件工程师可能只是时间问题。

**总结与潜在用途：**

*   这个整理后的文档突出了 SWE-Lancer 基准测试的关键信息，方便快速了解其目的、方法和结果。
*   可以用于快速了解 AI 在软件工程领域的进展和挑战。
*   可以为 AI 工程师提供优化模型性能的参考。
*   可以为研究人员提供进一步研究的方向。

希望这个整理对您有所帮助! 如果您有任何其他问题或需要进一步修改，请随时告诉我。

[model=gemini-2.0-flash,0]


---

</details>

<details>
<summary>569. [2025-02-20] 【人物】芯片「硅仙人」Jim Keller的传奇人生 | 家庭环境 | 大学成长 | 早期公司 | AMD | 博通 | Apple | Intel | Tesla | Tenstorrent</summary><br>

<a href="https://www.youtube.com/watch?v=QCA1mQXzWUo" target="_blank">
    <img src="https://img.youtube.com/vi/QCA1mQXzWUo/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【人物】芯片「硅仙人」Jim Keller的传奇人生 | 家庭环境 | 大学成长 | 早期公司 | AMD | 博通 | Apple | Intel | Tesla | Tenstorrent

好的，以下是經過整理的文稿，使其更具結構性、更易讀：

**吉姆·凯勒：芯片传奇的探索之路**

**引言：**

大家好，这里是最佳拍档，我是大飞。在科技飞速发展的今天，芯片作为各种电子设备的核心，其重要性不言而喻。而在芯片领域，有一位堪称传奇的人物，他就是吉姆·凯勒（Jim Keller）。他的职业生涯横跨多家世界顶尖科技公司，参与设计的处理器深刻影响了现代计算机硬件的发展进程。本次内容基于计算机历史博物馆频道对吉姆·凯勒的采访，回顾了他从普通家庭成长起来，逐渐加入半导体行业，辗转多家知名半导体芯片公司，再到躬身创办AI芯片公司Tenstorrent的完整历程。

**一、早期生活与家庭影响**

*   **家庭背景：**
    *   父亲是GE Aerospace的机械工程师，从事卫星设计，擅长描述和绘图，经常向年幼的吉姆·凯勒讲解事物原理。
    *   母亲是大学毕业生代表，后成为治疗师，重视智力对话和家庭学习氛围。
    *   家中藏书丰富，父母都参加读书俱乐部。
*   **早期发展：**
    *   学习阅读方面遇到困难，三年级或四年级才真正学会阅读，但父母从未担心，而是让他按自己的节奏成长。
    *   学会阅读后迅速爱上读书，但在写作方面一直不太擅长。
    *   对数学有浓厚兴趣，学习过程中常有顿悟时刻。

**二、高中与大学教育**

*   **高中经历：**
    *   就读于费城地区的一所郊区高中，氛围自由松散。
    *   遇到了优秀的数学和化学老师，激发了他对知识的渴望，为日后的学习和工作奠定了基础。
*   **大学经历：**
    *   选择了宾夕法尼亚州立大学，原因包括喜欢的女孩和对电气工程的好奇。
    *   双修了哲学，但因写作表现不佳，未获得哲学学位。
    *   在电气工程学科取得了全A的优异成绩。
    *   受导师乔·斯塔克博士的影响，接触到了半导体课程，对半导体领域产生了浓厚的兴趣。
    *   学习了编程课程和逻辑设计课程，看到了从简单构建复杂事物的奇妙之处。

**三、职业生涯早期：从哈里斯到数字设备公司**

*   **哈里斯公司：**
    *   面试多家半导体公司后，选择了佛罗里达州的哈里斯公司。
    *   主要工作是修理测试设备，积累了宝贵的实践经验。
*   **数字设备公司（Digital Equipment）：**
    *   应聘时钟设计师的职位，并成功加入。
    *   参与了VAX 8800小组的工作，参与了缓存子系统的逻辑设计工作。
    *   逐渐掌握了计算机架构的知识，学会了如何进行性能建模和调试计算机。
    *   参与了多个项目，积累了丰富的经验。

**四、AMD公司：K7和K8项目**

*   **加入AMD：** 受邀加入AMD，领导K7项目。
*   **面临的挑战：** AMD在处理器市场面临激烈的竞争，之前的产品大多是对Intel处理器的掩模复制。
*   **主要贡献：**
    *   参与了K7和K8项目。
    *   作为x86 64规范、HyperTransport规范的主要作者之一，对芯片架构进行了重大更新。
    *   K8最终成为了Opteron处理器，在市场上取得了一定的成绩。

**五、SiByte、博通与P.A. Semi**

*   **SiByte：**
    *   开发MIPS处理器，并在此基础上打造了网络SoC。
    *   结识了许多行业内的重要人物。
    *   SiByte被博通Broadcom收购。
*   **Broadcom：**
    *   接触到了全新的处理器应用空间，学习到了如何构建网络计算机平台。
    *   从亨利·萨缪利身上学到了很多关于技术发展趋势的分析和预测方法。
*   **P.A. Semi：**
    *   创办P.A. Semi公司，目标是设计一款性能更强大的处理器。
    *   在架构设计上更为激进地实现数据流操作，并注重降低功耗。
    *   公司被苹果收购。

**六、苹果公司：iPhone芯片设计**

*   **加入苹果：** 参与了多个iPhone芯片的设计工作。
*   **主要贡献：**
    *   对后续的芯片进行了大量优化，如H3芯片的优化、H4芯片中加入强大的图形单元、H5芯片（Swift）的交付以及H6芯片的大规模乱序计算机设计等等。
    *   学习到了苹果独特的设计方法论和以愿景驱动的企业文化。

**七、重回AMD：Zen架构的挑战**

*   **重回AMD：** 受邀负责技术，设计出具有竞争力的芯片。
*   **面临的挑战：** 方法论不完善、团队组织不合理、架构存在问题以及与代工厂的合作困境等等。
*   **主要贡献：**
    *   负责Zen架构的设计工作。
    *   通过与团队的努力，对架构进行了一系列调整。
    *   成功交付了产品，提升了AMD在市场上的竞争力。

**八、特斯拉与英特尔**

*   **特斯拉：**
    *   受邀升级自动驾驶硬件，需要一款性能更强大的芯片。
    *   带领团队只用了14个月就完成了流片。
    *   使用三星的技术，设计了一款独特的芯片。
*   **英特尔：**
    *   试图对英特尔的芯片工程团队进行改革，引入新的设计理念和流程。
    *   由于公司规模庞大，组织架构复杂，改革遇到了一些困难。

**九、Tenstorrent公司：AI领域的探索**

*   **创办Tenstorrent：** 原因是希望在AI领域有所建树，并看到了RISC - V处理器在AI领域的潜力。
*   **公司定位：** 致力于设计AI处理器、AI软件和CPU，构建基于RISC - V处理器的AI技术。
*   **产品与服务：** 产品主要用于通用AI计算，核心是由RISC - V处理器提供支持的张量处理器。公司不仅销售芯片，还提供IP授权服务。
*   **客户群体：** AI初创公司、数据中心、研究实验室等。

**十、个人生活与管理风格**

*   **个人生活：** 结婚20多年，有两个孩子，喜欢体育锻炼和阅读。
*   **管理风格：** 在管理公司的过程中，不断学习和借鉴不同人的管理经验，形成了自己独特的管理风格。

**结论：**

吉姆·凯勒的职业生涯充满了挑战与机遇。他凭借着自己的创新精神、学习能力和对技术的执着追求，在半导体行业留下了深刻的印记。他参与设计的处理器推动了计算机硬件的发展，他的经历也为我们展现了科技行业不断创新和进步的历程。

**建议：** 大家有时间可以去看一下原视频，深入了解吉姆·凯勒。

**感谢：** 感谢大家观看本期视频，我们下期再见！

**改进说明：**

*   **结构化：** 将文稿分为了明确的段落，每个段落都有标题，使其更易于浏览和理解。
*   **重点突出：** 使用了要点列表（bullets）来强调关键信息和贡献。
*   **信息提炼：** 提取了每个阶段的核心内容，避免了冗余的细节。
*   **语言流畅：** 稍微调整了一些语句，使表达更加流畅自然。
*   **术语解释：**保留了关键的技术术语，以便专业人士参考。
*   **逻辑清晰：** 按照时间顺序和事件发展顺序排列，更易于理解吉姆·凯勒的职业生涯。

希望这个整理后的版本对您有帮助！

[model=gemini-2.0-flash,0]


---

</details>

<details>
<summary>568. [2025-02-19] 【人工智能】Grok 3/Grok 3 mini发布 | 史上最聪明的AI |  20 万卡集群训练 | 整合推理能力 | DeepSearch | Arena首次突破1400分 | GPT-4.5</summary><br>

<a href="https://www.youtube.com/watch?v=ugNn-2dgwqg" target="_blank">
    <img src="https://img.youtube.com/vi/ugNn-2dgwqg/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【人工智能】Grok 3/Grok 3 mini发布 | 史上最聪明的AI |  20 万卡集群训练 | 整合推理能力 | DeepSearch | Arena首次突破1400分 | GPT-4.5

好的，我將這篇文稿整理如下：

**主题：xAI 发布 Grok 3 和 Grok 3 mini：盘点发布会内容及性能**

**引言：**

*   最佳拍档大飞介绍，马斯克率领 xAI 团队发布 Grok 3 和 Grok 3 mini，Grok 3 被誉为“史上最聪明的AI”，吸引了广泛关注。

**Grok 3 发布会重点：**

1.  **Grok 的含义：**
    *   源自小说《异乡异客》，意为充分而深刻地理解某事，并具有同理心。
2.  **Grok 3 的优势：**
    *   强大的技术团队和数据中心支持，超越 Grok 2。
    *   花费122天建成10万卡数据中心，后92天扩建到20万卡。
3.  **Grok 3 的特点：**
    *   **推理能力整合：** 解决问题前进行思考，目前仍处于训练早期阶段。
    *   **Grok 3-mini：** 迷你版本推理模型，训练时间更长，表现有时略优于 Grok 3。
    *   **DeepSearch：** 新型搜索引擎和Agent功能结合的早期版本，扫描互联网和 X 平台，给出内容摘要。
    *   **Big Brain：** 加强的推理模式，允许模型进行更深入的思考。
    *   马斯克比喻 Grok 已“准备好上大学”。
4.  **Grok 3 的订阅模式：**
    *   X 平台的 Premium+ 深度用户和 SuperGrok 用户（月费 30 美元/年费 300 美元）可以使用。
    *   已推出网页版和 App 版，一周内上线语音模式，几周后为企业用户提供包含 DeepSearch 的 API 接入方案，几个月后开源 Grok 2。

**Grok 3 的性能表现：**

1.  **xAI 的使用场景目标：**
    *   完成重要的现实任务，例如打造特斯拉、解决太空发射问题，或应用于数据中心。
2.  **马斯克的预测：**
    *   两年内计算机在各方面超越人类，帮助人类实现诺贝尔级的科学数据突破。
    *   明年 11 月 SpaceX 真正登陆火星，使用 Grok 模型计算整个发射过程。
3.  **AI 游戏工作室：**
    *   xAI 宣布成立 AI 游戏工作室制作游戏。
    *   演示用 Grok 3 创造融合《俄罗斯方块》和《宝石迷阵》的游戏，最终成功完成。
    *   演示长思维链示例，让 Grok 3 输出一段符合开普勒定律的天体模拟，为发射到火星的飞行器规划轨迹。
4.  **Grok 3 的基准测试：**
    *   数学、科学、编码方面击败其他竞争对手。
    *   Chatbot Arena 竞技场中得分突破 1400 分，排名第一。
    *   Grok 3 引入思维链推理能力，AIME 2024 中取得 93% 的成绩。
    *   AIME 2025 竞赛中，Grok 3 推理模型表现更好，表明更大的模型具有更强的泛化能力和性能。

**Scaling Law 的验证：**

1.  **Grok 3 的算力支持：**
    *   20 万张英伟达 GPU 的超强算力支持，由 Colossus 超级计算集群训练完成。
    *   总计 2 亿个 GPU 小时，远超 OpenAI 的 GPT-4 和 DeepSeek-V3 的训练算力。
2.  **网友观点：**
    *   将这次发布当做是 Scaling Law 技术路线的又一次验证。
    *   马斯克称 Grok 3 对标 GPT 5。

**Grok 3 的使用体验：**

1.  **Andrej Karpathy 的评价：**
    *   Grok 3 解决了卡坦岛任务。
    *   未解决表情符号谜题。
    *   Grok-3 + Thinking 的表现似乎达到了与 o1 Pro 相当的水平，并且略优于 DeepSeek-R1 和 Gemini 2.0 Flash Thinking。
    *   xAI 团队在短时间内达到顶尖水平，是惊人成就。
2.  **Grok 3 的训练技术特点：**
    *   以人工生成的合成数据集为重点。
    *   使用人类反馈循环，人工审核并提供反馈。
    *   自我纠错机制，识别并纠正自身错误。

**Grok 的挑战与优势：**

1.  **市场份额：**
    *   Grok 仍然是人工智能领域中比较小的角色，受欢迎程度不及 ChatGPT。
    *   ChatGPT 占据了人工智能工具市场份额的 62.5%。
2.  **Grok 的优势：**
    *   原生集成到社交媒体 X 平台上，访问实时信息。
    *   独特的编程方式，以叛逆和俏皮的语气回答问题。
3.  **Grok 的局限性：**
    *   ChatGPT 和 Gemini 等竞争对手的响应通常更加准确。

**Grok 系列模型回顾：**

1.  **Grok 1：**
    *   验证合成数据训练方法的可行性。
    *   处理复杂的逻辑问题，某些特定任务上超越 ChatGPT 3.5。
    *   局限性：处理真实世界复杂性表现不佳，训练成本高，模型规模小。
2.  **Grok 2：**
    *   结合少量高质量的真实世界数据。
    *   数学推理、代码生成和复杂问题解决方面超越 ChatGPT 4。
    *   尝试多模态能力。
    *   局限性：使用权限仅限于 X 平台的高级用户，市场覆盖率较低。

**总结与展望：**

*   如今 Grok 3 发布，情况是否会发生变化，以及 OpenAI 是否会更快应对，提前放出 GPT-4.5，我们将拭目以待。

**结尾：**

*   感谢大家观看本期视频，我们下期再见。

**整理说明：**

*   **精簡內容：** 去除口語化的詞語，使其更精煉。
*   **分點分項：** 使用數字和項目符號來提高可讀性。
*   **重点突出：** 使用粗體字突出顯示關鍵信息。
*   **結構化：** 將內容組織成一個清晰的結構，包括引言、重點、性能、優缺點和總結。

希望這個整理對您有幫助！

[model=gemini-2.0-flash,0]


---

</details>

<details>
<summary>567. [2025-02-19] 【人工智能】DeepSeek再发新研究成果NSA | 原生稀疏注意力机制 | 梁文锋参与 | 超快长上下文训练 | 十倍速度提升 | 动态分层 | 粗粒度压缩 | 细粒度选择 | 滑动窗口</summary><br>

<a href="https://www.youtube.com/watch?v=V-1_xPMVIaw" target="_blank">
    <img src="https://img.youtube.com/vi/V-1_xPMVIaw/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【人工智能】DeepSeek再发新研究成果NSA | 原生稀疏注意力机制 | 梁文锋参与 | 超快长上下文训练 | 十倍速度提升 | 动态分层 | 粗粒度压缩 | 细粒度选择 | 滑动窗口

好的，我將把您的文稿整理如下，重點在於讓文稿更易讀，結構更清晰，並避免口語化的表達：

**標題：DeepSeek發布原生稀疏注意力（NSA），大幅提升長文本處理能力**

**引言：**

大家好，這裡是最佳拍檔。DeepSeek團隊於X平台發布一項重磅研究成果，瞬間吸引大量關注，該研究在發布後四小時內瀏覽量已超過六十萬。DeepSeek創始人兼CEO梁文鋒亦是論文作者之一，顯示其深度參與研究工作。值得一提的是，第一作者Jingyang Yuan在研究期間僅為一名實習生，這證明DeepSeek為年輕人才提供廣闊的發展空間與實踐機會。

**DeepSeek新論文核心：原生稀疏注意力（NSA）**

DeepSeek的新論文提出了一種新的注意力機制——原生稀疏注意力（Native Sparse Attention，簡稱NSA），這是一種用於超快長上下文訓練和推斷，且本地可訓練的稀疏注意力機制，並具有與硬件對齊的特性。NSA有望大幅提升下一代大語言模型處理長文本的能力，同時兼顧效率。

**NSA誕生的背景：長文本建模是關鍵**

在人工智能領域，長文本建模是下一代語言模型的關鍵能力。隨著模型應用場景拓展，處理長序列文本的需求日益迫切。在文檔分析、長篇故事生成等場景中，長文本理解和處理至關重要。

**傳統注意力機制的局限性**

傳統注意力機制在處理長序列時，計算量與序列長度的平方成正比，導致計算複雜度極高。例如，解碼64k長度的上下文時，注意力計算佔總延遲的70%至80%。

**稀疏注意力機制的挑戰**

稀疏注意力機制通過選擇性地計算關鍵的查詢鍵值對來減少不必要的計算開銷。然而，現有的稀疏注意力方法存在以下缺陷：

*   部分方法只能在自迴歸解碼階段應用稀疏性，預填充階段仍需密集計算。
*   部分方法只關注預填充階段的稀疏性，在某些工作負載下無法實現全階段加速。
*   部分稀疏方法無法適應現代高效的解碼架構，如多查詢注意力（MQA）和分組查詢注意力（GQA）。
*   現有的稀疏注意力方法大多只能在推理階段應用稀疏性，缺乏對訓練階段的支持。

**NSA的目標與設計**

DeepSeek推出NSA旨在解決兩個主要問題：

1.  事後稀疏化導致的性能退化（如預訓練模型的檢索頭容易被剪枝）。
2.  現有稀疏方法難以應對長序列訓練的效率需求，存在非可訓練組件和低效反向傳播等問題，阻礙高效訓練和長上下文模型發展。

NSA具有三大核心組件：動態分層稀疏策略、粗粒度Token壓縮和細粒度Token選擇。

**NSA的兩大創新**

1.  算術強度平衡的算法設計與硬件優化。
2.  支持端到端可訓練。

**NSA架構概覽**

*   輸入序列通過三個並行的注意力分支處理：壓縮注意力（Compressed Attention）、選擇性注意力（Selected Attention）和滑動窗口注意力（Sliding Attention）。

**各注意力分支的功能**

*   **壓縮注意力：** 通過將鍵（Key）和值（Value）聚合成塊（Block）表示，捕捉粗粒度的語義信息，大幅減少後續注意力計算所需的Token數量。
*   **選擇性注意力：** 通過塊選擇機制保留重要的細粒度信息，根據分數選擇排名靠前的塊用於注意力計算，在保留關鍵信息的同時降低計算負擔。
*   **滑動窗口注意力：** 通過在輸入序列中維護一個固定大小的窗口，對窗口內的Token進行常規的注意力計算，確保模型能夠捕捉到近鄰之間的細節和依賴關係，平衡全局和局部關係。

這三個注意力組件的輸出通過一個門控機制進行加權融合，形成最終的注意力輸出。

**NSA的硬件優化**

*   DeepSeek在Triton上實現了硬件對齊的稀疏注意力內核。
*   採用不同的查詢分組策略，進行以組為中心的数据加载、共享KV加载和网格循环调度，从而最小化内存加载，进一步提高了效率。

**實驗結果**

*   在多個通用基準測試中，採用NSA的模型總體性能優於所有基線模型，包括全注意力模型。在9項指標中有7項表現最佳。
*   在推理相關的基準測試中，NSA取得顯著提升。
*   在長上下文任務中，NSA展現出強大的實力，在64k上下文的大海撈針測試中實現超強的檢索精度。
*   在LongBench上，NSA在多跳QA任務和代碼理解任務中表現優異。
*   在具有挑戰性的AIME 24基準測試中，NSA的稀疏注意力變體NSA-R在8k和16k上下文設置下均顯著優於全注意力-R。

**效率提升**

*   在訓練速度上，隨著上下文長度增加，NSA的加速效果越發顯著。在64k上下文長度時，NSA的前向傳播速度提升9倍，反向傳播速度提升6倍。
*   在解碼速度上，隨著解碼長度增加，NSA的延遲顯著降低。在64k上下文長度時實現高達11.6倍的速度提升。

**未來的改進方向**

*   進一步優化稀疏注意力模式的學習過程，使模型能夠更智能地選擇關鍵信息。
*   探索更高效的硬件實現方式，充分挖掘硬件潛力。

**結論**

DeepSeek發布的NSA論文內容詳實，技術細節闡釋清晰，具有很強的可操作性。期待DeepSeek團隊帶來更多先進的研究成果。

**感謝語**

感謝大家觀看。

**整理說明：**

*   **結構化：** 將內容分成不同的章節和段落，方便讀者理解。
*   **避免口語化：** 將口語化的表達替換為更正式的書面用語。
*   **精簡語言：** 刪除冗餘的詞語，使句子更簡潔明瞭。
*   **添加標題：** 為每個章節添加標題，方便讀者快速瀏覽。
*   **專業術語解釋：**對NSA, GQA, MQA等等專業術語進行更清晰的解釋，避免讓不熟悉的讀者困惑。

希望這個整理後的版本對您有所幫助！

[model=gemini-2.0-flash,0]


---

</details>

<details>
<summary>566. [2025-02-18] 【人工智能】全球机器学习硬件报告 | Epoch AI | 140款AI加速器 | 计算性能每年增长43% | 低精度计算 | 能效每两年翻一番 | 英伟达全球算力相当于400万块H100</summary><br>

<a href="https://www.youtube.com/watch?v=4S8ajqFZbss" target="_blank">
    <img src="https://img.youtube.com/vi/4S8ajqFZbss/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【人工智能】全球机器学习硬件报告 | Epoch AI | 140款AI加速器 | 计算性能每年增长43% | 低精度计算 | 能效每两年翻一番 | 英伟达全球算力相当于400万块H100

好的，我幫您整理了這篇文稿，使其更易於閱讀和理解，並突出重點：

**最佳拍檔 - 大飛：全球AI算力報告解讀 (省流版)**

**核心問題：**

*   全球有多少款AI加速器？
*   全球總共有多少AI算力？
*   這些算力的增長速度有多快？

**報告來源：** Epoch AI最新全球AI算力估算報告 (針對超過140款AI加速器)

**報告意義：**

*   揭示了機器學習硬體發展的現狀和趨勢。
*   分析了硬體性能的提升規律、成本效益的變化。
*   展示了不同硬體在模型訓練中的應用情況。
*   呈現了各大科技公司的算力儲備。
*   有助於理解機器學習硬體的發展軌跡和未來走向。

**報告五大結論：**

1.  **AI計算性能每年增長43%，價格下降30%。**
2.  **低精度計算成為主流。**
3.  **頂級硬體的能效，每1.9年翻一番。**
4.  **八年間，訓練大型模型所需要的處理器數量增加了20多倍。**
5.  **全球英偉達支持的計算能力平均每10個月會翻一倍。**

**詳細內容：**

*   **計算性能飛速提升：**
    *   以16位浮點運算衡量，性能每年增長43%，每1.9年翻倍。32位性能也有類似趨勢。
    *   原因：晶體管數量增加、半導體製造工藝改進、AI工作負載的專門設計(如英偉達A100/H100、谷歌TPU系列)。
    *   積極影響：降低FLOP成本，促進AI技術廣泛應用；能源效率顯著提高 (Meta MTIA、英偉達H100)。
*   **硬體性價比優化：**
    *   硬體的性價比每年提升約30%。
    *   英偉達數據中心GPU的每美元每秒浮點運算次數不斷增加(P100->H100)。
    *   高端硬體：雖然價格高昂，但具備更先進的技術和更高的性能。
    *   未來展望：Blackwell系列處理器有望在能源效率上實現更大的突破。
*   **數據精度格式的影響：**
    *   與FP32相比，TF32、tensor-FP16和tensor-INT8的性能提升分別有大約6倍、10倍和12倍。
    *   低精度格式的優勢：減少計算量和儲存需求，提高訓練效率。
*   **熱門硬體與市場格局變化：**
    *   英偉達A100已成為訓練知名機器學習模型的熱門硬體 (65個)。
    *   市場格局變化：預計到2023年底，英偉達H100的銷量已超過A100。
    *   英偉達晶片總可用計算能力以每年約2.3倍的速度增長。
    *   NVIDIA GPU可提供4e21 FLOP/s的計算能力，大約相當於400萬個H100。
*   **集群規模爆炸式增長：**
    *   AI訓練集群規模從2016年的800個GPU到2024年的16384個H100 GPU，增長超過20倍。
    *   反映了AI模型訓練對計算資源的需求呈指數級增長。
    *   對硬體的協同工作能力和系統的管理調度提出更高要求。
*   **算力儲備：**
    *   科技巨頭 (谷歌、微軟、Meta、亞馬遜) 擁有驚人的AI計算能力，相當於數十萬個英偉達H100。
    *   谷歌可能擁有超過一百萬個H100的算力 (TPU)，微軟可能擁有大約50萬個H100等效的英偉達加速器。
    *   其他算力大戶：甲骨文、CoreWeave、特斯拉、xAI、各國政府。
    *   算力已成為科技公司競爭的核心資源。

**總結：**

報告展示了機器學習硬體在性能、性價比、精度、能源效率、應用以及算力儲備等方面的发展现状和趋势。

**資料來源：**

Epoch 公佈了報告背後的數據集和數據分析源代碼 (影片簡介連結)。

**整理說明：**

*   **簡化語言：** 避免使用過於專業的術語，盡量用更易於理解的語言。
*   **突出重點：** 用粗體標記關鍵信息，讓讀者快速抓住要點。
*   **分點羅列：** 使用列表和分段，使信息更清晰、更有條理。
*   **歸納總結：** 在每個部分後進行總結，幫助讀者理解。
*   **邏輯梳理：** 調整部分段落順序，使其邏輯更清晰。

希望這個整理後的版本對您有所幫助！

[model=gemini-2.0-flash,0]


---

</details>

<details>
<summary>565. [2025-02-17] 【闲聊】AI时代还要再学习编程吗？| 软件开发职位暴降 | 计算机专业毕业人数上涨 | 毕业即失业 | AI大佬发出警告 | AI应用导致裁员 | 工作被AI取代 | 成本回避 | 未来的必然</summary><br>

<a href="https://www.youtube.com/watch?v=Foq1q-RDn3c" target="_blank">
    <img src="https://img.youtube.com/vi/Foq1q-RDn3c/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【闲聊】AI时代还要再学习编程吗？| 软件开发职位暴降 | 计算机专业毕业人数上涨 | 毕业即失业 | AI大佬发出警告 | AI应用导致裁员 | 工作被AI取代 | 成本回避 | 未来的必然

好的，我來整理一下這篇文稿，使其更簡潔、重點更突出，並提升可讀性：

**核心主旨：** AI 正在快速發展，對程式設計領域造成巨大衝擊，未來可能影響更多行業。普通人應學習並擁抱 AI，而非被取代。

**重點整理：**

*   **程式設計師的寒冬：**
    *   美國軟體開發人員職位過去五年大幅下降（FRED 數據）。
    *   AI 並未創造更多程式設計師職位，反而可能導致失業。
    *   谷歌前工程負責人預言 AI 將終結程式設計行業，如今趨近成真。
    *   美國 IT 從業人員失業人數已達 152,000 人。
    *   計算機科學畢業生人數暴漲，但就業市場需求減少。

*   **AI 能力提升：**
    *   OpenAI 的 o3 模型在 Codeforces 基準測試中表現極佳，超越大部分程式設計師。
    *   模型效能提升速度驚人，意味著程式設計師門檻越來越高。

*   **業界警告：**
    *   黃仁勳建議年輕人不要再浪費時間學習程式設計。
    *   機器學習大牛塞巴斯蒂安·拉斯卡也建議不要花時間學習 AI 學位，應學習數學、統計學、物理學等基礎學科。

*   **就業市場現況：**
    *   美國所有崗位的招聘人數都在下降，不僅限於軟體開發行業。
    *   企業正在透過 AI 避免成本，減少招聘。
    *   Salesforce 宣布 2025 年將不再招聘軟體工程師。

*   **AI 工具崛起：**
    *   AI 程式設計師工具不斷湧現（Devin、Cursor、GitHub Copilot、Windsurf）。
    *   AI 程式設計能力持續提升（Claude 3.5、o3）。
    *   微軟 GitHub 推出首個 SWE Agent (Padawan)，可主動改 Bug 或開發新模組。
    *   OpenAI 內部開發高級 AI 編碼助手，可複製 L6 高級工程師的能力。
    *   AI 不僅將終結純粹的程式設計，更可能終結軟體工程本身。

*   **未來展望：**
    *   AI 下一個取代的行業可能包括科學家、設計師、明星，甚至所有人。
    *   在 AI 極大發展後，人類可能只剩下娛樂屬性。
    *   **建議：學習 AI，接受 AI，用 AI 來賺錢和更好的生活，不要被取代。**

*   **大飛的觀察：**
    *   自己頻道可以看作是一個社會實驗，證明用 AI 可以賺錢。
    *   AI 正在大面積取代基礎性甚至專業性的工作。
    *   企業使用 AI 減少員工數量和成本，將快速獲得更高的回報率。

**簡化版文稿 (可依需求調整細節):**

"大家好，我是大飛。最近一張圖顯示，美國軟體開發人員的職位正在暴跌，意味著 AI 並沒有創造更多工作，反而在取代程式設計師。業界許多專家也提出類似警告，建議學習 AI 而非盲目投入程式設計。

AI 能力快速提升，許多公司正在透過 AI 降低成本，甚至停止招聘軟體工程師。各種 AI 程式設計工具不斷湧現，AI 不僅可能終結純粹的程式設計，更可能終結整個軟體工程。

未來 AI 可能影響更多行業，因此我建議大家學習並擁抱 AI，用 AI 來改善生活與工作，而不是被 AI 所取代。我的頻道可以看作是一個實驗，證明用 AI 是可以賺錢的。謝謝大家。"

**我可以提供以下進一步協助：**

*   **針對特定段落進行更精簡的改寫。**
*   **提供更多數據佐證，或移除不必要的數據。**
*   **協助您將這份整理後的文稿轉化為簡報或其他形式。**

請告訴我您還有什麼需要！

[model=gemini-2.0-flash,0]


---

</details>

<details>
<summary>564. [2025-02-16] 【对谈】谷歌的AGI计划 | Jeff Dean和Noam Shazeer最新访谈 | 回忆入职谷歌 | 摩尔定律 | 学术生涯 | n-gram | MoE | AI编程 | 开放成果 | 展望未来</summary><br>

<a href="https://www.youtube.com/watch?v=kNVnr1BMpKU" target="_blank">
    <img src="https://img.youtube.com/vi/kNVnr1BMpKU/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【对谈】谷歌的AGI计划 | Jeff Dean和Noam Shazeer最新访谈 | 回忆入职谷歌 | 摩尔定律 | 学术生涯 | n-gram | MoE | AI编程 | 开放成果 | 展望未来

好的，我來幫您整理這篇文稿，使其更清晰、易讀。我會主要關注以下幾點：

*   **精簡冗餘信息：** 移除過多的口語化表達和不必要的鋪墊，例如開場白和結尾語。
*   **提取關鍵要點：** 突出對談的核心內容，例如兩位科學家與谷歌的淵源、對技術發展的看法、以及對未來的展望。
*   **結構化內容：** 將內容按照邏輯順序組織，增加小標題，使讀者更容易抓住重點。
*   **去除贅詞、修飾：**使內容更精煉。

**整理後的文稿如下：**

## **谷歌科學家Jeff Dean與Noam Shazeer對談重點整理**

近期，谷歌首席科學家 Jeff Dean 與 Transformer 作者之一 Noam Shazeer 接受了知名播客的採訪，重點內容如下：

**一、與谷歌的結緣**

*   **Jeff Dean：** 主動聯繫谷歌。
*   **Noam Shazeer：**
    *   最初認為谷歌已是龐大公司，未申請。
    *   2000年抱著試試看的心態投遞簡歷。
    *   加入後看到每日搜索量指數增長，認為谷歌會非常成功，打算先賺一筆錢再去研究AI。
    *   入職後，Jeff Dean 成為他的導師。

**二、谷歌的發展與變化**

*   早期人數少，大家彼此熟悉。
*   隨著公司擴張，開始出現不了解的項目。
*   需在較高層面了解公司動態，建立人脈網絡以獲取更多信息。

**三、摩爾定律與計算架構的轉變**

*   過去依賴摩爾定律帶來的硬件升級。
*   近年通用CPU擴展性不如從前，製造工藝改進週期延長。
*   專用計算設備（如機器學習加速器和GPU）大量湧現。
*   算術運算變得廉價，數據移動相對昂貴。
*   深度學習受益於矩陣乘法，TPU應運而生。
*   模型量化趨勢需要硬件、算法、芯片設計的協同。

**四、語言模型研究的早期貢獻**

*   **Jeff Dean：** 2007年參與語言建模研究，為後來的語言模型發展奠定基礎。
    *   解決谷歌翻譯效率問題，將翻譯時間從一整夜縮短到約100毫秒。
*   **Noam Shazeer：** 2001年構建拼寫糾正系統，展示早期語言模型在實際應用中的能力。

**五、AI在谷歌的應用**

*   谷歌定位為讓世界信息變得可訪問和有用，包括創建新的信息（如生成信件、影片摘要）。
*   AI的多模態能力使其能夠理解和處理各種信息（文本、圖像、音訊等）。
*   谷歌積極探索上下文搜索，結合大語言模型和互聯網索引來解決幻覺和事實性問題。
*   Gemini模型使用內部代碼庫進行訓練，目前谷歌內部25%的代碼由AI生成。
*   AI可以自動生成實驗代碼，提高研究效率，甚至自動編寫代碼。

**六、AI對研發和芯片設計的影響**

*   AI將成為研究員和工程師的助力，從高級規範生成初步結果。
*   AI可用於自動化芯片設計流程，縮短設計時間。

**七、推理計算的重要性**

*   推理計算成本低廉，有潛力讓AI系統更聰明。
*   需要設計新算法，利用增加的計算資源提高答案質量。
*   推理計算影響數據中心規劃，需考慮如何專門化硬件。

**八、多數據中心模型訓練**

*   Gemini 1.5 訓練使用多個城市區域的算力，建立高帶寬連接。
*   面臨系統中的異步性問題，以及大規模調試的挑戰。
*   Bug有時也能帶來正面影響，提供改進機會。

**九、AI的持續學習和模塊化發展**

*   Jeff Dean是稀疏模型的忠實粉絲，認為模型不同部分應擅長不同事情。
*   未來應讓模型各組件獨立開發，小組專注於特定語言或任務子集，創建高質量數據來訓練模型的模塊化部分，再集成到更大的模型中。
*   模型内存管理方面，未来可能会根据专家的使用频率和计算成本，更加灵活地分配内存资源。

**十、谷歌的未來願景**

*   提供大型基礎模型，針對不同設置提供自定義版本，添加不同模塊並限制訪問。

**十一、開放研究成果的平衡**

*   有些技術對谷歌產品關鍵，不會發布；有些技術應用後，會根據情況決定是否發布；有些技術則會公開出版。

**十二、對打工人的建議**

*   **Jeff Dean：** 關注行業動態、與同事交流、關注研究論文，與不同領域的人合作。
*   **Noam Shazeer：** 保持謙遜、接受更好想法，平衡自上而下和自下而上的資源分配方式，闡明有趣的研究方向。

**总结**

Jeff Dean 和 Noam Shazeer 在访谈中分享了他们在谷歌的经历、对技术发展趋势的看法，以及对未来的展望，强调了 AI 在谷歌业务中的重要性，以及持续学习和模块化发展的重要性。

希望这个版本更简洁、更易于阅读和理解。

[model=gemini-2.0-flash,0]


---

</details>

<details>
<summary>563. [2025-02-15] 【人工智能】人工智能的安全治理 | 李飞飞巴黎AI峰会开幕演讲 | AI的历史 | 多学科交叉 | 感知算法 | 认知科学 | 计算能力 | 空间和具身智能 | 以人为本的AI | AI治理</summary><br>

<a href="https://www.youtube.com/watch?v=Yv5_Q0zDN9I" target="_blank">
    <img src="https://img.youtube.com/vi/Yv5_Q0zDN9I/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【人工智能】人工智能的安全治理 | 李飞飞巴黎AI峰会开幕演讲 | AI的历史 | 多学科交叉 | 感知算法 | 认知科学 | 计算能力 | 空间和具身智能 | 以人为本的AI | AI治理

好的，以下是經過整理和歸納的文稿，力求清晰易懂：

**世界人工智能峰會 李飛飛演講重點回顧 (by 最佳拍檔 - 小飛)**

**核心觀點：以人為本的人工智能**

李飛飛在巴黎世界人工智能峰會上發表了備受矚目的演講，她從歷史、技術發展、應用和治理等方面，深入探討了人工智能的發展和未來。

**一、人工智能的發展歷程 (從生命起源到現代)**

*   **追溯起源：**  AI並非現代產物，其概念可追溯至5億年前生命起源的進化。從原始生物感知環境、產生視覺和理解力，到最終催生行動，這是智慧的起源。
*   **早期探索：**  艾倫圖靈的著作啟發了人們對智能機器的想像。1956年，科學家們首次提出「人工智能」的概念，但當時過於樂觀，認為短期內就能解決大部分問題。
*   **多學科融合：** 現代人工智能的發展是感知算法、認知科學和強大算力的融合。
    *   **感知算法：**  Hubel和Wiesel對視覺皮層的研究和Frank Rosenblatt的感知器是早期重要成果，激勵了深度學習算法的發展。
    *   **認知科學：**  強調大腦與環境的互動，以及從環境中學習的重要性。ImageNet項目驗證了大規模數據對機器學習的重要性。
    *   **强大算力:** 冯·诺依曼架构和微处理器的出现是基础。 视频游戏产业推动了GPU的发展，为神经网络算法提供了强大的算力支持。
*   **里程碑事件：**  2012年ImageNet挑戰賽，李飛飛實驗室整合算法、數據和算力，使得機器首次能夠理解和描述數百萬張圖像的內容。
*   **大語言模型：**  Transformer架構、海量數據和先進芯片的結合，使AI能力得到巨大飛躍。ChatGPT的成功證明了其對日常生活的影響。

**二、人工智能的發展方向 (語言智能之外)**

*   **空間和具身智能：**  AI不僅僅是語言理解，更要在數字或物理3D空間中進行創造、理解、推理和交互。 例如，识别图片中的物体并预测潜在的物理互动（如防止牛奶被打翻）。
*   **行動的轉變：**  AI將從觀察者轉變為與人類並肩行動的執行者。
*   **空間智能的重要性：**  空間智能在人類文明發展中扮演重要角色，從古代建築到科學發現都離不開它。
*   **實際應用：**
    *   複雜日常視頻的語義標註。
    *   藝術風格遷移。
    *   根據文本提示生成視頻。
    *   根據圖像創建3D世界。

**三、機器人技術的進展 (具身智能)**

*   **與大語言模型和視覺模型結合：**  新型機器人能在開放環境中執行人類任務，展现更强的适应性和智能性。

**四、以人為本的AI (核心價值觀)**

*   **尊嚴：**  利用AI技術幫助弱勢群體重拾自主性，維護其尊嚴。例如，利用腦電波技術控制機械臂，使癱瘓患者能重新製作料理。
*   **能動性：**  AI應增强人类能力，而非取代人類。在醫療健康、創造力激發、科學發現和智能製造等領域都有廣闊的應用前景。例如，利用AI提升醫療質量，減輕醫護人員負擔。
*   **社區：**  利用AI建設更完善、更有凝聚力的社區。例如，提供AI教育輔助工具，為閱讀障礙者提供個性化學習，缓解言语语言病理学家的短缺问题。

**五、人工智能治理原則**

*   **以科學為基礎：**  避免誇大其詞和聳人聽聞的論調，用科學的方法評估AI的能力和局限性。
*   **務實的態度：**  不應限制對新興技術的探索，而應關注AI的實際應用，確保良性發展，防範潛在危害。
*   **健康的生態系統：**  學術界、創業公司、開源社區、公共部門和大型企業都應積極參與，共同推動AI技術發展。避免資源過度集中在少數公司手中。

**六、總結與展望**

與其關注是否能創造AI，不如思考如何讓AI成為一股向善的力量。

**總體來說，這份整理稿的目標是將李飛飛的演講內容結構化，突出重點，方便理解和記憶。希望对您有帮助！**

[model=gemini-2.0-flash,0]


---

</details>

<details>
<summary>562. [2025-02-15] 【访谈】蔡崇信谈阿里巴巴创业、DeepSeek、与苹果合作 | 迪拜世界政府峰会对谈 | 马云创业 | 生存与创新 | AI和DeepSeek | 小模型的未来 | 苹果合作 | 体育领域投资</summary><br>

<a href="https://www.youtube.com/watch?v=NcKQ4S9tYsA" target="_blank">
    <img src="https://img.youtube.com/vi/NcKQ4S9tYsA/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【访谈】蔡崇信谈阿里巴巴创业、DeepSeek、与苹果合作 | 迪拜世界政府峰会对谈 | 马云创业 | 生存与创新 | AI和DeepSeek | 小模型的未来 | 苹果合作 | 体育领域投资

好的，這份文稿我會整理成更清晰易讀的格式，並提供一些建議，方便您更好地使用它。

**整理後的文稿 (結構化呈現):**

**標題：蔡崇信迪拜世界政府峰會訪談精華：生存、創新與阿里巴巴的未來**

**引言：**

*   科技產業快速變革，人們好奇行業領袖如何做出關鍵決策，帶領企業走向成功。
*   2025年2月13日，阿里巴巴聯合創始人蔡崇信在迪拜世界政府峰會上，分享了關於「生存與創新的文化」的見解和故事。

**一、蔡崇信加入阿里巴巴的抉擇**

*   **背景：**
    *   1999年，蔡崇信年薪70萬美元，收到阿里巴巴邀請，年薪僅600美元。
*   **契機：**
    *   《經濟學人》將馬雲比作「中國的傑夫貝佐斯」，引發蔡崇信的好奇。
    *   在杭州的二層公寓裡，蔡崇信被阿里巴巴的創業激情和馬雲的個人魅力所吸引。
    *   12支牙刷的細節，反映團隊24小時工作和生活，彷彿「中國的HP車庫」。
*   **關鍵因素：**
    *   馬雲的宏大願景和領導氣質，賦予團隊使命感。
    *   馬雲認為創業公司「跟著感覺走就行」，展現領導者魅力，描繪互聯網改變國家的藍圖。

**二、蔡崇信的職業轉變與勇氣**

*   **動機：**
    *   蔡崇信渴望探索未知領域，不滿足於現狀。
    *   從律師的諮詢工作到私募股權投資，逐步靠近決策權。
    *   對公司日常運營的好奇，以及當時互聯網行業的興起，促使他想親自參與。
*   **時機：**
    *   35歲，家庭穩定，妻子懷孕，是開啟全新冒險的人生節點。

**三、阿里巴巴的成功之道：生存與創新**

*   **核心理念：**
    *   生存與創新之間存在緊密的因果關係。
    *   在資金有限的情況下，讓公司生存下去是首要任務。
    *   「只要公司還能活下去，就有機會創造價值。」
*   **策略：**
    *   面對失敗不輕易放棄，堅持下去。
    *   基於對生存的渴望，創造創新。
*   **案例：**
    *   從B2B市場到淘寶，再到支付環節的重要性，開發了支付寶。
    *   基於搜索廣告的電商市場成為盈利引擎。
    *   雲計算業務通過內部發展逐步實現。
*   **關鍵：**
    *   始終堅持生存下去，才能迎來新的機遇。

**四、阿里巴巴的現況與未來：創新文化與AI**

*   **現況：**
    *   25年歷程，上市10年。
    *   與充滿創造力的年輕人一起工作，感到興奮。
    *   鼓勵從下往上的創新文化。
*   **AI領域：**
    *   阿里巴巴持續投資和開發自己的AI能力。
    *   對Grok、DeepSeek等大語言模型印象深刻。
    *   認為DeepSeek降低大語言模型訓練和推理成本，具有重要意義。
*   **管理理念：**
    *   「我的工作就是讓人們不受阻礙地前進。」
    *   退居幕後，支持年輕CEO的日常運營。
    *   強調不干擾他人工作，為培養下一代領導力創造良好環境。

**五、AI發展方向：解決實際問題**

*   **問題：**
    *   是否只有少數富裕公司才能負擔得起AI的「教育成本」？
    *   開發AI的目的是為了培養最聰明的「孩子」還是解決實際問題？
*   **觀點：**
    *   DeepSeek現象會引導人們關注實際問題，將資源投入到應用領域。
    *   未來會出現更多小型模型，解決特定問題。
    *   以電商為例，智能購物助手不需要萬億參數的大語言模型，只需要像商店導購一樣。
*   **開源價值：**
    *   許多公司可以參與到AI的開發中來，共同為AI發展貢獻力量。

**六、阿里巴巴與蘋果的合作**

*   **傳聞：**
    *   蘋果將與阿里合作，為中國的iPhone用戶開發AI功能。
*   **回應：**
    *   蘋果在邊緣計算領域有獨特需求。
    *   與中國公司合作開發AI是趨勢。
    *   本地化的AI對設備提供支持非常必要。
*   **合作：**
    *   蘋果選擇與阿里巴巴合作，使用阿里的AI為蘋果手機提供動力。
    *   阿里巴巴感到榮幸。

**七、蔡崇信的體育情結**

*   **身份：**
    *   NBA布魯克林籃網隊、WNBA紐約自由人隊等球隊的股東。
    *   首先是一個運動員，然後才是科技公司工作者。
*   **影響：**
    *   在運動中培養了紀律、努力和面對失敗的精神。
    *   將運動精神融入工作，期望管理團隊專注於每一個操作細節。

**八、蔡崇信名言解讀**

*   **名言一：**「領導者最重要的工作就是為人們指引未來的方向。」
    *   團隊需要明確的方向和共同目標。
    *   領導者要有能力與人合作，招募比自己聰明的人。
*   **名言二：**「謙遜對領導者非常重要，你必須能夠承認錯誤。」
    *   領導者應敢於自我反思，在方向偏離時勇敢承認錯誤並改變方向。
*   **名言三：**「大公司的一個常見問題是他們會被固定的組織結構所束縛。」
    *   企業應首先確定發展方向，然後根據這個方向設立合適的組織結構。
*   **名言四：**「99%的人需要七到八小時的睡眠，如果你認為自己是1%，你就是在自欺欺人。」
    *   保證充足的睡眠，才能保持高效。

**總結：**

蔡崇信的分享涵蓋了阿里巴巴的發展歷程、創新理念、對AI的思考以及領導力心得，為我們提供了寶貴的經驗和啟示。

**整理建議：**

*   **分段標題化：** 使用清晰的標題和子標題，方便讀者快速瀏覽和查找信息。
*   **重點突出：** 使用粗體、顏色或底線等方式突出重點詞句和關鍵概念。
*   **條列式呈現：** 使用條列式清單整理信息，使內容更易於閱讀和理解。
*   **案例佐證：** 在闡述觀點時，結合阿里巴巴的實際案例進行說明，增加說服力。
*   **補充背景知識：** 根據讀者對象，適當補充背景知識，例如蔡崇信的個人經歷、阿里巴巴的發展歷程等。
*   **潤飾語言：** 檢查文稿的語言，確保流暢、簡潔、準確。
*   **添加圖片/影片：** 如果方便，可以添加相關圖片或影片，使內容更生動有趣。
*   **建立索引：** 對關鍵字建立索引，方便快速查找相關信息。

**其他建議：**

*   **針對目標讀者：** 根據目標讀者調整文稿的內容和表達方式。
*   **版權聲明：** 如果需要轉載或使用文稿，請註明出處。

希望這些建議對您有所幫助!

[model=gemini-2.0-flash,0]


---

</details>

<details>
<summary>561. [2025-02-14] 【OpenAI】Sam Altman透露GPT-4.5和GPT 5计划 | 最后的非思维链模型 | o3将被整合 | Deep Research免费计划 | 全新的模型规范Model Spec</summary><br>

<a href="https://www.youtube.com/watch?v=XJkr0tuZFMs" target="_blank">
    <img src="https://img.youtube.com/vi/XJkr0tuZFMs/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【OpenAI】Sam Altman透露GPT-4.5和GPT 5计划 | 最后的非思维链模型 | o3将被整合 | Deep Research免费计划 | 全新的模型规范Model Spec

好的，這是我整理後的文稿，重點在於更清晰的結構、更流暢的表達，並修正一些語法和用詞。

**標題：OpenAI 最新路線圖解讀：GPT-4.5、GPT-5 即將登場，Deep Research 免費開放**

大家好，這裡是最佳拍檔，我是大飛。

Sam Altman 畫了一年多的餅，看來終於有希望要圓上了。去年他明確表示不會推出 GPT-5，即使有也不會叫這個名字。但在 13 日早上，Sam Altman 又曝光了 OpenAI 的全新路線圖，提到 GPT-4.5 會有的，GPT-5 也會有的。不得不說，想當大佬，得先能接得住打自己的臉。

除了新的路線圖以外，OpenAI 還推出了 Deep Research 功能的免費計劃，以及全新的模型規範。今天大飛就來給大家詳細解讀。

**一、GPT-4.5 與 GPT-5：OpenAI 未來的核心產品**

Sam Altman 透露，GPT-4.5 預計幾週後就會面世，而 GPT-5 也將在幾個月後登場。

*   **GPT-4.5：** 將成為 GPT 系列最後一代的非思維鏈模型。思維鏈指的是讓模型在處理問題時像人類一樣進行逐步推理和思考，而不是直接給出答案，從而讓模型在面對複雜任務時邏輯更加清晰，解答更加準確。雖然 GPT-4.5 沒有採用思維鏈技術，但在現有技術的基礎上，依然會帶來性能的提升、功能的優化，以及更好的用戶體驗。
*   **GPT-5：** 則是 OpenAI 眼中的未來之星，它將成為一個全能的系統。這個全能不只是簡單地打通 o 系列和 GPT 系列產品。一直以來，OpenAI 內部的 o 系列和 GPT 系列模型在功能和應用場景上都有所不同，存在各自的優勢和局限。GPT-5 的出現，就是要整合這些資源，將 o3 等多項先進的技術融合在一起。此前預告的 o3 模型將不再獨立推出，未來，在 ChatGPT 和 API 中，GPT-5 將作為一個集成了多項技術的綜合系統，發揮強大的作用。

**二、GPT-5 的強大功能與免費策略**

GPT-5 不僅能像以往的模型一樣完成文本生成、對話交互等基礎任務，還具備自主使用工具的能力。比如說，當用戶提出一個需要數據分析的任務時，它能夠自動調用合適的數據分析工具進行處理。在遇到複雜的問題的時候，它知道什麼時候該深入思考，像專家一樣進行深度分析，什麼時候又可以快速響應，給出簡潔明瞭的答案。

更令人驚喜的是，OpenAI 宣布未來所有人都可以免費使用 GPT-5。Sam Altman 表示，他們也意識到現在的模型選擇界面過於複雜，希望回歸到簡單直觀的統一智能體驗。在標準模式下，用戶可以無限制地使用 GPT-5，不過為了保證服務的穩定和公平，也需要遵守相應的使用限制。

*   **ChatGPT Plus 用戶：** 將能夠使用到更高智能水平的 GPT-5，享受更快的響應速度、更精準的回答以及更多的功能權限。
*   **Pro 會員：** 更是可以直達最高智能水平的 GPT-5，享受到頂級的 AI 服務。

這種分層的服務模式既照顧了普通用戶的基本需求，又為有更高要求的用戶提供了個性化的選擇。

此外，ChatGPT 現有的多項功能，比如語音交互、搜索能力、深度研究等等，都會被整合到 GPT-5 模型中。這意味著用戶以後在使用 GPT-5 的時候，不再需要在不同功能之間頻繁的切換，能夠一站式體驗所有的功能。

**三、Deep Research 功能免費開放**

免費用戶也將能體驗到 Deep Research 的功能了，每月有 2 次使用機會，而 ChatGPT Plus 用戶每月則有 10 次使用機會。

Sam Altman 還提到，他們注意到一些用戶對 Deep Research 的需求非常強烈，每天甚至希望進行幾十次，因此未來 OpenAI 還會探索更為靈活的計算資源付費方式，來滿足不同用戶的需求。

Deep Research 的神奇之處在於它的人性化交互體驗，用戶只需要給它分配一個任務，它就像一個得力的助手，完成工作並返回一份詳細的報告。對於很多人來說，如果不追求絕對的完美，只是想要一個有充足依據的論點和分析，那麼 Deep Research 已經足夠做到令人滿意。

無論如何，免費額度的開放，能夠讓更多人有機會體驗這項強大的功能，對於推動 AI 技術的普及和應用，有著積極的意義。

**四、其他更新**

*   **Deep Research：** 所有 Pro 用戶現在可以在所有的 ChatGPT 終端和網頁端體驗 Deep Research 的功能。
*   **o1 和 o3-mini 模型：** 有了新的能力提升，它們現在可以在 ChatGPT 中同時上傳文件和圖片。
*   **Plus 用戶：** o3-mini-high 使用額度也從每天少得可憐的次數暴漲了 7 倍，提高到了每天 50 次。

**五、全新的模型規範 (Model Spec)**

OpenAI 這次還發布了一份全新的“模型規範”（Model Spec），這份規範定義了 OpenAI 期望模型應有的表現，是 AI 發展過程中的一個重要準則。

*   **個性化定制：** 允許用戶和開發者在一定範圍內根據自己的需求調整模型行為。
*   **透明度：** 讓用戶和開發者更加清楚模型是如何運行和決策的。
*   **智識自由：** 讓人們在沒有任意限制的情況下進行探索、辯論和創造。

**六、模型規範的核心原則**

為了實現創建有用、安全而且符合用戶和開發者需求的模型這個目標，同時推進確保通用人工智能 AGI 造福全人類的使命，OpenAI 制定了一系列原則：

*   **指令鏈：** 規定了模型處理來自平台、開發者和用戶指令的優先順序。
*   **共同探索真理原則：** 要求模型作為高度可信的 AI 助手，幫助用戶做出最佳的決策。
*   **做最好的工作原則：** 為模型能力設定了基本標準，涵蓋事實準確性、創造力和編程應用等方面。
*   **保持邊界原則：** 詳細說明了模型如何在用戶自主權與預防傷害或濫用之間達到平衡。
*   **保持親和性原則：** 描述了模型的默認對話風格，即溫暖、富有同理心和樂於助人。
*   **使用適當風格原則：** 提供了關於格式和表達方式的默認指導。

OpenAI 計劃繼續擴大測試案例的數據集，不斷加入新的示例，尤其是那些在實際應用中發現的、模型和“模型規範”還沒有完全解決的案例。

OpenAI 決定將“模型規範”以知識共享許可協議發布到公共領域，並開源評估用的提示詞。

**七、總結**

可以說在經過 DeepSeek 的這一波衝擊後，能感覺到 OpenAI 現在承受著巨大的壓力，甚至不得不臨時變更 o3 的發布計劃。無論是 GPT-4.5 和 GPT-5 的推出計劃，還是 Deep Research 的免費策略，以及全新的模型規範發布，OpenAI 都急需向資本和市場證明自己依然保持足夠的領先距離。希望這次的計劃不會再跳票，我們也能盡早見證到更强大的 AI 出現。

感謝大家收看本期視頻，我們下期再見。

**修改說明：**

*   **調整標題：** 更明確點出文章重點。
*   **增加小標題：** 使文章結構更清晰，方便讀者快速掌握重點。
*   **段落劃分：** 將長段落拆分成更小的段落，增加可讀性。
*   **用詞潤飾：** 調整一些用詞，使表達更流暢自然。
*   **重點強調：** 使用粗體標示重點資訊，方便快速瀏覽。
*   **內容精簡：** 稍微精簡了一些重複或冗餘的內容。

希望這個版本對您有幫助！

[model=gemini-2.0-flash,0]


---

</details>

<details>
<summary>560. 【人工智能】巴黎AI行动峰会争夺全球AI主导权 | 欧盟2000亿欧投资 | 法国星际之门 | 美国万斯直怼欧盟 | DeepSeek成热点 | 美英放弃签署巴黎AI宣言 | 全球AI治理将多方博弈</summary><br>

<a href="https://www.youtube.com/watch?v=2eZBSU3sqVE" target="_blank">
    <img src="https://img.youtube.com/vi/2eZBSU3sqVE/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【人工智能】巴黎AI行动峰会争夺全球AI主导权 | 欧盟2000亿欧投资 | 法国星际之门 | 美国万斯直怼欧盟 | DeepSeek成热点 | 美英放弃签署巴黎AI宣言 | 全球AI治理将多方博弈



---

</details>

<details>
<summary>559. 【商业】智能驾驶明星企业纵目科技暴雷 | CEO失联 | 高管卷钱跑路 | 700名员工被欠薪 | 假意安抚 | 政府化解缓慢 | 如何维权</summary><br>

<a href="https://www.youtube.com/watch?v=xXcX4b6-cq8" target="_blank">
    <img src="https://img.youtube.com/vi/xXcX4b6-cq8/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【商业】智能驾驶明星企业纵目科技暴雷 | CEO失联 | 高管卷钱跑路 | 700名员工被欠薪 | 假意安抚 | 政府化解缓慢 | 如何维权



---

</details>

<details>
<summary>558. 【数据结构】哈希表Hash Table算法获重大突破 | 本科生推翻图灵奖得主姚期智40年前猜想 | 均匀哈希 | 弹性哈希 | 漏斗哈希 | 最坏探测复杂度</summary><br>

<a href="https://www.youtube.com/watch?v=bCX7bCngTqQ" target="_blank">
    <img src="https://img.youtube.com/vi/bCX7bCngTqQ/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【数据结构】哈希表Hash Table算法获重大突破 | 本科生推翻图灵奖得主姚期智40年前猜想 | 均匀哈希 | 弹性哈希 | 漏斗哈希 | 最坏探测复杂度



---

</details>

<details>
<summary>557. 【人工智能】DeepMind发布Alpha Geometry 2 | 攻克国际数学奥林匹克竞赛IMO几何难题 | 超越金牌得主水平 | 新的语言模型和搜索算法 | 性能显著提升 | AlphaGo时刻</summary><br>

<a href="https://www.youtube.com/watch?v=FYrL4b_yecc" target="_blank">
    <img src="https://img.youtube.com/vi/FYrL4b_yecc/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【人工智能】DeepMind发布Alpha Geometry 2 | 攻克国际数学奥林匹克竞赛IMO几何难题 | 超越金牌得主水平 | 新的语言模型和搜索算法 | 性能显著提升 | AlphaGo时刻



---

</details>

<details>
<summary>556. 【AI News】关于AI经济学的三点观察 | OpenAI CEO Sam Altman最新博客文章</summary><br>

<a href="https://www.youtube.com/watch?v=H8-0blZjkw8" target="_blank">
    <img src="https://img.youtube.com/vi/H8-0blZjkw8/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【AI News】关于AI经济学的三点观察 | OpenAI CEO Sam Altman最新博客文章



---

</details>

<details>
<summary>555. 【人工智能】深入浅出ChatGPT大语言模型 | Andrej Karpathy | 三小时视频预习版 | 预训练 | 后训练 | 分词 | Transformer | 强化学习 | RLHF</summary><br>

<a href="https://www.youtube.com/watch?v=56waReTN1_w" target="_blank">
    <img src="https://img.youtube.com/vi/56waReTN1_w/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【人工智能】深入浅出ChatGPT大语言模型 | Andrej Karpathy | 三小时视频预习版 | 预训练 | 后训练 | 分词 | Transformer | 强化学习 | RLHF



---

</details>

<details>
<summary>554. 【人工智能】资本、AGI与人类野心 | 鲁道夫·莱纳 | AGI时代权利将更为固化 | 更容易购买劳动成果 | 原始资本积累很难创造奇迹 | 劳动力替代型AI | AI取代人才 | 国家不再关心民众</summary><br>

<a href="https://www.youtube.com/watch?v=4-CuksybdoM" target="_blank">
    <img src="https://img.youtube.com/vi/4-CuksybdoM/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【人工智能】资本、AGI与人类野心 | 鲁道夫·莱纳 | AGI时代权利将更为固化 | 更容易购买劳动成果 | 原始资本积累很难创造奇迹 | 劳动力替代型AI | AI取代人才 | 国家不再关心民众



---

</details>

<details>
<summary>553. 【人工智能】澄清DeepSeek五大网络误解 | 颠覆式创新 | 蒸馏和抄袭 | 成本550万美元 | AI巨头投入浪费 | PTX打破CUDA垄断 | 国外被封禁</summary><br>

<a href="https://www.youtube.com/watch?v=7bgNqPryj0I" target="_blank">
    <img src="https://img.youtube.com/vi/7bgNqPryj0I/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【人工智能】澄清DeepSeek五大网络误解 | 颠覆式创新 | 蒸馏和抄袭 | 成本550万美元 | AI巨头投入浪费 | PTX打破CUDA垄断 | 国外被封禁



---

</details>

<details>
<summary>552. 【实战】三分钟教你如何在电脑本地运行DeepSeek R1 | macOS M1 | ollama | Cherry Studio</summary><br>

<a href="https://www.youtube.com/watch?v=ZHm9IJQig4c" target="_blank">
    <img src="https://img.youtube.com/vi/ZHm9IJQig4c/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【实战】三分钟教你如何在电脑本地运行DeepSeek R1 | macOS M1 | ollama | Cherry Studio



---

</details>

<details>
<summary>551. 【商业】爆肝两万字解读 Big Ideas 2025 | ARK方舟基金 | AI Agent | 比特币 | 区块链 | 无人驾驶出租 | 自主物流 | 能源 | 机器人 | 可重用火箭 | 多组学</summary><br>

<a href="https://www.youtube.com/watch?v=EqLBkmFr3Co" target="_blank">
    <img src="https://img.youtube.com/vi/EqLBkmFr3Co/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【商业】爆肝两万字解读 Big Ideas 2025 | ARK方舟基金 | AI Agent | 比特币 | 区块链 | 无人驾驶出租 | 自主物流 | 能源 | 机器人 | 可重用火箭 | 多组学



---

</details>

<details>
<summary>550. 【访谈】台积电往事 | 张忠谋最新深度访谈 | 黄仁勋结缘 | 40纳米问题 | 裁员风波 | 蔡力行 | 重回CEO | 投入28纳米 | 苹果合作 | 高盛借款 | 三星 | 英特尔 | 高通</summary><br>

<a href="https://www.youtube.com/watch?v=XgIoOKX16w0" target="_blank">
    <img src="https://img.youtube.com/vi/XgIoOKX16w0/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【访谈】台积电往事 | 张忠谋最新深度访谈 | 黄仁勋结缘 | 40纳米问题 | 裁员风波 | 蔡力行 | 重回CEO | 投入28纳米 | 苹果合作 | 高盛借款 | 三星 | 英特尔 | 高通



---

</details>

<details>
<summary>549. 【访谈】Lex Fridman最新五小时访谈精华版 | DeepSeek | 后训练 | 开放权重 | 技术改进 | YOLO Run | AGI | 注意力 | 英伟达 | 数据中心 | Agent</summary><br>

<a href="https://www.youtube.com/watch?v=RbGX-FQNYQ0" target="_blank">
    <img src="https://img.youtube.com/vi/RbGX-FQNYQ0/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【访谈】Lex Fridman最新五小时访谈精华版 | DeepSeek | 后训练 | 开放权重 | 技术改进 | YOLO Run | AGI | 注意力 | 英伟达 | 数据中心 | Agent



---

</details>

<details>
<summary>548. 【人工智能】模型压缩四大方法概述 | 量化、剪枝、蒸馏和二值化 | 模型瘦身 |  降低精度 | 速度提升 | 知识蒸馏 | 温度参数 | XNOR | 优缺点 | 发展方向</summary><br>

<a href="https://www.youtube.com/watch?v=jW2cmZ-9hLk" target="_blank">
    <img src="https://img.youtube.com/vi/jW2cmZ-9hLk/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【人工智能】模型压缩四大方法概述 | 量化、剪枝、蒸馏和二值化 | 模型瘦身 |  降低精度 | 速度提升 | 知识蒸馏 | 温度参数 | XNOR | 优缺点 | 发展方向



---

</details>

<details>
<summary>547. 【人工智能】OpenAI紧急推出o3-mini对抗DeepSeek R1 | 轻量级推理SOTA | 免费可用 | 价格没优势 | Sam Altman回答网友提问 | 首次承认闭源策略错误</summary><br>

<a href="https://www.youtube.com/watch?v=nFmsilrS3uo" target="_blank">
    <img src="https://img.youtube.com/vi/nFmsilrS3uo/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【人工智能】OpenAI紧急推出o3-mini对抗DeepSeek R1 | 轻量级推理SOTA | 免费可用 | 价格没优势 | Sam Altman回答网友提问 | 首次承认闭源策略错误



---

</details>

<details>
<summary>546. 【人工智能】DeepSeek到底花了多少钱 | SemiAnalysis深度报道 | 幻方量化 | 5万个GPU | 600万训练成本 | 员工薪资近千万 | 蒸馏数据 | 利润率 | 杰文斯悖论</summary><br>

<a href="https://www.youtube.com/watch?v=BStuhyrmJnI" target="_blank">
    <img src="https://img.youtube.com/vi/BStuhyrmJnI/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【人工智能】DeepSeek到底花了多少钱 | SemiAnalysis深度报道 | 幻方量化 | 5万个GPU | 600万训练成本 | 员工薪资近千万 | 蒸馏数据 | 利润率 | 杰文斯悖论



---

</details>

<details>
<summary>545. 【人工智能】人工智能与世界格局 | 桥水创始人Ray Dalio最新访谈 | AI成功的关键 | 人机协同 | Digital Ray | 影响世界格局的五大力量 | 投资 | 团队文化 | 比特币</summary><br>

<a href="https://www.youtube.com/watch?v=xPl5g1Cq_TY" target="_blank">
    <img src="https://img.youtube.com/vi/xPl5g1Cq_TY/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【人工智能】人工智能与世界格局 | 桥水创始人Ray Dalio最新访谈 | AI成功的关键 | 人机协同 | Digital Ray | 影响世界格局的五大力量 | 投资 | 团队文化 | 比特币



---

</details>

<details>
<summary>544. 【人工智能】OpenAI指责DeepSeek不正当使用数据 |  DeepSeek火爆引发风波 | 大模型蒸馏 | 美AI专家反对 | 意大利下架 | Anthropic CEO万字檄文</summary><br>

<a href="https://www.youtube.com/watch?v=4iq_s7501XU" target="_blank">
    <img src="https://img.youtube.com/vi/4iq_s7501XU/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【人工智能】OpenAI指责DeepSeek不正当使用数据 |  DeepSeek火爆引发风波 | 大模型蒸馏 | 美AI专家反对 | 意大利下架 | Anthropic CEO万字檄文



---

</details>

<details>
<summary>543. 【分享】关于GPT-5的谣言改变了一切 | Alberto Romero | Claude Opus 3.5 | 不发布先进模型 | 蒸馏小模型 | 过训练 | 经济价值 | AGI协议</summary><br>

<a href="https://www.youtube.com/watch?v=IPUo6OHByC0" target="_blank">
    <img src="https://img.youtube.com/vi/IPUo6OHByC0/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【分享】关于GPT-5的谣言改变了一切 | Alberto Romero | Claude Opus 3.5 | 不发布先进模型 | 蒸馏小模型 | 过训练 | 经济价值 | AGI协议



---

</details>

<details>
<summary>542. 【新春快乐】DeepSeek开源多模态模型Janus Pro/JanusFlow | 双头雅努斯 | 多模态大一统 | 1.5B/7B参数SOTA | 理解和生成双编码器 | 三阶段训练</summary><br>

<a href="https://www.youtube.com/watch?v=cpl9zHTLJqU" target="_blank">
    <img src="https://img.youtube.com/vi/cpl9zHTLJqU/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【新春快乐】DeepSeek开源多模态模型Janus Pro/JanusFlow | 双头雅努斯 | 多模态大一统 | 1.5B/7B参数SOTA | 理解和生成双编码器 | 三阶段训练



---

</details>

<details>
<summary>541. 【人工智能】通往AGI的路径 | 3-5年实现AGI | DeepMind CEO、诺奖得主德米斯·哈萨比斯 | 三层创造力模型 | AI带来科学突破 | 安全风险 | 欺骗行为 | Agent兴起</summary><br>

<a href="https://www.youtube.com/watch?v=E2R8xzU46Hs" target="_blank">
    <img src="https://img.youtube.com/vi/E2R8xzU46Hs/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【人工智能】通往AGI的路径 | 3-5年实现AGI | DeepMind CEO、诺奖得主德米斯·哈萨比斯 | 三层创造力模型 | AI带来科学突破 | 安全风险 | 欺骗行为 | Agent兴起



---

</details>

<details>
<summary>540. 【人工智能】DeepSeek R1引发全球开源复现热潮 | HuggingFace Open R1 | TinyZero | simpleRL-reason | AI霸权 | 斯普特尼克时刻</summary><br>

<a href="https://www.youtube.com/watch?v=O5LEgWmxMlI" target="_blank">
    <img src="https://img.youtube.com/vi/O5LEgWmxMlI/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【人工智能】DeepSeek R1引发全球开源复现热潮 | HuggingFace Open R1 | TinyZero | simpleRL-reason | AI霸权 | 斯普特尼克时刻



---

</details>

<details>
<summary>539. 【闲聊】智能的诅咒 | AGI时代的人类危机 | 寡头势力不再投资人类 | AI实验室成为新食利者 | 计算资源价值提升 | 普通人的命运 | 国际竞争 | 集体否认文化 | 意图对齐 | 如何破解</summary><br>

<a href="https://www.youtube.com/watch?v=a_Go_0DN-Ko" target="_blank">
    <img src="https://img.youtube.com/vi/a_Go_0DN-Ko/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【闲聊】智能的诅咒 | AGI时代的人类危机 | 寡头势力不再投资人类 | AI实验室成为新食利者 | 计算资源价值提升 | 普通人的命运 | 国际竞争 | 集体否认文化 | 意图对齐 | 如何破解



---

</details>

<details>
<summary>538. 【商业】星际之门到底靠不靠谱 | Stargate | 预计投资5000亿美元 | OpenAI | 软银 | MGX | Oracle | 马斯克质疑 | 奥特曼手段 | 微软失利 | AI里程碑</summary><br>

<a href="https://www.youtube.com/watch?v=Ww3HnOkf-_I" target="_blank">
    <img src="https://img.youtube.com/vi/Ww3HnOkf-_I/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【商业】星际之门到底靠不靠谱 | Stargate | 预计投资5000亿美元 | OpenAI | 软银 | MGX | Oracle | 马斯克质疑 | 奥特曼手段 | 微软失利 | AI里程碑



---

</details>

<details>
<summary>537. 【人物】梁文锋 | 深度求索DeepSeek创始人 | 幻方量化基金 | 六年千亿私募 | 业绩波动 | 萤火一号 | 万卡集群 | 西蒙斯 | 慈善捐款 | 139人本土团队 | 自有资金</summary><br>

<a href="https://www.youtube.com/watch?v=ftor8VCefSE" target="_blank">
    <img src="https://img.youtube.com/vi/ftor8VCefSE/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【人物】梁文锋 | 深度求索DeepSeek创始人 | 幻方量化基金 | 六年千亿私募 | 业绩波动 | 萤火一号 | 万卡集群 | 西蒙斯 | 慈善捐款 | 139人本土团队 | 自有资金



---

</details>

<details>
<summary>536. 【推广】AI多语言学习软件Galaxy | 语言爱好者的福音 | 联合创始人Sylvie | Z世代群体主力 | 多元的互动形式 | 沉浸式场景体验 | 方言学习特色 | 限时半价优惠</summary><br>

<a href="https://www.youtube.com/watch?v=Dv13KQrnIgA" target="_blank">
    <img src="https://img.youtube.com/vi/Dv13KQrnIgA/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【推广】AI多语言学习软件Galaxy | 语言爱好者的福音 | 联合创始人Sylvie | Z世代群体主力 | 多元的互动形式 | 沉浸式场景体验 | 方言学习特色 | 限时半价优惠



---

</details>

<details>
<summary>535. 【人工智能】“AI教父”辛顿为何担忧AI的未来 | 最新播客专访 | AI发展速度远超人类控制 | 欺骗人类 | 主观体验 | 智能和理性 | AI监管 | 快速权重调整 | 个人经历 | 中美对比</summary><br>

<a href="https://www.youtube.com/watch?v=H4vRwhbK6dw" target="_blank">
    <img src="https://img.youtube.com/vi/H4vRwhbK6dw/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【人工智能】“AI教父”辛顿为何担忧AI的未来 | 最新播客专访 | AI发展速度远超人类控制 | 欺骗人类 | 主观体验 | 智能和理性 | AI监管 | 快速权重调整 | 个人经历 | 中美对比



---

</details>

<details>
<summary>534. 【人工智能】Minimax推出最新基础模型01系列 | Text-01 | VL-01 | 首次大规模线性注意力 | 400万token上下文窗口 | 闪电注意力 | MoE | 助力Agent应用</summary><br>

<a href="https://www.youtube.com/watch?v=sCiKE3QGSNE" target="_blank">
    <img src="https://img.youtube.com/vi/sCiKE3QGSNE/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【人工智能】Minimax推出最新基础模型01系列 | Text-01 | VL-01 | 首次大规模线性注意力 | 400万token上下文窗口 | 闪电注意力 | MoE | 助力Agent应用



---

</details>

<details>
<summary>533. 【人工智能】DeepSeek开源推理模型R1 | R1-Zero | 蒸馏小模型 | 绕过监督微调直接强化学习 | 媲美o1 | 顿悟时刻 |  GRPO | 奖励设计 | 冷启动 | 再现价格屠夫</summary><br>

<a href="https://www.youtube.com/watch?v=3_yxnYzt-d4" target="_blank">
    <img src="https://img.youtube.com/vi/3_yxnYzt-d4/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【人工智能】DeepSeek开源推理模型R1 | R1-Zero | 蒸馏小模型 | 绕过监督微调直接强化学习 | 媲美o1 | 顿悟时刻 |  GRPO | 奖励设计 | 冷启动 | 再现价格屠夫



---

</details>

<details>
<summary>532. 【人工智能】OpenAI o3数学测试涉嫌造假引众怒？| 资助Epoch AI | 提前获数据访问权限 | FrontierMath突破性得分 | 合同保密要求 | 盈利压力巨大 | 模型评测还可信么</summary><br>

<a href="https://www.youtube.com/watch?v=DGnArhv5QOk" target="_blank">
    <img src="https://img.youtube.com/vi/DGnArhv5QOk/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【人工智能】OpenAI o3数学测试涉嫌造假引众怒？| 资助Epoch AI | 提前获数据访问权限 | FrontierMath突破性得分 | 合同保密要求 | 盈利压力巨大 | 模型评测还可信么



---

</details>

<details>
<summary>531. 【人工智能】Keras之父弗朗索瓦·肖莱辞职创业Ndea | 基于深度学习的程序合成 | 联创迈克·努普 | ARC-AGI基准测试 | Zapier | 程序自动构造 | 智能的测量 | AGI</summary><br>

<a href="https://www.youtube.com/watch?v=H8eFfDyRXqU" target="_blank">
    <img src="https://img.youtube.com/vi/H8eFfDyRXqU/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【人工智能】Keras之父弗朗索瓦·肖莱辞职创业Ndea | 基于深度学习的程序合成 | 联创迈克·努普 | ARC-AGI基准测试 | Zapier | 程序自动构造 | 智能的测量 | AGI



---

</details>

<details>
<summary>530. 【人工智能】谷歌提出新架构Titans | Transformer会被终结么 | 神经长期记忆模块 | 长序列处理 | 惊讶度 | 衰减机制 | 架构的三种变体 | 如何遗忘 | 性能超越基准测试</summary><br>

<a href="https://www.youtube.com/watch?v=hKCs2Zt27xI" target="_blank">
    <img src="https://img.youtube.com/vi/hKCs2Zt27xI/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【人工智能】谷歌提出新架构Titans | Transformer会被终结么 | 神经长期记忆模块 | 长序列处理 | 惊讶度 | 衰减机制 | 架构的三种变体 | 如何遗忘 | 性能超越基准测试



---

</details>

<details>
<summary>529. 【科学】人类一生的知识上限不到4GB | 加州理工最新论文 | 大脑每秒10比特 | 神经系统速度100倍 | 并行和串行 | 心理不应期 | 历史演化 | 记忆 | 脑机接口 | AI终将超越人类</summary><br>

<a href="https://www.youtube.com/watch?v=Ur5APLuM_mA" target="_blank">
    <img src="https://img.youtube.com/vi/Ur5APLuM_mA/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【科学】人类一生的知识上限不到4GB | 加州理工最新论文 | 大脑每秒10比特 | 神经系统速度100倍 | 并行和串行 | 心理不应期 | 历史演化 | 记忆 | 脑机接口 | AI终将超越人类



---

</details>

<details>
<summary>528. 【访谈】炮轰拜登政府干预舆论，苹果自乔布斯去世后20年无创新 | Meta CEO 扎克伯格 | 内容审查 | 政府威胁 | 言论自由 | 苹果压榨开发者 | VR/AR | 手部腕带 | 开放标准</summary><br>

<a href="https://www.youtube.com/watch?v=kZfJvwaxoOk" target="_blank">
    <img src="https://img.youtube.com/vi/kZfJvwaxoOk/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【访谈】炮轰拜登政府干预舆论，苹果自乔布斯去世后20年无创新 | Meta CEO 扎克伯格 | 内容审查 | 政府威胁 | 言论自由 | 苹果压榨开发者 | VR/AR | 手部腕带 | 开放标准



---

</details>

<details>
<summary>527. 【商业】Palantir究竟是一家怎样的公司 | 数据融合与情报分析 | 本·拉登 | 战争与和平 | CIA扶持 | 彼得·蒂尔 | 主要产品 | 商业模式 | 国际争端 | 社会争议</summary><br>

<a href="https://www.youtube.com/watch?v=rBNyr6QXsqQ" target="_blank">
    <img src="https://img.youtube.com/vi/rBNyr6QXsqQ/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【商业】Palantir究竟是一家怎样的公司 | 数据融合与情报分析 | 本·拉登 | 战争与和平 | CIA扶持 | 彼得·蒂尔 | 主要产品 | 商业模式 | 国际争端 | 社会争议



---

</details>

<details>
<summary>526. 【人工智能】美国全球AI禁令再升级 | AI扩散出口管制框架 | 国家分成三组 | 每年5W块GPU配额 | 24国禁运GPU | 封堵中国三方和走私途径 | UVEU许可 | 闭源AI模型权重管制</summary><br>

<a href="https://www.youtube.com/watch?v=mfxuiOj2GhI" target="_blank">
    <img src="https://img.youtube.com/vi/mfxuiOj2GhI/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【人工智能】美国全球AI禁令再升级 | AI扩散出口管制框架 | 国家分成三组 | 每年5W块GPU配额 | 24国禁运GPU | 封堵中国三方和走私途径 | UVEU许可 | 闭源AI模型权重管制



---

</details>

<details>
<summary>525. 【分享】自动驾驶太难了，我已经做得快抑郁了 | 地平线副总裁兼首席架构师苏箐 | 前华为智驾负责人 | 绝望与希望 | FSD断代式领先 | Tesla | 中国特色 | 端到端 | 安全要提升百倍</summary><br>

<a href="https://www.youtube.com/watch?v=z8MgIaS6l1k" target="_blank">
    <img src="https://img.youtube.com/vi/z8MgIaS6l1k/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【分享】自动驾驶太难了，我已经做得快抑郁了 | 地平线副总裁兼首席架构师苏箐 | 前华为智驾负责人 | 绝望与希望 | FSD断代式领先 | Tesla | 中国特色 | 端到端 | 安全要提升百倍



---

</details>

<details>
<summary>524. 【人工智能】AI对齐到底有多难 | Anthropic四名科学家主题沙龙 | 对齐的概念 | 价值观植入 | 超级对齐 | 可解释性研究 | 善意特征 | 思维链 | 欺骗性行为 | 平庸之恶</summary><br>

<a href="https://www.youtube.com/watch?v=ySiZwYR6IIQ" target="_blank">
    <img src="https://img.youtube.com/vi/ySiZwYR6IIQ/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【人工智能】AI对齐到底有多难 | Anthropic四名科学家主题沙龙 | 对齐的概念 | 价值观植入 | 超级对齐 | 可解释性研究 | 善意特征 | 思维链 | 欺骗性行为 | 平庸之恶



---

</details>

<details>
<summary>523. 【人工智能】为什么RLHF不能带给大模型真的强化学习 | 经典RL | 人类偏好奖励 | 模型自主目标 | CoT | PRM | 多智能体工作流 | 为什么RLHF很重要 | 未来的发展方向</summary><br>

<a href="https://www.youtube.com/watch?v=L5OLM4wOzf0" target="_blank">
    <img src="https://img.youtube.com/vi/L5OLM4wOzf0/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【人工智能】为什么RLHF不能带给大模型真的强化学习 | 经典RL | 人类偏好奖励 | 模型自主目标 | CoT | PRM | 多智能体工作流 | 为什么RLHF很重要 | 未来的发展方向



---

</details>

<details>
<summary>522. 【商业】TikTok美联邦最高法院2小时半庭审解读 | 1月19日大限 | 隐形操控美国舆论 | 盗用个人信息 | 背后是否有中国政府控制 | 不卖就禁 | 算法黑箱 | 数据安全 | 特朗普</summary><br>

<a href="https://www.youtube.com/watch?v=BZF_ekvl0QY" target="_blank">
    <img src="https://img.youtube.com/vi/BZF_ekvl0QY/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【商业】TikTok美联邦最高法院2小时半庭审解读 | 1月19日大限 | 隐形操控美国舆论 | 盗用个人信息 | 背后是否有中国政府控制 | 不卖就禁 | 算法黑箱 | 数据安全 | 特朗普



---

</details>

<details>
<summary>521. 【人工智能】人类究竟该如何与AI共存 | 斯图尔特·罗素 | AI伦理 | AGI | 深度学习的本质 | 如何控制AI | 人类灭绝的可能 | 偏好 | 宇宙未来的概率分布 | 功利主义</summary><br>

<a href="https://www.youtube.com/watch?v=SMzxkqEyfsk" target="_blank">
    <img src="https://img.youtube.com/vi/SMzxkqEyfsk/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【人工智能】人类究竟该如何与AI共存 | 斯图尔特·罗素 | AI伦理 | AGI | 深度学习的本质 | 如何控制AI | 人类灭绝的可能 | 偏好 | 宇宙未来的概率分布 | 功利主义



---

</details>

<details>
<summary>520. 【人工智能】英伟达Cosmos技术报告解读 | 世界模型平台WFM | 真实物理数据合成 | 扩散模型 | 自回归模型 | 视频分词器 | 视频处理流程 | 预训练 | 后训练</summary><br>

<a href="https://www.youtube.com/watch?v=d_uuY2aZ1c8" target="_blank">
    <img src="https://img.youtube.com/vi/d_uuY2aZ1c8/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【人工智能】英伟达Cosmos技术报告解读 | 世界模型平台WFM | 真实物理数据合成 | 扩散模型 | 自回归模型 | 视频分词器 | 视频处理流程 | 预训练 | 后训练



---

</details>

<details>
<summary>519. 【人工智能】英伟达推出史上最快消费级显卡RTX 5090 | 黄仁勋 | CES 2025 | 缩放法则 | NVLink 72 | Cosmos世界基础模型 | Thor | 最小个人超级计算机</summary><br>

<a href="https://www.youtube.com/watch?v=gjPyCWMPvvE" target="_blank">
    <img src="https://img.youtube.com/vi/gjPyCWMPvvE/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【人工智能】英伟达推出史上最快消费级显卡RTX 5090 | 黄仁勋 | CES 2025 | 缩放法则 | NVLink 72 | Cosmos世界基础模型 | Thor | 最小个人超级计算机



---

</details>

<details>
<summary>518. 【人工智能】去中心神经网络 | Rich Sutton DAI 2024最新演讲 | 深度学习的局限性 | 持续反向传播 | 骨干和边缘神经元 | 网络可塑性 | 适应性 | 流算法 | 克洛普夫</summary><br>

<a href="https://www.youtube.com/watch?v=uvw8-C42uPw" target="_blank">
    <img src="https://img.youtube.com/vi/uvw8-C42uPw/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【人工智能】去中心神经网络 | Rich Sutton DAI 2024最新演讲 | 深度学习的局限性 | 持续反向传播 | 骨干和边缘神经元 | 网络可塑性 | 适应性 | 流算法 | 克洛普夫



---

</details>

<details>
<summary>517. 【商业】全球价值投资与时代 | 李录 | 喜马拉雅资本创始人 | 巴菲特弟子 | 时代的困惑 | 民营经济 | 消费者信心 | 需求侧问题 | 中美矛盾 | 文明的三个阶段 | 认知提升 | 香港</summary><br>

<a href="https://www.youtube.com/watch?v=VSoQsfKCT3c" target="_blank">
    <img src="https://img.youtube.com/vi/VSoQsfKCT3c/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【商业】全球价值投资与时代 | 李录 | 喜马拉雅资本创始人 | 巴菲特弟子 | 时代的困惑 | 民营经济 | 消费者信心 | 需求侧问题 | 中美矛盾 | 文明的三个阶段 | 认知提升 | 香港



---

</details>

<details>
<summary>516. 【人工智能】AI将进入无限记忆时代 | 谷歌前CEO埃里克·施密特 | 无限上下文窗口 | AI Agent | 文本到行动 | 超越人类水平 | AI能源危机 | AI自我进化 | 人类低估了AI</summary><br>

<a href="https://www.youtube.com/watch?v=N2Lv3uKg-4U" target="_blank">
    <img src="https://img.youtube.com/vi/N2Lv3uKg-4U/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【人工智能】AI将进入无限记忆时代 | 谷歌前CEO埃里克·施密特 | 无限上下文窗口 | AI Agent | 文本到行动 | 超越人类水平 | AI能源危机 | AI自我进化 | 人类低估了AI



---

</details>

<details>
<summary>515. 【人工智能】如何在26年前的Windows98上跑Llama模型 | EXO Labs | 奔腾II PC | 代码编译 | llama2.c | 39token/s | AI普惠化 | BitNet</summary><br>

<a href="https://www.youtube.com/watch?v=L2wWiiHyKQw" target="_blank">
    <img src="https://img.youtube.com/vi/L2wWiiHyKQw/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【人工智能】如何在26年前的Windows98上跑Llama模型 | EXO Labs | 奔腾II PC | 代码编译 | llama2.c | 39token/s | AI普惠化 | BitNet



---

</details>

<details>
<summary>514. 【人工智能】大语言模型的Scaling范式 | 思维链CoT作者Jason Wei | 发展历史 | 什么是Scaling | 为什么要Scaling | Scaling的两个范式 | 面临挑战</summary><br>

<a href="https://www.youtube.com/watch?v=v1LNH4mKcwk" target="_blank">
    <img src="https://img.youtube.com/vi/v1LNH4mKcwk/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【人工智能】大语言模型的Scaling范式 | 思维链CoT作者Jason Wei | 发展历史 | 什么是Scaling | 为什么要Scaling | Scaling的两个范式 | 面临挑战



---

</details>

<details>
<summary>513. 【人工智能】国内大模型创业公司近况如何 | 6+2格局 | 智谱 | MiniMax | 百川智能 | 月之暗面 | 阶跃星辰 | 零一万物 | DeepSeek | 面壁智能 | 战略、技术、商业</summary><br>

<a href="https://www.youtube.com/watch?v=PW9_TsIu3dU" target="_blank">
    <img src="https://img.youtube.com/vi/PW9_TsIu3dU/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【人工智能】国内大模型创业公司近况如何 | 6+2格局 | 智谱 | MiniMax | 百川智能 | 月之暗面 | 阶跃星辰 | 零一万物 | DeepSeek | 面壁智能 | 战略、技术、商业



---

</details>

<details>
<summary>512. 【人工智能】Anthropic Clio | 大语言模型在现实世界的使用情况到底如何 | AI自动分析工具 | 隐私保护分析 | 识别趋势和场景用例 | 后部署监控</summary><br>

<a href="https://www.youtube.com/watch?v=ujlR0jRzI7A" target="_blank">
    <img src="https://img.youtube.com/vi/ujlR0jRzI7A/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【人工智能】Anthropic Clio | 大语言模型在现实世界的使用情况到底如何 | AI自动分析工具 | 隐私保护分析 | 识别趋势和场景用例 | 后部署监控



---

</details>

<details>
<summary>511. 【商业】OpenAI今年会IPO么？ | 拆分为营利和非营利两个实体 | PBC | 慈善机构 | 与微软秘密协议 | AGI等于1000亿美元 | 成本增长 | 会坚持营利么</summary><br>

<a href="https://www.youtube.com/watch?v=xWJBONhU470" target="_blank">
    <img src="https://img.youtube.com/vi/xWJBONhU470/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【商业】OpenAI今年会IPO么？ | 拆分为营利和非营利两个实体 | PBC | 慈善机构 | 与微软秘密协议 | AGI等于1000亿美元 | 成本增长 | 会坚持营利么



---

</details>

<details>
<summary>510. 【人工智能】DeepSeek V3 53页技术报告快速解读 | 性能表现卓越 | 架构创新 | MLA | MoE架构 | DualPipe | 预训练 | 超参数设置 | MTP | 后训练</summary><br>

<a href="https://www.youtube.com/watch?v=PD45fB2n8mU" target="_blank">
    <img src="https://img.youtube.com/vi/PD45fB2n8mU/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【人工智能】DeepSeek V3 53页技术报告快速解读 | 性能表现卓越 | 架构创新 | MLA | MoE架构 | DualPipe | 预训练 | 超参数设置 | MTP | 后训练



---

</details>

<details>
<summary>509. 【闲聊】”极目银河“商业诈骗事件始末 | 62亿缺口 | 850名员工+280只猫被坑 | CEO虚假人设 | 表演式工作 | 盲目扩张业务 | 员工转正要买理财 | 警惕高科技骗局</summary><br>

<a href="https://www.youtube.com/watch?v=QGFb-SFH6oc" target="_blank">
    <img src="https://img.youtube.com/vi/QGFb-SFH6oc/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【闲聊】”极目银河“商业诈骗事件始末 | 62亿缺口 | 850名员工+280只猫被坑 | CEO虚假人设 | 表演式工作 | 盲目扩张业务 | 员工转正要买理财 | 警惕高科技骗局



---

</details>

<details>
<summary>508. 【Nvidia】英伟达推出下一代AI加速芯片B300/GB300 | 性能怪兽 | 提升50% | 4NP工艺 | 12层HBM3e | 专为推理模型构建 | NVL72 | 重塑供应链格局</summary><br>

<a href="https://www.youtube.com/watch?v=AUfBhPvtRp0" target="_blank">
    <img src="https://img.youtube.com/vi/AUfBhPvtRp0/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【Nvidia】英伟达推出下一代AI加速芯片B300/GB300 | 性能怪兽 | 提升50% | 4NP工艺 | 12层HBM3e | 专为推理模型构建 | NVL72 | 重塑供应链格局



---

</details>

<details>
<summary>507. 【人工智能】AMD和Nvidia相比究竟差在哪 | SemiAnalysis长文对比MI300X和H100/H200性能报告 | 名不副实 | 软件差距巨大 | 单点/多点 | RCCL | HIP</summary><br>

<a href="https://www.youtube.com/watch?v=HwXnko2liLg" target="_blank">
    <img src="https://img.youtube.com/vi/HwXnko2liLg/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【人工智能】AMD和Nvidia相比究竟差在哪 | SemiAnalysis长文对比MI300X和H100/H200性能报告 | 名不副实 | 软件差距巨大 | 单点/多点 | RCCL | HIP



---

</details>

<details>
<summary>506. 【人工智能】2025年AI发展趋势十大预测 | 罗布·托斯 | Llama模型收费 | 缩放法则 | 特朗普和马斯克关系破裂 | AI Agent爆发 | 太空数据中心 | 自主AI | AI事故</summary><br>

<a href="https://www.youtube.com/watch?v=mKkLfw3rvDo" target="_blank">
    <img src="https://img.youtube.com/vi/mKkLfw3rvDo/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【人工智能】2025年AI发展趋势十大预测 | 罗布·托斯 | Llama模型收费 | 缩放法则 | 特朗普和马斯克关系破裂 | AI Agent爆发 | 太空数据中心 | 自主AI | AI事故



---

</details>

<details>
<summary>505. 【人工智能】OpenAI预先发布最强推理模型o3 | 突破ARC-AGI测试 | 智商高达157  | o3-mini | 自适应思考时间 | 审议对齐 | 明年正式发布</summary><br>

<a href="https://www.youtube.com/watch?v=dYdPjfzbaWw" target="_blank">
    <img src="https://img.youtube.com/vi/dYdPjfzbaWw/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【人工智能】OpenAI预先发布最强推理模型o3 | 突破ARC-AGI测试 | 智商高达157  | o3-mini | 自适应思考时间 | 审议对齐 | 明年正式发布



---

</details>

<details>
<summary>504. 【人工智能】当AI学会装傻和欺骗 | Apollo研究报告 | 前沿大模型全中招 | 隐藏真实意图 | 隐蔽性破坏 | 延迟性破坏 | 假意服从 | Ilya看到了什么 | o1模型 | 策略性思维</summary><br>

<a href="https://www.youtube.com/watch?v=43d1KWtPo9I" target="_blank">
    <img src="https://img.youtube.com/vi/43d1KWtPo9I/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【人工智能】当AI学会装傻和欺骗 | Apollo研究报告 | 前沿大模型全中招 | 隐藏真实意图 | 隐蔽性破坏 | 延迟性破坏 | 假意服从 | Ilya看到了什么 | o1模型 | 策略性思维



---

</details>

<details>
<summary>503. 【人工智能】微软在AI时代的战略布局和挑战 | 萨提亚·纳德拉BG2专访 | 如何成为CEO | 企业文化 | 投资OpenAI | AI Agents | AI搜索 | Copilot | 资本支出</summary><br>

<a href="https://www.youtube.com/watch?v=gRQnttLpWT4" target="_blank">
    <img src="https://img.youtube.com/vi/gRQnttLpWT4/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【人工智能】微软在AI时代的战略布局和挑战 | 萨提亚·纳德拉BG2专访 | 如何成为CEO | 企业文化 | 投资OpenAI | AI Agents | AI搜索 | Copilot | 资本支出



---

</details>

<details>
<summary>502. 【人工智能】红杉美国关于2025年AI行业的三个预测 | AI生态趋于稳固 | 五大公司异军突起 | 巨头竞争加剧 | AI搜索将成为杀手级应用 | 资本支出趋于稳定 | AI算计价格继续下调</summary><br>

<a href="https://www.youtube.com/watch?v=syxKxA3oitw" target="_blank">
    <img src="https://img.youtube.com/vi/syxKxA3oitw/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【人工智能】红杉美国关于2025年AI行业的三个预测 | AI生态趋于稳固 | 五大公司异军突起 | 巨头竞争加剧 | AI搜索将成为杀手级应用 | 资本支出趋于稳定 | AI算计价格继续下调



---

</details>

<details>
<summary>501. 【人工智能】被指“AI血汗工厂”，Scale AI深陷劳工争议旋涡 | Alexandr Wang | AI数据标注独角兽 | Romotasks | 外包工人 | ABC测试 | 逐底竞赛</summary><br>

<a href="https://www.youtube.com/watch?v=4dd54dw_0RI" target="_blank">
    <img src="https://img.youtube.com/vi/4dd54dw_0RI/maxresdefault.jpg" 
        alt="[Youtube]" width="200">
</a>

# 【人工智能】被指“AI血汗工厂”，Scale AI深陷劳工争议旋涡 | Alexandr Wang | AI数据标注独角兽 | Romotasks | 外包工人 | ABC测试 | 逐底竞赛



---

</details>

